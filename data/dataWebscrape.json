{"0": "from fuzzywuzzy import fuzz\nimport numpy as np\nimport pandas as pd\nimport sqlite3 as sql\nimport sys\n\nif len(sys.argv) < 3:\n    print('Usage: python merge_coordinates.py [pris_link] [webscrape_link]')\n\n\ndef get_cursor(file_name):\n    \"\"\" Connects and returns a cursor to an sqlite output file\n\n    Parameters\n    ----------\n    file_name: str\n        name of the sqlite file\n\n    Returns\n    -------\n    sqlite cursor\n    \"\"\"\n    con = sql.connect(file_name)\n    con.row_factory = sql.Row\n    return con.cursor()\n\n\ndef import_pris(pris_link):\n    \"\"\" Opens pris_csv using Pandas. Adds Latitude and Longitude\n    columns\n\n    Parameters\n    ----------\n    pris_link: str\n        path to reactors_pris_2016.original.csv file\n\n    Returns\n    -------\n    pris: pd.Dataframe\n        pris database\n    \"\"\"\n    pris = pd.read_csv(pris_link,\n                       delimiter=',',\n                       encoding='iso-8859-1'\n                       )\n    pris.insert(13, 'Latitude', np.nan)\n    pris.insert(14, 'Longitude', np.nan)\n    pris = pris.replace(np.nan, '')\n    return pris\n\n\ndef import_webscrape_data(scrape_link):\n    \"\"\" Returns sqlite content of webscrape by performing an\n    sqlite query\n\n    Parameters\n    ----------\n    scrape_link: str\n        path to webscrape.sqlite file\n\n    Returns\n    -------\n    coords: sqlite cursor\n        sqlite cursor containing webscrape data\n    \"\"\"\n    cur = get_cursor(scrape_link)\n    coords = cur.execute(\"SELECT name, long, lat FROM reactors_coordinates\")\n    return coords\n\n\ndef get_edge_cases():\n    \"\"\" Returns a dictionary of edge cases that fuzzywuzzy is\n    unable to catch. This could be because PRIS database stores\n    reactor names and Webscrape database fetches power plant names,\n    or because PRIS reactor names are abbreviated.\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n    others: dict\n        dictionary of edge cases with \"key=pris_reactor_name, and\n        value=webscrape_plant_name\"\n    \"\"\"\n    others = {'OHI-': '\u014ci',\n              'ASCO-': 'Asc\u00f3',\n              'ROVNO-': 'Rivne',\n              'SHIN-KORI-': 'Kori',\n              'ANO-': 'Arkansas One',\n              'HANBIT-': 'Yeonggwang',\n              'FERMI-': 'Enrico Fermi',\n              'BALTIC-': 'Kaliningrad',\n              'COOK-': 'Donald C. Cook',\n              'HATCH-': 'Edwin I. Hatch',\n              'HARRIS-': 'Shearon Harris',\n              'SHIN-WOLSONG-': 'Wolseong',\n              'ST. ALBAN-': 'Saint-Alban',\n              'LASALLE-': 'LaSalle County',\n              'SUMMER-': 'Virgil C. Summer',\n              'FARLEY-': 'Joseph M. Farley',\n              'ST. LAURENT ': 'Saint-Laurent',\n              'HADDAM NECK': 'Connecticut Yankee',\n              'HIGASHI DORI-1 (TOHOKU)': 'Higashid\u014dri',\n              }\n    return others\n\n\ndef sanitize_webscrape_name(name):\n    \"\"\" Sanitizes webscrape powerplant names by removing unwanted\n    strings (listed in blacklist), applying lower case, and deleting\n    trailing whitespace.\n\n    Parameters\n    ----------\n    name: str\n        webscrape plant name\n\n    Returns\n    -------\n    name: str\n        sanitized name for use with fuzzywuzzy\n    \"\"\"\n    blacklist = ['nuclear', 'power',\n                 'plant', 'generating',\n                 'station', 'reactor', 'atomic',\n                 'energy', 'center', 'electric']\n    name = name.lower()\n    for blacklisted in blacklist:\n        name = name.replace(blacklisted, '')\n    name = name.strip()\n    name = ' '.join(name.split())\n    return name\n\n\ndef merge_coordinates(pris_link, scrape_link):\n    \"\"\" Merges webscrape data with pris data performed by string\n    comparison of reactor names from pris and webscrape. Returns\n    updated pris database with coordinates.\n\n    Parameters\n    ----------\n    pris_link: str\n        path to reactors_pris_2016.original.csv file\n    scrape_link: str\n        path to webscrape.sqlite file\n\n    Returns\n    -------\n    pris: pd.DataFrame\n        updated PRIS database with latitude and longitude info\n    \"\"\"\n    others = get_edge_cases()\n    pris = import_pris(pris_link)\n    coords = import_webscrape_data(scrape_link)\n    for web in coords:\n        for idx, prs in pris.iterrows():\n            webscrape_name = sanitize_webscrape_name(web['name'])\n            pris_name = prs[1].lower()\n            if fuzz.ratio(webscrape_name, pris_name) > 64:\n                prs[13] = web['lat']\n                prs[14] = web['long']\n            else:\n                for other in others.keys():\n                    edge_case_key = other.lower()\n                    edge_case_value = others[other].lower()\n                    if (fuzz.ratio(pris_name, edge_case_key) > 80 and\n                            fuzz.ratio(webscrape_name, edge_case_value) > 75):\n                        prs[13] = web['lat']\n                        prs[14] = web['long']\n    return pris\n\n\ndef save_output(pris):\n    \"\"\" Saves updated PRIS database as 'reactors_pris_2016.csv'\n\n    Parameters\n    ----------\n    pris: pd.DataFrame\n        updated PRIS database with latitude and longitude info\n\n    Returns\n    -------\n\n    \"\"\"\n    pris.to_csv('reactors_pris_2016.csv',\n                index=False,\n                sep=',',\n                )\n\n\ndef main(pris_link, scrape_link):\n    \"\"\" Calls all required functions to merge PRIS and webscrape\n\n    Parameters\n    ----------\n    pris_link: str\n        path to reactors_pris_2016.original.csv file\n    scrape_link: str\n        path to webscrape.sqlite file\n\n    Returns\n    -------\n\n    \"\"\"\n    pris = merge_coordinates(pris_link, scrape_link)\n    save_output(pris)\n\n\nif __name__ == \"__main__\":\n    main(sys.argv[1], sys.argv[2])\n", "1": "# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'webscrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "2": "# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'webscrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "3": "# Scrapy settings for amazon_webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'amazon_webscrape'\n\nSPIDER_MODULES = ['amazon_webscrape.spiders']\nNEWSPIDER_MODULE = 'amazon_webscrape.spiders'\n\nFEED_EXPORT_ENCODING='utf-8'\nFEED_FORMAT = 'csv'\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'amazon_webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\nDOWNLOAD_DELAY = 10\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'amazon_webscrape.middlewares.AmazonWebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'amazon_webscrape.middlewares.AmazonWebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'amazon_webscrape.pipelines.AmazonWebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "4": "import twint\nimport pandas as pd\n\nfrom pytz import timezone\nfrom datetime import datetime as dt\nfrom datetime import timedelta\n\nfrom src.topic_search import topic_for_search\n\n\ndef fetch_dataframe() -> pd.DataFrame():\n    \"\"\"Returns the dataframe accessed on SQL\n\n    Args:\n\n    Returns:\n        (pd.DataFrame) : Tweets dataframe from host.engine\n    \"\"\"\n    try:\n        df = pd.read_csv('tweets.csv', sep='\\t', names=[\"id\", \"user_id\", \"username\", \"date\", \"tweet\"])\n    except:\n        df = pd.DataFrame()\n    return df\n\n\nclass ClassTwitterScrape:\n    def __init__(self):\n        self.webscrape()\n\n    def webscrape(self) -> None:\n        \"\"\"Webscrapes\n\n        Args:\n            None\n\n        Returns:\n            scrape_df (pd.DataFrame): scraped dataframe with keep columns\n        \"\"\"\n\n        # create the config\n        c = twint.Config()\n\n        # defining the topic to be searched\n        c.Search = topic_for_search\n\n        # Time\n        eu_paris = timezone(\"Europe/Paris\")  # set Paris timezone\n        week_ago = (dt.now(eu_paris) - timedelta(days=7)).strftime(\n            \"%Y-%m-%d\"\n        )  # today minus 7 days\n\n        c.Since = week_ago  # researches up to a week ago until now\n\n        c.Limit = 1000  # number of Tweets to scrape\n        c.Lang = \"fr\"  # search for french text\n        c.Pandas = True\n        c.Hide_output = True\n\n        twint.run.Search(c)\n\n        webscrape_df = twint.output.panda.Tweets_df\n        webscrape_df['date'] = pd.to_datetime(webscrape_df['date'])\n        webscrape_df['tweet'] = webscrape_df['tweet'].map(lambda x: x.encode('unicode-escape').decode('utf-8')) # TODO: fix the accents and french language stuff\n        webscrape_df['username'] = webscrape_df['username'].map(lambda x: x.encode('unicode-escape').decode('utf-8'))\n        keep_columns = [\"id\", \"user_id\", \"username\", \"date\", \"tweet\"]\n\n        webscrape_df = webscrape_df[keep_columns]\n        file_name = open('tweets.csv', 'a+')\n        webscrape_df.to_csv(file_name, sep='\\t', header=False)\n\n\n", "5": "#-----------------------------------------------------------------------\n# webscrape.py\n# Author: Julia Douvas\n# Description: Webscrapes PsychologyToday therapist and psychiatrist\n#              profiles in the Princeton area and stores detailed info\n#              about each provider into moreinfo_newest.csv and \n#              psy_moreinfo_newest.csv. Code takes about 1 hour to run.\n# Results: creation of files: ids.csv, ids.json, moreinfo_newer.csv,\n#          moreinfo_newest,csv, moreinfo.csv, moreinfo.json, psy_ids.csv,\n#          psy_ids.json, psy_moreinfo_newer.csv, psy_moreinfo_newest.csv,\n#          psy_moreinfo.json\n#-----------------------------------------------------------------------\nimport therapist_webscrape_basic\nimport therapist_webscrape_moreinfo\nimport psy_webscrape_basic\nimport psy_webscrape_moreinfo\n\ndef main():\n    # run therapist_webscrape_basic.py\n    therapist_webscrape_basic.main()\n\n    # run psy_webscrape_basic.py\n    psy_webscrape_basic.main()\n\n    # run therapist_webscrape_moreinfo.py\n    therapist_webscrape_moreinfo.main()\n\n    # run psy_webscrape_moreinfo.py\n    psy_webscrape_moreinfo.main()\n\nif __name__ == '__main__':\n    main()\n", "6": "Python 3.6.9 (default, Nov  7 2019, 10:44:02) \n[GCC 8.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license()\" for more information.\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\n['Autocourse', '1962', 'Autocourse', '1967', 'Autocourse', '1968']\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nTraceback (most recent call last):\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 10, in \n    print(kwr())\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 4, in kwr\n    data = f.readlines().split()\nAttributeError: 'list' object has no attribute 'split'\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nTraceback (most recent call last):\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 10, in \n    print(kwr())\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 4, in kwr\n    data = fn.split()\nAttributeError: '_io.TextIOWrapper' object has no attribute 'split'\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\n\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nTraceback (most recent call last):\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 10, in \n    print(kwr())\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 4, in kwr\n    data = fn()\nTypeError: '_io.TextIOWrapper' object is not callable\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nTraceback (most recent call last):\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 10, in \n    print(kwr())\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 6, in kwr\n    return (data)\nNameError: name 'data' is not defined\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962\n\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962\n\nAutocourse 1967\n\nAutocourse 1968\n\nTraceback (most recent call last):\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 10, in \n    print(kwr())\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 7, in kwr\n    f.close()\nNameError: name 'f' is not defined\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962\n\nAutocourse 1967\n\nAutocourse 1968\n\nTraceback (most recent call last):\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 7, in \n    f.close()\nNameError: name 'f' is not defined\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962\n\nAutocourse 1967\n\nAutocourse 1968\n\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962 200\n\nAutocourse 1967 222\n\nAutocourse 1968 225\n\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962 200\n\nAutocourse 1967 222\n\nAutocourse 1968 225\n\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\n['Autocourse 1962 200\\n']\n['Autocourse 1967 222\\n']\n['Autocourse 1968 225\\n']\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\n['Autocourse 1962 200\\n']\n['Autocourse 1967 222\\n']\n['Autocourse 1968 225\\n']\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\n['Autocourse', '1962']\n['Autocourse', '1967']\n['Autocourse', '1968']\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\n['Autocourse', '1962']\n['Autocourse', '1967']\n['Autocourse', '1968']\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nTraceback (most recent call last):\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 7, in \n    st = st.join(\",\")\nAttributeError: 'list' object has no attribute 'join'\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nTraceback (most recent call last):\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 7, in \n    st = st.join()\nAttributeError: 'list' object has no attribute 'join'\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\n['Autocourse', '1962']\n['Autocourse', '1967']\n['Autocourse', '1968']\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse,1962\nAutocourse,1967\nAutocourse,1968\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse1962\nAutocourse1967\nAutocourse1968\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962\nAutocourse 1967\nAutocourse 1968\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nTraceback (most recent call last):\n  File \"/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py\", line 9, in \n    print[ls]\nTypeError: 'builtin_function_or_method' object is not subscriptable\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\n['A', 'u', 't', 'o', 'c', 'o', 'u', 'r', 's', 'e', ' ', '1', '9', '6', '2']\n['A', 'u', 't', 'o', 'c', 'o', 'u', 'r', 's', 'e', ' ', '1', '9', '6', '7']\n['A', 'u', 't', 'o', 'c', 'o', 'u', 'r', 's', 'e', ' ', '1', '9', '6', '8']\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962\nAutocourse 1967\nAutocourse 1968\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962\nAutocourse 1967\nAutocourse 1968\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962 Autocourse 1962\nAutocourse 1967 Autocourse 1967\nAutocourse 1968 Autocourse 1968\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1968 Autocourse 1968\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962\nAutocourse 1967\nAutocourse 1968\n>>> \n====== RESTART: /home/drpi/Python/WebScrapeRM/webscrape-rm/bin/test1.py ======\nAutocourse 1962\nAutocourse 1967\nAutocourse 1968\n>>> \n", "7": "#!/usr/bin/env python\n\nimport os\nimport sys\nimport json\n\nsys.path.insert(0, 'src')\nfrom etl import convert_txt\nfrom model import autophrase\nfrom weight_phrases import change_weight\nfrom webscrape import webscrape\nfrom website import activate_website\nfrom utils import convert_report\n\n#def main(targets):\ndef main():\n    data_config = json.load(open('config/data-params.json'))\n    model_config = json.load(open('config/model-params.json'))\n    weight_config = json.load(open('config/weight-params.json'))\n    webscrape_config = json.load(open('config/webscrape-params.json'))\n    website_config = json.load(open('config/website-params.json'))\n    report_config = json.load(open('config/report-params.json'))\n    test_config = json.load(open('config/test-params.json'))\n\n    os.system('git submodule update --init')\n    \n    # Getting the target\n    # If no target is given, then run 'website'\n    if len(sys.argv) == 1:\n        targets = 'website'\n    else:\n        targets = sys.argv[1]\n        \n    if 'data' in targets:\n        convert_txt(**data_config)\n    if 'autophrase' in targets:\n        autophrase(data_config['outdir'], data_config['pdfname'], model_config['outdir'], model_config['filename'])\n    if 'weight' in targets:\n        try:\n            unique_key = '_' + sys.argv[2]\n            change_weight(unique_key, **weight_config)\n        except:\n            change_weight(unique_key='', **weight_config)\n    if 'webscrape' in targets:\n        try:\n            unique_key = '_' +  sys.argv[2]\n            webscrape(unique_key, **webscrape_config)\n        except:\n            webscrape(unique_key='', **webscrape_config)\n    if 'report' in targets:\n        convert_report(report_config['experiment_in_path'], report_config['experiment_out_path'])\n        convert_report(report_config['analysis_in_path'], report_config['analysis_out_path'])\n    if 'website' in targets:\n        activate_website(**website_config)\n    if 'test' in targets:\n        convert_txt(test_config['indir'], data_config['outdir'], test_config['pdfname'],)\n        autophrase(data_config['outdir'], test_config['pdfname'], model_config['outdir'], model_config['filename'])\n        change_weight(unique_key='', **weight_config)\n        webscrape(unique_key='', **webscrape_config)\n        convert_report(report_config['experiment_in_path'], report_config['experiment_out_path'])\n        convert_report(report_config['analysis_in_path'], report_config['analysis_out_path'])\n    return\n\n#if __name__ == '__main__':\n#    # run via:\n#    # python main.py data features model\n#    targets = sys.argv\n#    main(targets)\nmain()\n", "8": "import pandas as pd\n\nfrom utils import *\n\n# Injury Table\nInjury = pd.read_html('https://www.basketball-reference.com/friv/injuries.fcgi#injuries')\nInjury_df = pd.DataFrame(Injury[0])\nInjury_array = np.array(Injury_df['Player']).astype(str)\n\n# CBS Schedule\n\nclass parameters:\n    season_year = '2022'\n    team_array = [\n        'BOS', 'BRK', 'TOR', 'PHI',\n        'NYK', 'MIN', 'OKC', 'DEN',\n        'POR', 'UTA', 'CHI', 'CLE',\n        'DET', 'IND', 'MIL', 'LAL',\n        'LAC', 'GSW', 'PHO', 'SAC',\n        'WAS', 'ATL', 'ORL', 'MIA',\n        'CHO', 'MEM', 'NOP', 'HOU',\n        'SAS', 'DAL'\n    ]\n    offence_variables_for_average = [\n        'FG', '3P', '2P',\n        'FT', 'ORB'\n    ]\n    defence_variables_for_average = [\n        'STL', 'BLK', 'DRB',\n        'PF', 'TOV'\n    ]\n    team_name_abbrevations = {\n    'Celtics':'BOS',\n    'Nets':'BRK',\n    'Raptors':'TOR',\n    '76ers':'PHI',\n    'Knicks':'NYK',\n    'Timberwolves':'MIN',\n    'Thunder':'OKC',\n    'Nuggets':'DEN',\n    'Trail Blazers':'POR',\n    'Jazz':'UTA',\n    'Bulls':'CHI',\n    'Cavaliers':'CLE',\n    'Pistons':'DET',\n    'Pacers':'IND',\n    'Bucks':'MIL',\n    'Lakers':'LAL',\n    'Clippers':'LAC',\n    'Warriors':'GSW',\n    'Suns':'PHO',\n    'Kings':'SAC',\n    'Wizards':'WAS',\n    'Hawks':'ATL',\n    'Magic':'ORL',\n    'Heat':'MIA',\n    'Hornets':'CHO',\n    'Grizzlies':'MEM',\n    'Pelicans':'NOP',\n    'Rockets':'HOU',\n    'Spurs':'SAS',\n    'Mavericks':'DAL'\n    }\nclass offence_defence_functions:\n    @staticmethod\n    def offence_dataset(df):\n        df = df.loc[0:12, parameters.offence_variables_for_average]\n        return df\n    @staticmethod\n    def defence_dataset(df):\n        df = df.loc[0:12, parameters.defence_variables_for_average]\n        return df\n    @staticmethod\n    def offence_function(FG, ThreeP, TwoP, FT, ORB):\n        final_result = (0.1 * FG) + (0.5 * ThreeP) + (0.25 * TwoP) + \\\n                       + (0.1 * FT) + (0.05 * ORB)\n        return final_result\n    @staticmethod\n    def defence_function(Stl, Blk, Drb, Pf, Tov):\n        final_result = (0.2 * Stl) + (0.1 * Blk) + \\\n                       (0.1 * Drb) - ((0.3 * Pf) + (0.3 * Tov))\n        return final_result\n    @staticmethod\n    def offence_defence_combo(team_number):\n        total_team_score = offence_defence_functions.offence_function(\n            normalization(team_dataset_offence.get(parameters.team_array[team_number]),\n                          parameters.offence_variables_for_average[0]),\n            normalization(team_dataset_offence.get(parameters.team_array[team_number]),\n                          parameters.offence_variables_for_average[1]),\n            normalization(team_dataset_offence.get(parameters.team_array[team_number]),\n                          parameters.offence_variables_for_average[2]),\n            normalization(team_dataset_offence.get(parameters.team_array[team_number]),\n                          parameters.offence_variables_for_average[3]),\n            normalization(team_dataset_offence.get(parameters.team_array[team_number]),\n                          parameters.offence_variables_for_average[4])) + \\\n                           offence_defence_functions.defence_function(\n                               normalization(team_dataset_defence.get(parameters.team_array[team_number]),\n                                             parameters.defence_variables_for_average[0]),\n                               normalization(team_dataset_defence.get(parameters.team_array[team_number]),\n                                             parameters.defence_variables_for_average[1]),\n                               normalization(team_dataset_defence.get(parameters.team_array[team_number]),\n                                             parameters.defence_variables_for_average[2]),\n                               normalization(team_dataset_defence.get(parameters.team_array[team_number]),\n                                             parameters.defence_variables_for_average[3]),\n                               normalization(team_dataset_defence.get(parameters.team_array[team_number]),\n                                             parameters.defence_variables_for_average[4]))\n        return total_team_score\nclass webscrape_functions:\n    todays_date = date.today().strftime(\"%Y%m%d\")\n    random_date_for_testing = '20211013'\n    Schedule = pd.read_html('https://www.cbssports.com/nba/scoreboard/' + todays_date + '/')\n    Schedule.pop(15)\n    def webscrape_dataset_function(team_array, season_year):\n        df = pd.read_html(\n            'https://www.basketball-reference.com/teams/' + team_array + '/' + season_year + '.html#advanced')\n        team_table_number = 1\n        df = pd.DataFrame(df[team_table_number])\n        df = injury_function(df)\n        df = df.loc[0:12, ['FG', '3P', '3PA', '2P', '2PA', 'FT', 'FTA', 'ORB',\n                           'DRB', 'STL', 'BLK', 'PF', 'TOV']].fillna(0)\n        return df\n    @staticmethod\n    def concat_schedule_teams():\n        todays_scheduled_teams = list()\n        append_scheduled_teams = list()\n        for i in range(0, len(webscrape_functions.Schedule)):\n            if i % 3 == 0:\n                todays_scheduled_teams.append(webscrape_functions.Schedule[i])\n        for k in range(0, len(todays_scheduled_teams)):\n            append_scheduled_teams.append(todays_scheduled_teams[k].T.iloc[0])\n        todays_scheduled_teams_df = pd.DataFrame(append_scheduled_teams)\n        todays_scheduled_teams_df.columns = ['Away', 'Home']\n        todays_scheduled_teams_df_away_no_numbers = pd.DataFrame(\n            todays_scheduled_teams_df['Away'].str[:-3]).reset_index(drop=True)\n        todays_scheduled_teams_df_home_no_numbers = pd.DataFrame(\n            todays_scheduled_teams_df['Home'].str[:-3]).reset_index(drop=True)\n        final_schedule_df = pd.merge(todays_scheduled_teams_df_away_no_numbers['Away'].map(parameters.team_name_abbrevations),\n                                     todays_scheduled_teams_df_home_no_numbers['Home'].map(parameters.team_name_abbrevations),\n                                     left_index=True, right_index=True)\n        return final_schedule_df\n\ndef normalization(df,variable):\n    variable_normalzied = float(np.sum(((df.loc[:, [variable]] - pd.DataFrame.mean(df.loc[:, [variable]])) /\n                                        pd.DataFrame.std(df.loc[:, [variable]]))))\n    return variable_normalzied\ndef injury_function(dataset):\n    dataset = dataset[~dataset.isin(Injury_df)]\n    dataset.dropna(subset=[('Unnamed: 1')], inplace = True)\n    return dataset\n\ntodays_away_teams_df = webscrape_functions.concat_schedule_teams()['Away']\ntodays_home_teams_df = webscrape_functions.concat_schedule_teams()['Home']\n\nteam_dataset_offence = {\n    'BOS': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[0],season_year=parameters.season_year)),\n    'BRK': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[1],season_year=parameters.season_year)),\n    'TOR': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[2],season_year=parameters.season_year)),\n    'PHI': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[3],season_year=parameters.season_year)),\n    'NYK': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[4],season_year=parameters.season_year)),\n    'MIN': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[5],season_year=parameters.season_year)),\n    'OKC': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[6],season_year=parameters.season_year)),\n    'DEN': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[7],season_year=parameters.season_year)),\n    'POR': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[8],season_year=parameters.season_year)),\n    'UTA': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[9],season_year=parameters.season_year)),\n    'CHI': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[10],season_year=parameters.season_year)),\n    'CLE': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[11],season_year=parameters.season_year)),\n    'DET': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[12],season_year=parameters.season_year)),\n    'IND': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[13],season_year=parameters.season_year)),\n    'MIL': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[14],season_year=parameters.season_year)),\n    'LAL': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[15],season_year=parameters.season_year)),\n    'LAC': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[16],season_year=parameters.season_year)),\n    'GSW': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[17],season_year=parameters.season_year)),\n    'PHO': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[18],season_year=parameters.season_year)),\n    'SAC': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[19],season_year=parameters.season_year)),\n    'WAS': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[20],season_year=parameters.season_year)),\n    'ATL': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[21],season_year=parameters.season_year)),\n    'ORL': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[22],season_year=parameters.season_year)),\n    'MIA': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[23],season_year=parameters.season_year)),\n    'CHO': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[24],season_year=parameters.season_year)),\n    'MEM': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[25],season_year=parameters.season_year)),\n    'NOP': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[26],season_year=parameters.season_year)),\n    'HOU': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[27],season_year=parameters.season_year)),\n    'SAS': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[28],season_year=parameters.season_year)),\n    'DAL': offence_defence_functions.offence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[29],season_year=parameters.season_year))\n}\nteam_dataset_defence = {\n    'BOS': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[0],season_year=parameters.season_year)),\n    'BRK': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[1],season_year=parameters.season_year)),\n    'TOR': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[2],season_year=parameters.season_year)),\n    'PHI': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[3],season_year=parameters.season_year)),\n    'NYK': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[4],season_year=parameters.season_year)),\n    'MIN': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[5],season_year=parameters.season_year)),\n    'OKC': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[6],season_year=parameters.season_year)),\n    'DEN': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[7],season_year=parameters.season_year)),\n    'POR': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[8],season_year=parameters.season_year)),\n    'UTA': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[9],season_year=parameters.season_year)),\n    'CHI': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[10],season_year=parameters.season_year)),\n    'CLE': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[11],season_year=parameters.season_year)),\n    'DET': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[12],season_year=parameters.season_year)),\n    'IND': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[13],season_year=parameters.season_year)),\n    'MIL': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[14],season_year=parameters.season_year)),\n    'LAL': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[15],season_year=parameters.season_year)),\n    'LAC': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[16],season_year=parameters.season_year)),\n    'GSW': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[17],season_year=parameters.season_year)),\n    'PHO': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[18],season_year=parameters.season_year)),\n    'SAC': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[19],season_year=parameters.season_year)),\n    'WAS': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[20],season_year=parameters.season_year)),\n    'ATL': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[21],season_year=parameters.season_year)),\n    'ORL': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[22],season_year=parameters.season_year)),\n    'MIA': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[23],season_year=parameters.season_year)),\n    'CHO': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[24],season_year=parameters.season_year)),\n    'MEM': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[25],season_year=parameters.season_year)),\n    'NOP': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[26],season_year=parameters.season_year)),\n    'HOU': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[27],season_year=parameters.season_year)),\n    'SAS': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[28],season_year=parameters.season_year)),\n    'DAL': offence_defence_functions.defence_dataset(webscrape_functions.webscrape_dataset_function(\n        parameters.team_array[29],season_year=parameters.season_year))\n}\nteam_final_result = {\n    'BOS': offence_defence_functions.offence_defence_combo(0),\n    'BRK': offence_defence_functions.offence_defence_combo(1),\n    'TOR': offence_defence_functions.offence_defence_combo(2),\n    'PHI': offence_defence_functions.offence_defence_combo(3),\n    'NYK': offence_defence_functions.offence_defence_combo(4),\n    'MIN': offence_defence_functions.offence_defence_combo(5),\n    'OKC': offence_defence_functions.offence_defence_combo(6),\n    'DEN': offence_defence_functions.offence_defence_combo(7),\n    'POR': offence_defence_functions.offence_defence_combo(8),\n    'UTA': offence_defence_functions.offence_defence_combo(9),\n    'CHI': offence_defence_functions.offence_defence_combo(10),\n    'CLE': offence_defence_functions.offence_defence_combo(11),\n    'DET': offence_defence_functions.offence_defence_combo(12),\n    'IND': offence_defence_functions.offence_defence_combo(13),\n    'MIL': offence_defence_functions.offence_defence_combo(14),\n    'LAL': offence_defence_functions.offence_defence_combo(15),\n    'LAC': offence_defence_functions.offence_defence_combo(16),\n    'GSW': offence_defence_functions.offence_defence_combo(17),\n    'PHO': offence_defence_functions.offence_defence_combo(18),\n    'SAC': offence_defence_functions.offence_defence_combo(19),\n    'WAS': offence_defence_functions.offence_defence_combo(20),\n    'ATL': offence_defence_functions.offence_defence_combo(21),\n    'ORL': offence_defence_functions.offence_defence_combo(22),\n    'MIA': offence_defence_functions.offence_defence_combo(23),\n    'CHO': offence_defence_functions.offence_defence_combo(24),\n    'MEM': offence_defence_functions.offence_defence_combo(25),\n    'NOP': offence_defence_functions.offence_defence_combo(26),\n    'HOU': offence_defence_functions.offence_defence_combo(27),\n    'SAS': offence_defence_functions.offence_defence_combo(28),\n    'DAL': offence_defence_functions.offence_defence_combo(29)\n}", "9": "import schedule\nimport time\nfrom pushbullet import Pushbullet\nfrom Pris import Pris\nfrom WebScrape import Webscrape\n\ndef main():\n    prisen = Pris()\n    prisen.currentPris = Webscrape.webScrape()\n    prisen.forjePris = prisen.currentPris\n    pb = Pushbullet(\"skriv inn api key her\")\n\n    while Webscrape.kjorProg:\n        if prisen.forjePris != prisen.currentPris:\n            Webscrape.kjorProg = False\n            print(Webscrape.kjorProg)\n            print(\"Pris er oppdatert:\")\n        else:\n            prisen.forjePris = prisen.currentPris\n            prisen.currentPris = Webscrape.webScrape()\n            print(prisen.currentPris)\n            schedule.run_pending()\n            time.sleep(14400) #sleep 4 timer kj\u00f8r igjen.\n    if Webscrape.kjorProg is False:\n        push = pb.push_note(\"Prisendring\", \"Prisen er n\u00e5 {} \".format(str(prisen.currentPris)))\n\nmain()\n", "10": "# This script is to import the data\nfrom utils import *\n\nyear_selected = '2022' # Note: Make sure too adjust this as the new season starts.\nTeam_array = [\n    'ARI',\t'TBR',\t'HOU',\t'NYY',\n    'CHW',\t'BOS',\t'OAK',\t'SEA',\n    'TOR',\t'CLE',\t'LAA',\t'DET',\n    'KCR',\t'MIN',\t'TEX',\t'BAL',\n    'SFG',\t'LAD',\t'MIL',\t'CIN',\n    'ATL',\t'SDP',\t'STL',\t'PHI',\n    'NYM',\t'COL',\t'WSN',\t'CHC',\n    'MIA',\t'PIT'\n]\nteam_abbrev = {\n    'Arizona':'ARI', 'Minnesota':'MIN', 'Washington':'WSN',\n    'Philadelphia':'PHI', 'N.Y. Yankees':'NYY', 'Oakland':'OAK',\n    'San Francisco':'SFG', 'Tampa Bay':'TBR', 'L.A. Angels' : 'LAA',\n    'Detroit':'DET', 'St. Louis':'STL', 'Chi. Cubs':'CHC',\n    'Atlanta':'ATL', 'Houston':'HOU', 'Cincinnati':'CIN',\n    'N.Y. Mets':'NYM', 'Boston':'BOS', 'Pittsburgh':'PIT',\n    'Miami':'MIA', 'Baltimore':'BAL', 'Toronto':'TOR',\n    'Cleveland':'CLE', 'Chi. White Sox':'CHW', 'Texas':'TEX',\n    'Kansas City':'KCR', 'Milwaukee':'MIL', 'Colorado':'COL',\n    'San Diego':'SDP', 'Seattle':'SEA', 'L.A. Dodgers':'LAD'\n}\n\nclass offence_defence:\n    def webscrape_link(team, season_year):\n        Link = 'https://www.baseball-reference.com/teams/' + team + '/' + season_year + '.shtml#team_batting'\n        return Link\n    def batting_df(web_link):\n        batting_Link = pd.read_html(web_link)\n        batting_Df = pd.DataFrame(batting_Link[(len(batting_Link) - 2)])\n        batting_Df = batting_Df.loc[0:len(batting_Df), ['R', 'H', '2B', '3B', 'RBI', 'SB']]\n        batting_Df = batting_Df.drop(labels=range(21, len(batting_Df)),\n                                     axis=0)\n        batting_Df = batting_Df.loc[(batting_Df['R'] != 'R')]\n        batting_Df = batting_Df.dropna()\n        return batting_Df\n    def pitching_df(web_link):\n        pitching_Link = pd.read_html(web_link)\n        pitching_Df = pd.DataFrame(pitching_Link[(len(pitching_Link) - 1)])\n        pitching_Df = pitching_Df.loc[0:len(pitching_Df), ['R', 'H', 'ER', 'HR', 'BB', 'SO']]\n        pitching_Df = pitching_Df.drop(labels=range(21, len(pitching_Df)),\n                                       axis=0)\n        pitching_Df = pitching_Df.loc[(pitching_Df['H'] != 'H')]\n        pitching_Df = pitching_Df.dropna()\n        return pitching_Df\n\nclass todays_games:\n    todays_date = date.today().strftime(\"%Y%m%d\")\n    link_to_todays_games = pd.read_html('https://www.cbssports.com/mlb/schedule/' + todays_date)\n    @staticmethod\n    def home_teams():\n        todays_home_teams_df = todays_games.link_to_todays_games[0]['Home'].map(team_abbrev)\n        return todays_home_teams_df\n    @staticmethod\n    def away_teams():\n        todays_away_teams_df = todays_games.link_to_todays_games[0]['Away'].map(team_abbrev)\n        return todays_away_teams_df\n\nclass teams:\n    class pitching:\n        # Arizona = ARI\n        ARI = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[0],\n                                                                                  season_year=year_selected))\n        # Tampa Bay = TBR\n        TBR = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[1],\n                                                                                  season_year=year_selected))\n        # Houston = HOU\n        HOU = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[2],\n                                                                                  season_year=year_selected))\n        # NY Yankees = NYY\n        NYY = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[3],\n                                                                                  season_year=year_selected))\n        # Chicago White Sox = CHW\n        CHW = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[4],\n                                                                                  season_year=year_selected))\n        # Boston = BOS\n        BOS = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[5],\n                                                                                  season_year=year_selected))\n        # Oakland = OAK\n        OAK = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[6],\n                                                                                  season_year=year_selected))\n        # Seattle = SEA\n        SEA = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[7],\n                                                                                  season_year=year_selected))\n        # Toronto = TOR\n        TOR = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[8],\n                                                                                  season_year=year_selected))\n        # Cleveland = CLE\n        CLE = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[9],\n                                                                                  season_year=year_selected))\n        # LA Angels = LAA\n        LAA = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[10],\n                                                                                  season_year=year_selected))\n        # Detroit = DET\n        DET = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[11],\n                                                                                  season_year=year_selected))\n        # Kansas City = KCR\n        KCR = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[12],\n                                                                                  season_year=year_selected))\n        # Minnesota = MIN\n        MIN = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[13],\n                                                                                  season_year=year_selected))\n        # Texas = TEX\n        TEX = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[14],\n                                                                                  season_year=year_selected))\n        # Baltimore = BAL\n        BAL = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[15],\n                                                                                  season_year=year_selected))\n        # SF Giants = SFG\n        SFG = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[16],\n                                                                                  season_year=year_selected))\n        # LA Dodgers = LAD\n        LAD = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[17],\n                                                                                  season_year=year_selected))\n        # Milwauke = MIL\n        MIL = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[18],\n                                                                                  season_year=year_selected))\n        # Cincinnati = CIN\n        CIN = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[19],\n                                                                                  season_year=year_selected))\n        # Atlanta = ATL\n        ATL = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[20],\n                                                                                  season_year=year_selected))\n        # San Diego = SDP\n        SDP = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[21],\n                                                                                  season_year=year_selected))\n        # St. Louis\n        STL = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[22],\n                                                                                  season_year=year_selected))\n        # Philadelphia = PHI\n        PHI = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[23],\n                                                                                  season_year=year_selected))\n        # NY Mets = NYM\n        NYM = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[24],\n                                                                                  season_year=year_selected))\n        # Colorado = COL\n        COL = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[25],\n                                                                                  season_year=year_selected))\n        # Washington = WSN\n        WSN = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[26],\n                                                                                  season_year=year_selected))\n        # Chicago Cubs = CHC\n        CHC = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[27],\n                                                                                  season_year=year_selected))\n        # Miami = MIA\n        MIA = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[28],\n                                                                                  season_year=year_selected))\n        # Pittsburgh = PIT\n        PIT = offence_defence.pitching_df(web_link=offence_defence.webscrape_link(team=Team_array[29],\n                                                                                  season_year=year_selected))\n    class batting:\n# Arizona = ARI\n        ARI = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[0],\n                                                                                 season_year=year_selected))\n# Tampa Bay = TBR\n        TBR = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[1],\n                                                                                 season_year=year_selected))\n# Houston = HOU\n        HOU = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[2],\n                                                                                 season_year=year_selected))\n# NY Yankees = NYY\n        NYY = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[3],\n                                                                                 season_year=year_selected))\n# Chicago White Sox = CHW\n        CHW = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[4],\n                                                                                 season_year=year_selected))\n# Boston = BOS\n        BOS = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[5],\n                                                                                 season_year=year_selected))\n# Oakland = OAK\n        OAK = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[6],\n                                                                                 season_year=year_selected))\n# Seattle = SEA\n        SEA = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[7],\n                                                                                 season_year=year_selected))\n# Toronto = TOR\n        TOR = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[8],\n                                                                                 season_year=year_selected))\n# Cleveland = CLE\n        CLE = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[9],\n                                                                                 season_year=year_selected))\n#LA Angels = LAA\n        LAA = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[10],\n                                                                                 season_year=year_selected))\n# Detroit = DET\n        DET = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[11],\n                                                                                 season_year=year_selected))\n# Kansas City = KCR\n        KCR = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[12],\n                                                                                 season_year=year_selected))\n# Minnesota = MIN\n        MIN = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[13],\n                                                                                 season_year=year_selected))\n# Texas = TEX\n        TEX = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[14],\n                                                                                 season_year=year_selected))\n# Baltimore = BAL\n        BAL = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[15],\n                                                                                 season_year=year_selected))\n# SF Giants = SFG\n        SFG = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[16],\n                                                                                 season_year=year_selected))\n# LA Dodgers = LAD\n        LAD = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[17],\n                                                                                 season_year=year_selected))\n# Milwauke = MIL\n        MIL = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[18],\n                                                                                 season_year=year_selected))\n# Cincinnati = CIN\n        CIN = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[19],\n                                                                                 season_year=year_selected))\n# Atlanta = ATL\n        ATL = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[20],\n                                                                                 season_year=year_selected))\n# San Diego = SDP\n        SDP = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[21],\n                                                                                 season_year=year_selected))\n# St. Louis\n        STL = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[22],\n                                                                                 season_year=year_selected))\n# Philadelphia = PHI\n        PHI = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[23],\n                                                                                 season_year=year_selected))\n# NY Mets = NYM\n        NYM = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[24],\n                                                                                 season_year=year_selected))\n# Colorado = COL\n        COL = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[25],\n                                                                                 season_year=year_selected))\n# Washington = WSN\n        WSN = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[26],\n                                                                                 season_year=year_selected))\n# Chicago Cubs = CHC\n        CHC = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[27],\n                                                                                 season_year=year_selected))\n# Miami = MIA\n        MIA = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[28],\n                                                                                 season_year=year_selected))\n# Pittsburgh = PIT\n        PIT = offence_defence.batting_df(web_link=offence_defence.webscrape_link(team=Team_array[29],\n                                                                                 season_year=year_selected))\n\ntodays_home_teams_df = todays_games.home_teams()\ntodays_away_teams_df = todays_games.away_teams()\n", "11": "# This page is to create the dataset needed to predict football (american) games.\n\nfrom utils import *\n\nteam_array_for_website = ['CRD', 'ATL', 'BUF', 'MIA', 'NWE',\n                          'NYJ', 'OTI', 'CLT', 'HTX', 'JAX',\n                          'PIT', 'RAV', 'CLE', 'CIN', 'KAN',\n                          'RAI', 'SDG', 'DEN', 'WAS', 'NYG',\n                          'DAL', 'PHI', 'NOR', 'TAM', 'CAR',\n                          'GNB', 'CHI', 'MIN', 'DET', 'SEA',\n                          'RAM', 'SFO']\n\nteam_array = ['ARI', 'ATL', 'BUF', 'MIA', 'NWE',\n              'NYJ', 'TEN', 'IND', 'HOU', 'JAX',\n              'PIT', 'BAL', 'CLE', 'CIN', 'KAN',\n              'LVR', 'LAC', 'DEN', 'WSH', 'NYG',\n              'DAL', 'PHI', 'NOR', 'TBB', 'CAR',\n              'GNB', 'CHI', 'MIN', 'DET', 'SEA',\n              'LAR', 'SFO']\n\nseason_year = 2021\n\ndef webscrape_data_manipulation_function(team_array_position):\n    latest_season = pd.read_html('https://www.pro-football-reference.com/teams/'\n                                 + str.lower(team_array_for_website[team_array_position]) + '/'\n                                 + str(season_year) + '.htm#passing')[1]\n    latest_season_minus_1 = pd.read_html('https://www.pro-football-reference.com/teams/'\n                                         + str.lower(team_array_for_website[team_array_position])\n                                         + '/' + str(season_year-1) + '.htm#passing')[1]\n    latest_season_minus_2 = pd.read_html('https://www.pro-football-reference.com/teams/'\n                                         + str.lower(team_array_for_website[team_array_position])\n                                         + '/' + str(season_year-2) + '.htm#passing')[1]\n    latest_season_minus_3 = pd.read_html('https://www.pro-football-reference.com/teams/'\n                                         + str.lower(team_array_for_website[team_array_position])\n                                         + '/' + str(season_year-3) + '.htm#passing')[1]\n    latest_season_minus_4 = pd.read_html('https://www.pro-football-reference.com/teams/'\n                                         + str.lower(team_array_for_website[team_array_position])\n                                         + '/' + str(season_year-4) + '.htm#passing')[1]\n    df = pd.concat([pd.DataFrame(latest_season_minus_4), pd.DataFrame(latest_season_minus_3),\n                    pd.DataFrame(latest_season_minus_2), pd.DataFrame(latest_season_minus_1),\n                    pd.DataFrame(latest_season)])\n    df = df.get(('Score', 'Tm'))\n    df = pd.DataFrame(df).dropna().reset_index(drop=True)\n    return df\n\ndef normalization(df):\n    normalized_values = np.log(df.shift(1) / df)\n    normalized_values = normalized_values.dropna()\n    normalized_values = normalized_values.replace([np.inf, -np.inf], 0)\n    return normalized_values\n\nteams_datasets = {\n    'ARI': webscrape_data_manipulation_function(0),\n    'ATL': webscrape_data_manipulation_function(1),\n    'BUF': webscrape_data_manipulation_function(2),\n    'MIA': webscrape_data_manipulation_function(3),\n    'NWE': webscrape_data_manipulation_function(4),\n    'NYJ': webscrape_data_manipulation_function(5),\n    'TEN': webscrape_data_manipulation_function(6),\n    'IND': webscrape_data_manipulation_function(7),\n    'HOU': webscrape_data_manipulation_function(8),\n    'JAX': webscrape_data_manipulation_function(9),\n    'PIT': webscrape_data_manipulation_function(10),\n    'BAL': webscrape_data_manipulation_function(11),\n    'CLE': webscrape_data_manipulation_function(12),\n    'CIN': webscrape_data_manipulation_function(13),\n    'KAN': webscrape_data_manipulation_function(14),\n    'LVR': webscrape_data_manipulation_function(15),\n    'LAC': webscrape_data_manipulation_function(16),\n    'DEN': webscrape_data_manipulation_function(17),\n    'WSH': webscrape_data_manipulation_function(18),\n    'NYG': webscrape_data_manipulation_function(19),\n    'DAL': webscrape_data_manipulation_function(20),\n    'PHI': webscrape_data_manipulation_function(21),\n    'NOR': webscrape_data_manipulation_function(22),\n    'TBB': webscrape_data_manipulation_function(23),\n    'CAR': webscrape_data_manipulation_function(24),\n    'GNB': webscrape_data_manipulation_function(25),\n    'CHI': webscrape_data_manipulation_function(26),\n    'MIN': webscrape_data_manipulation_function(27),\n    'DET': webscrape_data_manipulation_function(28),\n    'SEA': webscrape_data_manipulation_function(29),\n    'LAR': webscrape_data_manipulation_function(30),\n    'SFO': webscrape_data_manipulation_function(31)\n}\nteams_normalized_datasets = {\n    'ARI': normalization(teams_datasets.get(team_array[0])),\n    'ATL': normalization(teams_datasets.get(team_array[1])),\n    'BUF': normalization(teams_datasets.get(team_array[2])),\n    'MIA': normalization(teams_datasets.get(team_array[3])),\n    'NWE': normalization(teams_datasets.get(team_array[4])),\n    'NYJ': normalization(teams_datasets.get(team_array[5])),\n    'TEN': normalization(teams_datasets.get(team_array[6])),\n    'IND': normalization(teams_datasets.get(team_array[7])),\n    'HOU': normalization(teams_datasets.get(team_array[8])),\n    'JAX': normalization(teams_datasets.get(team_array[9])),\n    'PIT': normalization(teams_datasets.get(team_array[10])),\n    'BAL': normalization(teams_datasets.get(team_array[11])),\n    'CLE': normalization(teams_datasets.get(team_array[12])),\n    'CIN': normalization(teams_datasets.get(team_array[13])),\n    'KAN': normalization(teams_datasets.get(team_array[14])),\n    'LVR': normalization(teams_datasets.get(team_array[15])),\n    'LAC': normalization(teams_datasets.get(team_array[16])),\n    'DEN': normalization(teams_datasets.get(team_array[17])),\n    'WSH': normalization(teams_datasets.get(team_array[18])),\n    'NYG': normalization(teams_datasets.get(team_array[19])),\n    'DAL': normalization(teams_datasets.get(team_array[20])),\n    'PHI': normalization(teams_datasets.get(team_array[21])),\n    'NOR': normalization(teams_datasets.get(team_array[22])),\n    'TBB': normalization(teams_datasets.get(team_array[23])),\n    'CAR': normalization(teams_datasets.get(team_array[24])),\n    'GNB': normalization(teams_datasets.get(team_array[25])),\n    'CHI': normalization(teams_datasets.get(team_array[26])),\n    'MIN': normalization(teams_datasets.get(team_array[27])),\n    'DET': normalization(teams_datasets.get(team_array[28])),\n    'SEA': normalization(teams_datasets.get(team_array[29])),\n    'LAR': normalization(teams_datasets.get(team_array[30])),\n    'SFO': normalization(teams_datasets.get(team_array[31]))\n}", "12": "import unittest\n\nfrom src.webscrape_trails import WebscrapeTrails\nimport pandas as pd\nimport numpy as np\n\n\nclass TestWebscrapeTrails(unittest.TestCase):\n\n    def __init__(self, *args, **kwargs):\n        super(TestWebscrapeTrails, self).__init__(*args, **kwargs)\n        self.webscrape = WebscrapeTrails()\n        \n    def test_make_tables(self):\n        \"\"\"\n        GIVEN a URL for trails at a resort\n\n        WHEN trails are webscraped and processed\n\n        THEN test if data structure is a DataFrame\n            and if specified columns exist in DataFrame\n        \"\"\"\n    \n        TEST_URL = \"https://jollyturns.com/resort/united-states-of-america/beaver-creek-resort/skiruns-green\"\n        \n        df_trails = self.webscrape.make_tables(URL=TEST_URL)\n\n        self.assertIsInstance(df_trails, pd.core.frame.DataFrame)\n\n        test_cols = ['Trail Name', 'Bottom Elev (ft)', 'Top Elev (ft)', 'Vertical Drop (ft)', 'Length (mi)', 'URL']\n\n        self.assertTrue(all([col in df_trails.columns for col in test_cols]))\n    \n    def test_rename_resorts(self):\n        \"\"\"\n        GIVEN a Pandas DataFrame of trail data\n\n        WHEN resorts are renamed based on the URL\n\n        THEN test if data structure is a DataFrame\n            and Resort column is in DataFrame\n            and tested resort name is Beaver Creek\n        \"\"\"\n\n        df_trails = pd.DataFrame({'Trail Name': {0: '\\xa0 Anderson Way ', 1: '\\xa0 Bear Paw '},\n                                  'Bottom Elev (ft)': {0: '8025 ft', 1: '8501 ft'},\n                                  'Top Elev (ft)': {0: '8238 ft', 1: '8547 ft'},\n                                  'Vertical Drop (ft)': {0: '213 ft', 1: '43 ft'},\n                                  'Length (mi)': {0: '0.73 mi', 1: '0.17 mi'},\n                                  'URL': {0: 'https://jollyturns.com/resort/united-states-of-america/beaver-creek-resort/skiruns-green',\n                                          1: 'https://jollyturns.com/resort/united-states-of-america/beaver-creek-resort/skiruns-green'}})\n        \n        df_trails = self.webscrape.rename_resorts(df=df_trails)\n        \n        self.assertIsInstance(df_trails, pd.core.frame.DataFrame)\n\n        self.assertTrue(\"Resort\" in list(df_trails.columns))\n        self.assertTrue(all(df_trails[\"Resort\"] == \"Beaver Creek\"))\n", "13": "import time\nfrom collections import defaultdict\nimport bs4\nimport pandas as pd\nimport selenium.webdriver as webd\nfrom bs4 import BeautifulSoup\nfrom sqlalchemy import create_engine\nimport pymysql\nimport sentiment_analysis\n\n\ndef webscrape(airline):\n    \"\"\" Input: airline: text used to go to reviews website for particular airline\n    Output: reviews: list of HTML segments that contains all relevant review information \"\"\"\n    reviews = []\n    browser = webd.Chrome(executable_path=\"C:/chromeDriver/chromedriver.exe\")\n\n    for page_num in range(1, 50):\n        url = f'http://www.airlinequality.com/airline-reviews/{airline}/page/{page_num}/?sortby=post_date%3ADesc&pagesize=100'\n        browser.get(url)\n        time.sleep(5)\n        html = browser.page_source\n        soup = BeautifulSoup(html, 'html.parser')\n        if soup.select('div.col-content article article.list-item'):\n            reviews.append(soup.select('div.col-content article article.list-item'))\n        else:\n            break\n\n    return reviews\n\n\ndef parse_review(review: bs4.element.Tag) -> dict:\n    \"\"\"\n    Input: review: HTML segment that contains all relevant review information\n    Output: d: dictionary of relevant review information\n    \"\"\"\n\n    d = {}\n    if review.select_one(\"div.rating-10 span\"):\n        d['rating'] = int(review.select_one(\"div.rating-10 span\").text)\n    d['headline'] = review.select_one(\"h2.text_header\").text\n    try:\n        d['country'] = review.select_one('h3.text_sub_header').text\\\n            .replace(')', '(').split('(')[1]\n    except IndexError:\n        d['country'] = 'None'\n    d['body'] = review.select_one(\"div.text_content\").text.strip()\n    rows = review.select('tr')\n    for row in rows:\n        if row.select('td')[1].attrs['class'][0] == 'review-rating-stars':\n            for x in row.select('span'):\n                try:\n                    if x.attrs['class'] == ['star', 'fill']:\n                        num = int(x.text)\n                        d[row.td.attrs['class'][1]] = num\n                except KeyError:\n                    continue\n        else:\n            d[row.td.attrs['class'][1]] = row.select('td')[1].text\n    return d\n\n\ndef webscrape_manager(airline_list):\n    \"\"\"\n    Input: airline_list: list of airline names as strings\n    Output: webscrape_info_dict: dictionary with keys as airline names and values as webscraped html code\n    \"\"\"\n    webscrape_info_dict = {}\n\n    for airline in airline_list:\n        webscrape_info_dict[airline] = webscrape(airline)\n\n    return webscrape_info_dict\n\n\ndef review_parser(airline_list, webscrape_info_dict):\n    \"\"\"\n    Input: airline_list: list of airline names as strings\n    webscrape_info_dict: dictionary with keys as airline names and values as webscraped html code\n    Output: airline_dict: dictionary with keys as airline names and values as their respective reviews\n    \"\"\"\n    airline_dict = defaultdict(list)\n    i = 0\n    for reviews in webscrape_info_dict.values():\n        for review in reviews:\n            for r in review:\n                airline_dict[airline_list[i]].append(parse_review(r))\n        i += 1\n    return airline_dict\n\n\ndef copy_to_sql(airline_list, airline_dict, engine):\n    \"\"\"\n    Input: airline_list: list of airline names as strings\n    airline_dict: dictionary with keys as airline names and values as their respective reviews\n    engine: directory to SQL database to store webscraped review data\n    \"\"\"\n    for airline in airline_list:\n        pd.DataFrame(airline_dict[airline]).to_sql(airline.split('-')[0], con=engine)\n\n\ndef getReviews(airline, engine):\n    airline_list = [airline]\n    webscrape_info_dict = webscrape_manager(airline_list)\n    airline_dict = review_parser(airline_list, webscrape_info_dict)\n    copy_to_sql(airline_list, airline_dict, engine)", "14": "# Scrapy settings for Webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'Webscrape'\n\nSPIDER_MODULES = ['Webscrape.spiders']\nNEWSPIDER_MODULE = 'Webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'Webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'Webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'Webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n    'Webscrape.pipelines.WebscrapePipeline': 300,\n}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "15": "import requests\nimport json\nimport datetime\nfrom html_scraper import scrape_url\n\n\ndef magicId (content):\n    if content is not None:\n        from magic_lib import Magic\n        m = Magic()\n        return m.from_buffer(content)\n    else: pass\n\ndef whatTheFile (uri):\n    if uri is not None:\n        uri = \"https://arweave.net/\" + uri\n        data = requests.get(uri)\n        return magicId(data.content)\n    else: pass\n\n# def testWebscrape(tx_number):\n#         webscrape = WebScraper(tx_number)\n#         webscrape = webscrape.run()\n        # data[\"named_entities\"] = str(webscrape[\"named_entities\"])\n        # data[\"source_language\"] = webscrape[\"source_language\"]\n        # data[\"source_text\"] = webscrape[\"source_text\"]\n        # data[\"english_text\"] = webscrape[\"english_text\"]\n\n# x = testWebscrape(\"IWdMfjLQo2mFlQ3ZBBVGwAghRI419mEw-HxufRIwF2g\")\ndef runArweaveAPI():\n    with open('height.txt', 'r') as f:\n        last_block = f.readlines()[-1]\n\n    url = 'https://arweave.net/graphql'\n\n    height = int(last_block)+1\n    query = \"\"\"query {\n        transactions(block: {min: \"\"\"+str(height)+\"\"\", max: \"\"\"+str(height)+\"\"\"}) {\n            edges {\n                node {\n                    id,\n                    data {\n                        size\n                        type\n                    },\n                    tags {\n                        name,\n                        value\n                    },\n                    block {\n                        id\n                        timestamp\n                        height\n                        previous\n                    }\n                }\n            }\n        }\n    }\"\"\"\n    response = requests.post(url, json={'query': query})\n    print(\"arweave_api response received {}\".format(str(response.status_code)))\n    json_data = json.loads(str(response.text))[\"data\"][\"transactions\"][\"edges\"]\n    arweave_api_data = []\n    for i in json_data:\n        # print(\"working through each tx: {} of {}\".format(i, str(len(json_data))))\n        tx_id = i[\"node\"][\"id\"]\n        timestamp = i[\"node\"][\"block\"][\"timestamp\"]\n        filetype = whatTheFile(tx_id)\n\n        data = {\"timestamp\": str(datetime.datetime.fromtimestamp(timestamp)),\n                \"tx_id\": str(tx_id),\n                \"block_height\": str(i[\"node\"][\"block\"][\"height\"]),\n                \"file_type\": str(filetype),\n                \"data_type\": str(i[\"node\"][\"data\"][\"type\"]),\n                \"data_size\": str(i[\"node\"][\"data\"][\"size\"])\n                }\n        for tag in i[\"node\"][\"tags\"]:\n            if \"_\" in str(tag[\"name\"]):\n                name = str(tag[\"name\"]).replace(\":\",\"_\")\n            else:\n                name = tag[\"name\"]\n            data[tag[\"name\"]] = str(tag[\"value\"])\n\n        # # try:\n        if \"text\" in str(filetype):\n            webscrape = scrape_url(tx_id)\n            # data[\"named_entities\"] = str(webscrape[\"named_entities\"])\n            # data[\"source_language\"] = webscrape[\"source_language\"]\n            # data[\"source_text\"] = webscrape[\"source_text\"]\n            data[\"source_text\"] = webscrape[\"source_text\"]\n            # # except:\n            # #     pass\n            print(\"done with webscrape stuff\")\n        arweave_api_data.append(data)\n    print(\"about to open height.txt to write new height\")\n    with open('height.txt', 'a') as f:\n        f.write(\"\\n\"+str(height))\n    print(\"about to return arweave_api_data\")\n    # return json.dumps(arweave_api_data, indent=4)\n    return arweave_api_data\n\n# if __name__ == '__main__':\n#     runArweaveAPI()\n\n\n# print(runArweaveAPI())\n", "16": "import psycopg2\n\nfrom database.methods import return_user\n\n\ndef init_connection():\n    conn = psycopg2.connect(\n        host=\"localhost\", database=\"mydb\", user=\"zaki\", password=\"password123\"\n    )\n    conn.autocommit = True\n    return conn\n\n\ndef create_schema(conn):\n    \"\"\"After manually creating and setting up psql database, run this\"\"\"\n    cur = conn.cursor()\n    cur.execute(\"CREATE SCHEMA IF NOT EXISTS webscrape AUTHORIZATION zaki\")\n    cur.close()\n    return\n\n\ndef create_tables(conn):\n    \"\"\"look into storing passwords, and its hashes\"\"\"\n    cur = conn.cursor()\n    # cur.execute(\"CREATE TABLE IF NOT EXISTS webscrape.test (name text, age integer)\")\n    cur.execute(\n        \"CREATE TABLE IF NOT EXISTS webscrape.users (\"\n        \"id serial,\"\n        \"email text,\"\n        \"password text,\"\n        \"PRIMARY KEY (id),\"\n        \"UNIQUE (email)\"\n        \")\"\n    )\n    cur.execute(\n        \"CREATE TABLE IF NOT EXISTS webscrape.items(\"\n        \"asin text,\"\n        \"title text,\"\n        \"current_amount numeric(6,2),\"\n        \"currency text,\"\n        \"PRIMARY KEY (asin)\"\n        \")\"\n    )\n    cur.execute(\n        \"CREATE TABLE IF NOT EXISTS webscrape.alerts (\"\n        \"asin text REFERENCES webscrape.items (asin),\"\n        \"id serial REFERENCES webscrape.users (id),\"\n        \"target_amount numeric(6, 2)\"\n        \")\"\n    )\n    cur.execute(\n        \"CREATE TABLE IF NOT EXISTS webscrape.price_history (\"\n        \"asin text REFERENCES webscrape.items (asin),\"\n        \"date date,\"\n        \"amount numeric(6, 2),\"\n        \"currency text\"\n        \")\"\n    )\n\n    cur.close()\n    return\n\n\ndef get_alerts_to_email(conn):\n    cur = conn.cursor()\n    # three table join\n    cur.execute(\n        \"\"\"\n    SELECT users.email, alerts.asin, items.title, items.current_amount, alerts.target_amount\n    FROM webscrape.users\n    INNER JOIN webscrape.alerts ON users.id=alerts.id\n    INNER JOIN webscrape.items ON alerts.asin=items.asin\n    WHERE items.current_amount > alerts.target_amount\n    \"\"\"\n    )\n    # for person in full_list:\n    #     (email, asin, title, current_amount, target_amount) = person\n    #     print(current_amount)\n\n    return cur.fetchall()\n\n\nif __name__ == \"__main__\":\n\n    conn = init_connection()\n    # create_schema(conn)\n    # create_tables(conn)\n    # mock_data(conn)\n    # a = get_alerts_to_email(conn)\n    # print(a)\n\n    a = return_user(conn, \"tm@gmail.com\")\n    # print(a)\n    if a:\n        print(\"user exists\")\n    else:\n        print(\"no user\")\n    conn.close()\n", "17": "from sqlalchemy import create_engine\nfrom collections import defaultdict\nimport selenium.webdriver as webd\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport selenium\nimport time\nimport bs4\n\n\ndef webscrape(airline):\n    \"\"\"\n    INPUT:\n    airline: text used to go to reviews website for particular airline\n\n    OUTPUT:\n    reviews: list of HTML segments that contains all relevant review\n        information\n    \"\"\"\n    reviews = []\n    browser = webd.Chrome()\n\n    for page_num in range(1, 50):\n        url = f'http://www.airlinequality.com/airline-reviews/{airline}/page/{page_num}/?sortby=post_date%3ADesc&pagesize=100'\n        browser.get(url)\n        time.sleep(5)\n        html = browser.page_source\n        soup = BeautifulSoup(html, 'html.parser')\n        if soup.select('div.col-content article article.list-item'):\n            reviews.append(soup.select('div.col-content article article.list-item'))\n        else:\n            break\n\n    return reviews\n\n\ndef parse_review(review: bs4.element.Tag) -> dict:\n    \"\"\"\n    INPUT:\n    review: HTML segment that contains all relevant review information\n\n    OUTPUT:\n    d: dictionary of relevant review information\n    \"\"\"\n\n    d = {}\n    if review.select_one(\"div.rating-10 span\"):\n        d['rating'] = int(review.select_one(\"div.rating-10 span\").text)\n    d['headline'] = review.select_one(\"h2.text_header\").text\n    try:\n        d['country'] = review.select_one('h3.text_sub_header').text\\\n            .replace(')', '(').split('(')[1]\n    except IndexError:\n        d['country'] = 'None'\n    d['body'] = review.select_one(\"div.text_content\").text.strip()\n    rows = review.select('tr')\n    for row in rows:\n        if row.select('td')[1].attrs['class'][0] == 'review-rating-stars':\n            for x in row.select('span'):\n                try:\n                    if x.attrs['class'] == ['star', 'fill']:\n                        num = int(x.text)\n                        d[row.td.attrs['class'][1]] = num\n                except KeyError:\n                    continue\n        else:\n            d[row.td.attrs['class'][1]] = row.select('td')[1].text\n    return d\n\n\ndef webscrape_manager(airline_list):\n    \"\"\"\n    INPUT:\n    airline_list: list of airline names as strings\n\n    OUTPUT:\n    webscrape_info_dict: dictionary with keys as airline names and values as\n        webscraped html code\n    \"\"\"\n    webscrape_info_dict = {}\n\n    for airline in airline_list:\n        webscrape_info_dict[airline] = webscrape(airline)\n\n    return webscrape_info_dict\n\n\ndef review_parser(airline_list, webscrape_info_dict):\n    \"\"\"\n    INPUT:\n    airline_list: list of airline names as strings\n    webscrape_info_dict: dictionary with keys as airline names and values as\n        webscraped html code\n\n    OUTPUT:\n    airline_dict: dictionary with keys as airline names and values as their\n        respective reviews\n    \"\"\"\n    airline_dict = defaultdict(list)\n    i = 0\n    for reviews in webscrape_info_dict.values():\n        for review in reviews:\n            for r in review:\n                airline_dict[airline_list[i]].append(parse_review(r))\n        i += 1\n    return airline_dict\n\n\ndef copy_to_sql(airline_list, airline_dict, engine):\n    \"\"\"\n    INPUT:\n    airline_list: list of airline names as strings\n    airline_dict: dictionary with keys as airline names and values as their\n        respective reviews\n    engine: directory to SQL database to store webscraped review data\n\n    OUTPUT:\n    None\n    \"\"\"\n    for airline in airline_list:\n        pd.DataFrame(airline_dict[airline]).to_sql(airline.split('-')[0],\n                                                   con=engine)\n\n\nif __name__ == \"__main__\":\n\n    airline_list = [\n        'southwest-airlines', 'american-airlines', 'delta-air-lines',\n        'united-airlines', 'ana-all-nippon-airways', 'japan-airlines',\n        'qatar-airways'\n    ]\n\n    webscrape_info_dict = webscrape_manager(airline_list)\n    airline_dict = review_parser(airline_list, webscrape_info_dict)\n\n    connection_string = 'postgresql://manuelcollazo:manuelcollazo@localhost:5432/airlines'\n\n    engine = create_engine(connection_string)\n    copy_to_sql(airline_list, airline_dict, engine)\n", "18": "from flask import Flask, jsonify, render_template, request\nimport json\nimport Webscrape\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef home():\n    return render_template(\"index.html\")\n\n@app.route(\"/webscrape\", methods=[\"POST\"])\ndef webscrape():\n    message = request.get_json(force=True)\n    \n    Webscrape.webscrape(message['search'])\n    response = json.load(open('product_info.json', 'r'))\n\n    return jsonify(response)\n\n@app.errorhandler(404)\ndef not_found(e):\n    return render_template(\"404.html\")\n\nif __name__ == '__main__':\n    app.run(debug=True)\n", "19": "from functions import webscrape_by_link, webscrape_by_span, webscraping_results, webscraping_results_nltk_bi, \\\n    display_results, webscraping_results_nltk_tri\nimport collections\nimport pandas\nfrom collections import Counter\nfrom variables import MAIN_RESULT\nimport matplotlib.pyplot as plt\n\n\"\"\"function to webscrape, extract and determine the most common words used in 5 main Polish news portals\"\"\"\n\n\n# wp.pl\nwebscrape_by_link('https://www.wp.pl/', 'a', 'class', \"pcfpmu-0 fxuaLn\")\n# gazeta.pl\nwebscrape_by_link('https://www.gazeta.pl/', 'a', 'class', \"timeline__link\")\n# interia\nwebscrape_by_link('https://www.interia.pl/', 'a', 'class', 'news-a')\n# onot.pl\nwebscrape_by_span('https://www.onet.pl/', 'span', 'class', 'title')\n# o2.pl\nwebscrape_by_span('https://www.o2.pl/', 'span', 'data-testid', 'teaserText')\n# tvn.pl\nwebscrape_by_link('https://tvn24.pl/', 'a', 'class', 'default-teaser__link')\n\n\n\n\nwebscraping_results()\ndisplay_results(webscraping_results())\n\nprint('nlt-bi:', webscraping_results_nltk_bi())\nprint('nlt-tri:', webscraping_results_nltk_tri())\nword_counter = Counter(webscraping_results()).most_common(10)\n\n\n\n\n\n# # # draw a chart\n# names = [i[0] for i in word_counter]\n#\n# values = [i[1] for i in word_counter]\n#\n# plt.figure(figsize=(10, 4))\n#\n# plt.subplot(131)\n# plt.bar(names, values)\n# plt.suptitle('Most common words')\n# plt.show()", "20": "# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'webscrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "21": "from webscrape import WebScrape\r\nfrom spotify import SpotPlaylist\r\n\r\ndate = input(\"Which year would you like to travel to? Type the date in this format YYYY-MM-DD: \")\r\nprint(\"Grabbing Data from Billboard of that Date.\")\r\nwebscrape = WebScrape(date)\r\nspotipy = SpotPlaylist()\r\nprint(\"Searching to see if songs exist on Spotify\")\r\nsong_uri_list = []\r\n\r\nfor i in range(0, len(webscrape.top_100)):\r\n    artist = webscrape.artist[i]\r\n    song = webscrape.top_100[i]\r\n    song_uri_list.append(spotipy.song_list_creation(song_name=song, artist=artist))\r\n\r\nplaylist_id = spotipy.playlist_generation(date)\r\nprint(\"Adding songs to Playlist\")\r\n\r\nfor songs in song_uri_list:\r\n    spotipy.adding_songs(playlist_id=playlist_id, tracks=songs)\r\n\r\nprint(f\"Completed! Enjoy your playlist https://open.spotify.com/playlist/{playlist_id}\")\r\n", "22": "from flask import Flask, jsonify, render_template, request\nimport json\nimport Webscrape\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef home():\n    return render_template(\"index.html\")\n\n@app.route(\"/webscrape\", methods=[\"POST\"])\ndef webscrape():\n    message = request.get_json(force=True)\n    Webscrape.webscrape(message['search'])\n    response = json.load(open('product_info.json', 'r'))\n    return jsonify(response)\n\n@app.errorhandler(404)\ndef not_found(e):\n    return render_template(\"404.html\")\n\nif __name__ == '__main__':\n    app.run(debug=True)\n", "23": "from django.contrib import admin\nfrom django.urls import path, include\nfrom core.views import CadastroViewSet\nfrom rest_framework import routers\nfrom core.webscrapping import webscrape\n#from .views import lista_itens\n#definicao de rotas para acesso pelo navegador\nrouter = routers.DefaultRouter()\nrouter.register('cadastros', CadastroViewSet)\n\nurlpatterns = [\n    path('', include('core.urls')),\n    #path('', include(router.urls)),\n    path('webscrape/', webscrape, name='webscrape'),\n    path('admin/', admin.site.urls),\n    path('api-auth/', include('rest_framework.urls')),\n]\n", "24": "# Scrapy settings for WebScrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'WebScrape'\n\nSPIDER_MODULES = ['WebScrape.spiders']\nNEWSPIDER_MODULE = 'WebScrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'WebScrape (+http://www.yourdomain.com)'\nUSER_AGENT = 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n#PROXY_POOL_ENABLED = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'WebScrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'WebScrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n\n\n\n\n#DOWNLOADER_MIDDLEWARES = {\n    # ...\n#    'scrapy_proxy_pool.middlewares.ProxyPoolMiddleware': 610,\n#    'scrapy_proxy_pool.middlewares.BanDetectionMiddleware': 620,\n#    # ...\n#}\n\n\n\n\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'WebScrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "25": "# Work with Python 3.6\nimport discord\nimport json\nimport WebScrape\nimport GenerateGraph\nfrom discord import File\n\nwith open('Secret.json') as json_file:  \n    data = json.load(json_file)\n    TOKEN = data['discordKey']\n\nclient = discord.Client()\n\n@client.event\nasync def on_message(message):\n    # we do not want the bot to reply to itself\n    if message.author == client.user:\n        return\n\n    if message.content.startswith('!hello'):\n        msg = 'Hello {0.author.mention}'.format(message)\n        # await message.channel.send(msg)\n        testFile = File('options.csv')\n        await message.channel.send(file = testFile)\n\n    elif message.content.startswith('!recordanewmixtape'):\n        msg = 'No {0.author.mention}, you do it'.format(message)\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!tickers'):\n        cmd = message.content\n        cmd = cmd[9:]\n        print('tickers command ticker: ' + cmd)\n        try:\n            tickers = WebScrape.printTickers(cmd)\n            msg = ''\n            for tick in tickers:\n                msg = msg + tick + '\\n'\n        except:\n            msg = 'Not a valid option\\nLook at !listOptions'\n            \n        await message.channel.send(msg)\n\n    elif message.content.startswith('!setOption'):\n        cmd = message.content\n        cmdParts = cmd.split()\n        WebScrape.setOption(cmdParts[1], cmdParts[2])\n        msg = '{0.author.mention} new option added '.format(message) + cmdParts[2]\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!removeOption'):\n        cmd = message.content\n        cmdParts = cmd.split()\n        WebScrape.removeOption(cmdParts[1])\n        msg = '{0.author.mention} option was removed '.format(message) + cmdParts[1]\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!listOptions'):\n        options = WebScrape.getOptions()\n        msg = ''\n        if options == 'There are no current options setup use !setOptions to set a new option':\n            msg = options\n        else:\n            for opt in options:\n                msg = msg + opt + '\\n'\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!stock'):\n        cmd = message.content\n        cmdParts = cmd.split()\n        GenerateGraph.makeGraph(cmdParts[1])\n        graphFile = File('stock_information.html')\n        await message.channel.send(file = graphFile)\n\n\n    elif message.content.startswith('!help'):\n        msg = 'Remember to seperate each part of the command with a space\\n'\n        msg = msg + '!setOption  \\n'\n        msg = msg + '!removeOption \\n'\n        msg = msg + '!listOptions\\n'\n        msg = msg + '!tickers \\n'\n        msg = msg + '!stock \\n'\n        await message.channel.send(msg)\n\n@client.event\nasync def on_ready():\n    print('Logged in as')\n    print(client.user.name)\n    print(client.user.id)\n    print('------')\n\nclient.run(TOKEN)", "26": "import csv\nimport dateutil.parser as date\nimport jinja2\nimport numpy as np\nimport os\nimport pathlib\nimport pandas as pd\nimport sqlite3 as sql\nfrom fuzzywuzzy import fuzz\nfrom pyne import nucname as nn\n\n\ndef get_cursor(file_name):\n    \"\"\" Connects and returns a cursor to an sqlite output file\n\n    Parameters\n    ----------\n    file_name: str\n        name of the sqlite file\n\n    Returns\n    -------\n    sqlite cursor3\n    \"\"\"\n    con = sql.connect(file_name)\n    con.row_factory = sql.Row\n    return con.cursor()\n\n\ndef import_pris(pris_link):\n    \"\"\" Opens pris_csv using Pandas. Adds Latitude and Longitude\n    columns\n\n    Parameters\n    ----------\n    pris_link: str\n        path to reactors_pris_2016.original.csv file\n\n    Returns\n    -------\n    pris: pd.Dataframe\n        pris database\n    \"\"\"\n    pris = pd.read_csv(pris_link,\n                       delimiter=',',\n                       encoding='iso-8859-1'\n                       )\n    pris.insert(13, 'Latitude', np.nan)\n    pris.insert(14, 'Longitude', np.nan)\n    pris = pris.replace(np.nan, '')\n    return pris\n\n\ndef import_webscrape_data(scrape_link):\n    \"\"\" Returns sqlite content of webscrape by performing an\n    sqlite query\n\n    Parameters\n    ----------\n    scrape_link: str\n        path to webscrape.sqlite file\n\n    Returns\n    -------\n    coords: sqlite cursor\n        sqlite cursor containing webscrape data\n    \"\"\"\n    cur = get_cursor(scrape_link)\n    coords = cur.execute(\"SELECT name, long, lat FROM reactors_coordinates\")\n    return coords\n\n\ndef get_edge_cases():\n    \"\"\" Returns a dictionary of edge cases that fuzzywuzzy is\n    unable to catch. This could be because PRIS database stores\n    reactor names and Webscrape database fetches power plant names,\n    or because PRIS reactor names are abbreviated.\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n    others: dict\n        dictionary of edge cases with \"key=pris_reactor_name, and\n        value=webscrape_plant_name\"\n    \"\"\"\n    others = {'OHI-': '\u014ci',\n              'ASCO-': 'Asc\u00f3',\n              'ROVNO-': 'Rivne',\n              'SHIN-KORI-': 'Kori',\n              'ANO-': 'Arkansas One',\n              'HANBIT-': 'Yeonggwang',\n              'FERMI-': 'Enrico Fermi',\n              'BALTIC-': 'Kaliningrad',\n              'COOK-': 'Donald C. Cook',\n              'HATCH-': 'Edwin I. Hatch',\n              'HARRIS-': 'Shearon Harris',\n              'SHIN-WOLSONG-': 'Wolseong',\n              'ST. ALBAN-': 'Saint-Alban',\n              'LASALLE-': 'LaSalle County',\n              'ZAPOROZHYE-': 'Zaporizhzhya',\n              'ROBINSON-': 'H. B. Robinson',\n              'SUMMER-': 'Virgil C. Summer',\n              'FARLEY-': 'Joseph M. Farley',\n              'ST. LAURENT ': 'Saint-Laurent',\n              'HADDAM NECK': 'Connecticut1 Yankee',\n              'FITZPATRICK': 'James A. FitzPatrick',\n              'HIGASHI DORI-1 (TOHOKU)': 'Higashid\u014dri',\n              }\n    return others\n\n\ndef sanitize_webscrape_name(name):\n    \"\"\" Sanitizes webscrape powerplant names by removing unwanted\n    strings (listed in blacklist), applying lower case, and deleting\n    trailing whitespace.\n\n    Parameters\n    ----------\n    name: str\n        webscrape plant name\n\n    Returns\n    -------\n    name: str\n        sanitized name for use with fuzzywuzzy\n    \"\"\"\n    blacklist = ['nuclear', 'power',\n                 'plant', 'generating',\n                 'station', 'reactor', 'atomic',\n                 'energy', 'center', 'electric']\n    name = name.lower()\n    for blacklisted in blacklist:\n        name = name.replace(blacklisted, '')\n    name = name.strip()\n    name = ' '.join(name.split())\n    return name\n\n\ndef sanitize_pris_name(name):\n    pris_name = name.lower()\n    if pris_name.find('-') != -1 and is_int(pris_name[-1]):\n        if pris_name[pris_name.find('-') + 1:].find('-') != -1:\n            idx = pris_name.find('-')\n            idx += pris_name[pris_name.find('-') + 1:].find('-')\n            pris_name = pris_name[:idx]\n        else:\n            pris_name = pris_name[:pris_name.find('-')]\n    return pris_name\n\n\ndef is_int(str):\n    \"\"\" Checks if input string is a number rather than a letter\n\n    Parameters\n    ----------\n    str: str\n        string to test\n\n    Returns\n    -------\n    answer: bool\n        returns True if string is a number; False if string is not\n    \"\"\"\n    answer = False\n    try:\n        int(str)\n    except ValueError:\n        return answer\n    answer = True\n    return answer\n\n\ndef merge_coordinates(pris_link, scrape_link):\n    \"\"\" Obtains coordinates from webscrape.sqlite and\n    writes them to matching reactors in PRIS reactor file.\n\n    Parameters\n    ----------\n    pris_link: str\n        path and name of pris reactor text file\n    scrape: str\n        path and name of webscrape sqlite file\n\n    Returns\n    -------\n    null\n        Writes pris text file with coordinates\n    \"\"\"\n    others = get_edge_cases()\n    pris = import_pris(pris_link)\n    coords = import_webscrape_data(scrape_link)\n    for web in coords:\n        for i, prs in pris.iterrows():\n            webscrape_name = sanitize_webscrape_name(web['name'])\n            pris_name = sanitize_pris_name(prs[1])\n            if fuzz.ratio(webscrape_name, pris_name) > 78:\n                prs[13] = web['lat']\n                prs[14] = web['long']\n            else:\n                for other in others.keys():\n                    edge_case_key = other.lower()\n                    edge_case_value = others[other].lower()\n                    if fuzz.ratio(pris_name, edge_case_key) > 80:\n                        if fuzz.ratio(webscrape_name, edge_case_value) > 75:\n                            prs[13] = web['lat']\n                            prs[14] = web['long']\n    pris.to_csv('reactors_pris_2016.csv', index=False, sep=',')\n\n\ndef import_csv(in_csv, delimit):\n    \"\"\" Imports contents of a csv text file to a list of\n    lists.\n\n    Parameters\n    ---------\n    in_csv: str\n        path and name of input csv file\n    delimit: str\n        delimiter of the csv file\n\n    Returns\n    -------\n    data_list: list\n        list of lists containing the csv data\n    \"\"\"\n    with open(in_csv, encoding='utf-8') as source:\n        sourcereader = csv.reader(source, delimiter=delimit)\n        data_list = []\n        for row in sourcereader:\n            data_list.append(row)\n    return data_list\n\n\ndef load_template(in_template):\n    \"\"\" Returns a jinja2 template from file.\n\n    Parameters\n    ---------\n    in_template: str\n        path and name of jinja2 template\n\n    Returns\n    -------\n    output_template: jinja template object\n    \"\"\"\n    with open(in_template, 'r') as default:\n        output_template = jinja2.Template(default.read())\n    return output_template\n\n\ndef get_composition_fresh(in_list, burnup):\n    \"\"\" Returns a dictionary of isotope and composition (in mass fraction)\n    using vision_recipes for fresh UOX fuel.\n\n    Parameters\n    ---------\n    in_list: list\n        list containing vision_recipes\n    burnup: int\n        burnup\n\n    Returns\n    -------\n    data_dict: dict\n        dictionary with key=[isotope],\n        and value=[composition]\n    \"\"\"\n    data_dict = {}\n    for i in range(len(in_list)):\n        if i > 1:\n            if burnup == 33:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][1])})\n            elif burnup == 51:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][3])})\n            else:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][5])})\n    return data_dict\n\n\ndef get_composition_spent(in_list, burnup):\n    \"\"\" Returns a dictionary of isotope and composition (in mass fraction)\n    using vision_recipes for spent nuclear fuel\n\n    Parameters\n    ---------\n    in_list: list\n        list containing vision_recipes data\n    burnup: int\n        burnup\n\n    Returns\n    -------\n    data_dict: dict\n        dictionary with key=[isotope],\n        and value=[composition]\n    \"\"\"\n    data_dict = {}\n    for i in range(len(in_list)):\n        if i > 1:\n            if burnup == 33:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][2])})\n            elif burnup == 51:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][4])})\n            else:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][6])})\n    return data_dict\n\n\ndef write_recipes(fresh_dict, spent_dict, in_template, burnup, region):\n    \"\"\" Renders jinja template using fresh and spent fuel composition.\n\n    Parameters\n    ---------\n    fresh_dict: dict\n        dictionary with key=[isotope], and\n        value=[composition] for fresh UOX\n    spent_dict: dict\n        dictionary with key=[isotope], and\n        value=[composition] for spent fuel\n    in_template: jinja template object\n        jinja template object to be rendered\n    burnup: int\n        amount of burnup\n\n    Returns\n    -------\n    null\n        generates recipe files for cyclus.\n    \"\"\"\n    out_path = 'cyclus/input/' + region + '/recipes/'\n    pathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n    rendered = in_template.render(fresh=fresh_dict,\n                                  spent=spent_dict)\n    with open(out_path + '/uox_' + str(burnup) + '.xml', 'w') as output:\n        output.write(rendered)\n\n\ndef produce_recipes(in_csv, recipe_template, burnup):\n    \"\"\" Generates commodity composition xml input for cyclus.\n\n    Parameters\n    ---------\n    in_csv: str\n        path and name of recipe file\n    recipe_template: str\n        path and name of recipe template\n    burnup: int\n        amount of burnup\n\n    Returns\n    -------\n    null\n        Generates commodity composition xml input for cyclus.\n    \"\"\"\n    recipe = import_csv(in_csv, ',')\n    write_recipes(get_composition_fresh(recipe, burnup),\n                  get_composition_spent(recipe, burnup),\n                  load_template(recipe_template), burnup)\n\n\ndef confirm_deployment(date_str, capacity):\n    \"\"\" Confirms if reactor is to be deployed for CYCLUS by\n    checking if the capacity > 400 and if the commercial date\n    is a proper date format.\n\n    Parameters\n    ----------\n    date_str: str\n            the commercial date string from PRIS data file\n    capacity: str\n            capacity in MWe from RPIS data file\n\n    Returns\n    -------\n    is_deployed: bool\n            determines whether the reactor will be deployed\n            in CYCLUS\n    \"\"\"\n    is_deployed = False\n    if len(date_str) > 4 and float(capacity) > 400:\n        try:\n            date.parse(date_str)\n            is_deployed = True\n        except:\n            pass\n    return is_deployed\n\n\ndef select_region(in_list, region):\n    \"\"\" Returns a list of reactors that will be deployed for\n    CYCLUS by checking the capacity and commercial date\n\n    Parameters\n    ----------\n    in_list: list\n            imported csv file in list format\n    region: str\n            name of the region\n\n    Returns\n    -------\n    reactor_list: list\n            list of reactors from PRIS\n    \"\"\"\n    ASIA = {'IRAN', 'JAPAN', 'KAZAKHSTAN',\n            'BANGLADESH', 'CHINA', 'INDIA',\n            'UNITED ARAB EMIRATES', 'VIETNAM',\n            'PAKISTAN', 'PHILIPPINES', 'SOUTH KOREA'\n            }\n    UNITED_STATES = {'UNITED STATES'}\n    SOUTH_AMERICA = {'ARGENTINA', 'BRAZIL'}\n    NORTH_AMERICA = {'CANADA', 'MEXICO', 'UNITED STATES'}\n    EUROPE = {'UKRAINE', 'UNITED KINGDOM',\n              'POLAND', 'ROMANIA', 'RUSSIA',\n              'BELARUS', 'BELGIUM', 'BULGARIA',\n              'GERMANY', 'ITALY', 'NETHERLANDS',\n              'SWEDEN', 'SWITZERLAND', 'TURKEY',\n              'SLOVENIA', 'SOVIET UNION', 'SPAIN',\n              'CZECHOSLOVAKIA', 'FINLAND', 'FRANCE'\n              }\n    AFRICA = {'EGYPT', 'MOROCCO', 'SOUTH AFRICA', 'TUNISIA'}\n    ALL = (SOUTH_AMERICA | NORTH_AMERICA |\n           EUROPE | ASIA | AFRICA | UNITED_STATES)\n    regions = {'ASIA': ASIA,\n               'AFRICA': AFRICA,\n               'EUROPE': EUROPE,\n               'SOUTH_AMERICA': SOUTH_AMERICA,\n               'NORTH_AMERICA': NORTH_AMERICA,\n               'UNITED_STATES': UNITED_STATES,\n               'ALL': ALL}\n    if region.upper() not in regions.keys():\n        raise ValueError(region + 'is not a valid region')\n    reactor_list = []\n    for row in in_list:\n        country = row[0]\n        if country.upper() in regions[region.upper()]:\n            capacity = row[3]\n            start_date = row[10]\n            if confirm_deployment(start_date, capacity):\n                reactor_list.append(row)\n    return reactor_list\n\n\ndef get_lifetime(in_row):\n    \"\"\" Calculates the lifetime of a reactor using first\n    commercial date and shutdown date. Defaults to 720 months\n    if shutdown date is not available.\n\n    Parameters\n    ----------\n    in_row: list\n        single row from PRIS data that contains reactor\n        information\n\n    Returns\n    -------\n    lifetime: int\n        lifetime of reactor\n    \"\"\"\n    comm_date = in_row[10]\n    shutdown_date = in_row[11]\n    if not shutdown_date.strip():\n        return 720\n    else:\n        n_days_month = 365.0 / 12\n        delta = (date.parse(shutdown_date) - date.parse(comm_date)).days\n        return int(delta / n_days_month)\n\n\ndef write_reactors(in_list, out_path, reactor_template):\n    \"\"\" Renders CYCAMORE::reactor specifications using jinja2.\n\n    Parameters\n    ----------\n    in_list: list\n        list containing PRIS data\n    out_path: str\n        output path for reactor files\n    reactor_template: str\n        path to reactor template\n\n    Returns\n    -------\n    null\n        writes xml files with CYCAMORE::reactor config\n    \"\"\"\n    if out_path[-1] != '/':\n        out_path += '/'\n    pathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n    reactor_template = load_template(reactor_template)\n    for row in in_list:\n        capacity = float(row[3])\n        if capacity >= 400:\n            name = row[1].replace(' ', '_')\n            assem_per_batch = 0\n            assem_no = 0\n            assem_size = 0\n            reactor_type = row[2]\n            latitude = row[13] if row[13] != '' else 0\n            longitude = row[14] if row[14] != '' else 0\n            if reactor_type in ['BWR', 'ESBWR']:\n                assem_no = 732\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 138000 / assem_no\n            elif reactor_type in ['GCR', 'HWGCR']:  # Need batch number\n                assem_no = 324\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 114000 / assem_no\n            elif reactor_type == 'HTGR':  # Need batch number\n                assem_no = 3944\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 39000 / assem_no\n            elif reactor_type == 'PHWR':\n                assem_no = 390\n                assem_per_batch = int(assem_no / 45)\n                assem_size = 80000 / assem_no\n            elif reactor_type == 'VVER':  # Need batch number\n                assem_no = 312\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 41500 / assem_no\n            elif reactor_type == 'VVER-1200':  # Need batch number\n                assem_no = 163\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 80000 / assem_no\n            else:\n                assem_no = 241\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 103000 / assem_no\n            config = reactor_template.render(name=name,\n                                             lifetime=get_lifetime(row),\n                                             assem_size=assem_size,\n                                             n_assem_core=assem_no,\n                                             n_assem_batch=assem_per_batch,\n                                             power_cap=row[3],\n                                             lon=longitude,\n                                             lat=latitude)\n            with open(out_path + name.replace(' ', '_') + '.xml',\n                      'w') as output:\n                output.write(config)\n\n\ndef obtain_reactors(in_csv, region, reactor_template):\n    \"\"\" Writes xml files for individual reactors in a given\n    region.\n\n    Parameters\n    ----------\n    in_csv: str\n        csv file name\n    region: str\n        region name\n    reactor_template: str\n        path to CYCAMORE::reactor config template file\n\n    Returns\n    -------\n    null\n        Writes xml files for individual reactors in region.\n    \"\"\"\n    in_data = import_csv(in_csv, ',')\n    reactor_list = select_region(in_data, region)\n    out_path = 'cyclus/input/' + region + '/reactors'\n    write_reactors(reactor_list, out_path, reactor_template)\n\n\ndef write_deployment(in_dict, out_path, deployinst_template,\n                     inclusions_template):\n    \"\"\" Renders jinja template using dictionary of reactor name and buildtime.\n    Outputs an xml file that uses xinclude to include the reactor xml files\n    located in cyclus_input/reactors.\n\n    Parameters\n    ---------\n    in_dict: dictionary\n        dictionary with key=[reactor name], and value=[buildtime]\n    out_path: str\n        output path for files\n    deployinst_template: str\n        path to deployinst template\n    inclusions_template: str\n        path to inclusions template\n\n    Returns\n    -------\n    null\n        generates input files that have deployment and xml inclusions\n    \"\"\"\n    if out_path[-1] != '/':\n        out_path += '/'\n    pathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n    deployinst_template = load_template(deployinst_template)\n    inclusions_template = load_template(inclusions_template)\n    country_list = {value[0] for value in in_dict.values()}\n    for nation in country_list:\n        temp_dict = {}\n        for reactor in in_dict.keys():\n            if in_dict[reactor][0].upper() == nation.upper():\n                temp_dict.update({reactor: in_dict[reactor][1]})\n        pathlib.Path(out_path + nation.replace(' ', '_') +\n                     '/').mkdir(parents=True, exist_ok=True)\n        deployinst = deployinst_template.render(reactors=temp_dict)\n        with open(out_path + nation.replace(' ', '_') +\n                  '/deployinst.xml', 'w') as output1:\n            output1.write(deployinst)\n    inclusions = inclusions_template.render(reactors=in_dict)\n    with open(out_path + 'inclusions.xml', 'w') as output2:\n        output2.write(inclusions)\n\n\ndef get_buildtime(in_list, start_year, path_list):\n    \"\"\" Calculates the buildtime required for reactor\n    deployment in months.\n\n    Parameters\n    ----------\n    in_list: list\n        list of reactors\n    start_year: int\n        starting year of simulation\n    path_list: list\n        list of paths to reactor files\n\n    Returns\n    -------\n    buildtime_dict: dict\n        dictionary with key=[name of reactor], and\n        value=[set of country and buildtime]\n    \"\"\"\n    buildtime_dict = {}\n    for row in in_list:\n        comm_date = date.parse(row[10])\n        start_date = [comm_date.year, comm_date.month, comm_date.day]\n        delta = ((start_date[0] - int(start_year)) * 12 +\n                 (start_date[1]) +\n                 round(start_date[2] / (365.0 / 12)))\n        for index, reactor in enumerate(path_list):\n            name = row[1].replace(' ', '_')\n            country = row[0]\n            file_name = (reactor.replace(\n                os.path.dirname(path_list[index]), '')).replace('/', '')\n            if (name + '.xml' == file_name):\n                buildtime_dict.update({name: (country, delta)})\n    return buildtime_dict\n\n\ndef deploy_reactors(in_csv, region, start_year, deployinst_template,\n                    inclusions_template, reactors_path, deployment_path):\n    \"\"\" Generates xml files that specify the reactors that will be included\n    in a CYCLUS simulation.\n\n    Parameters\n    ---------\n    in_csv: str\n        path to pris reactor database\n    region: str\n        region name\n    start_year: int\n        starting year of simulation\n    deployinst_template: str\n        path to deployinst template\n    inclusions_template: str\n        path to inclusions template\n    reactors_path: str\n        path containing reactor files\n    deployment_path: str\n        output path for deployinst xml\n\n    Returns\n    -------\n    buildtime_dict: dict\n        dictionary with key=[name of reactor], and\n        value=[set of country and buildtime]\n    \"\"\"\n    lists = []\n    if reactors_path[-1] != '/':\n        reactors_path += '/'\n    for files in os.listdir(reactors_path):\n        lists.append(reactors_path + files)\n    in_data = import_csv(in_csv, ',')\n    reactor_list = select_region(in_data, region)\n    buildtime = get_buildtime(reactor_list, start_year, lists)\n    write_deployment(buildtime, deployment_path, deployinst_template,\n                     inclusions_template)\n    return buildtime\n\n\ndef render_cyclus(cyclus_template, region, in_dict, out_path):\n    \"\"\" Renders final CYCLUS input file with xml base, and institutions\n    for each country\n\n    Parameters\n    ----------\n    cyclus_template: str\n        path to CYCLUS input file template\n    region: str\n        region chosen for CYCLUS simulation\n    in_dict: dictionary\n        in_dict should be buildtime_dict from get_buildtime function\n    out_path: str\n        output path for CYCLUS input file\n    output_name:\n\n    Returns\n    -------\n    null\n        writes CYCLUS input file in out_path\n    \"\"\"\n    if out_path[-1] != '/':\n        out_path += '/'\n    cyclus_template = load_template(cyclus_template)\n    country_list = {value[0].replace(' ', '_') for value in in_dict.values()}\n    rendered = cyclus_template.render(countries=country_list,\n                                      base_dir=os.path.abspath(out_path) + '/')\n    with open(out_path + region + '.xml', 'w') as output:\n        output.write(rendered)\n", "27": "# Work with Python 3.6\nimport discord\nimport json\nimport WebScrape\nimport GenerateGraph\nfrom discord import File\n\nwith open('Secret.json') as json_file:  \n    data = json.load(json_file)\n    TOKEN = data['discordKey']\n\nclient = discord.Client()\n\n@client.event\nasync def on_message(message):\n    # we do not want the bot to reply to itself\n    if message.author == client.user:\n        return\n\n    if message.content.startswith('!hello'):\n        msg = 'Hello {0.author.mention}'.format(message)\n        # await message.channel.send(msg)\n        testFile = File('options.csv')\n        await message.channel.send(file = testFile)\n\n    elif message.content.startswith('!recordanewmixtape'):\n        msg = 'No {0.author.mention}, you do it'.format(message)\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!tickers'):\n        cmd = message.content\n        cmd = cmd[9:]\n        print('tickers command ticker: ' + cmd)\n        try:\n            tickers = WebScrape.printTickers(cmd)\n            msg = ''\n            for tick in tickers:\n                msg = msg + tick + '\\n'\n        except:\n            msg = 'Not a valid option\\nLook at !listOptions'\n            \n        await message.channel.send(msg)\n\n    elif message.content.startswith('!setOption'):\n        cmd = message.content\n        cmdParts = cmd.split()\n        WebScrape.setOption(cmdParts[1], cmdParts[2])\n        msg = '{0.author.mention} new option added '.format(message) + cmdParts[2]\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!removeOption'):\n        cmd = message.content\n        cmdParts = cmd.split()\n        WebScrape.removeOption(cmdParts[1])\n        msg = '{0.author.mention} option was removed '.format(message) + cmdParts[1]\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!listOptions'):\n        options = WebScrape.getOptions()\n        msg = ''\n        if options == 'There are no current options setup use !setOptions to set a new option':\n            msg = options\n        else:\n            for opt in options:\n                msg = msg + opt + '\\n'\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!stock'):\n        cmd = message.content\n        cmdParts = cmd.split()\n        GenerateGraph.makeGraph(cmdParts[1])\n        graphFile = File('stock_information.html')\n        await message.channel.send(file = graphFile)\n\n\n    elif message.content.startswith('!help'):\n        msg = 'Remember to seperate each part of the command with a space\\n'\n        msg = msg + '!setOption  \\n'\n        msg = msg + '!removeOption \\n'\n        msg = msg + '!listOptions\\n'\n        msg = msg + '!tickers \\n'\n        msg = msg + '!stock \\n'\n        await message.channel.send(msg)\n\n@client.event\nasync def on_ready():\n    print('Logged in as')\n    print(client.user.name)\n    print(client.user.id)\n    print('------')\n\nclient.run(os.environ[\"TOKEN\"])\n", "28": "from wikipedia import webscrapeWikipedia, getResponse, getDF\nfrom schedules_query_processor import Processor\nfrom classification import Preprocessor\nimport json\nimport glob\nimport pandas as pd\nimport re\n\ndef initMain():\n    \"\"\"\n    Initializes all data needed for the main function including\n    preprocessing, schedules, and wikipedia related objects\n    \"\"\"\n    if True: # TODO: have a condition so it only webscrapes when needed\n        completeWebscrape()\n\n    # Get DF and related info for wikipedia\n    wikiRet = getDF()\n\n    # Get DF and related info for schedules\n    schDf = {}\n    df_files = glob.glob(\"data/*.csv\")\n    for file in df_files:\n        df_name = re.split(\"\\\\\\\\|/|\\.\", file)[1]\n        schDf[df_name] = pd.read_csv(file, encoding=\"ISO-8859-1\")\n    with open(\"data/keywords.json\") as json_file:\n        schDict = json.load(json_file)\n\n    # initialize objects and variables\n    p = Preprocessor(schDict)\n    schedulesProcessor = Processor(schDf)\n\n    return p, wikiRet, schedulesProcessor\n\ndef completeWebscrape():\n    webscrapeWikipedia()\n    # TODO add webscrape schedules in this function\n\ndef handleQuery(query, p, schP, wikiRet):\n    qp = p.preprocessAndClassifyQuery(query)\n    resp = \"\"\n    if qp[\"classification\"] == \"schedules\":\n        resp = schP.getResponse(qp)\n        if resp == \"\":\n            # If schedules can't find it, try wikipedia\n            resp = getResponse(wikiRet[0], wikiRet[1], wikiRet[2], qp)\n    elif qp[\"classification\"] == \"wikipedia\":\n        resp = getResponse(wikiRet[0], wikiRet[1], wikiRet[2], qp)\n    if resp == \"\":\n        resp = \"Sorry I don't know how to respond to that.\"\n    return resp\n", "29": "# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n# USER_AGENT = 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,\n}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'webscrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "30": "from flask import Flask, jsonify\nfrom flask_restful import Resource, Api\n\nimport webscrape\n\napp = Flask(__name__)\napi = Api(app)\n\nclass WebScrape(Resource):\n    def get(self):\n        return jsonify(webscrape.get_special_menu_json())\n\napi.add_resource(WebScrape, '/specials')\n\nif __name__ == '__main__':\n    app.run(threaded=True, port=5000)", "31": "# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'webscrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "32": "from flask import (\n    Flask, Blueprint, flash, g, redirect, render_template, request, url_for\n)\nfrom werkzeug.exceptions import abort\nfrom werkzeug.utils import secure_filename\nimport pandas as pd\nimport numpy as np\nimport webscrape_script as wbscr\nimport image_script as imgscr\nimport nlanguage_script as langscr\nimport requests\n#import urlib2\n\napp = Flask(__name__, instance_relative_config=True, static_folder='static', static_url_path='/static')\n@app.route('/', methods=['GET', 'POST'])\ndef main():\n    if request.method == 'POST':\n        newsub = request.form['subreddit']\n        #print(newsub)\n        data = runWebscrape(newsub)\n        title = get_title(data, 0)\n        url = get_link(data, 0)\n        sentiment = np.round(get_sentiment(data),5)\n        sub_link = \"https://www.reddit.com/r/\" + newsub\n        #exists = check_url(sub_link)\n        #if not exists:\n        #    sub_link =  \"https://www.reddit.com/r/worldnews/\"\n        exists = True\n        return render_template(\"index.html\", headline = title, link = url, sentiment = sentiment, subreddit = newsub, sub_link= sub_link)\n    else:\n        #a = get database info\n        data = runWebscrape(\"WorldNews\")\n        #print(data.to_string())\n        title = get_title(data, 0)\n        url = get_link(data, 0)\n        sentiment = np.round(get_sentiment(data), 5)\n        #hlOneImg = data.iloc[0]['title']\n        #hlTwoImg = data.iloc[1]['title']\n        #imgscr.main(hlOneImg);\n        #imgscr.main(hlTwoImg);\n        return render_template('index.html', headline = title, link = url, sentiment = sentiment, subreddit=\"WorldNews\", sub_link = \"https://www.reddit.com/r/worldnews/\", exists=True)\n\n\n@app.route(\"/method\")\ndef method():\n    return render_template('method.html')\n\ndef runWebscrape( subreddit ):\n    dict = wbscr.main(subreddit)\n    return dict\n\ndef runLangScript( phrase ):\n    return langscr.main( phrase )\n\ndef get_sentiment( data ):\n    lst = [runLangScript(get_title(data, index)) for index in range(8)]\n    return np.average(lst)\n\ndef get_title(data, index):\n    new_data = data['title']\n    title = str(new_data[index])\n    return title\n\ndef get_link(data, index):\n    new_data = data['url']\n    url = str(new_data[index])\n    return url\n\ndef check_url(link):\n    request = requests.get(link)\n    if request.status_code == 200:\n        return True\n    else:\n        return False\n\n\n\nif __name__ == '__main__':\n    app.run(host='127.0.0.1', port=8080, debug=True)\n", "33": "from concurrent.futures import ThreadPoolExecutor\nfrom webbrowser import get\nimport re\nimport requests\nfrom .models import PortHoldings\nfrom .web_scrape import WebScrape\n\nheaders = {'User-agent': 'your bot 0.1'}\n\ndef get_airbus_value():\n    #Get beautiful soup objects\n    airbus = WebScrape('https://markets.businessinsider.com/stocks/airbus-stock?op=1', {\n        'Access-Control-Allow-Origin': '*',\n        'Access-Control-Allow-Methods': 'GET',\n        'Access-Control-Allow-Headers': 'Content-Type',\n        'Access-Control-Max-Age': '3600',\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n        })\n    airbusPrice = airbus.scraper.find('span', class_=\"price-section__current-value\")\n    airbusHoldings = float(airbusPrice.text)\n    #Convert to AUD\n    eur_audConverter = WebScrape('https://www.x-rates.com/calculator/?from=EUR&to=AUD&amount=1', headers)\n    euro = eur_audConverter.scraper.find('span', class_ ='ccOutputRslt').text\n    rates = re.findall(r'\\d+\\.\\d+', euro)\n    airbusAUDTotal = float(rates[0]) * float(airbusHoldings)\n    AirAmt = PortHoldings.objects.get(nameFund=\"Airbus\")\n    AirValue = float(AirAmt.numHoldings) * float(airbusAUDTotal)\n\n    return AirAmt, AirValue\n\ndef get_etherium_value():\n    etherium = WebScrape('https://www.independentreserve.com/market/eth', headers)\n    ethPrice = etherium.scraper.find('span', class_=\"currency-value__amount\")\n    EthAmt = PortHoldings.objects.get(nameFund=\"Etherium\")\n    EthValue = float(EthAmt.numHoldings) * float(ethPrice.text.replace(',','').strip('$'))\n\n    return EthAmt, EthValue\n\ndef get_bitcoin_value():\n    bitcoin = WebScrape('https://www.independentreserve.com/market/btc', headers)\n    bitcoinPrice = bitcoin.scraper.find('span', class_=\"currency-value__amount\")\n    BitAmt = PortHoldings.objects.get(nameFund=\"Bitcoin\")\n    BitValue = float(BitAmt.numHoldings) * float(bitcoinPrice.text.replace(',','').strip('$'))\n    \n    return BitAmt, BitValue\n\ndef get_VDHG_value():\n    vdhg = WebScrape('https://www.google.com/finance/quote/VDHG:ASX', headers)\n    #Scrape stock price value from page\n    vdhgPrice = vdhg.scraper.find('div', class_=\"YMlKec fxKbKc\")\n    #Calculate total price\n    VDHGAmt = PortHoldings.objects.get(nameFund=\"VDHG\", institution=\"CommSec\")\n    VDHGValue = float(VDHGAmt.numHoldings) * float(vdhgPrice.text.replace(',','').strip('$'))\n\n    return VDHGAmt, VDHGValue\n\ndef get_VESG_value():\n    vesg = WebScrape('https://www.google.com/finance/quote/VESG:ASX', headers)\n    vesgPrice = vesg.scraper.find('div', class_=\"YMlKec fxKbKc\")\n    VESGAmt = PortHoldings.objects.get(nameFund=\"VESG\")\n    VESGValue = float(VESGAmt.numHoldings) * float(vesgPrice.text.replace(',','').strip('$'))\n\n    return VESGAmt, VESGValue\n\ndef get_vanguard_value():\n    vanguard = WebScrape('https://www.morningstar.com.au/Fund/FundReportPrint/5402', headers)\n    vanguardPrice = vanguard.scraper.find_all('span', class_=\"YMWpadright\")\n    VanguardAmt = PortHoldings.objects.get(institution=\"Vanguard\")\n    VanguardValue = float(VanguardAmt.numHoldings) * float(vanguardPrice[4].text.replace(',','').strip('$'))\n\n    return VanguardAmt, VanguardValue\n\ndef get_hbar_value():\n    hbar = WebScrape('https://coinmarketcap.com/currencies/hedera/hbar/aud/', headers)\n    hbarPrice = hbar.scraper.find('div', class_=\"priceValue\")\n    HbarAmt = PortHoldings.objects.get(nameFund=\"Hbar\")\n    HbarValue = float(HbarAmt.numHoldings) * float(hbarPrice.text.replace(',','').strip('$'))\n\n    return HbarAmt, HbarValue\n    \ndef scrapeData():    \n\n    # Using ThreadPoolExecutor so all the webscraping functions can run at once, drastically increasing render time\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        f1 = executor.submit(get_airbus_value)\n        f2 = executor.submit(get_etherium_value)\n        f3 = executor.submit(get_VESG_value)\n        f4 = executor.submit(get_VDHG_value)\n        f5 = executor.submit(get_bitcoin_value)\n        f6 = executor.submit(get_vanguard_value)\n        f7 = executor.submit(get_hbar_value)\n\n    AirAmt, AirValue = f1.result()\n    EthAmt, EthValue = f2.result()\n    VESGAmt, VESGValue = f3.result()\n    VDHGAmt, VDHGValue = f4.result()\n    BitAmt, BitValue = f5.result()\n    VanguardAmt, VanguardValue = f6.result()\n    HbarAmt, HbarValue = f7.result()\n\n    holdingsDatabase = PortHoldings.objects.all().values()\n    output = \"\"\n    for x in holdingsDatabase:\n        output += x['nameFund']        \n\n    totalValue = (AirValue + EthValue + BitValue + VanguardValue + VESGValue + VDHGValue + HbarValue)\n\n    holdings = {'funds':[{'fundName': VESGAmt.nameFund, \"value\": VESGValue},\n                    {'fundName': VDHGAmt.nameFund, \"value\": VDHGValue},\n                    {'fundName': EthAmt.nameFund, \"value\": EthValue},\n                    {'fundName': BitAmt.nameFund, \"value\": BitValue},\n                    {'fundName': AirAmt.nameFund, \"value\": AirValue},\n                    {'fundName': VanguardAmt.institution, \"value\": VanguardValue},\n                    {'fundName': HbarAmt.nameFund, \"value\": HbarValue},]\n                } \n    return holdings, totalValue, holdingsDatabase\n\nif __name__ == '__main__':\n    response = requests.get('https://www.independentreserve.com/api2/market/best?primary=Eth&secondary=Aud', headers)\n    print(response.bestOffer)", "34": "# Scrapy settings for WebScrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'WebScrape'\n\nSPIDER_MODULES = ['WebScrape.spiders']\nNEWSPIDER_MODULE = 'WebScrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'WebScrape (+http://www.yourdomain.com)'\nUSER_AGENT = 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n#PROXY_POOL_ENABLED = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'WebScrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'WebScrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n\n\n\n\n#DOWNLOADER_MIDDLEWARES = {\n    # ...\n#    'scrapy_proxy_pool.middlewares.ProxyPoolMiddleware': 610,\n#    'scrapy_proxy_pool.middlewares.BanDetectionMiddleware': 620,\n#    # ...\n#}\n\n\n\n\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'WebScrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "35": "import requests\r\nfrom bs4 import BeautifulSoup\r\nimport pandas\r\nimport webscrape\r\nimport argparse\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--dbname\", help=\"Enter number of pages to parse \", type=int)\r\nargs = parser.parse_args()\r\n\r\nmb_url = \"https://www.magicbricks.com/property-for-sale/residential-real-estate?bedroom=&proptype=Multistorey\" \\\r\n         \"-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment&cityName=North-area-Bangalore\"\r\nreq = requests.get(mb_url)\r\ncontent = req.content\r\n\r\nsoup = BeautifulSoup(content, \"html.parser\")\r\n\r\nall_flat = soup. find_all(\"div\", {\"class\": \"mb-srp__card\"})\r\nscrapped_info_list = []\r\nwebscrape.connect(args.dbname)\r\n\r\n\r\nfor flat in all_flat:\r\n    flat_dic = {}\r\n    flat_dic[\"type\"] = flat.find(\"h2\", {\"class\": \"mb-srp__card--title\"}).text\r\n    try:\r\n        flat_dic[\"name\"] = flat.find(\"a\", {\"class\": \"mb-srp__card__society--name\"}).text\r\n    except AttributeError:\r\n        flat_dic[\"name\"] = None\r\n\r\n    scrapped_info_list.append(flat_dic)\r\n    webscrape.insert_into_table(args.dbname, tuple(flat_dic.values()))\r\n\r\ndataFrame = pandas.DataFrame(scrapped_info_list)\r\nprint(\"Creating CSV File.....\")\r\ndataFrame.to_csv(\"M_B.csv\")\r\nwebscrape.get_flat_info(args.dbname)\r\n", "36": "from . import scraping\nfrom flask import render_template, request\nfrom .scrapes import WebScrape\nfrom app.feeds import BlogFeeds\n\n@scraping.get('/geeksforgeeks')\ndef geeksforgeeks():\n  blog = BlogFeeds()\n\n  link = request.args.get('links')\n\n  scrape = WebScrape()\n  articles = scrape.geeks(link)\n\n  programming_articles = blog.programming_section()\n  opensource_articles = blog.opensource_section()\n\n  return render_template('article/geeks.html', data=articles, title=\"Geeksforgeeks\", programming=programming_articles, opensource=opensource_articles)\n\n@scraping.get('/github_blog')\ndef github():\n  blog = BlogFeeds()\n  link = request.args.get('links')\n\n  scrape = WebScrape()\n  articles = scrape.github_blog(link)\n\n  programming_articles = blog.programming_section()\n  opensource_articles = blog.opensource_section()\n\n  return render_template('article/github.html', data=articles, title=\"Github\",programming=programming_articles, opensource=opensource_articles)\n\n@scraping.get('/codingdojo')\ndef codingdojo():\n  blog = BlogFeeds()\n  link = request.args.get('links')\n\n  scrape = WebScrape()\n  articles = scrape.codingdojo(link)\n\n  programming_articles = blog.programming_section()\n  opensource_articles = blog.opensource_section()\n\n  return render_template('article/codingdojo.html', data=articles, title=\"CodingDojo\", programming=programming_articles, opensource=opensource_articles)\n\n@scraping.get('/fosslinux')\ndef fosslinux():\n  blog = BlogFeeds()\n  link = request.args.get('links')\n\n  scrape = WebScrape()\n  articles = scrape.fosslinux(link)\n\n  programming_articles = blog.programming_section()\n  opensource_articles = blog.opensource_section()\n\n  return render_template('article/fosslinux.html', data=articles, title=\"FossLinux\",programming=programming_articles, opensource=opensource_articles)\n\n@scraping.get('/linuxhint')\ndef linuxhint():\n  blog = BlogFeeds()\n  link = request.args.get('links')\n\n  scrape = WebScrape()\n  articles = scrape.linuxhint(link)\n\n  programming_articles = blog.programming_section()\n  opensource_articles = blog.opensource_section()\n\n  return render_template('article/linuxhint.html', data=articles, title=\"Linux Hint\", programming=programming_articles, opensource=opensource_articles)\n\n@scraping.get('/itsfoss')\ndef itsfoss():\n  blog = BlogFeeds()\n  link = request.args.get('links')\n\n  scrape = WebScrape()\n  articles = scrape.itsfoss(link)\n\n  programming_articles = blog.programming_section()\n  opensource_articles = blog.opensource_section()\n\n\n  return render_template('article/itsfoss.html', data=articles, title=\"It\\'s Foss\", programming=programming_articles, opensource=opensource_articles)\n\n\n# @scraping.get('/ostechnix')\n# def ostechnix():\n#   link = request.args.get('links')\n\n#   scrape = WebScrape()\n#   articles = scrape.ostechnix(link)\n\n#   return render_template('article/ostechnix.html', data=articles, title=\"OSTechNix\")\n", "37": "import requests\nfrom bs4 import BeautifulSoup\nimport pprint\nimport sys\n\n\ndef response_and_soup(url_link):\n    resp = requests.get(url_link)\n    soup = BeautifulSoup(resp.text, 'html.parser')\n    return soup\n\n\ndef get_link(soup):\n    return soup.select('.storylink')\n\n\ndef get_subtext(soup):\n    return soup.select('.subtext')\n\n\ndef sort_stories_by_votes(hnlist):\n    return sorted(hnlist, key=lambda k: k['votes'], reverse=True)\n\n\ndef create_custom_fn(links, subtext):\n    hn = []\n    for idx, item in enumerate(links):\n        title = links[idx].getText()\n        href = links[idx].get('href', None)\n        vote = subtext[idx].select('.score')\n        if len(vote):\n            points = int(vote[0].getText().replace(' points', ''))\n            if points > 99:\n                hn.append({'title': title, 'votes': points, 'link': href})\n    return sort_stories_by_votes(hn)\n\n\ndef get_mega_links_and_subtext(urls):\n    mega_link = []\n    mega_subtext = []\n    for url in urls:\n        mega_link.extend(get_link(response_and_soup(url)))\n        mega_subtext.extend(get_subtext(response_and_soup(url)))\n    pprint.pprint(create_custom_fn(mega_link, mega_subtext))\n    print(\"DONE !!!\")\n\n\ndef input_urls():\n    urls_to_webscrape = []\n    url = 'https://' + input('Please enter the url to scrape from: ')\n    urls_to_webscrape.append(url)\n    while True:\n        more = input('Do you have more links? y/n: ').lower()\n        if more == 'n':\n            break\n        elif more == 'y':\n            url = 'https://' + input('Please enter the url to scrape from: ')\n            urls_to_webscrape.append(url)\n        else:\n            print('Invalid input! Try again')\n\n    return get_mega_links_and_subtext(urls_to_webscrape)\n\n\nif __name__ == '__main__':\n    sys.exit(input_urls())\n", "38": "from django.http import Http404\nfrom django.shortcuts import render, redirect\nfrom .forms import WebscrapeForm, UserLoginForm\nfrom .models import WebScrape\n# Create your views here.\n\n\ndef home(request):\n    form = UserLoginForm(data=request.POST or None)\n    if request.method == 'POST':\n        if form.is_valid():\n            form.save()\n            return redirect('Account')\n\n    context = {\n        'form': form\n    }\n    return render(request, 'Resellers_MarketWatch/MarketWatch_home.html', context)\n\n\ndef account(request):\n    form = WebscrapeForm(data=request.POST or None)\n    if request.method == 'POST':\n        print('Method is POST')\n        if form.is_valid():\n            print('Form is valid')\n            form.save()\n            return redirect('Listview')\n\n    context = {\n        'form': form\n    }\n    return render(request, 'Resellers_MarketWatch/AccountPage.html', context)\n\n\ndef all_webscrape(request):\n    dataset = WebScrape.WebScrape_db.all()\n    context = {\n        'dataset': dataset\n    }\n    return render(request, 'Resellers_MarketWatch/Listview.html', context)\n\n\ndef detailsview(request, pk):\n    try:\n        data = WebScrape.WebScrape_db.get(id=pk)\n    except WebScrape.DoesNotExist:\n        raise Http404('Data does not exist')\n\n    return render(request, 'Resellers_MarketWatch/DetailView.html', {'data': data})", "39": "# Scrapy settings for Webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'Webscrape'\n\nSPIDER_MODULES = ['Webscrape.spiders']\nNEWSPIDER_MODULE = 'Webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'Webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'Webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'Webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n    'Webscrape.pipelines.WebscrapePipeline': 300,\n}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "40": "import discord\r\nimport os\r\nimport WebScrape\r\n\r\nfrom dotenv import load_dotenv\r\nload_dotenv()\r\nTOKEN = os.getenv(\"DISCORD_BOT_TOKEN\")\r\n\r\nclient = discord.Client()\r\n\r\ncommand_list = {\r\n    \"hello\" : \"Miko says hello back\",\r\n    \"livemiko\" : \"Checks live status of Miko's stream\",\r\n    \"help\" : \"displays help\",\r\n    \"malfav\" : \"Syntax: $malfav (username) (category name). Shows the favorites of a user based on their MyAnimeList\",\r\n    \"malscore\" : \"Syntax: $malscore (username) (anime name). Shows score user gave on anime\"\r\n}\r\n\r\n@client.event\r\n\r\nasync def on_message(message):\r\n\r\n    if message.author == client.user:\r\n        return\r\n    \r\n    #message.content = message.content.lower()\r\n\r\n    if message.content.lower() == (f'$hello'):\r\n        await message.channel.send(\"Hello nye\")\r\n\r\n    if message.content.lower() == (f'$livemiko'):\r\n        if WebScrape.is_liveYT(\"https://www.youtube.com/channel/UC-hM6YJuNYVAmUWxeIr9FeA/live\") == True:\r\n            await message.channel.send(\"Nye :cherry_blossom: \\n https://www.youtube.com/channel/UC-hM6YJuNYVAmUWxeIr9FeA/live\")\r\n        else:\r\n            await message.channel.send(\"No nye :cry: \")\r\n\r\n    if message.content.lower() == (f\"$help\"):\r\n        for command in command_list:\r\n            await message.channel.send(\"$\" + command + \": \" + command_list[command])\r\n    \r\n    if (f\"apex legends\") in message.content.lower():\r\n            await message.add_reaction(u\"\\U0001F92E\")\r\n    \r\n    if \"$malfav\" in message.content.lower():\r\n        msg = message.content.split(\" \")\r\n        if(len(msg) == 3):\r\n            if message.content.split(\" \")[2].lower() == \"character\":\r\n                characters = WebScrape.getUserFavoriteCharacter(message.content.split(\" \")[1])\r\n\r\n                count=0\r\n                printString= \"**\" + msg[1] + \"'s** favorite characters: \\n\" + \"---------------------\\n\"\r\n                for character in characters:\r\n                    count+=1\r\n                    if count==1:\r\n                        printString+=str(count) + \". \" + character + \"\u00f0\u0178\u00a5\u2021 \\n\"\r\n                        continue \r\n                    if count==2:\r\n                        printString+=str(count) + \". \" + character + \"\u00f0\u0178\u00a5\u02c6 \\n\"\r\n                        continue\r\n                    if count==3:\r\n                        printString+=str(count) + \". \" + character + \"\u00f0\u0178\u00a5\u2030 \\n\"\r\n                        continue\r\n                    printString+=str(count) + \". \" + character + \"\\n\"\r\n                await message.channel.send(printString)\r\n                if count==0:\r\n                    await message.channel.send(\"This user doesn't have a favorite character!\")\r\n\r\n            elif message.content.split(\" \")[2] == \"anime\":\r\n                animes = WebScrape.getUserFavoriteAnimes(message.content.split(\" \")[1])\r\n\r\n                count=0\r\n                printString= \"**\" + msg[1] + \"'s** favorite animes: \\n\" \"---------------------\\n\"\r\n                for anime in animes:\r\n                    count+=1\r\n                    if count==1:\r\n                        printString+=str(count) + \". \" + anime + \"\u00f0\u0178\u00a5\u2021 \\n\"\r\n                        continue \r\n                    if count==2:\r\n                        printString+=str(count) + \". \" + anime + \"\u00f0\u0178\u00a5\u02c6 \\n\"\r\n                        continue\r\n                    if count==3:\r\n                        printString+=str(count) + \". \" + anime + \"\u00f0\u0178\u00a5\u2030 \\n\"\r\n                        continue\r\n                    printString+=str(count) + \". \" + anime + \"\\n\"\r\n                await message.channel.send(printString)\r\n                if count==0:\r\n                    await message.channel.send(\"This user doesn't have a favorite anime!\")\r\n\r\n            elif message.content.split(\" \")[2] == \"manga\":\r\n                mangas = WebScrape.getUserFavoriteMangas(message.content.split(\" \")[1])\r\n\r\n                count=0\r\n                printString= \"**\" + msg[1] + \"'s** favorite mangas: \\n\" \"---------------------\\n\"\r\n                for manga in mangas:\r\n                    count+=1\r\n                    if count==1:\r\n                        printString+=str(count) + \". \" + manga + \"\u00f0\u0178\u00a5\u2021 \\n\"\r\n                        continue \r\n                    if count==2:\r\n                        printString+=str(count) + \". \" + manga + \"\u00f0\u0178\u00a5\u02c6 \\n\"\r\n                        continue\r\n                    if count==3:\r\n                        printString+=str(count) + \". \" + manga + \"\u00f0\u0178\u00a5\u2030 \\n\"\r\n                        continue\r\n                    printString+=str(count) + \". \" + manga + \"\\n\"\r\n                await message.channel.send(printString)\r\n                if count==0:\r\n                    await message.channel.send(\"This user doesn't have a favorite manga!\")\r\n            \r\n            elif message.content.split(\" \")[2] == \"people\":\r\n                people = WebScrape.getUserFavoritePeople(message.content.split(\" \")[1])\r\n\r\n                count=0\r\n                printString= \"**\" + msg[1] + \"'s** favorite people: \\n\" + \"---------------------\\n\"\r\n                for person in people:\r\n                    count+=1\r\n                    if count==1:\r\n                        printString+=str(count) + \". \" + person + \"\u00f0\u0178\u00a5\u2021 \\n\"\r\n                        continue \r\n                    if count==2:\r\n                        printString+=str(count) + \". \" + person + \"\u00f0\u0178\u00a5\u02c6 \\n\"\r\n                        continue\r\n                    if count==3:\r\n                        printString+=str(count) + \". \" + person + \"\u00f0\u0178\u00a5\u2030 \\n\"\r\n                        continue\r\n                    printString+=str(count) + \". \" + person + \"\\n\"\r\n                await message.channel.send(printString)\r\n                if count==0:\r\n                    await message.channel.send(\"This user doesn't have a favorite person!\")\r\n            \r\n            elif message.content.split(\" \")[2] == \"producer\":\r\n                producers = WebScrape.getUserFavoriteCompanies(message.content.split(\" \")[1])\r\n\r\n                count=0\r\n                printString= \"**\" + msg[1] + \"'s** favorite producers: \\n\" + \"---------------------\\n\"\r\n                for producer in producers:\r\n                    count+=1\r\n                    if count==1:\r\n                        printString+=str(count) + \". \" + producer + \"\u00f0\u0178\u00a5\u2021 \\n\"\r\n                        continue \r\n                    if count==2:\r\n                        printString+=str(count) + \". \" + producer + \"\u00f0\u0178\u00a5\u02c6 \\n\"\r\n                        continue\r\n                    if count==3:\r\n                        printString+=str(count) + \". \" + producer + \"\u00f0\u0178\u00a5\u2030 \\n\"\r\n                        continue\r\n                    printString+=str(count) + \". \" + producer + \"\\n\"\r\n                await message.channel.send(printString)\r\n                if count==0:\r\n                    await message.channel.send(\"This user doesn't have a favorite producer!\")\r\n        \r\n        elif(len(msg) == 2):\r\n            everything = WebScrape.getUserTopFavorites(message.content.split(\" \")[1])\r\n            for entry in everything:\r\n                await message.channel.send(entry)\r\n        else:\r\n            await message.channel.send(\"Correct Syntax: $malfav (username) (category)\")\r\n    \r\n    if f\"$malscore\" in message.content.lower():\r\n        msg = message.content.split(\" \")\r\n        username = msg[1]\r\n        anime = msg[2:]\r\n        animeName = \"\"\r\n        for word in anime:\r\n            if anime.index(word) != len(anime)-1:\r\n                animeName = animeName + word + \" \"\r\n            else:\r\n                animeName = animeName + word\r\n        \r\n        animeList = WebScrape.getUserScoreAnime(username, animeName)[0]\r\n        scores = WebScrape.getUserScoreAnime(username, animeName)[1]\r\n        status = WebScrape.getUserScoreAnime(username, animeName)[2]\r\n\r\n        await message.channel.send(\"Searching **\" + username + \"'s** list for \\\"\" + animeName + \"\\\"... \\n\")\r\n\r\n        if len(animeList) == 0:\r\n            await message.channel.send(\"**\" + username + \"** does not have a rating for \\\"\" + animeName + \"\\\"\")\r\n\r\n        else:\r\n            output = \"\"\r\n            counter=0\r\n            for entry in animeList:\r\n\r\n                if status[counter] == '2':\r\n                    if scores[counter] == '0':\r\n                        output = output + \":white_check_mark:  : \" + entry + \" : **\" + \"-\" + \"**\\n\"\r\n                    else:\r\n                        output = output + \":white_check_mark:  : \" + entry + \" : **\" + scores[counter] + \"**\\n\"\r\n                    \r\n\r\n                elif status[counter] == '3':\r\n                    if scores[counter] == '0':\r\n                        output = output + \"\u00e2\u0153\u2039 : \" + entry + \" : **\" + \"-\" + \"**\\n\"\r\n                    else:\r\n                        output = output + \"\u00e2\u0153\u2039 : \" + entry + \" : **\" + scores[counter] + \"**\\n\"\r\n\r\n                elif status[counter] == '4':\r\n                    if scores[counter] == '0':\r\n                        output = output + \"\u00f0\u0178\u203a\u2018 : \" + entry + \" : **\" + \"-\" + \"**\\n\"\r\n                    else:\r\n                        output = output + \"\u00f0\u0178\u203a\u2018 : \" + entry + \" : **\" + scores[counter] + \"**\\n\"\r\n\r\n                counter+=1\r\n            \r\n            await message.channel.send(output)\r\n\r\n\r\n@client.event\r\n\r\nasync def on_ready():\r\n    print(f'{client.user} is online nye')\r\n\r\nclient.run(TOKEN)\r\n", "41": "from utils import *\n\n# Directory & File Name Function\ndef create_directory(directory_name, xlsx_name):\n    dirname = os.path.dirname(__file__)\n    path = os.path.join(dirname, directory_name)\n    filename = os.path.join(path, xlsx_name + '.xlsx')\n    if not os.path.exists(path):\n        os.mkdir(path)\n    return(filename)\n\nclass position_functions:\n    def offence_function(df):\n        offence = 0.65 * (np.sum(df.loc[0]) + 0.35 * (np.sum(df.loc[1])))\n        return offence\n\n    def defence_function(df):\n        defence = 0.35 * (np.sum(df.loc[0])) + 0.25 * (np.sum(df.loc[1])) + 0.4 * (np.sum(df.loc[2]))\n        return defence\n\n    def goalie_function(df):\n        goalie = np.sum(df)\n        return goalie\nclass dataframe_manipulation:\n    def offence_dataframe(df, offence_defence_table_number):\n        team_df = df[offence_defence_table_number].loc[:len(df[offence_defence_table_number]).__index__()-1,\n                  [('Scoring', 'G'), ('Unnamed: 17_level_0', 'S')]]\n        return team_df\n\n    def defence_dataframe(df, offence_defence_table_number):\n        defence_df = pd.DataFrame(df[offence_defence_table_number])\n        team_defence_df = defence_df.loc[:len(df[offence_defence_table_number]).__index__()-1,\n                          [('Unnamed: 24_level_0', 'BLK'), ('Unnamed: 25_level_0', 'HIT'),\n                           ('Unnamed: 26_level_0', 'FOW'), ('Unnamed: 27_level_0', 'FOL')]]\n        team_defence_df['Unnamed: 26_level_0', 'XXX'] = team_defence_df['Unnamed: 26_level_0', 'FOW'] -\\\n                                                        team_defence_df['Unnamed: 27_level_0', 'FOL']\n        team_defence_df.drop(columns=[('Unnamed: 26_level_0', 'FOW'), ('Unnamed: 27_level_0', 'FOL')])\n        return team_defence_df\n\n    def goalie_dataframe(df, goalie_table_number):\n        team_goalie_df = df[goalie_table_number].loc[0:2, [('Goalie Stats', 'GA')]]\n        return team_goalie_df\n\n    def concat_teams(dictionary):\n        for i in list(dictionary.keys()):\n            df = pd.concat(dictionary.get(i))\n            return df\n\ndef webscrape_function(team_name, season_year):\n    df = pd.read_html('https://www.hockey-reference.com/teams/' + team_name + '/' + season_year + '.html')\n    return df\ndef normalization_function(df):\n    normalized_value = ( df - pd.DataFrame.mean(df) ) / pd.DataFrame.std(df)\n    return normalized_value\nteam_array = ['CAR', 'FLA', 'TBL', 'NSH',\n              'DAL', 'CHI', 'CBJ', 'DET',\n              'PIT', 'WSH', 'BOS', 'NYI',\n              'NYR', 'PHI', 'NJD', 'BUF',\n              'TOR', 'EDM', 'WPG', 'MTL',\n              'CGY', 'OTT', 'VAN', 'COL',\n              'VEG', 'MIN', 'STL', 'ARI',\n              'SJS', 'LAK', 'ANA', 'SEA']\nteam_abbrev = {\n    'Boston': 'BOS',\n    'Buffalo': 'BUF',\n    'Detroit': 'DET',\n    'Florida': 'FLA',\n    'Montreal': 'MTL',\n    'Ottawa': 'OTT',\n    'Tampa Bay': 'TBL',\n    'Toronto': 'TOR',\n    'Carolina': 'CAR',\n    'Columbus': 'CBJ',\n    'N.Y. Islanders': 'NYI',\n    'N.Y. Rangers': 'NYR',\n    'New Jersey': 'NJD',\n    'Philadelphia': 'PHI',\n    'Pittsburgh': 'PIT',\n    'Washington': 'WSH',\n    'Arizona': 'ARI',\n    'Chicago': 'CHI',\n    'Colorado': 'COL',\n    'Dallas': 'DAL',\n    'Minnesota': 'MIN',\n    'Nashville': 'NSH',\n    'St. Louis': 'STL',\n    'Winnipeg': 'WPG',\n    'Anaheim': 'ANA',\n    'Calgary': 'CGY',\n    'Edmonton': 'EDM',\n    'Los Angeles': 'LAK',\n    'San Jose': 'SJS',\n    'Seattle': 'SEA',\n    'Vancouver': 'VAN',\n    'Vegas': 'VEG'\n}\n\nclass todays_games:\n    todays_date = date.today().strftime(\"%Y%m%d\")\n    link_to_todays_games = pd.read_html('https://www.cbssports.com/nhl/schedule/' + todays_date)\n    @staticmethod\n    def home_teams():\n        todays_home_teams_df = todays_games.link_to_todays_games[0]['Home'].map(team_abbrev)\n        return todays_home_teams_df\n    @staticmethod\n    def away_teams():\n        todays_away_teams_df = todays_games.link_to_todays_games[0]['Away'].map(team_abbrev)\n        return todays_away_teams_df\n\n\n# Variables to declare the column numbers, season year, and number of players to use for the weighted average.\nseason_year = '2022'\ninjury_table_number = 0\nno_injury_table = None # in case a team has no injuries\nfirst_offence_defence_table_number = 4\nsecond_offence_defence_table_number = 3\nfirst_goalie_table_number = 5\nsecond_goalie_table_number = 4\n\n# Webscraping the team data\nteam_datasets = {\n    'CAR_df' : webscrape_function(team_array[0], season_year=season_year),\n    'FLA_df' : webscrape_function(team_array[1], season_year=season_year),\n    'TBL_df' : webscrape_function(team_array[2], season_year=season_year),\n    'NSH_df' : webscrape_function(team_array[3], season_year=season_year),\n    'DAL_df' : webscrape_function(team_array[4], season_year=season_year),\n    'CHI_df' : webscrape_function(team_array[5], season_year=season_year),\n    'CBJ_df' : webscrape_function(team_array[6], season_year=season_year),\n    'DET_df' : webscrape_function(team_array[7], season_year=season_year),\n    'PIT_df' : webscrape_function(team_array[8], season_year=season_year),\n    'WSH_df' : webscrape_function(team_array[9], season_year=season_year),\n    'BOS_df' : webscrape_function(team_array[10], season_year=season_year),\n    'NYI_df' : webscrape_function(team_array[11], season_year=season_year),\n    'NYR_df' : webscrape_function(team_array[12], season_year=season_year),\n    'PHI_df' : webscrape_function(team_array[13], season_year=season_year),\n    'NJD_df' : webscrape_function(team_array[14], season_year=season_year),\n    'BUF_df' : webscrape_function(team_array[15], season_year=season_year),\n    'TOR_df' : webscrape_function(team_array[16], season_year=season_year),\n    'EDM_df' : webscrape_function(team_array[17], season_year=season_year),\n    'WPG_df' : webscrape_function(team_array[18], season_year=season_year),\n    'MTL_df' : webscrape_function(team_array[19], season_year=season_year),\n    'CGY_df' : webscrape_function(team_array[20], season_year=season_year),\n    'OTT_df' : webscrape_function(team_array[21], season_year=season_year),\n    'VAN_df' : webscrape_function(team_array[22], season_year=season_year),\n    'COL_df' : webscrape_function(team_array[23], season_year=season_year),\n    'VEG_df' : webscrape_function(team_array[24], season_year=season_year),\n    'MIN_df' : webscrape_function(team_array[25], season_year=season_year),\n    'STL_df' : webscrape_function(team_array[26], season_year=season_year),\n    'ARI_df' : webscrape_function(team_array[27], season_year=season_year),\n    'SJS_df' : webscrape_function(team_array[28], season_year=season_year),\n    'LAK_df' : webscrape_function(team_array[29], season_year=season_year),\n    'ANA_df' : webscrape_function(team_array[30], season_year=season_year),\n    'SEA_df' : webscrape_function(team_array[31], season_year=season_year)\n}\n# Normalization of the data & final weighted averages\nfinal_team_averages = {\n    'CAR': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('CAR_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('CAR_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('CAR_df'), first_goalie_table_number)))),\n\n    'FLA': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('FLA_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('FLA_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('FLA_df'), first_goalie_table_number)))),\n\n    'TBL': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('TBL_df'), second_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('TBL_df'), second_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('TBL_df'), second_goalie_table_number)))),\n\n    'NSH': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('NSH_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('NSH_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('NSH_df'), first_goalie_table_number)))),\n\n    'DAL': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('DAL_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('DAL_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('DAL_df'), first_goalie_table_number)))),\n\n    'CHI': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('CHI_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('CHI_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('CHI_df'), first_goalie_table_number)))),\n\n    'CBJ': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('CBJ_df'), first_offence_defence_table_number))) + \\\n                 position_functions.defence_function(normalization_function(\n                     dataframe_manipulation.defence_dataframe(team_datasets.get('CBJ_df'),\n                                                              first_offence_defence_table_number))) + \\\n                 position_functions.goalie_function(normalization_function(\n                     dataframe_manipulation.goalie_dataframe(team_datasets.get('CBJ_df'), first_goalie_table_number)))),\n\n    'DET': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('DET_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('DET_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('DET_df'), first_goalie_table_number)))),\n\n    'WSH': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('WSH_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('WSH_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('WSH_df'), first_goalie_table_number)))),\n\n    'PIT': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('PIT_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('PIT_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('PIT_df'), first_goalie_table_number)))),\n\n    'BOS': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('BOS_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('BOS_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('BOS_df'), first_goalie_table_number)))),\n\n    'NYI': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('NYI_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('NYI_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('NYI_df'), first_goalie_table_number)))),\n\n    'NYR': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('NYR_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('NYR_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('NYR_df'), first_goalie_table_number)))),\n\n    'PHI': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('PHI_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('PHI_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('PHI_df'), first_goalie_table_number)))),\n\n    'NJD': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('NJD_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('NJD_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('NJD_df'), first_goalie_table_number)))),\n\n    'BUF': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('BUF_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('BUF_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('BUF_df'), first_goalie_table_number)))),\n\n    'TOR': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('TOR_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('TOR_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('TOR_df'), first_goalie_table_number)))),\n\n    'EDM': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('EDM_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('EDM_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('EDM_df'), first_goalie_table_number)))),\n\n    'WPG': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('WPG_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('WPG_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('WPG_df'), first_goalie_table_number)))),\n\n    'MTL': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('MTL_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('MTL_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('MTL_df'), first_goalie_table_number)))),\n\n    'CGY': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('CGY_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('CGY_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('CGY_df'), first_goalie_table_number)))),\n\n    'OTT': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('OTT_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('OTT_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('OTT_df'), first_goalie_table_number)))),\n\n    'VAN': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('VAN_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('VAN_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('VAN_df'), first_goalie_table_number)))),\n\n    'COL': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('COL_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('COL_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('COL_df'), first_goalie_table_number)))),\n\n    'VEG': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('VEG_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('VEG_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('VEG_df'), first_goalie_table_number)))),\n\n    'MIN': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('MIN_df'), second_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('MIN_df'), second_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('MIN_df'), second_goalie_table_number)))),\n\n    'STL': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('STL_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('STL_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('STL_df'), first_goalie_table_number)))),\n\n    'ARI': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('ARI_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('ARI_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('ARI_df'), first_goalie_table_number)))),\n\n    'SJS': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('SJS_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('SJS_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('SJS_df'), first_goalie_table_number)))),\n\n    'LAK': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('LAK_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('LAK_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('LAK_df'), first_goalie_table_number)))),\n\n    'ANA': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('ANA_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('ANA_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('ANA_df'), first_goalie_table_number)))),\n\n    'SEA': float(position_functions.offence_function(normalization_function(\n        dataframe_manipulation.offence_dataframe(team_datasets.get('SEA_df'), first_offence_defence_table_number))) +\\\n           position_functions.defence_function(normalization_function(\n               dataframe_manipulation.defence_dataframe(team_datasets.get('SEA_df'), first_offence_defence_table_number))) +\\\n           position_functions.goalie_function(normalization_function(\n               dataframe_manipulation.goalie_dataframe(team_datasets.get('SEA_df'), first_goalie_table_number))))\n\n}\n\ntodays_home_teams_df = todays_games.home_teams()\ntodays_away_teams_df = todays_games.away_teams()", "42": "#!/usr/bin/python3\n\nimport json\nimport pprint\nimport os.path\nimport os\nimport pymysql.cursors\n\n#from __main__ import *\nfrom webscrape import response_champs\nfrom webscrape import champs\nfrom webscrape import response_items\nfrom webscrape import items\nfrom webscrape import response_summoners\nfrom webscrape import summoners\nfrom webscrape import skip_maps\nif skip_maps==False:\n\tfrom webscrape import maps\n\nconn=pymysql.connect(host='localhost', user='ec2-user', passwd='pdcd', db='bravery')\n\nportrait_path = 'images/portraits/'\nskill_path = 'images/skills/'\npassive_path = 'images/passives/'\nitem_path = 'images/items/'\nsummoner_path = 'images/summoners/'\ntry:\n\twith conn.cursor() as cursor:\n\t\tfor i in champs:\n\t\t\tchamp_id = response_champs['data'][i]['id']\n\t\t\tchamp_name = response_champs['data'][i]['name']\n\t\t\tportrait = portrait_path+i+'.png'\n\t\t\tq_skill = response_champs['data'][i]['spells'][0]['image']['full']\n\t\t\tw_skill = response_champs['data'][i]['spells'][1]['image']['full']\n\t\t\te_skill = response_champs['data'][i]['spells'][2]['image']['full']\n\t\t\tr_skill = response_champs['data'][i]['spells'][3]['image']['full']\n\t\t\tpassive_icon = response_champs['data'][i]['passive']['image']['full']\n\t\t\tskins = str(len(response_champs['data'][i]['skins'])-1)\n\t\t\tsql = \"INSERT IGNORE INTO champions (id, name, portrait, q, w, e, r, passive, skins) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\"\n\t\t\tcursor.execute(sql, (champ_id,i, portrait, skill_path+q_skill, skill_path+w_skill, skill_path+e_skill, skill_path+r_skill, passive_path+passive_icon, skins))\n\t\t\t#cursor.execute(sql, (skins, champ_id))\t\tThis line and the one above it are used to update how many skins a champ has once they are already in the database. FIXIT\n\t\t\tconn.commit()\n\t\tfor i in items:\n\t\t\t# id | name | gold | crystal_scar | twisted_treeline | summoners_rift | howling_abyss | icon \n\t\t\titem_id = response_items['data'][i]['id']\n\t\t\tif 'name' in response_items['data'][i]:  # fix for item 3632 because it has no name. wtf rito\n\t\t\t\titem_name = response_items['data'][i]['name'] \n\t\t\titem_gold = response_items['data'][i]['gold']['total']\n\t\t\titem_icon = response_items['data'][i]['image']['full']\n\t\t\tif 'tags' in response_items['data'][i]:  # not all items have tags and was used for Bilgewater event\n\t\t\t\titem_tags = response_items['data'][i]['tags']\n\t\t\titem_map_cs = False\n\t\t\titem_map_tt = True\n\t\t\titem_map_sr = True\n\t\t\titem_map_ha = True\n\t\t\tcounter = 0\n#\t\t\tif skip_maps==False:\n#\t\t\t\tfor map_number in maps:\n#\t\t\t\t\tfor counter in range(len(maps[map_number]['unpurchasableItemList'])):\n#\t\t\t\t\t\tif item_id == maps[map_number]['unpurchasableItemList'][counter]:\n#\t\t\t\t\t\t\tif map_number == '10':\n#\t\t\t\t\t\t\t\titem_map_tt = False\n#\t\t\t\t\t\t\telif map_number == '11':\n#\t\t\t\t\t\t\t\titem_map_sr = False\n#\t\t\t\t\t\t\telif map_number == '12':\n#\t\t\t\t\t\t\t\titem_map_ha = False\n#\t\t\tif 'Bilgewater' not in item_tags:\t\t\t### Here in case another event is introduced\n\t\t\tsql = \"INSERT IGNORE INTO items (id, name , gold, crystal_scar, twisted_treeline, summoners_rift, howling_abyss, icon) VALUES (%r, %s, %r, %r, %r, %r, %r, %s);\"\n\t\t\tcursor.execute(sql, (item_id, item_name, item_gold, item_map_cs, item_map_tt, item_map_sr, item_map_ha, item_path+item_icon))\n\t\t\tconn.commit()\n\n\t\tfor i in summoners:\n\t\t\tsummoner_id = response_summoners['data'][i]['id']\n\t\t\tsummoner_name = response_summoners['data'][i]['name']\n\t\t\tsummoner_icon = response_summoners['data'][i]['image']['full']\n\t\t\tsummoner_map_cs = False\n\t\t\tsummoner_map_tt = False\n\t\t\tsummoner_map_sr = False\n\t\t\tsummoner_map_ha = False\n\t\t\tcounter = 0\n\t\t\tfor mode in response_summoners['data'][i]['modes']:\n\t\t\t\tif mode == 'CLASSIC':\n\t\t\t\t\tsummoner_map_tt = True\n\t\t\t\t\tsummoner_map_sr = True\n\t\t\t\telif mode == 'ARAM':\n\t\t\t\t\tsummoner_map_ha = True\n\t\t\t\telif mode == 'ODIN':\n\t\t\t\t\tsummoner_map_cs = True\n\t\t\tsql = \"INSERT IGNORE INTO summoners (id, name, crystal_scar, twisted_treeline, summoners_rift, howling_abyss, icon) VALUES (%s, %s, %r, %r, %r, %r, %s);\"\n\t\t\tcursor.execute(sql, (summoner_id, summoner_name, summoner_map_cs, summoner_map_tt, summoner_map_sr, summoner_map_ha, summoner_path+summoner_icon))\n\t\t\tconn.commit()\nfinally:\n\tconn.close()\n\n", "43": "from database.data_classes import Alerts, Items, Price_History, User\n\n#################\n# conn = psycopg2.connect(host=\"localhost\", database=\"mydb\", user=\"zaki\", password=\"password123\")\n\n\ndef return_user(conn, email):\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM webscrape.users WHERE email = %s\", (email,))\n    return cur.fetchone()\n\n\ndef add_user(conn, person: User):\n    \"\"\"add user to db\"\"\"\n    cur = conn.cursor()\n    cur.execute(\n        \"INSERT INTO webscrape.users (email, password) VALUES (%s, %s)\",\n        (person.email, person.password),\n    )\n    cur.close()\n    return\n\n\ndef add_item(conn, item: Items):\n    cur = conn.cursor()\n    cur.execute(\n        \"INSERT INTO webscrape.items (asin, title, current_amount, currency) VALUES (%s, %s, %s, %s)\",\n        (item.asin, item.title, item.current_amount, item.currency),\n    )\n    cur.close()\n    return\n\n\ndef add_alert(conn, alert: Alerts):\n    \"\"\"add alert to db\"\"\"\n    cur = conn.cursor()\n    cur.execute(\n        \"INSERT INTO webscrape.alerts (asin, id, target_amount) VALUES (%s, %s, %s)\",\n        (alert.asin, alert.id, alert.target_amount),\n    )\n    cur.close()\n    return\n\n\ndef add_price_history(conn, ph: Price_History):\n    \"\"\"add ph to db\"\"\"\n    cur = conn.cursor()\n    cur.execute(\n        \"INSERT INTO webscrape.price_history (asin, date, amount, currency) VALUES (%s, %s, %s, %s)\",\n        (ph.asin, ph.date, ph.amount, ph.currency),\n    )\n    cur.close()\n    return\n\n\ndef list_of_unique_asins(conn):\n    # the asins have empty space, caused by the psql table defintions\n    cur = conn.cursor()\n    cur.execute(\"SELECT DISTINCT asin FROM webscrape.alerts\")\n    x = [i[0] for i in cur.fetchall()]\n    cur.close()\n    return x\n\n\ndef print_table(conn):\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM webscrape.alerts\")\n    a = cur.fetchall()\n    print(a)\n    cur.close()\n    return\n\n\n\"\"\"\nDatabase notes:\n\nadd to database:\nalerts: email, asin, and target price DONE\nuser: make email unique\n\nall database interactions:\nfront end:\n    user (email, password):\n        current asins and target price\n        change email/password\n    alerts (email, asin, target price):\n        add/remove asin\n        change target price\n\nChange to tables:\n    add target price to Alerts\n\ndatabase queries:\n    Automated back-end\n        - select all unique asins (for price history backend function)  DONE\n        - select all grouped emails associated with each unique asin,\n            the target price for each email, and its latest price from price history\n            (then send emails)\n\n    get all asins in alert, and its current price\n    get all emails/target_price attached to unique asin (alerts), and current amount (ph) -> join alerts and p_h on asin\n    - given a user, select all asins and current/target price, associated with an email (for displaying at user front end)\n\nemailing:\n    for each unique asin in alerts:\n        get current price\n        for each id tracking asin:\n            if tracking price <= target price\n                send email\n\n\"\"\"\n\n\n# sort out uniques in db\n# https://www.psycopg.org/docs/usage.html#passing-parameters-to-sql-queries\n", "44": "# CST 205\n# Cathy Hsu, Christiana Libhart, Jaclyn Libhart, Deborah Meda, Charlie Nguyen\n# This file, is the the main file for our CST 205 final project.\n#\n# Citation\n# https://stackoverflow.com/questions/21217475/get-selected-text-from-a-form-using-wtforms-selectfield\n# https://hackersandslackers.com/flask-wtforms-forms/\n# https://stackoverflow.com/questions/44055471/how-can-i-add-a-flask-wtforms-selectfield-to-my-html\n# https://datatofish.com/delete-file-folder-python/\n# https://www.geeksforgeeks.org/python-opencv-cv2-imwrite-method/\n# https://discord.com/channels/778798502020513813/778798503408304141/788202133630877717\n#https://discord.com/channels/778798502020513813/778798503408304141/788202159614853150\n\nfrom flask import Flask, render_template, flash, redirect\nfrom flask_bootstrap import Bootstrap\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, SubmitField, SelectField\nfrom wtforms.validators import DataRequired\nfrom webscrape_recipe_file import website_recipe_info\nfrom pprint import pprint\nimport urllib.request       # for saving images\nfrom PIL import Image\nimport cv2\nimport webscrape                        # webscrape\n\n\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'csumb-otter'\nbootstrap = Bootstrap(app)\n\n# Class is used to save the users search term and their image format.\nclass RecipeSearchTerm(FlaskForm):\n    search_term = StringField(\n        'Search Term', \n        validators=[DataRequired()]\n    )\n\n    image_format = SelectField(\"Choose an option\", choices=[(\"none\", \"None\"), (\"grayscale\", \"Grayscale\"), (\"negative\", \"Negative\"), (\"sephia\", \"Sephia\"), (\"thumbnail\", \"Thumbnail\"), (\"winter\", \"Winter\")])\n\n# Instance Variables\nrecipes = []\nmatched_recipes = []\nimage_filter = \"none\"\n\n# run webscrape file\n# We arent running this function at the moment, as all our data is already pulled\n# Charlie created\ndef run_webscrape():\n    webscrape.webscrape_function()\n\n# split search term into separate words and convert words to lower case\n# Cathy created\ndef store_search_term(token):\n    search_term = token.lower().split()\n    return search_term\n\n# clean webscrape data\n# Cathy created\ndef preprocess(): \n    for recipe in website_recipe_info: \n        recipes.append({})\n        recipes[-1]['title'] = recipe['title']\n        recipes[-1]['recipe_url'] = recipe['recipe_url']\n        # split tag into separate words and convert words to lower case \n        recipes[-1]['tags'] = recipe['tags'].lower().split()\n        recipes[-1]['image_url'] = recipe['image_url']\n\n# This functions searches for recipes that matches a search term\n# Cathy created\ndef search_for_recipe_matches(search_term): \n    # empty array so recipes that matched previous search term aren't included\n    matched_recipes.clear()\n    # used to check if a recipe has already been added into matched_recipes\n    already_matched_recipe_titles = []\n\n    # for every word in the search_term, go through each recipe's tags and check if the word matches any word in the \n    # tag. If there is a match and the recipe has not already been added, add it to matched_recipes\n    for word in search_term: \n        for recipe in recipes: \n            for tag in recipe['tags']: \n                if (tag == word and recipe['title'] not in already_matched_recipe_titles):\n                    matched_recipes.append({})\n                    already_matched_recipe_titles.append(recipe['title'])\n                    matched_recipes[-1]['title'] = recipe['title']\n                    matched_recipes[-1]['recipe_url'] = recipe['recipe_url']\n                    matched_recipes[-1]['tags'] = recipe['tags']\n                    matched_recipes[-1]['image_url'] = recipe['image_url']\n\n# This function applies a gray scale filter on the specified image.\n# Christiana and Jaclyn created\ndef apply_grayscale(image_name, recipe_title): \n    im = Image.open(image_name)\n    grayscale_list = [ ( (a[0]+a[1]+a[2])//3, ) * 3\n                  for a in im.getdata() ]\n    im.putdata(grayscale_list)\n    im.save(\"static/images/grayscale/\" + recipe_title + \".jpg\")\n\n# This function applies a negative filter on the specified image.\n# Christiana and Jaclyn created\ndef apply_negative(image_name, recipe_title): \n    im = Image.open(image_name)\n    negative_list = [(255 - p[0], 255 - p[1], 255 - p[2]) for p in im.getdata()]\n    im.putdata(negative_list)\n    im.save(\"static/images/negative/\" + recipe_title + \".jpg\")\n\n# This function applies a thumbnail scale on the specified image.\n# The image will show a canvas color\n# Christiana and Jaclyn created\ndef apply_thumbnail(image_name, recipe_title):\n    source = Image.open(image_name)\n    w,h = source.width, source.height\n    target = Image.new('RGB', (w, h), 'rosybrown')\n\n    target_x = 0\n    for source_x in range(0, source.width, 2):\n        target_y = 0\n        for source_y in range(0, source.height, 2):\n            pixel = source.getpixel((source_x, source_y))\n            target.putpixel((target_x, target_y), pixel)\n            target_y += 1\n        target_x += 1\n    target.save(\"static/images/thumbnail/\" + recipe_title + \".jpg\")\n\n# This function applies a sephia filter on the specified image.\n# Christiana and Jaclyn created\ndef apply_sephia(image_name, recipe_title): \n    im = Image.open(image_name)\n    sepia_list = [(255 + pixel[0], pixel[1], pixel[2])\n                    for pixel in im.getdata()]\n    im.putdata(sepia_list)\n    im.save(\"static/images/sephia/\" + recipe_title + \".jpg\")\n\n# This function applies a winter filter (color map) on the specified image.\n# Christiana and Jaclyn created\ndef apply_winter(image_name, recipe_title): \n    image_winter = cv2.imread(image_name,cv2.IMREAD_GRAYSCALE)\n    image_remap = cv2.applyColorMap(image_winter, cv2.COLORMAP_WINTER)\n    cv2.imwrite(\"static/images/winter/\" + recipe_title + \".jpg\", image_remap)\n\n# This function encapsulates all the filters into one wrapper function for ease of use\n# Cathy created\ndef apply_filter(image_filter):\n    # Index to index into matched recipes :3c\n    i = 0\n    pprint(matched_recipes)\n    \n    for recipe in matched_recipes:\n        if image_filter == \"grayscale\":\n            matched_recipes[i][\"image_url\"] = \"static/images/grayscale/\" + recipe['title'] + \".jpg\"\n        \n        if image_filter == \"negative\":\n            matched_recipes[i][\"image_url\"] = \"static/images/negative/\" + recipe['title'] + \".jpg\"\n\n        if image_filter == \"sephia\":\n            matched_recipes[i][\"image_url\"] = \"static/images/sephia/\" + recipe['title'] + \".jpg\"\n\n        if image_filter == \"winter\":\n            matched_recipes[i][\"image_url\"] = \"static/images/winter/\" + recipe['title'] + \".jpg\"\n\n        if image_filter == \"thumbnail\":\n            matched_recipes[i][\"image_url\"] = \"static/images/thumbnail/\" + recipe['title'] + \".jpg\"\n\n        i += 1\n\n\n# this will be run once to save all the images in our directory the filter images\n# Cathy created\ndef create_filter_images(): \n    index = 0 \n    for recipe in recipes:\n        print(\"inside of matched_recipes loop\")\n        #print(recipe)\n        image_name = \"static/images/\" + recipe[\"title\"] + \".jpg\"\n        current_image_url = recipe[\"image_url\"]\n        # retrieve the image and save it\n        urllib.request.urlretrieve(current_image_url, image_name)\n        im = Image.open(image_name)\n        # filters\n        apply_grayscale(image_name, recipe[\"title\"])\n        apply_negative(image_name, recipe[\"title\"])\n        apply_thumbnail(image_name, recipe['title'])\n        apply_sephia(image_name, recipe['title'])\n        apply_winter(image_name, recipe['title'])\n        index += 1\n        \n# Home route or landing page\n# Cathy created\n@app.route('/', methods=('GET', 'POST'))\ndef index():\n    preprocess()\n    form = RecipeSearchTerm()\n    # when the user hits submit we will grab out the information we need\n    if form.validate_on_submit():\n        search_term = store_search_term(form.search_term.data)\n        image_filter = form.image_format.data\n        print(image_filter)\n        search_for_recipe_matches(search_term)\n        # create_filter_images()   \n        apply_filter(image_filter)\n        return redirect('/result')\n    return render_template('index.html', form=form)\n\n# result route\n# Debbie created\n@app.route('/result')\ndef vp():\n    pprint(matched_recipes)\n    return render_template('result.html', matched_recipes=matched_recipes)", "45": "# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = False\n\n# Configure maximum concurrent requests performed by Scrapy\nCONCURRENT_REQUESTS = 16\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'webscrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "46": "from argparse import ArgumentParser\r\nimport argparse\r\nfrom bs4 import BeautifulSoup\r\nfrom collections import Counter\r\nimport csv\r\nimport datetime as dt\r\nfrom datetime import datetime\r\nimport gensim\r\nfrom gensim.utils import simple_preprocess\r\nfrom gensim.models import CoherenceModel\r\nfrom gensim.models.coherencemodel import CoherenceModel\r\nfrom gensim import corpora as corpora\r\nfrom gensim.models import LsiModel\r\nimport json\r\nimport logging\r\nimport logging.config\r\nfrom itertools import chain\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib\r\nimport nltk\r\nfrom nltk import word_tokenize\r\nfrom nltk.tokenize import RegexpTokenizer\r\nfrom nltk.corpus import stopwords\r\nfrom nltk.stem.porter import PorterStemmer\r\nimport numpy as np\r\nimport os.path\r\nimport os\r\nimport pandas as pd\r\nfrom PIL import Image\r\nimport PIL\r\nfrom pprint import pprint\r\nimport pyLDAvis\r\nimport pyLDAvis.gensim\r\nimport re\r\nimport requests\r\nfrom requests import get\r\nfrom schema import SCHEMA\r\nimport selenium\r\nfrom selenium.webdriver import Chrome\r\nfrom selenium.webdriver.common.keys import Keys\r\nfrom selenium import webdriver as wd\r\nimport shutil\r\nimport smtplib\r\nimport string\r\nfrom string import Template\r\nimport wordcloud\r\nimport time\r\nimport unittest\r\nfrom urllib.request import urlopen\r\nimport urllib\r\nimport win32com.client\r\nfrom wordcloud import WordCloud, ImageColorGenerator\r\nfrom wordcloud import STOPWORDS\r\nimport xlsxwriter\r\n\r\npunctuations = '!()-[]{};:\"\\,<>./?@#$%^&*_~'\r\n\r\n# import pywin32\r\n\r\nx = datetime.today()\r\n\r\n\r\nurl_indeed = []\r\nurl_indeed.append(\"https://ca.indeed.com/cmp/Caa/reviews\")\r\nurl_indeed.append(\"https://ca.indeed.com/cmp/Caa/reviews?start=20\")\r\nurl_indeed.append(\"https://ca.indeed.com/cmp/Caa/reviews?start=40\")\r\nurl_glassdoor = \"https://www.glassdoor.ca/Reviews/CAA-South-Central-Ontario-Reviews-E150598.htm\"\r\n\r\nstars = []\r\ntitle = []\r\ndescription = []\r\ndate = []\r\nposition = []\r\nlocation = []\r\npros = []\r\ncons = []\r\n\r\nnumber_of_topics = 5\r\nwords = 10\r\n\r\nstart = time.time()\r\n\r\nDEFAULT_URL = ('https://www.glassdoor.ca/Reviews/CAA-South-Central-Ontario-Reviews-E150598.htm')\r\n\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument('-u', '--url',\r\n                        help='URL of the company\\'s Glassdoor landing page.',\r\n                        default=DEFAULT_URL)\r\nparser.add_argument('-f', '--file', default='glassdoor_ratings.csv',\r\n                        help='Output file.')\r\nparser.add_argument('--headless', action='store_true',\r\n                    help='Run Chrome in headless mode.')\r\nparser.add_argument('--username', help='Email address used to sign in to GD.')\r\nparser.add_argument('-p', '--password', help='Password to sign in to GD.')\r\nparser.add_argument('-c', '--credentials', help='Credentials file')\r\nparser.add_argument('-l', '--limit', default=152,\r\n                        action='store', type=int, help='Max reviews to scrape')\r\nparser.add_argument('--start_from_url', action='store_true',\r\n                        help='Start scraping from the passed URL.')\r\nparser.add_argument(\r\n    '--max_date', help='Latest review date to scrape.\\\r\n    Only use this option with --start_from_url.\\\r\n    You also must have sorted Glassdoor reviews ASCENDING by date.',\r\n    type=lambda s: dt.datetime.strptime(s, \"%Y-%m-%d\"))\r\nparser.add_argument(\r\n    '--min_date', help='Earliest review date to scrape.\\\r\n    Only use this option with --start_from_url.\\\r\n    You also must have sorted Glassdoor reviews DESCENDING by date.',\r\n    type=lambda s: dt.datetime.strptime(s, \"%Y-%m-%d\"))\r\nargs = parser.parse_args()\r\n\r\nif not args.start_from_url and (args.max_date or args.min_date):\r\n    raise Exception(\r\n       'Invalid argument combination:\\\r\n        No starting url passed, but max/min date specified.'\r\n    )\r\nelif args.max_date and args.min_date:\r\n    raise Exception(\r\n          'Invalid argument combination:\\\r\n           Both min_date and max_date specified.'\r\n     )\r\n\r\nif args.credentials:\r\n    with open(args.credentials) as f:\r\n        d = json.loads(f.read())\r\n        args.username = d['username']\r\n        args.password = d['password']\r\nelse:\r\n    try:\r\n        with open('secrets.txt') as f:\r\n            d = json.loads(f.read())\r\n            args.username = d['username']\r\n            args.password = d['password']\r\n    except FileNotFoundError:\r\n        msg = 'Please provide Glassdoor credentials.\\\r\n        Credentials can be provided as a secret.txt file in the working\\\r\n        directory, or passed at the command line using the --username and\\\r\n        --password flags.'\r\n        raise Exception(msg)\r\nlogger = logging.getLogger(__name__)\r\nlogger.setLevel(logging.INFO)\r\nch = logging.StreamHandler()\r\nch.setLevel(logging.INFO)\r\nlogger.addHandler(ch)\r\nformatter = logging.Formatter(\r\n    '%(asctime)s %(levelname)s %(lineno)d\\\r\n    :%(filename)s(%(process)d) - %(message)s')\r\nch.setFormatter(formatter)\r\n\r\nlogging.getLogger('selenium').setLevel(logging.CRITICAL)\r\nlogging.getLogger('selenium').setLevel(logging.CRITICAL)\r\n\r\nmyy_dictionary = {\r\n    \"1\": 'January',\r\n    \"2\": 'February',\r\n    \"3\": 'March',\r\n    \"4\": 'April',\r\n    \"5\": 'May',\r\n    \"6\": 'June',\r\n    \"7\": 'July',\r\n    \"8\": 'August',\r\n    \"9\": 'September',\r\n    \"10\": 'October',\r\n    \"11\": 'November',\r\n    \"12\": 'December'\r\n}\r\n\r\ndef make_directory():\r\n    # # dir = os.path.join(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n    # # if not os.path.exists(dir):\r\n    # #     os.mkdir(dir)\r\n    dir_move = os.path.join(\"C:\\\\Users\\\\asha1\\\\Previous\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n    if not os.path.exists(dir_move):\r\n        os.mkdir(dir_move)\r\n\r\n# Moves old files from the same day to \"previous\" location\r\ndef old_file_move():\r\n    files = []\r\n    filespath = []\r\n    countloop = int(0)\r\n    path = os.path.join(\"C:\\\\Users\\\\asha1\\\\WebScrape\")\r\n    dst = os.path.join(\"C:\\\\Users\\\\asha1\\\\Previous\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n\r\n    for r, d, f in os.walk(path):\r\n        for file in f:\r\n            if 'glassdoor' in file or 'indeed' in file or \"coherence\" in file:\r\n                files.append(file)\r\n                filespath.append(r)\r\n\r\n    for f in files:\r\n        src = str(filespath[countloop])\r\n        shutil.move(os.path.join(src,f), os.path.join(dst,f))\r\n        countloop = countloop + 1\r\n\r\n# Webscrapes CAA indeed pages\r\ndef web_scrape_indeed():\r\n    for page in range(0, 3):\r\n        print(\"New Page\")\r\n        response = get(url_indeed[page])\r\n        html_soup = BeautifulSoup(response.text, 'html.parser')\r\n        # print(html_soup.prettify())\r\n\r\n        # Finds all the elements on the page\r\n        star_containers = html_soup.find_all('div', class_='cmp-ReviewRating-text')\r\n        review_containers = html_soup.find_all('div', class_='cmp-Review-text')\r\n        title_containers = html_soup.find_all('div', class_='cmp-Review-title')\r\n        # pros_containers = html_soup.find_all('div', class_='cmp-ReviewProsCons-prosText')\r\n        # cons_containers = html_soup.find_all('div', class_='cmp-ReviewProsCons-consText')\r\n        position_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n        general_review = html_soup.find_all('div', class_='cmp-Review-content')\r\n        # location_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n        # date_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n\r\n        #Test to see if the containers are getting the proper amount of reviews\r\n        print(len(star_containers))\r\n        print(len(review_containers))\r\n        print(len(title_containers))\r\n        print(len(position_containers))\r\n        print(len(general_review))\r\n        # print(len(cons_containers))\r\n        # print(len(location_containers))\r\n        # print(len(date_containers))\r\n\r\n        # Adds each element on the page to list in order\r\n        for i in range(0, len(title_containers) - 1):\r\n            first_star = star_containers[i].text\r\n            stars.append(first_star)\r\n\r\n            first_review_text = review_containers[i].span.span.text\r\n            description.append(first_review_text)\r\n\r\n            first_title_text = title_containers[i].text\r\n            title.append(first_title_text)\r\n\r\n            first_position_text = position_containers[i].text\r\n            position.append(first_position_text)\r\n\r\n            try:\r\n                pros_containers = general_review[i].find('div', class_='cmp-ReviewProsCons-prosText')\r\n                first_pros_text = pros_containers.span.text\r\n                pros.append(first_pros_text)\r\n            except Exception as e:\r\n                pros.append(\"N/A\")\r\n\r\n            try:\r\n                cons_containers = general_review[i].find('div', class_='cmp-ReviewProsCons-consText')\r\n                first_cons_text = cons_containers.span.text\r\n                cons.append(first_cons_text)\r\n            except Exception as e:\r\n                cons.append(\"N/A\")\r\n            # first_location_text = location_containers[i].a.text.\r\n            # location.append(first_location_text)\r\n            # #\r\n            # first_date_text = date_containers[i].a.text\r\n            # date.append(first_date_text)\r\n    # Creates dataframe for all given elements and exports to excel\r\n    test_df = pd.DataFrame({'Stars': stars, 'Title': title, 'Description': description, 'Position, Date, Location': position, \"Pros\": pros, \"Cons\": cons})\r\n                             # 'Date': date, 'Location': location})\r\n    # test_df.head(10)\r\n    # print(test_df.head(10))\r\n\r\n    # test_df.loc[:, 'Stars'] = test_df['Stars'].str[0:3]\r\n\r\n    export_csv = test_df.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeed.csv')\r\n\r\n\r\n# Emails to HRC using VBA Script in Excel Document\r\ndef email_attachments():\r\n    if os.path.exists(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\Email.xlsm\"):\r\n        xl = win32com.client.Dispatch(\"Excel.Application\")\r\n        xl.Workbooks.Open(os.path.abspath(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\Email.xlsm\"), ReadOnly=1)\r\n        xl.Application.Run(\"Send_the_Email\")\r\n        del xl\r\n    else:\r\n        print(\"Path doesn't exist\")\r\n\r\n\r\n# Creates csv file with only the text from the glassdoor webscraping\r\ndef csv_convert(path, file_name):\r\n    df = pd.read_csv(path + file_name)\r\n    saved_column_pros = df.pros\r\n    saved_column_cons = df.cons\r\n    saved_column_MainText = df.MainText\r\n    res = pd.DataFrame([], [])\r\n    res = res.append(saved_column_pros)\r\n    res = res.append(saved_column_cons)\r\n    res = res.append(saved_column_MainText)\r\n    print(Counter(\" \".join(df['pros']).split()).most_common(100))\r\n    res.to_csv(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoortext\"  + \".csv\")\r\n    print(res)\r\n    return 0\r\n\r\n\r\n# Outputs the most common words from glassdoor webscraping\r\n# Not used anymore because run time is too long and doesn't exlude punctuation\r\ndef most_common_glassdoor():\r\n    top_N = 60\r\n    df = pd.read_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoor'  + str(x.year) + str(x.month) + str(x.day) + '.csv',\r\n                     usecols=['pros', 'cons', 'MainText'])\r\n\r\n    txt = df.pros.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.cons.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.MainText.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\r\n    words = nltk.tokenize.word_tokenize(txt)\r\n    word_dist = nltk.FreqDist(words)\r\n\r\n    stopwords = nltk.corpus.stopwords.words('english')\r\n    words_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords)\r\n\r\n    rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\r\n                        columns=['Word', 'Frequency']).set_index('Word')\r\n    print(rslt)\r\n    rslt.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoorcommon'  + str(x.year) + str(x.month) + str(x.day) + '.csv')\r\n\r\n# Outputs the most common words from glassdoor webscraping\r\ndef most_common_glassdoor_text(texts):\r\n    # text_counter = collections.Counter(texts)\r\n    #     # Common = text_counter.most_common(300)\r\n    Common = Counter(chain.from_iterable(texts)).most_common(60)\r\n    print(Common)\r\n    df = pd.DataFrame(Common, columns=['Common Words in Glassdoor','Amount of Occurences'])\r\n    df.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoorcommonwords'  +  '.csv')\r\n\r\n\r\n\r\n# Outputs the most common words from indeed scraping\r\n# NOt used anymore because it takes too long and doesn't exclude punctuation\r\ndef most_common_indeed():\r\n    top_N = 60\r\n    df = pd.read_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeed'  + str(x.year) + str(x.month) + str(x.day) + '.csv',\r\n                     usecols=['Description', 'Title'])\r\n\r\n    txt = df.Description.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.Title.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\r\n    words = nltk.tokenize.word_tokenize(txt)\r\n    word_dist = nltk.FreqDist(words)\r\n\r\n    stopwords = nltk.corpus.stopwords.words('english')\r\n    words_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords)\r\n\r\n    rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\r\n                        columns=['Word', 'Frequency']).set_index('Word')\r\n    print(rslt)\r\n    rslt.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeedcommon'  + str(x.year) + str(x.month) + str(x.day) + '.csv')\r\n\r\n# Outputs the most common words from indeed scraping\r\ndef most_common_indeed_text(texts):\r\n    Common = Counter(chain.from_iterable(texts)).most_common(60)\r\n    print(Common)\r\n    df = pd.DataFrame(Common, columns=['Common Words in Indeed','Number of Occurences'])\r\n    df.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeedcommonwords'  +  '.csv')\r\n\r\n# Creates a wordcloud for the indeed text\r\ndef most_common_wordcloud_indeed(texts):\r\n    text = \"\"\r\n    for i in texts:\r\n        print (i)\r\n        text = text + str(i)\r\n    wordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"black\").generate(text)\r\n    plt.figure()\r\n    plt.imshow(wordcloud, interpolation= \"bilinear\")\r\n    plt.axis(\"off\")\r\n    # plt.show()\r\n    # wordcloud.to_file(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n    plt.savefig(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n    CAA_logo = np.array(Image.open(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_logo.png\"))\r\n    # CAA_logo = CAA_logo.reshape((CAA_logo.shape[0], CAA_logo.shape[1]), order='F')\r\n    transformed_CAA_logo = np.ndarray((CAA_logo.shape[0],CAA_logo.shape[1]), np.int32)\r\n\r\n    for i in range(len(CAA_logo)):\r\n        transformed_CAA_logo[i] = list(map(transform_format, CAA_logo[i]))\r\n    print(transformed_CAA_logo)\r\n    wc = WordCloud(background_color=\"black\", max_words=500, mask=CAA_logo, contour_width=3, contour_color=\"blue\")\r\n    wc.generate(text)\r\n    wc.to_file(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_transformed_indeed.png\")\r\n    plt.figure(figsize=[20,10])\r\n    plt.imshow(wc, interpolation=\"bilinear\")\r\n    plt.axis(\"off\")\r\n    # plt.show()\r\n    return 0\r\n\r\n# Creates a wordcloud for the glassdoor text\r\ndef most_common_wordcloud_glassdoor(texts):\r\n    # Extracts all the text information into a string variable\r\n    text = \"\"\r\n    for i in texts:\r\n        print(i)\r\n        text = text + str(i)\r\n    # Creates the wordcloud\r\n    wordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"white\").generate(text)\r\n    plt.figure()\r\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\r\n    plt.axis(\"off\")\r\n    # plt.show()\r\n    # wordcloud.to_file(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n    # Saves the wordcloud\r\n    plt.savefig(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforglassdoor.png\")\r\n\r\n    # Creates a logo in the shape of a CAA logo\r\n    # Opens the CAA logo and takes its shape\r\n    CAA_logo = np.array(Image.open(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_logo.png\"))\r\n    transformed_CAA_logo = np.ndarray((CAA_logo.shape[0], CAA_logo.shape[1]), np.int32)\r\n\r\n    # Changes the array values of the array to the value 255\r\n    for i in range(len(CAA_logo)):\r\n        transformed_CAA_logo[i] = list(map(transform_format, CAA_logo[i]))\r\n    print(transformed_CAA_logo)\r\n\r\n    # Creates the wordclud in the CAA logo\r\n    wc = WordCloud(background_color=\"black\", max_words=500, mask=CAA_logo, contour_width=3, contour_color=\"blue\")\r\n    wc.generate(text)\r\n    # Saves it to the file\r\n    wc.to_file(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_transformed_glassdoor.png\")\r\n    # Creates the dimensions\r\n    plt.figure(figsize=[20, 10])\r\n    plt.imshow(wc, interpolation=\"bilinear\")\r\n    # Makes sure that it has no axises\r\n    plt.axis(\"off\")\r\n    return 0\r\n\r\n# Transforms the values of the numpy array of the CAA logo from 0 to 255 to create the wordcloud\r\ndef transform_format(val):\r\n    return int(255)\r\n\r\n# All the functions  for the LSA analysis on the webscraped pages\r\n# Reads all the text from the indeed excel document\r\ndef load_data(path, file_name):\r\n    print(\"load_data\")\r\n    documents_list = []\r\n    titles = []\r\n    with open(os.path.join(path, file_name), \"r\", encoding='utf8', errors='ignore') as fin:\r\n        for line in fin.readlines():\r\n            text = line.strip()\r\n            documents_list.append(text)\r\n    print(len(documents_list))\r\n    titles.append(text[0:min(len(text), 250)])\r\n    return documents_list, titles\r\n\r\n# Reads all the text from the glassdoor excel document\r\ndef load_data_glassdoor(path, file_name):\r\n    print(\"load data\")\r\n    documents_list = []\r\n    titles = []\r\n    with open(os.path.join(path, file_name), \"r\", encoding='utf8', errors='ignore') as fin:\r\n        for line in fin.readlines():\r\n            text = line.strip()\r\n            documents_list.append(text)\r\n    print(len(documents_list))\r\n    titles.append(text[0:min(len(text), 250)])\r\n    return documents_list, titles\r\n\r\n# Gets rid of stop words, punctuation and makes text lower case\r\ndef preprocess_data(doc_set):\r\n    punctuations = '!()-[]{};:\"\\,<>./?@#$%^&*_~'\r\n    print(\"preprocess data\")\r\n    tokenizer = RegexpTokenizer(r'\\w+')\r\n    # create English stop words list\r\n    en_stop = set(stopwords.words('english'))\r\n    en_punctuation = set(string.punctuation)\r\n    # Create p_stemmer of class PorterStemmer\r\n    p_stemmer = PorterStemmer()\r\n    # list for tokenized documents in loop\r\n    texts = []\r\n    # loop through document list\r\n    for i in doc_set:\r\n        # clean and tokenize document string\r\n        raw = i.lower()\r\n        for x in raw:\r\n            if x in punctuations:\r\n                raw = raw.replace(x,\"\")\r\n        tokens = tokenizer.tokenize(raw)\r\n        # remove stop words from tokens\r\n        stopped_tokens = [i for i in tokens if i not in en_stop]\r\n        # stem tokens\r\n        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\r\n        # add tokens to list\r\n        texts.append(stemmed_tokens)\r\n    return texts\r\n\r\ndef negative_words():\r\n    punctuations = '!()-[]{};:\"\\,<>./?@#$%^&*_~'\r\n    tokenizer = RegexpTokenizer(r'\\w+')\r\n    en_stop = set(stopwords.words('english'))\r\n    en_punctuation = set(string.punctuation)\r\n    p_stemmer = PorterStemmer()\r\n    texts = []\r\n    negative_word_list = []\r\n    try:\r\n        with open(\"negative_words.txt\", \"r\", encoding='utf8', errors='ignore') as fin:\r\n            for line in fin.readlines():\r\n                line = line.replace(\"\\n\", \" \")\r\n                negative_word_list.append(line)\r\n    except Exception as e:\r\n        print(e)\r\n    for i in negative_word_list:\r\n        # clean and tokenize document string\r\n        raw = i.lower()\r\n        for x in raw:\r\n            if x in punctuations:\r\n                raw = raw.replace(x,\"\")\r\n        tokens = tokenizer.tokenize(raw)\r\n        # remove stop words from tokens\r\n        stopped_tokens = [i for i in tokens if i not in en_stop]\r\n        # stem tokens\r\n        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\r\n        # add tokens to list\r\n        texts.append(stemmed_tokens)\r\n    for text in texts:\r\n        for text_indiv in text:\r\n            print(text_indiv + \"TEXT_INDIVIDUAL\")\r\n    return texts\r\n\r\ndef negative_word_list_compare(negative_word_list, doc_clean):\r\n    doc_clean_neg = []\r\n    print(\"does it work\")\r\n    for i in doc_clean:\r\n        print(i)\r\n        for x in i:\r\n            print(x)\r\n            for neg_array in negative_word_list:\r\n                for neg_word in neg_array:\r\n                    if x == neg_word:\r\n                        doc_clean_neg.append(x)\r\n    return doc_clean_neg\r\n\r\ndef negative_word_common(doc_clean_neg):\r\n    Common = Counter(doc_clean_neg).most_common(30)\r\n    df = pd.DataFrame(Common, columns = [\"Most common negative words\", \"Number of Occurences of Words\"])\r\n    df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\negativewords.csv\", index=False)\r\n\r\n\r\n# Creates matrix of how often words occur and dictionary of all the words that exist\r\ndef prepare_corpus(doc_clean):\r\n    print(\"print corpus\")\r\n    \"\"\"\r\n      Input  : clean document\r\n      Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\r\n      Output : term dictionary and Document Term Matrix\r\n      \"\"\"\r\n    # Creating the term dictionary of our courpus, where every unique term is assigned an index.\r\n    dictionary = corpora.Dictionary(doc_clean)\r\n    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\r\n    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\r\n    # generate LDA model\r\n    return dictionary, doc_term_matrix\r\n\r\n# Machine learning model that creates topic modelling from the document\r\ndef create_gensim_lsa_model(doc_clean, number_of_topics, words):\r\n    \"\"\"\r\n        Input  : clean document, number of topics and number of words associated with each topic\r\n        Purpose: create LSA model using gensim\r\n        Output : return LSA model\r\n        \"\"\"\r\n    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\r\n    # generate LSA model\r\n    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)  # train model\r\n    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\r\n    return lsamodel\r\n\r\n# Computes the value of\r\ndef compute_coherence_values(dictionary, doc_term_matrix, doc_clean,  stop, start, step):\r\n\r\n    # Input : dictionary : Gensim dictionary\r\n    #         corpus: gensim corpus\r\n    #         texts: list of input texts\r\n    #         stop: max num of topics\r\n    # Purpse : Compute c_v coherence for different number of topics\r\n    # Output: model_list : List of LSA topic models\r\n    #         coherence_values : Coherence values corresponding to the lDA model with respective numbers\r\n\r\n    coherence_values = []\r\n    model_list = []\r\n    for num_topics in range(start, stop, step):\r\n        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)\r\n        model_list.append(model)\r\n        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\r\n        coherence_values.append(coherencemodel.get_coherence())\r\n    return model_list, coherence_values\r\n\r\n# Creates csv file with information on the topic modelling words and coherence scores for indeed\r\ndef LSA_indeed_model_to_csv(model_list):\r\n    # df = pd.DataFrame(data={\"Topic Modelling Indeed\":[model_list]})\r\n    # df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelindeed\"  +  \".csv\", sep=\" \", index=False)\r\n    df = pd.DataFrame.from_records(model_list)\r\n    df.columns = [\"Topic Modelling Indeed\", \"col 2\"]\r\n    df.applymap(str)\r\n    df['Topic Modelling Indeed'] = df['Topic Modelling Indeed'].astype(str)\r\n    # df['Topic Modelling Indeed'].apply(str)\r\n    # Removes coherence scores from the dataframe\r\n    print(df.dtypes)\r\n    df['Topic Modelling Indeed'] = df['Topic Modelling Indeed'].str.replace('\\d+', ' ')\r\n    df['col 2'] = df['col 2'].str.replace('\\d+', ' ')\r\n    df['col 2'] = df['col 2'].str.replace(r'[^\\w\\s]+', '')\r\n    # df.transpose()\r\n    df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelindeed\"  +  \".csv\", sep=\" \", index=False)\r\n\r\n# Creates csv file with information on the topic modelling words and coherence scores for indeed\r\ndef LSA_glassdoor_model_to_csv(model_list):\r\n    df = pd.DataFrame.from_records(model_list)\r\n    df.columns = [\"Topic Modelling Glassdoor\", \"col 2\"]\r\n    df.applymap(str)\r\n    df['Topic Modelling Glassdoor'] = df['Topic Modelling Glassdoor'].astype(str)\r\n    # Removes coherence scores from the dataframe\r\n    df['Topic Modelling Glassdoor'] = df['Topic Modelling Glassdoor'].str.replace('\\d+', ' ')\r\n    df['col 2'] = df['col 2'].str.replace('\\d+', ' ')\r\n    # Removes the punctuation from the dataframe\r\n    df['col 2'] = df['col 2'].str.replace(r'[^\\w\\s]+', '')\r\n    df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelglassdoor\"  +\".csv\", sep=\" \", index=False)\r\n\r\n# Creates LDA model specifically for indeed\r\ndef LDA_model_indeed(dictionaryy, doc_term_matrixx, doc_clean):\r\n    exclude = '!()-[]{};:\"\\,<>./?@#$%^&*_~+*'\r\n    lda_model = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrixx,\r\n                                                id2word=dictionaryy, per_word_topics=True, num_topics = 5)\r\n                                                # ,\r\n                                                # num_topics=6,\r\n                                                # random_state=100,\r\n                                                # update_every=1,\r\n                                                # chunksize=100,\r\n                                                # passes=10,\r\n                                                # alpha='auto',\r\n                                                # per_word_topics=True)\r\n    pprint(lda_model.print_topics())\r\n    doc_lda = lda_model[doc_term_matrixx]\r\n    coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_clean, dictionary=dictionaryy, coherence='c_v')\r\n    coherence_lda = coherence_model_lda.get_coherence()\r\n    print('\\nCoherence Score: ', coherence_lda)\r\n    raw = lda_model.print_topics()\r\n    # try:\r\n    #     raw = ''.join(ch for ch in x if ch not in exclude)\r\n    # except:\r\n    #     pass\r\n    df = pd.DataFrame(raw)\r\n    df.columns = [\"Topic Nummber\", \"Topic Words\"]\r\n    df.applymap(str)\r\n    df['Topic Words'] = df['Topic Words'].astype(str)\r\n    # Gets rid of all the numbers\r\n    df['Topic Words'] = df['Topic Words'].str.replace('\\d+', '')\r\n    # Gets rid of all the punctuation from the topic model\r\n    df['Topic Words'] = df['Topic Words'].str.replace(r'[^\\w\\s]+', '')\r\n    print(df.head(10))\r\n    df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\coherencescoresindeed\" + \".csv\", sep=\" \", index=False)\r\n    # mallet_path = 'C:\\\\Users\\\\asha1\\\\AppData\\\\Local\\\\Temp\\\\mallet-2.0.8.zip'  # update this path\r\n    # ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrixx, num_topics=20, id2word=dictionaryy)\r\n    #\r\n    # pprint(ldamallet.show_topics(formatted=False))\r\n    #\r\n    # coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=doc_clean, dictionary=dictionaryy,\r\n    #                                            coherence='c_v')\r\n    # coherence_ldamallet = coherence_model_ldamallet.get_coherence()\r\n    # print('\\nCoherence Score: ', coherence_ldamallet)\r\n\r\n# Creates LDA model specifically for glassdoor\r\ndef LDA_model_glassdoor(dictionaryy, doc_term_matrixx, doc_clean):\r\n    # Generates the lda model\r\n    lda_model = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrixx,\r\n                                                id2word=dictionaryy, per_word_topics=True, num_topics = 5)\r\n                                                # ,\r\n                                                # num_topics=6,\r\n                                                # random_state=100,\r\n                                                # update_every=1,\r\n                                                # chunksize=100,\r\n                                                # passes=10,\r\n                                                # alpha='auto',\r\n                                                # per_word_topics=True)\r\n    pprint(lda_model.print_topics())\r\n    doc_lda = lda_model[doc_term_matrixx]\r\n    coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_clean, dictionary=dictionaryy, coherence='c_v')\r\n    coherence_lda = coherence_model_lda.get_coherence()\r\n    print('Coherence Score:', coherence_lda)\r\n    df = pd.DataFrame(lda_model.print_topics())\r\n    # df.columns = [\"Topic Modelling Indeed\", \" \"]\r\n    # df['Coherence Score'] = df['Coherence Score'].str.replace('\\dt+', '')\r\n    # df['Topic Words'] = df['Topic Words'].replace('\\d+', '')\r\n    df.columns = [\"Topic Nummber\", \"Topic Words\"]\r\n    df.applymap(str)\r\n    # Converts the entire dataframe to an object so it can be interpreted as a string\r\n    df['Topic Words'] = df['Topic Words'].astype(str)\r\n    # Gets rid of all the numbers within the string\r\n    df['Topic Words'] = df['Topic Words'].str.replace('\\d+', '')\r\n    # Gets rid of all the punctuation within the dataframe (easier for user to read)\r\n    df['Topic Words'] = df['Topic Words'].str.replace(r'[^\\w\\s]+', '')\r\n    # Prints first 10 topics generated by the model\r\n    print(df.head(10))\r\n    # Makes the dataframe go to a csv file\r\n    df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\coherencescoresglassdoor\" + \".csv\", sep=\" \", index=False)\r\n    # mallet_path = 'C:\\\\Users\\\\asha1\\\\AppData\\\\Local\\\\Temp\\\\mallet-2.0.8.zip'  # update this path\r\n    # ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrixx, num_topics=20, id2word=dictionaryy)\r\n    #\r\n    # pprint(ldamallet.show_topics(formatted=False))\r\n    #\r\n    # coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=doc_clean, dictionary=dictionaryy,\r\n    #                                            coherence='c_v')\r\n    # coherence_ldamallet = coherence_model_ldamallet.get_coherence()\r\n    # print('\\nCoherence Score: ', coherence_ldamallet)\r\n\r\n# Plots graph of coherence scores and recommends how many topics to use for\r\n# Should not be used as helper function to determine amount of topics that should be used\r\ndef plot_graph(doc_clean, start, stop, step):\r\n    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\r\n    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start, step)\r\n\r\n#    Show graph\r\n    x = range(start, stop, step)\r\n    plt.plot(x, coherence_values)\r\n    plt.xlabel(\"Number of Topics\")\r\n    plt.ylabel(\"Coherence score\")\r\n    plt.legend(\"coherence_values\", loc='best')\r\n    plt.show()\r\n\r\n# Webscrape functions for glassdoor\r\ndef scrape(field, review, author):\r\n\r\n    def scrape_date(review):\r\n        return review.find_element_by_class_name(\"date\").text\r\n\r\n    def scrape_emp_title(review):\r\n        if 'Anonymous Employee' not in review.text:\r\n            try:\r\n                res = author.find_element_by_class_name(\r\n                    'authorJobTitle').text.split('-')[1]\r\n            except Exception:\r\n                res = np.nan\r\n        else:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_location(review):\r\n        try:\r\n            res = author.find_element_by_class_name(\r\n            'authorLocation').text\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_status(review):\r\n        try:\r\n            res = author.text.split('-')[0]\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_rev_title(review):\r\n        try:\r\n            res = review.find_element_by_class_name('summary').text\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_years(review):\r\n        try:\r\n            first_par = review.find_element_by_class_name(\r\n            'reviewBodyCell').text\r\n            res = first_par\r\n        except:\r\n            print(\"doesn't work\")\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_helpful(review):\r\n        try:\r\n            helpful = review.find_element_by_class_name('helpfulCount')\r\n            res = helpful[helpful.find('(') + 1: -1]\r\n        except Exception:\r\n            res = 0\r\n        return res\r\n\r\n    def expand_show_more(section):\r\n        try:\r\n            more_content = section.find_element_by_class_name('moreContent')\r\n            more_link = more_content.find_element_by_class_name('moreLink')\r\n            more_link.click()\r\n        except Exception:\r\n            pass\r\n\r\n    def scrape_pros(review):\r\n        try:\r\n            pros = review.find_element_by_css_selector(\"p.mt-0.mb-xsm.v2__EIReviewDetailsV2__bodyColor.v2__EIReviewDetailsV2__lineHeightLarge\")\r\n            expand_show_more(pros)\r\n            res = pros.text\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_cons(review):\r\n        try:\r\n            cons = review.find_elements_by_css_selector(\r\n                \"p.mt-0.mb-xsm.v2__EIReviewDetailsV2__bodyColor.v2__EIReviewDetailsV2__lineHeightLarge\")[1]\r\n            expand_show_more(cons)\r\n            res = cons.text\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_advice(review):\r\n        try:\r\n            advice = review.find_element_by_class_name('adviceMgmt')\r\n            expand_show_more(advice)\r\n            res = advice.text.replace('\\nShow Less', '')\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_overall_rating(review):\r\n        try:\r\n            ratings = review.find_element_by_class_name('gdStars')\r\n            overall = ratings.find_element_by_class_name(\r\n                'rating').find_element_by_class_name('value-title')\r\n            res = overall.get_attribute('title')\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def _scrape_subrating(i):\r\n        try:\r\n            ratings = review.find_element_by_class_name('gdStars')\r\n            subratings = ratings.find_element_by_class_name(\r\n                'subRatings').find_element_by_tag_name('ul')\r\n            this_one = subratings.find_elements_by_tag_name('li')[i]\r\n            res = this_one.find_element_by_class_name(\r\n                'gdBars').get_attribute('title')\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_work_life_balance(review):\r\n        return _scrape_subrating(0)\r\n\r\n    def scrape_culture_and_values(review):\r\n        return _scrape_subrating(1)\r\n\r\n    def scrape_career_opportunities(review):\r\n        return _scrape_subrating(2)\r\n\r\n    def scrape_comp_and_benefits(review):\r\n        return _scrape_subrating(3)\r\n\r\n    def scrape_senior_management(review):\r\n        return _scrape_subrating(4)\r\n\r\n    def scrape_maintext(review):\r\n        try:\r\n            maintext = review.find_element_by_class_name('mainText')\r\n            res = maintext.text\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    # All the functions for scraping within a list so that they are easier to call at once\r\n    funcs = [\r\n        scrape_date,\r\n        scrape_emp_title,\r\n        scrape_location,\r\n        scrape_status,\r\n        scrape_rev_title,\r\n        scrape_helpful,\r\n        scrape_pros,\r\n        scrape_cons,\r\n        scrape_maintext,\r\n        scrape_overall_rating,\r\n        scrape_work_life_balance,\r\n        scrape_culture_and_values,\r\n        scrape_career_opportunities,\r\n        scrape_comp_and_benefits,\r\n        scrape_senior_management\r\n    ]\r\n\r\n    # Calls all the functions for scraping and collects into a variavle for one review\r\n    fdict = dict((s, f) for (s, f) in zip(SCHEMA, funcs))\r\n\r\n    return fdict[field](review)\r\n\r\n# Calls scraping functions for glassdoor and creates dataframe\r\ndef extract_from_page():\r\n\r\n    # Extracts all the reviews from the webpages\r\n    def extract_review(review):\r\n        time.sleep(2)\r\n        author = review.find_element_by_css_selector('span.authorInfo')\r\n\r\n        res = {}\r\n        for field in SCHEMA:\r\n            res[field] = scrape(field, review, author)\r\n            time.sleep(0.1)\r\n\r\n        print(\"Extracting review\")\r\n        assert set(res.keys()) == set(SCHEMA)\r\n        return res\r\n\r\n    # Creates the pandas dataframe, and inputs the columns from the SCHEMA.py file\r\n    res = pd.DataFrame([], columns=SCHEMA)\r\n\r\n    reviews = browser.find_elements_by_class_name('empReview')\r\n    # Extracts all the reviews on the page and increases index length of array for each review scraped on page\r\n    for review in reviews:\r\n        data = extract_review(review)\r\n        res.loc[idx[0]] = data\r\n        idx[0] = idx[0] + 1\r\n        print(idx[0])\r\n        print(\"index length\")\r\n\r\n    print(\"Done extracting from page\")\r\n    #Arguments not passed for max and min date, but it would check and stop the process if not within bounds of dates passsed\r\n    if args.max_date and \\\r\n        (pd.to_datetime(res['date']).max() > args.max_date) or \\\r\n            args.min_date and \\\r\n            (pd.to_datetime(res['date']).min() < args.min_date):\r\n        date_limit_reached[0] = True\r\n\r\n    return res\r\n\r\n# Checks if there are more pages available to scrape\r\ndef more_pages():\r\n    next_ = browser.find_element_by_css_selector('li.pagination__PaginationStyle__next')\r\n    print(\"Found li tag\")\r\n    try:\r\n        next_.find_element_by_tag_name('a')\r\n        print(\"Element is found\")\r\n        return True\r\n    except selenium.common.exceptions.NoSuchElementException:\r\n        return False\r\n\r\n#Goes to next page if next page exists\r\ndef go_to_next_page():\r\n    next_ = browser.find_element_by_css_selector('li.pagination__PaginationStyle__next a')\r\n    browser.get(next_.get_attribute('href'))\r\n    page[0] = page[0] + 1\r\n\r\n\r\ndef no_reviews():\r\n    return False\r\n\r\n# Goes to initial start page for glassdoor scraping\r\ndef navigate_to_reviews():\r\n\r\n    browser.get(args.url)\r\n    time.sleep(1)\r\n\r\n    if no_reviews():\r\n        return False\r\n\r\n    print(\"Navigating to reviews\")\r\n    time.sleep(1)\r\n\r\n    return True\r\n\r\n# Signs into glassdoor account to prevent getting asked to sign up while scraping\r\ndef sign_in():\r\n    # logger.info(f'Signing in to {args.username}')\r\n\r\n    url = 'https://www.glassdoor.ca/profile/login_input.htm?userOriginHook=HEADER_SIGNIN_LINK'\r\n    browser.get(url)\r\n\r\n    email_field = browser.find_element_by_name('username')\r\n    password_field = browser.find_element_by_name('password')\r\n    submit_btn = browser.find_element_by_xpath('//button[@type=\"submit\"]')\r\n\r\n    email_field.send_keys(args.username)\r\n    password_field.send_keys(args.password)\r\n    submit_btn.click()\r\n\r\n    time.sleep(1)\r\n\r\n# Uses google chrome to webscrape\r\ndef get_browser():\r\n    chrome_options = wd.ChromeOptions()\r\n    # if args.headless:\r\n    chrome_options.add_argument('--headless')\r\n    chrome_options.add_argument('log-level=3')\r\n    browser = wd.Chrome(options=chrome_options)\r\n    return browser\r\n\r\ndef get_current_page():\r\n    paging_control = browser.find_element_by_class_name('pagingControls')\r\n    current = int(paging_control.find_element_by_xpath(\r\n        '//ul//li[contains\\\r\n        (concat(\\' \\',normalize-space(@class),\\' \\'),\\' current \\')]\\\r\n        //span[contains(concat(\\' \\',\\\r\n        normalize-space(@class),\\' \\'),\\' disabled \\')]')\r\n        .text.replace(',', ''))\r\n    return current\r\n\r\n# Not used since arguments for dates are not passed, but it would verify if the date are sorted and raise exceptions if not\r\ndef verify_date_sorting():\r\n    ascending = urllib.parse.parse_qs(\r\n        args.url)['sort.ascending'] == ['true']\r\n\r\n    if args.min_date and ascending:\r\n        raise Exception(\r\n            'min_date required reviews to be sorted DESCENDING by date.')\r\n    elif args.max_date and not ascending:\r\n        raise Exception(\r\n            'max_date requires reviews to be sorted ASCENDING by date.')\r\n\r\nbrowser = get_browser()\r\npage = [1]\r\nidx = [0]\r\ndate_limit_reached = [False]\r\n\r\n# Calls functions to webscrape glassdoor\r\ndef web_scrape_glassdoor():\r\n    res = pd.DataFrame([], columns=SCHEMA)\r\n    sign_in()\r\n\r\n    if not args.start_from_url:\r\n        reviews_exist = navigate_to_reviews()\r\n        if not reviews_exist:\r\n            return\r\n    elif args.max_date or args.min_date:\r\n        verify_date_sorting()\r\n        browser.get(args.url)\r\n        page[0] = get_current_page()\r\n        print(f'Starting from page {page[0]:,}.')\r\n        time.sleep(1)\r\n    else:\r\n        browser.get(args.url)\r\n        page[0] = get_current_page()\r\n        print(f'Starting from page {page[0]:,}.')\r\n        time.sleep(1)\r\n    time.sleep(4)\r\n    reviews_df = extract_from_page()\r\n    res = res.append(reviews_df)\r\n    count = int(0)\r\n\r\n    while more_pages() and len(res) < args.limit:\r\n        # not date_limit_reached[0]:\r\n        time.sleep(2)\r\n        go_to_next_page()\r\n        time.sleep(12)\r\n        reviews_df = extract_from_page()\r\n        res = res.append(reviews_df)\r\n        print(len(res))\r\n\r\n    res.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoor'  + '.csv', index=False, encoding='utf-8')\r\n# Addding a comment for testing purposes\r\n# Performs latent semantic analysis for scraped pages for indeed website\r\ndef LSA_indeed():\r\n    print(\"hi\")\r\n    number_of_topics = 4\r\n    words = 8\r\n    text_info, title_info = load_data('C:\\\\Users\\\\asha1\\\\WebScrape', 'indeed' + '.csv')\r\n    print(text_info)\r\n    clean_text = preprocess_data(text_info)\r\n    print(clean_text)\r\n    prep_dict, prep_matrix = prepare_corpus(clean_text)\r\n    print(prep_dict)\r\n    print(\"1\")\r\n    print(prep_matrix)\r\n    new_model = create_gensim_lsa_model(clean_text, 5, 10)\r\n    print(new_model)\r\n    new_list, num_topics = compute_coherence_values(prep_dict, prep_matrix, clean_text, 2, 10, 1)\r\n    print(new_list)\r\n    print(\"In between\")\r\n    # plot_graph(clean_text, 2, 10, 1)\r\n    # most_common_indeed()\r\n    common = most_common_indeed_text(clean_text)\r\n    print(common)\r\n    most_common_wordcloud_indeed(clean_text)\r\n    print(\"LSA model\")\r\n    print(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n    LSA_indeed_model_to_csv(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n    LDA_model_indeed(prep_dict, prep_matrix, clean_text)\r\n\r\n# Performs latent semantic analysis for scraped pages on glassdoor\r\n#  Test\r\ndef LSA_glassdoor():\r\n    number_of_topics = 5\r\n    words = 10\r\n    csv_convert('C:\\\\Users\\\\asha1\\\\WebScrape\\\\', 'glassdoor' + '.csv')\r\n    text_info, title_info = load_data_glassdoor('C:\\\\Users\\\\asha1\\\\WebScrape', 'glassdoortext' + '.csv')\r\n    print(text_info)\r\n    clean_text = preprocess_data(text_info)\r\n    print(clean_text)\r\n    prep_dict, prep_matrix = prepare_corpus(clean_text)\r\n    print(prep_dict)\r\n    print(\"1\")\r\n    print(prep_matrix)\r\n    new_model = create_gensim_lsa_model(clean_text, 5, 10)\r\n    print(new_model)\r\n    new_list, num_topics = compute_coherence_values(prep_dict, prep_matrix, clean_text, 5, 10, 1)\r\n    print(new_list)\r\n    print(new_model)\r\n    most_common_wordcloud_glassdoor(clean_text)\r\n    LSA_glassdoor_model_to_csv(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n    common = most_common_glassdoor_text(clean_text)\r\n    print(common)\r\n    LDA_model_glassdoor(prep_dict, prep_matrix, clean_text)\r\n\r\n    # plot_graph(clean_text, 2, 10, 1)\r\n\r\n\r\n# Runs the program\r\ndef run_the_program():\r\n    # Brings old files to old directory and allows space for new files to exist in current directoryis\r\n    make_directory()\r\n    # #\r\n    # # Webscrapes indeed and glassdoor respectively and saves dataframe to csv file\r\n    web_scrape_indeed()\r\n    time.sleep(3)\r\n    web_scrape_glassdoor()\r\n    #\r\n    # # Runs the latent semantic analysis\r\n    LSA_indeed()\r\n    LSA_glassdoor()\r\n\r\n    # # Emails the files to Recruitment\r\n    email_attachments()\r\n    old_file_move()\r\n\r\n#Starts the program\r\n# if __name__ == '__main__':\r\n#     run_the_program()\r\n", "47": "#!/usr/bin/env python3\n\nimport os\nfrom dotenv import load_dotenv\nimport psycopg2\n\n\n'''\ncreate/edit .env file (name has to be exactly \".env\")\nsample config:\n\nDB_NAME=\"testdb\"\nDB_USERNAME=\"test\"\nDB_PASSWORD=\"password\"\n'''\n\nclass Db:\n\n    def __init__(self):\n        # AUTHENTICATION\n        load_dotenv()\n        self.dbname = os.getenv('DB_NAME')\n        self.user = os.getenv('DB_USERNAME')\n        self.password = os.getenv('DB_PASSWORD')\n        self.host = os.getenv('HOST')\n    \n    def connect(self):\n        self.conn = psycopg2.connect(\"dbname={0} user={1} password={2} host={3}\".format(self.dbname, self.user, self.password, self.host))\n        self.cur = self.conn.cursor()\n        \n    def create_table(self, school: str):\n\n    # https://stackoverflow.com/questions/19812597/postgresql-string-escaping-settings\n    # https://stackoverflow.com/questions/41396195/what-is-the-difference-between-single-quotes-and-double-quotes-in-postgresql\n    # backticks will not work\n    # table name should use underscore, but uses double quotes in case\n        cmd = \"CREATE TABLE IF NOT EXISTS \\\"{0}\\\" ( \\\n                name    varchar(10), \\\n                department  varchar(5), \\\n                course  varchar(5), \\\n                prof    varchar(30), \\\n                difficulty  real, \\\n                size    int \\\n                );\".format(school)\n\n        self.cur.execute(cmd)\n    def execute(self, cmd):\n        self.cur.execute(cmd)\n    \n    def close(self):\n        self.conn.commit()\n        self.cur.close()\n        self.conn.close()\n\ndb = Db()\ndb.connect()\ndb.create_table(\"de_anza\")\ndb.execute(\"SELECT * FROM \\\"de_anza\\\";\")\nprint(db.cur.fetchall())\ndb.close()\n'''\nEXAMPLE\ndb = Db()\ndb.connect()\ndb.execute(\"SELECT * FROM courses;\")\nprint(db.cur.fetchall())\ndb.close()\n'''\n\n''' \nDONT DELETE THIS, THIS IS FOR REST OF MAN DOCUMENTATION FOR LATER\n'''\n\n'''\nDESIGN\nTABLE: college_name (de anza)\ncourse name (PHYS 4A)\ndepartment (PHYS)\ncourse number (4A)\ndifficulty (3)\nprof (Rick Taylor)\nsize (# of ratings)\n\nthere should be a different functions based off for writing, follow how jellyfin updates metadata\n\nfunction: (function time)\n> webscraping is much faster than getting data from rmp api (* means all data)\n\nrefresh function: overwrites/refreshes all data (webscrape * + rmp api *)\nget function: only gets new entries, doesnt overwrite current entries (webscrape * + rmp api (newdata) )\nupdate function: only updates current data (rmp api (current data) || webscrape (current data))\n\n> either update ratings, or update professor data like maybe timings in the future\n> could be a cron task, where every week webscrape data is updated, and rmp data updated only every quarter \n'''\n\n# if var char size is not enough: https://stackoverflow.com/questions/22668024/how-to-change-column-size-of-varchar-type-in-mysql\n# EOL format: https://stackoverflow.com/questions/59387001/mysql-command-in-python-yields-syntaxerror-eol-while-scanning-string-literal\n\n'''\ninsert if not exists\nhttps://www.postgresql.org/docs/current/sql-insert.html\n\n Im not sure if this is really better than official postgresql documentation (where not exists): https://stackoverflow.com/questions/5288283/sql-server-insert-if-not-exists-best-practice\n\ndo we even need this for our functions for overriding we dont need it, for updating, we should be first be comparing\n the webscrape entry to the database\n\n'get function: only gets new entries, doesnt overwrite current entries (webscrape * + rmp api (newdata) )'\nif course is not in SQL database: run api, insert into db (the check is already happening in python, no need\nfor sql to check with if not)\n\n'update function: only updates current data (rmp api (current data) || webscrape (current data))'\nUPDATE SQL function\n\non conflict will not work without a index in the table, however we are checking for a string tho\ncur.execute(\"INSERT into courses (name, department, course, prof, difficulty, size) \\\n        VALUES ('CALC 1C', 'CALC', '1C', 'RICK', 3.2, 10) \\\n        ON CONFLICT (name) DO NOTHING;\")\n'''\n\n'''\ncur.execute(\"INSERT into courses (name, department, course, prof, difficulty, size) \\\n        VALUES ('CALC 1C', 'CALC', '1C', 'RICK', 3.2, 10);\")\n        '''\n\n'''\ncur.execute(\"SELECT * FROM courses;\")\nprint(cur.fetchall())\nprint(\"DISTINCT\")\ncur.execute(\"SELECT DISTINCT ON (name) * FROM courses;\")\nprint(cur.fetchall())\n'''\n\n'''\nTEST\n#cur.execute(\"CREATE TABLE IF NOT EXISTS test (id serial PRIMARY KEY, num integer, data varchar);\")\ncur.execute(\"INSERT INTO test (num, data) VALUES (%s, %s)\", (100, \"abc'def\"))\n#cu.execute(\"INSERT INTO test (num) VALUES (3);\")\ncur.execute(\"SELECT * FROM test;\")\nprint(cur.fetchone())\n'''\n\n", "48": "from argparse import ArgumentParser\r\nimport argparse\r\nfrom bs4 import BeautifulSoup\r\nfrom collections import Counter\r\nimport csv\r\nimport datetime as dt\r\nfrom datetime import datetime\r\nimport gensim\r\nfrom gensim.utils import simple_preprocess\r\nfrom gensim.models import CoherenceModel\r\nfrom gensim.models.coherencemodel import CoherenceModel\r\nfrom gensim import corpora as corpora\r\nfrom gensim.models import LsiModel\r\nimport json\r\nimport logging\r\nimport logging.config\r\nfrom itertools import chain\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib\r\nimport nltk\r\nfrom nltk import word_tokenize\r\nfrom nltk.tokenize import RegexpTokenizer\r\nfrom nltk.corpus import stopwords\r\nfrom nltk.stem.porter import PorterStemmer\r\nimport numpy as np\r\nimport os.path\r\nimport os\r\nimport pandas as pd\r\nfrom PIL import Image\r\nimport PIL\r\nfrom pprint import pprint\r\nimport pyLDAvis\r\nimport pyLDAvis.gensim\r\nimport re\r\nimport requests\r\nfrom requests import get\r\nfrom schema import SCHEMA\r\nimport selenium\r\nfrom selenium.webdriver import Chrome\r\nfrom selenium.webdriver.common.keys import Keys\r\nfrom selenium import webdriver as wd\r\nimport shutil\r\nimport smtplib\r\nimport string\r\nfrom string import Template\r\nimport wordcloud\r\nimport time\r\nimport unittest\r\nfrom urllib.request import urlopen\r\nimport urllib\r\nimport win32com.client\r\nfrom wordcloud import WordCloud, ImageColorGenerator\r\nfrom wordcloud import STOPWORDS\r\nimport xlsxwriter\r\n\r\npunctuations = '!()-[]{};:\"\\,<>./?@#$%^&*_~'\r\n\r\n# import pywin32\r\n\r\nx = datetime.today()\r\n\r\n\r\nurl_indeed = []\r\nurl_indeed.append(\"https://ca.indeed.com/cmp/Caa/reviews\")\r\nurl_indeed.append(\"https://ca.indeed.com/cmp/Caa/reviews?start=20\")\r\nurl_indeed.append(\"https://ca.indeed.com/cmp/Caa/reviews?start=40\")\r\nurl_glassdoor = \"https://www.glassdoor.ca/Reviews/CAA-South-Central-Ontario-Reviews-E150598.htm\"\r\n\r\nstars = []\r\ntitle = []\r\ndescription = []\r\ndate = []\r\nposition = []\r\nlocation = []\r\npros = []\r\ncons = []\r\n\r\nnumber_of_topics = 5\r\nwords = 10\r\n\r\nstart = time.time()\r\n\r\nDEFAULT_URL = ('https://www.glassdoor.ca/Reviews/CAA-South-Central-Ontario-Reviews-E150598.htm')\r\n\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument('-u', '--url',\r\n                        help='URL of the company\\'s Glassdoor landing page.',\r\n                        default=DEFAULT_URL)\r\nparser.add_argument('-f', '--file', default='glassdoor_ratings.csv',\r\n                        help='Output file.')\r\nparser.add_argument('--headless', action='store_true',\r\n                    help='Run Chrome in headless mode.')\r\nparser.add_argument('--username', help='Email address used to sign in to GD.')\r\nparser.add_argument('-p', '--password', help='Password to sign in to GD.')\r\nparser.add_argument('-c', '--credentials', help='Credentials file')\r\nparser.add_argument('-l', '--limit', default=152,\r\n                        action='store', type=int, help='Max reviews to scrape')\r\nparser.add_argument('--start_from_url', action='store_true',\r\n                        help='Start scraping from the passed URL.')\r\nparser.add_argument(\r\n    '--max_date', help='Latest review date to scrape.\\\r\n    Only use this option with --start_from_url.\\\r\n    You also must have sorted Glassdoor reviews ASCENDING by date.',\r\n    type=lambda s: dt.datetime.strptime(s, \"%Y-%m-%d\"))\r\nparser.add_argument(\r\n    '--min_date', help='Earliest review date to scrape.\\\r\n    Only use this option with --start_from_url.\\\r\n    You also must have sorted Glassdoor reviews DESCENDING by date.',\r\n    type=lambda s: dt.datetime.strptime(s, \"%Y-%m-%d\"))\r\nargs = parser.parse_args()\r\n\r\nif not args.start_from_url and (args.max_date or args.min_date):\r\n    raise Exception(\r\n       'Invalid argument combination:\\\r\n        No starting url passed, but max/min date specified.'\r\n    )\r\nelif args.max_date and args.min_date:\r\n    raise Exception(\r\n          'Invalid argument combination:\\\r\n           Both min_date and max_date specified.'\r\n     )\r\n\r\nif args.credentials:\r\n    with open(args.credentials) as f:\r\n        d = json.loads(f.read())\r\n        args.username = d['username']\r\n        args.password = d['password']\r\nelse:\r\n    try:\r\n        with open('secrets.txt') as f:\r\n            d = json.loads(f.read())\r\n            args.username = d['username']\r\n            args.password = d['password']\r\n    except FileNotFoundError:\r\n        msg = 'Please provide Glassdoor credentials.\\\r\n        Credentials can be provided as a secret.txt file in the working\\\r\n        directory, or passed at the command line using the --username and\\\r\n        --password flags.'\r\n        raise Exception(msg)\r\nlogger = logging.getLogger(__name__)\r\nlogger.setLevel(logging.INFO)\r\nch = logging.StreamHandler()\r\nch.setLevel(logging.INFO)\r\nlogger.addHandler(ch)\r\nformatter = logging.Formatter(\r\n    '%(asctime)s %(levelname)s %(lineno)d\\\r\n    :%(filename)s(%(process)d) - %(message)s')\r\nch.setFormatter(formatter)\r\n\r\nlogging.getLogger('selenium').setLevel(logging.CRITICAL)\r\nlogging.getLogger('selenium').setLevel(logging.CRITICAL)\r\n\r\nmyy_dictionary = {\r\n    \"1\": 'January',\r\n    \"2\": 'February',\r\n    \"3\": 'March',\r\n    \"4\": 'April',\r\n    \"5\": 'May',\r\n    \"6\": 'June',\r\n    \"7\": 'July',\r\n    \"8\": 'August',\r\n    \"9\": 'September',\r\n    \"10\": 'October',\r\n    \"11\": 'November',\r\n    \"12\": 'December'\r\n}\r\n\r\ndef make_directory():\r\n    # # dir = os.path.join(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n    # # if not os.path.exists(dir):\r\n    # #     os.mkdir(dir)\r\n    dir_move = os.path.join(\"C:\\\\Users\\\\asha1\\\\Previous\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n    if not os.path.exists(dir_move):\r\n        os.mkdir(dir_move)\r\n\r\n# Moves old files from the same day to \"previous\" location\r\ndef old_file_move():\r\n    files = []\r\n    filespath = []\r\n    countloop = int(0)\r\n    path = os.path.join(\"C:\\\\Users\\\\asha1\\\\WebScrape\")\r\n    dst = os.path.join(\"C:\\\\Users\\\\asha1\\\\Previous\\\\\" + str(myy_dictionary[str(x.month)]) + \" \" + str(x.day) + \" \" + str(x.year))\r\n\r\n    for r, d, f in os.walk(path):\r\n        for file in f:\r\n            if 'glassdoor' in file or 'indeed' in file or \"coherence\" in file:\r\n                files.append(file)\r\n                filespath.append(r)\r\n\r\n    for f in files:\r\n        src = str(filespath[countloop])\r\n        shutil.move(os.path.join(src,f), os.path.join(dst,f))\r\n        countloop = countloop + 1\r\n\r\n# Webscrapes CAA indeed pages\r\ndef web_scrape_indeed():\r\n    for page in range(0, 3):\r\n        print(\"New Page\")\r\n        response = get(url_indeed[page])\r\n        html_soup = BeautifulSoup(response.text, 'html.parser')\r\n        # print(html_soup.prettify())\r\n\r\n        # Finds all the elements on the page\r\n        star_containers = html_soup.find_all('div', class_='cmp-ReviewRating-text')\r\n        review_containers = html_soup.find_all('div', class_='cmp-Review-text')\r\n        title_containers = html_soup.find_all('div', class_='cmp-Review-title')\r\n        # pros_containers = html_soup.find_all('div', class_='cmp-ReviewProsCons-prosText')\r\n        # cons_containers = html_soup.find_all('div', class_='cmp-ReviewProsCons-consText')\r\n        position_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n        general_review = html_soup.find_all('div', class_='cmp-Review-content')\r\n        # location_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n        # date_containers = html_soup.find_all('span', class_='cmp-ReviewAuthor')\r\n\r\n        #Test to see if the containers are getting the proper amount of reviews\r\n        print(len(star_containers))\r\n        print(len(review_containers))\r\n        print(len(title_containers))\r\n        print(len(position_containers))\r\n        print(len(general_review))\r\n        # print(len(cons_containers))\r\n        # print(len(location_containers))\r\n        # print(len(date_containers))\r\n\r\n        # Adds each element on the page to list in order\r\n        for i in range(0, len(title_containers) - 1):\r\n            first_star = star_containers[i].text\r\n            stars.append(first_star)\r\n\r\n            first_review_text = review_containers[i].span.span.text\r\n            description.append(first_review_text)\r\n\r\n            first_title_text = title_containers[i].text\r\n            title.append(first_title_text)\r\n\r\n            first_position_text = position_containers[i].text\r\n            position.append(first_position_text)\r\n\r\n            try:\r\n                pros_containers = general_review[i].find('div', class_='cmp-ReviewProsCons-prosText')\r\n                first_pros_text = pros_containers.span.text\r\n                pros.append(first_pros_text)\r\n            except Exception as e:\r\n                pros.append(\"N/A\")\r\n\r\n            try:\r\n                cons_containers = general_review[i].find('div', class_='cmp-ReviewProsCons-consText')\r\n                first_cons_text = cons_containers.span.text\r\n                cons.append(first_cons_text)\r\n            except Exception as e:\r\n                cons.append(\"N/A\")\r\n            # first_location_text = location_containers[i].a.text.\r\n            # location.append(first_location_text)\r\n            # #\r\n            # first_date_text = date_containers[i].a.text\r\n            # date.append(first_date_text)\r\n    # Creates dataframe for all given elements and exports to excel\r\n    test_df = pd.DataFrame({'Stars': stars, 'Title': title, 'Description': description, 'Position, Date, Location': position, \"Pros\": pros, \"Cons\": cons})\r\n                             # 'Date': date, 'Location': location})\r\n    # test_df.head(10)\r\n    # print(test_df.head(10))\r\n\r\n    # test_df.loc[:, 'Stars'] = test_df['Stars'].str[0:3]\r\n\r\n    export_csv = test_df.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeed.csv')\r\n\r\n\r\n# Emails to HRC using VBA Script in Excel Document\r\ndef email_attachments():\r\n    if os.path.exists(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\Email.xlsm\"):\r\n        xl = win32com.client.Dispatch(\"Excel.Application\")\r\n        xl.Workbooks.Open(os.path.abspath(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\Email.xlsm\"), ReadOnly=1)\r\n        xl.Application.Run(\"Send_the_Email\")\r\n        del xl\r\n    else:\r\n        print(\"Path doesn't exist\")\r\n\r\n\r\n# Creates csv file with only the text from the glassdoor webscraping\r\ndef csv_convert(path, file_name):\r\n    df = pd.read_csv(path + file_name)\r\n    saved_column_pros = df.pros\r\n    saved_column_cons = df.cons\r\n    saved_column_MainText = df.MainText\r\n    res = pd.DataFrame([], [])\r\n    res = res.append(saved_column_pros)\r\n    res = res.append(saved_column_cons)\r\n    res = res.append(saved_column_MainText)\r\n    print(Counter(\" \".join(df['pros']).split()).most_common(100))\r\n    res.to_csv(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoortext\"  + \".csv\")\r\n    print(res)\r\n    return 0\r\n\r\n\r\n# Outputs the most common words from glassdoor webscraping\r\n# Not used anymore because run time is too long and doesn't exlude punctuation\r\ndef most_common_glassdoor():\r\n    top_N = 60\r\n    df = pd.read_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoor'  + str(x.year) + str(x.month) + str(x.day) + '.csv',\r\n                     usecols=['pros', 'cons', 'MainText'])\r\n\r\n    txt = df.pros.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.cons.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.MainText.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\r\n    words = nltk.tokenize.word_tokenize(txt)\r\n    word_dist = nltk.FreqDist(words)\r\n\r\n    stopwords = nltk.corpus.stopwords.words('english')\r\n    words_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords)\r\n\r\n    rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\r\n                        columns=['Word', 'Frequency']).set_index('Word')\r\n    print(rslt)\r\n    rslt.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoorcommon'  + str(x.year) + str(x.month) + str(x.day) + '.csv')\r\n\r\n# Outputs the most common words from glassdoor webscraping\r\ndef most_common_glassdoor_text(texts):\r\n    # text_counter = collections.Counter(texts)\r\n    #     # Common = text_counter.most_common(300)\r\n    Common = Counter(chain.from_iterable(texts)).most_common(60)\r\n    print(Common)\r\n    df = pd.DataFrame(Common, columns=['Common Words in Glassdoor','Amount of Occurences'])\r\n    df.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoorcommonwords'  +  '.csv')\r\n\r\n\r\n\r\n# Outputs the most common words from indeed scraping\r\n# NOt used anymore because it takes too long and doesn't exclude punctuation\r\ndef most_common_indeed():\r\n    top_N = 60\r\n    df = pd.read_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeed'  + str(x.year) + str(x.month) + str(x.day) + '.csv',\r\n                     usecols=['Description', 'Title'])\r\n\r\n    txt = df.Description.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ') + df.Title.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\r\n    words = nltk.tokenize.word_tokenize(txt)\r\n    word_dist = nltk.FreqDist(words)\r\n\r\n    stopwords = nltk.corpus.stopwords.words('english')\r\n    words_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords)\r\n\r\n    rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\r\n                        columns=['Word', 'Frequency']).set_index('Word')\r\n    print(rslt)\r\n    rslt.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeedcommon'  + str(x.year) + str(x.month) + str(x.day) + '.csv')\r\n\r\n# Outputs the most common words from indeed scraping\r\ndef most_common_indeed_text(texts):\r\n    Common = Counter(chain.from_iterable(texts)).most_common(60)\r\n    print(Common)\r\n    df = pd.DataFrame(Common, columns=['Common Words in Indeed','Number of Occurences'])\r\n    df.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\indeedcommonwords'  +  '.csv')\r\n\r\n# Creates a wordcloud for the indeed text\r\ndef most_common_wordcloud_indeed(texts):\r\n    text = \"\"\r\n    for i in texts:\r\n        print (i)\r\n        text = text + str(i)\r\n    wordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"black\").generate(text)\r\n    plt.figure()\r\n    plt.imshow(wordcloud, interpolation= \"bilinear\")\r\n    plt.axis(\"off\")\r\n    # plt.show()\r\n    # wordcloud.to_file(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n    plt.savefig(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n    CAA_logo = np.array(Image.open(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_logo.png\"))\r\n    # CAA_logo = CAA_logo.reshape((CAA_logo.shape[0], CAA_logo.shape[1]), order='F')\r\n    transformed_CAA_logo = np.ndarray((CAA_logo.shape[0],CAA_logo.shape[1]), np.int32)\r\n\r\n    for i in range(len(CAA_logo)):\r\n        transformed_CAA_logo[i] = list(map(transform_format, CAA_logo[i]))\r\n    print(transformed_CAA_logo)\r\n    wc = WordCloud(background_color=\"black\", max_words=500, mask=CAA_logo, contour_width=3, contour_color=\"blue\")\r\n    wc.generate(text)\r\n    wc.to_file(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_transformed_indeed.png\")\r\n    plt.figure(figsize=[20,10])\r\n    plt.imshow(wc, interpolation=\"bilinear\")\r\n    plt.axis(\"off\")\r\n    # plt.show()\r\n    return 0\r\n\r\n# Creates a wordcloud for the glassdoor text\r\ndef most_common_wordcloud_glassdoor(texts):\r\n    # Extracts all the text information into a string variable\r\n    text = \"\"\r\n    for i in texts:\r\n        print(i)\r\n        text = text + str(i)\r\n    # Creates the wordcloud\r\n    wordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"white\").generate(text)\r\n    plt.figure()\r\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\r\n    plt.axis(\"off\")\r\n    # plt.show()\r\n    # wordcloud.to_file(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforindeed.png\")\r\n    # Saves the wordcloud\r\n    plt.savefig(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\reviewforglassdoor.png\")\r\n\r\n    # Creates a logo in the shape of a CAA logo\r\n    # Opens the CAA logo and takes its shape\r\n    CAA_logo = np.array(Image.open(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_logo.png\"))\r\n    transformed_CAA_logo = np.ndarray((CAA_logo.shape[0], CAA_logo.shape[1]), np.int32)\r\n\r\n    # Changes the array values of the array to the value 255\r\n    for i in range(len(CAA_logo)):\r\n        transformed_CAA_logo[i] = list(map(transform_format, CAA_logo[i]))\r\n    print(transformed_CAA_logo)\r\n\r\n    # Creates the wordclud in the CAA logo\r\n    wc = WordCloud(background_color=\"black\", max_words=500, mask=CAA_logo, contour_width=3, contour_color=\"blue\")\r\n    wc.generate(text)\r\n    # Saves it to the file\r\n    wc.to_file(\"C:\\\\Users\\\\asha1\\\\WebScrape\\\\CAA_transformed_glassdoor.png\")\r\n    # Creates the dimensions\r\n    plt.figure(figsize=[20, 10])\r\n    plt.imshow(wc, interpolation=\"bilinear\")\r\n    # Makes sure that it has no axises\r\n    plt.axis(\"off\")\r\n    return 0\r\n\r\n# Transforms the values of the numpy array of the CAA logo from 0 to 255 to create the wordcloud\r\ndef transform_format(val):\r\n    return int(255)\r\n\r\n# All the functions  for the LSA analysis on the webscraped pages\r\n# Reads all the text from the indeed excel document\r\ndef load_data(path, file_name):\r\n    print(\"load_data\")\r\n    documents_list = []\r\n    titles = []\r\n    with open(os.path.join(path, file_name), \"r\", encoding='utf8', errors='ignore') as fin:\r\n        for line in fin.readlines():\r\n            text = line.strip()\r\n            documents_list.append(text)\r\n    print(len(documents_list))\r\n    titles.append(text[0:min(len(text), 250)])\r\n    return documents_list, titles\r\n\r\n# Reads all the text from the glassdoor excel document\r\ndef load_data_glassdoor(path, file_name):\r\n    print(\"load data\")\r\n    documents_list = []\r\n    titles = []\r\n    with open(os.path.join(path, file_name), \"r\", encoding='utf8', errors='ignore') as fin:\r\n        for line in fin.readlines():\r\n            text = line.strip()\r\n            documents_list.append(text)\r\n    print(len(documents_list))\r\n    titles.append(text[0:min(len(text), 250)])\r\n    return documents_list, titles\r\n\r\n# Gets rid of stop words, punctuation and makes text lower case\r\ndef preprocess_data(doc_set):\r\n    punctuations = '!()-[]{};:\"\\,<>./?@#$%^&*_~'\r\n    print(\"preprocess data\")\r\n    tokenizer = RegexpTokenizer(r'\\w+')\r\n    # create English stop words list\r\n    en_stop = set(stopwords.words('english'))\r\n    en_punctuation = set(string.punctuation)\r\n    # Create p_stemmer of class PorterStemmer\r\n    p_stemmer = PorterStemmer()\r\n    # list for tokenized documents in loop\r\n    texts = []\r\n    # loop through document list\r\n    for i in doc_set:\r\n        # clean and tokenize document string\r\n        raw = i.lower()\r\n        for x in raw:\r\n            if x in punctuations:\r\n                raw = raw.replace(x,\"\")\r\n        tokens = tokenizer.tokenize(raw)\r\n        # remove stop words from tokens\r\n        stopped_tokens = [i for i in tokens if i not in en_stop]\r\n        # stem tokens\r\n        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\r\n        # add tokens to list\r\n        texts.append(stemmed_tokens)\r\n    return texts\r\n\r\ndef negative_words():\r\n    negative_word_list = []\r\n    try:\r\n        with open(\"negative_words.txt\") as f:\r\n            for line in fin.readLines():\r\n                word = text.strip()\r\n                negative_word_list.append(word)\r\n    for word in negative_word_list:\r\n        print(word)\r\n\r\ndef negative_word_compare(doc_clean):\r\n    return 0\r\n\r\n# Creates matrix of how often words occur and dictionary of all the words that exist\r\ndef prepare_corpus(doc_clean):\r\n    print(\"print corpus\")\r\n    \"\"\"\r\n      Input  : clean document\r\n      Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\r\n      Output : term dictionary and Document Term Matrix\r\n      \"\"\"\r\n    # Creating the term dictionary of our courpus, where every unique term is assigned an index.\r\n    dictionary = corpora.Dictionary(doc_clean)\r\n    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\r\n    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\r\n    # generate LDA model\r\n    return dictionary, doc_term_matrix\r\n\r\n# Machine learning model that creates topic modelling from the document\r\ndef create_gensim_lsa_model(doc_clean, number_of_topics, words):\r\n    \"\"\"\r\n        Input  : clean document, number of topics and number of words associated with each topic\r\n        Purpose: create LSA model using gensim\r\n        Output : return LSA model\r\n        \"\"\"\r\n    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\r\n    # generate LSA model\r\n    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)  # train model\r\n    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\r\n    return lsamodel\r\n\r\n# Computes the value of\r\ndef compute_coherence_values(dictionary, doc_term_matrix, doc_clean,  stop, start, step):\r\n\r\n    # Input : dictionary : Gensim dictionary\r\n    #         corpus: gensim corpus\r\n    #         texts: list of input texts\r\n    #         stop: max num of topics\r\n    # Purpse : Compute c_v coherence for different number of topics\r\n    # Output: model_list : List of LSA topic models\r\n    #         coherence_values : Coherence values corresponding to the lDA model with respective numbers\r\n\r\n    coherence_values = []\r\n    model_list = []\r\n    for num_topics in range(start, stop, step):\r\n        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)\r\n        model_list.append(model)\r\n        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\r\n        coherence_values.append(coherencemodel.get_coherence())\r\n    return model_list, coherence_values\r\n\r\n# Creates csv file with information on the topic modelling words and coherence scores for indeed\r\ndef LSA_indeed_model_to_csv(model_list):\r\n    # df = pd.DataFrame(data={\"Topic Modelling Indeed\":[model_list]})\r\n    # df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelindeed\"  +  \".csv\", sep=\" \", index=False)\r\n    df = pd.DataFrame.from_records(model_list)\r\n    df.columns = [\"Topic Modelling Indeed\", \"col 2\"]\r\n    df.applymap(str)\r\n    df['Topic Modelling Indeed'] = df['Topic Modelling Indeed'].astype(str)\r\n    # df['Topic Modelling Indeed'].apply(str)\r\n    # Removes coherence scores from the dataframe\r\n    print(df.dtypes)\r\n    df['Topic Modelling Indeed'] = df['Topic Modelling Indeed'].str.replace('\\d+', ' ')\r\n    df['col 2'] = df['col 2'].str.replace('\\d+', ' ')\r\n    df['col 2'] = df['col 2'].str.replace(r'[^\\w\\s]+', '')\r\n    # df.transpose()\r\n    df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelindeed\"  +  \".csv\", sep=\" \", index=False)\r\n\r\n# Creates csv file with information on the topic modelling words and coherence scores for indeed\r\ndef LSA_glassdoor_model_to_csv(model_list):\r\n    df = pd.DataFrame.from_records(model_list)\r\n    df.columns = [\"Topic Modelling Glassdoor\", \"col 2\"]\r\n    df.applymap(str)\r\n    df['Topic Modelling Glassdoor'] = df['Topic Modelling Glassdoor'].astype(str)\r\n    # Removes coherence scores from the dataframe\r\n    df['Topic Modelling Glassdoor'] = df['Topic Modelling Glassdoor'].str.replace('\\d+', ' ')\r\n    df['col 2'] = df['col 2'].str.replace('\\d+', ' ')\r\n    # Removes the punctuation from the dataframe\r\n    df['col 2'] = df['col 2'].str.replace(r'[^\\w\\s]+', '')\r\n    df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\TopicModelglassdoor\"  +\".csv\", sep=\" \", index=False)\r\n\r\n# Creates LDA model specifically for indeed\r\ndef LDA_model_indeed(dictionaryy, doc_term_matrixx, doc_clean):\r\n    exclude = '!()-[]{};:\"\\,<>./?@#$%^&*_~+*'\r\n    lda_model = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrixx,\r\n                                                id2word=dictionaryy, per_word_topics=True, num_topics = 5)\r\n                                                # ,\r\n                                                # num_topics=6,\r\n                                                # random_state=100,\r\n                                                # update_every=1,\r\n                                                # chunksize=100,\r\n                                                # passes=10,\r\n                                                # alpha='auto',\r\n                                                # per_word_topics=True)\r\n    pprint(lda_model.print_topics())\r\n    doc_lda = lda_model[doc_term_matrixx]\r\n    coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_clean, dictionary=dictionaryy, coherence='c_v')\r\n    coherence_lda = coherence_model_lda.get_coherence()\r\n    print('\\nCoherence Score: ', coherence_lda)\r\n    raw = lda_model.print_topics()\r\n    # try:\r\n    #     raw = ''.join(ch for ch in x if ch not in exclude)\r\n    # except:\r\n    #     pass\r\n    df = pd.DataFrame(raw)\r\n    df.columns = [\"Topic Nummber\", \"Topic Words\"]\r\n    df.applymap(str)\r\n    df['Topic Words'] = df['Topic Words'].astype(str)\r\n    # Gets rid of all the numbers\r\n    df['Topic Words'] = df['Topic Words'].str.replace('\\d+', '')\r\n    # Gets rid of all the punctuation from the topic model\r\n    df['Topic Words'] = df['Topic Words'].str.replace(r'[^\\w\\s]+', '')\r\n    print(df.head(10))\r\n    df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\coherencescoresindeed\" + \".csv\", sep=\" \", index=False)\r\n    # mallet_path = 'C:\\\\Users\\\\asha1\\\\AppData\\\\Local\\\\Temp\\\\mallet-2.0.8.zip'  # update this path\r\n    # ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrixx, num_topics=20, id2word=dictionaryy)\r\n    #\r\n    # pprint(ldamallet.show_topics(formatted=False))\r\n    #\r\n    # coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=doc_clean, dictionary=dictionaryy,\r\n    #                                            coherence='c_v')\r\n    # coherence_ldamallet = coherence_model_ldamallet.get_coherence()\r\n    # print('\\nCoherence Score: ', coherence_ldamallet)\r\n\r\n# Creates LDA model specifically for glassdoor\r\ndef LDA_model_glassdoor(dictionaryy, doc_term_matrixx, doc_clean):\r\n    # Generates the lda model\r\n    lda_model = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrixx,\r\n                                                id2word=dictionaryy, per_word_topics=True, num_topics = 5)\r\n                                                # ,\r\n                                                # num_topics=6,\r\n                                                # random_state=100,\r\n                                                # update_every=1,\r\n                                                # chunksize=100,\r\n                                                # passes=10,\r\n                                                # alpha='auto',\r\n                                                # per_word_topics=True)\r\n    pprint(lda_model.print_topics())\r\n    doc_lda = lda_model[doc_term_matrixx]\r\n    coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_clean, dictionary=dictionaryy, coherence='c_v')\r\n    coherence_lda = coherence_model_lda.get_coherence()\r\n    print('Coherence Score:', coherence_lda)\r\n    df = pd.DataFrame(lda_model.print_topics())\r\n    # df.columns = [\"Topic Modelling Indeed\", \" \"]\r\n    # df['Coherence Score'] = df['Coherence Score'].str.replace('\\dt+', '')\r\n    # df['Topic Words'] = df['Topic Words'].replace('\\d+', '')\r\n    df.columns = [\"Topic Nummber\", \"Topic Words\"]\r\n    df.applymap(str)\r\n    # Converts the entire dataframe to an object so it can be interpreted as a string\r\n    df['Topic Words'] = df['Topic Words'].astype(str)\r\n    # Gets rid of all the numbers within the string\r\n    df['Topic Words'] = df['Topic Words'].str.replace('\\d+', '')\r\n    # Gets rid of all the punctuation within the dataframe (easier for user to read)\r\n    df['Topic Words'] = df['Topic Words'].str.replace(r'[^\\w\\s]+', '')\r\n    # Prints first 10 topics generated by the model\r\n    print(df.head(10))\r\n    # Makes the dataframe go to a csv file\r\n    df.to_csv(\"C:\\\\Users\\\\asha1\\\\Webscrape\\\\coherencescoresglassdoor\" + \".csv\", sep=\" \", index=False)\r\n    # mallet_path = 'C:\\\\Users\\\\asha1\\\\AppData\\\\Local\\\\Temp\\\\mallet-2.0.8.zip'  # update this path\r\n    # ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrixx, num_topics=20, id2word=dictionaryy)\r\n    #\r\n    # pprint(ldamallet.show_topics(formatted=False))\r\n    #\r\n    # coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=doc_clean, dictionary=dictionaryy,\r\n    #                                            coherence='c_v')\r\n    # coherence_ldamallet = coherence_model_ldamallet.get_coherence()\r\n    # print('\\nCoherence Score: ', coherence_ldamallet)\r\n\r\n# Plots graph of coherence scores and recommends how many topics to use for\r\n# Should not be used as helper function to determine amount of topics that should be used\r\ndef plot_graph(doc_clean, start, stop, step):\r\n    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\r\n    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start, step)\r\n\r\n#    Show graph\r\n    x = range(start, stop, step)\r\n    plt.plot(x, coherence_values)\r\n    plt.xlabel(\"Number of Topics\")\r\n    plt.ylabel(\"Coherence score\")\r\n    plt.legend(\"coherence_values\", loc='best')\r\n    plt.show()\r\n\r\n# Webscrape functions for glassdoor\r\ndef scrape(field, review, author):\r\n\r\n    def scrape_date(review):\r\n        return review.find_element_by_class_name(\"date\").text\r\n\r\n    def scrape_emp_title(review):\r\n        if 'Anonymous Employee' not in review.text:\r\n            try:\r\n                res = author.find_element_by_class_name(\r\n                    'authorJobTitle').text.split('-')[1]\r\n            except Exception:\r\n                res = np.nan\r\n        else:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_location(review):\r\n        try:\r\n            res = author.find_element_by_class_name(\r\n            'authorLocation').text\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_status(review):\r\n        try:\r\n            res = author.text.split('-')[0]\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_rev_title(review):\r\n        try:\r\n            res = review.find_element_by_class_name('summary').text\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_years(review):\r\n        try:\r\n            first_par = review.find_element_by_class_name(\r\n            'reviewBodyCell').text\r\n            res = first_par\r\n        except:\r\n            print(\"doesn't work\")\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_helpful(review):\r\n        try:\r\n            helpful = review.find_element_by_class_name('helpfulCount')\r\n            res = helpful[helpful.find('(') + 1: -1]\r\n        except Exception:\r\n            res = 0\r\n        return res\r\n\r\n    def expand_show_more(section):\r\n        try:\r\n            more_content = section.find_element_by_class_name('moreContent')\r\n            more_link = more_content.find_element_by_class_name('moreLink')\r\n            more_link.click()\r\n        except Exception:\r\n            pass\r\n\r\n    def scrape_pros(review):\r\n        try:\r\n            pros = review.find_element_by_css_selector(\"p.mt-0.mb-xsm.v2__EIReviewDetailsV2__bodyColor.v2__EIReviewDetailsV2__lineHeightLarge\")\r\n            expand_show_more(pros)\r\n            res = pros.text\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_cons(review):\r\n        try:\r\n            cons = review.find_elements_by_css_selector(\r\n                \"p.mt-0.mb-xsm.v2__EIReviewDetailsV2__bodyColor.v2__EIReviewDetailsV2__lineHeightLarge\")[1]\r\n            expand_show_more(cons)\r\n            res = cons.text\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_advice(review):\r\n        try:\r\n            advice = review.find_element_by_class_name('adviceMgmt')\r\n            expand_show_more(advice)\r\n            res = advice.text.replace('\\nShow Less', '')\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_overall_rating(review):\r\n        try:\r\n            ratings = review.find_element_by_class_name('gdStars')\r\n            overall = ratings.find_element_by_class_name(\r\n                'rating').find_element_by_class_name('value-title')\r\n            res = overall.get_attribute('title')\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def _scrape_subrating(i):\r\n        try:\r\n            ratings = review.find_element_by_class_name('gdStars')\r\n            subratings = ratings.find_element_by_class_name(\r\n                'subRatings').find_element_by_tag_name('ul')\r\n            this_one = subratings.find_elements_by_tag_name('li')[i]\r\n            res = this_one.find_element_by_class_name(\r\n                'gdBars').get_attribute('title')\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    def scrape_work_life_balance(review):\r\n        return _scrape_subrating(0)\r\n\r\n    def scrape_culture_and_values(review):\r\n        return _scrape_subrating(1)\r\n\r\n    def scrape_career_opportunities(review):\r\n        return _scrape_subrating(2)\r\n\r\n    def scrape_comp_and_benefits(review):\r\n        return _scrape_subrating(3)\r\n\r\n    def scrape_senior_management(review):\r\n        return _scrape_subrating(4)\r\n\r\n    def scrape_maintext(review):\r\n        try:\r\n            maintext = review.find_element_by_class_name('mainText')\r\n            res = maintext.text\r\n        except Exception:\r\n            res = np.nan\r\n        return res\r\n\r\n    # All the functions for scraping within a list so that they are easier to call at once\r\n    funcs = [\r\n        scrape_date,\r\n        scrape_emp_title,\r\n        scrape_location,\r\n        scrape_status,\r\n        scrape_rev_title,\r\n        scrape_helpful,\r\n        scrape_pros,\r\n        scrape_cons,\r\n        scrape_maintext,\r\n        scrape_overall_rating,\r\n        scrape_work_life_balance,\r\n        scrape_culture_and_values,\r\n        scrape_career_opportunities,\r\n        scrape_comp_and_benefits,\r\n        scrape_senior_management\r\n    ]\r\n\r\n    # Calls all the functions for scraping and collects into a variavle for one review\r\n    fdict = dict((s, f) for (s, f) in zip(SCHEMA, funcs))\r\n\r\n    return fdict[field](review)\r\n\r\n# Calls scraping functions for glassdoor and creates dataframe\r\ndef extract_from_page():\r\n\r\n    # Extracts all the reviews from the webpages\r\n    def extract_review(review):\r\n        time.sleep(2)\r\n        author = review.find_element_by_css_selector('span.authorInfo')\r\n\r\n        res = {}\r\n        for field in SCHEMA:\r\n            res[field] = scrape(field, review, author)\r\n            time.sleep(0.1)\r\n\r\n        print(\"Extracting review\")\r\n        assert set(res.keys()) == set(SCHEMA)\r\n        return res\r\n\r\n    # Creates the pandas dataframe, and inputs the columns from the SCHEMA.py file\r\n    res = pd.DataFrame([], columns=SCHEMA)\r\n\r\n    reviews = browser.find_elements_by_class_name('empReview')\r\n    # Extracts all the reviews on the page and increases index length of array for each review scraped on page\r\n    for review in reviews:\r\n        data = extract_review(review)\r\n        res.loc[idx[0]] = data\r\n        idx[0] = idx[0] + 1\r\n        print(idx[0])\r\n        print(\"index length\")\r\n\r\n    print(\"Done extracting from page\")\r\n    #Arguments not passed for max and min date, but it would check and stop the process if not within bounds of dates passsed\r\n    if args.max_date and \\\r\n        (pd.to_datetime(res['date']).max() > args.max_date) or \\\r\n            args.min_date and \\\r\n            (pd.to_datetime(res['date']).min() < args.min_date):\r\n        date_limit_reached[0] = True\r\n\r\n    return res\r\n\r\n# Checks if there are more pages available to scrape\r\ndef more_pages():\r\n    next_ = browser.find_element_by_css_selector('li.pagination__PaginationStyle__next')\r\n    print(\"Found li tag\")\r\n    try:\r\n        next_.find_element_by_tag_name('a')\r\n        print(\"Element is found\")\r\n        return True\r\n    except selenium.common.exceptions.NoSuchElementException:\r\n        return False\r\n\r\n#Goes to next page if next page exists\r\ndef go_to_next_page():\r\n    next_ = browser.find_element_by_css_selector('li.pagination__PaginationStyle__next a')\r\n    browser.get(next_.get_attribute('href'))\r\n    page[0] = page[0] + 1\r\n\r\n\r\ndef no_reviews():\r\n    return False\r\n\r\n# Goes to initial start page for glassdoor scraping\r\ndef navigate_to_reviews():\r\n\r\n    browser.get(args.url)\r\n    time.sleep(1)\r\n\r\n    if no_reviews():\r\n        return False\r\n\r\n    print(\"Navigating to reviews\")\r\n    time.sleep(1)\r\n\r\n    return True\r\n\r\n# Signs into glassdoor account to prevent getting asked to sign up while scraping\r\ndef sign_in():\r\n    # logger.info(f'Signing in to {args.username}')\r\n\r\n    url = 'https://www.glassdoor.ca/profile/login_input.htm?userOriginHook=HEADER_SIGNIN_LINK'\r\n    browser.get(url)\r\n\r\n    email_field = browser.find_element_by_name('username')\r\n    password_field = browser.find_element_by_name('password')\r\n    submit_btn = browser.find_element_by_xpath('//button[@type=\"submit\"]')\r\n\r\n    email_field.send_keys(args.username)\r\n    password_field.send_keys(args.password)\r\n    submit_btn.click()\r\n\r\n    time.sleep(1)\r\n\r\n# Uses google chrome to webscrape\r\ndef get_browser():\r\n    chrome_options = wd.ChromeOptions()\r\n    # if args.headless:\r\n    chrome_options.add_argument('--headless')\r\n    chrome_options.add_argument('log-level=3')\r\n    browser = wd.Chrome(options=chrome_options)\r\n    return browser\r\n\r\ndef get_current_page():\r\n    paging_control = browser.find_element_by_class_name('pagingControls')\r\n    current = int(paging_control.find_element_by_xpath(\r\n        '//ul//li[contains\\\r\n        (concat(\\' \\',normalize-space(@class),\\' \\'),\\' current \\')]\\\r\n        //span[contains(concat(\\' \\',\\\r\n        normalize-space(@class),\\' \\'),\\' disabled \\')]')\r\n        .text.replace(',', ''))\r\n    return current\r\n\r\n# Not used since arguments for dates are not passed, but it would verify if the date are sorted and raise exceptions if not\r\ndef verify_date_sorting():\r\n    ascending = urllib.parse.parse_qs(\r\n        args.url)['sort.ascending'] == ['true']\r\n\r\n    if args.min_date and ascending:\r\n        raise Exception(\r\n            'min_date required reviews to be sorted DESCENDING by date.')\r\n    elif args.max_date and not ascending:\r\n        raise Exception(\r\n            'max_date requires reviews to be sorted ASCENDING by date.')\r\n\r\nbrowser = get_browser()\r\npage = [1]\r\nidx = [0]\r\ndate_limit_reached = [False]\r\n\r\n# Calls functions to webscrape glassdoor\r\ndef web_scrape_glassdoor():\r\n    res = pd.DataFrame([], columns=SCHEMA)\r\n    sign_in()\r\n\r\n    if not args.start_from_url:\r\n        reviews_exist = navigate_to_reviews()\r\n        if not reviews_exist:\r\n            return\r\n    elif args.max_date or args.min_date:\r\n        verify_date_sorting()\r\n        browser.get(args.url)\r\n        page[0] = get_current_page()\r\n        print(f'Starting from page {page[0]:,}.')\r\n        time.sleep(1)\r\n    else:\r\n        browser.get(args.url)\r\n        page[0] = get_current_page()\r\n        print(f'Starting from page {page[0]:,}.')\r\n        time.sleep(1)\r\n    time.sleep(4)\r\n    reviews_df = extract_from_page()\r\n    res = res.append(reviews_df)\r\n    count = int(0)\r\n\r\n    while more_pages() and len(res) < args.limit:\r\n        # not date_limit_reached[0]:\r\n        time.sleep(2)\r\n        go_to_next_page()\r\n        time.sleep(12)\r\n        reviews_df = extract_from_page()\r\n        res = res.append(reviews_df)\r\n        print(len(res))\r\n\r\n    res.to_csv('C:\\\\Users\\\\asha1\\\\WebScrape\\\\glassdoor'  + '.csv', index=False, encoding='utf-8')\r\n# Addding a comment for testing purposes\r\n# Performs latent semantic analysis for scraped pages on indeed\r\ndef LSA_indeed():\r\n    print(\"hi\")\r\n    number_of_topics = 4\r\n    words = 8\r\n    text_info, title_info = load_data('C:\\\\Users\\\\asha1\\\\WebScrape', 'indeed' + '.csv')\r\n    print(text_info)\r\n    clean_text = preprocess_data(text_info)\r\n    print(clean_text)\r\n    prep_dict, prep_matrix = prepare_corpus(clean_text)\r\n    print(prep_dict)\r\n    print(\"1\")\r\n    print(prep_matrix)\r\n    new_model = create_gensim_lsa_model(clean_text, 5, 10)\r\n    print(new_model)\r\n    new_list, num_topics = compute_coherence_values(prep_dict, prep_matrix, clean_text, 2, 10, 1)\r\n    print(new_list)\r\n    print(\"In between\")\r\n    # plot_graph(clean_text, 2, 10, 1)\r\n    # most_common_indeed()\r\n    common = most_common_indeed_text(clean_text)\r\n    print(common)\r\n    most_common_wordcloud_indeed(clean_text)\r\n    print(\"LSA model\")\r\n    print(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n    LSA_indeed_model_to_csv(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n    LDA_model_indeed(prep_dict, prep_matrix, clean_text)\r\n\r\n# Performs latent semantic analysis for scraped pages on glassdoor\r\ndef LSA_glassdoor():\r\n    number_of_topics = 5\r\n    words = 10\r\n    csv_convert('C:\\\\Users\\\\asha1\\\\WebScrape\\\\', 'glassdoor' + '.csv')\r\n    text_info, title_info = load_data_glassdoor('C:\\\\Users\\\\asha1\\\\WebScrape', 'glassdoortext' + '.csv')\r\n    print(text_info)\r\n    clean_text = preprocess_data(text_info)\r\n    print(clean_text)\r\n    prep_dict, prep_matrix = prepare_corpus(clean_text)\r\n    print(prep_dict)\r\n    print(\"1\")\r\n    print(prep_matrix)\r\n    new_model = create_gensim_lsa_model(clean_text, 5, 10)\r\n    print(new_model)\r\n    new_list, num_topics = compute_coherence_values(prep_dict, prep_matrix, clean_text, 5, 10, 1)\r\n    print(new_list)\r\n    print(new_model)\r\n    most_common_wordcloud_glassdoor(clean_text)\r\n    LSA_glassdoor_model_to_csv(new_model.print_topics(num_topics=number_of_topics, num_words=words))\r\n    common = most_common_glassdoor_text(clean_text)\r\n    print(common)\r\n    LDA_model_glassdoor(prep_dict, prep_matrix, clean_text)\r\n\r\n    # plot_graph(clean_text, 2, 10, 1)\r\n\r\n\r\n# Runs the program\r\ndef run_the_program():\r\n    # Brings old files to old directory and allows space for new files to exist in current directoryis\r\n    make_directory()\r\n    # #\r\n    # # Webscrapes indeed and glassdoor respectively and saves dataframe to csv file\r\n    web_scrape_indeed()\r\n    time.sleep(3)\r\n    web_scrape_glassdoor()\r\n    #\r\n    # # Runs the latent semantic analysis\r\n    LSA_indeed()\r\n    LSA_glassdoor()\r\n\r\n    # # Emails the files to Recruitment\r\n    email_attachments()\r\n    old_file_move()\r\n\r\n#Starts the program\r\n# if __name__ == '__main__':\r\n#     run_the_program()\r\n", "49": "from tkinter import *\nimport socket, platform, subprocess, os, sys\nimport requests, bs4\n\n#functions\ndef get_ip():\n\te.delete(0, END)\n\tx = socket.gethostbyname(socket.gethostname())\n\te.insert(0, str(x))\n\treturn\n\ndef get_telnet():\n\tports = [80, 443]\n\tx = os.system(\"telnet \" + str(\"www.icbc.com.cn\") + \" \" + str(ports[0]))\n\te.insert(0, str(x))\n\n\ndef get_hostname():\n\te.delete(0, END)\n\tx = socket.gethostname()\n\te.insert(0, str(x))\n\ndef get_ping():\n\turl = \"www.icbc.com.cn\"\n\tx = os.system(\"ping \" + url )\n\n\ndef get_trace():\n\turl = \"www.icbc.com.cn\"\n\tx = os.system(\"tracert \" + url)\n\n\ndef webscrape():\n\tres = requests.get('http://www.icbc.com.cn')\n\tsoup = bs4.BeautifulSoup(res.content, 'html.parser')\n\n\theaders = res.headers\n\tstatus = res.status_code\n\tx = soup.find(class_=\"main\")\n\n\tif len(headers) == 3 and status == 200:\n\t\ttext = x.get_text()\n\t\tprint(\"VPN = down, Bank = Up\")\n\t\tprint(text)\n\telif len(headers) == 3 and status != 200:\n\t\ttext = x.get_text()\n\t\tprint(\"VPN = down, Bank = Down\")\n\t\tprint(text)\n\telif len(headers) > 3 and status != 200:\n\t\tprint(\"VPN = Up, Bank = Down\")\n\telse:\n\t\tprint(\"VPN = Up, Bank = Up\")\n\n\n\t\t\n#create a root terminal\nroot = Tk()\nroot.title(\"network checker\")\nroot.geometry(\"500x500\") #You want the size of the app to be 500x500\nroot.resizable(0, 0) #Don't allow resizing in the x or y direction\n\n#create and use input\ne = Entry(root, width=100, borderwidth=5)\ne.grid(row=0, column=0, columnspan=3, padx=10, pady=10)\n\n\n    \n\n#create a widget\nlabel_ipadd = Button(root, text=\"Get IP\", padx=60, pady=20, command=get_ip)\nlabel_telnet = Button(root, text=\"telnet\", padx=60, pady=20, command=get_telnet)\nlabel_trace = Button(root, text=\"tracer\", padx=60, pady=20, command= get_trace)\nlabel_hostname = Button(root, text=\"hostname\", padx=60, pady=20, command=get_hostname)\nlabel_ping = Button(root, text=\"ping\", padx=60, pady=20, command= get_ping)\nlabel_webscrape = Button(root, text=\"webscrape\", padx=60, pady=20, command= webscrape)\n\n#put the widget on the screen / root terminal\nlabel_ipadd.grid(row=1, column=0)\nlabel_telnet.grid(row=1, column=1)\nlabel_trace.grid(row=2, column=0)\nlabel_hostname.grid(row=2, column=1)\nlabel_ping.grid(row=3, column=0)\nlabel_webscrape.grid(row=3, column=1)\n\n\nroot.mainloop()", "50": "from .webscrape import webscrape\r\ndef setup(bot)\r\n    bot.add_cog(webscrape())\r\n", "51": "import requests\nfrom bs4 import BeautifulSoup\nimport codecs\n\nurl = \"https://en.wikipedia.org/wiki/Pennsylvania_State_University\"\n\nresponse = requests.get(url)\n\nsoup = BeautifulSoup(response.content, 'html.parser')\ninfoBox = soup.find(\"table\", class_=\"infobox vcard\")\n\nwebScrape = {\"Univeristy\": \"The Pennsylvania State University\"}\nwantedInfo = [\"Motto\", \"Type\", \"Established\", \"Academic affiliations\",\n            \"Endowment\", \"Budget\", \"President\", \"Provost\", \n            \"Academic staff\", \"Students\", \"Undergraduates\", \n            \"Postgraduates\", \"Location\", \"Campus\", \"Newspaper\", \n            \"Colors\", \"Nickname\", \"Sporting affiliations\", \"Mascot\", \"Website\"]\n    \n#Get all of the data inside info box\nfor tr in infoBox.find_all(\"tr\"):\n    if len(tr.findChildren(\"th\", recursive=False)) > 0 and \\\n        len(tr.findChildren(\"td\", recursive=False)) > 0:\n            \n        #Grab table header and table data\n        header = tr.findChildren(\"th\", recursive=False)[0]\n        data = tr.findChildren(\"td\", recursive=False)[0]\n\n        #Add to dictionary if not in it already\n        if header.get_text() not in webScrape and header.get_text() in wantedInfo:\n            #Decompose unwanted tags\n            while data(\"sup\"):\n                data.find(\"sup\").decompose()\n            while data(\"span\") and header.get_text() != \"Website\":\n                data.find(\"span\").decompose()\n            webScrape[header.get_text()] = data.get_text()\n    \n#Writing to file\nwith codecs.open(\"webScrape.txt\", \"w\", encoding=\"utf-8\") as output_data:\n    for key in webScrape.keys():\n        output_data.write(\"{}: {}\\n\".format(key, webScrape[key]))", "52": "from fuzzywuzzy import fuzz\nimport numpy as np\nimport pandas as pd\nimport sqlite3 as sql\nimport sys\n\nif len(sys.argv) < 3:\n    print('Usage: python merge_coordinates.py [pris_link] [webscrape_link]')\n\n\ndef cursor(file_name):\n    \"\"\" Connects and returns a cursor to an sqlite output file\n\n    Parameters\n    ----------\n    file_name: str\n        name of the sqlite file\n\n    Returns\n    -------\n    sqlite cursor\n    \"\"\"\n    con = sql.connect(file_name)\n    con.row_factory = sql.Row\n    return con.cursor()\n\n\ndef import_pris(pris_link):\n    \"\"\" Opens pris_csv using Pandas. Adds Latitude and Longitude\n    columns\n\n    Parameters\n    ----------\n    pris_link: str\n        path to reactors_pris_2016.original.csv file\n\n    Returns\n    -------\n    pris: pd.Dataframe\n        pris database\n    \"\"\"\n    pris = pd.read_csv(pris_link,\n                       delimiter=',',\n                       encoding='iso-8859-1'\n                       )\n    pris.insert(13, 'Latitude', np.nan)\n    pris.insert(14, 'Longitude', np.nan)\n    pris = pris.replace(np.nan, '')\n    return pris\n\n\ndef import_webscrape_data(scrape_link):\n    \"\"\" Returns sqlite content of webscrape by performing an\n    sqlite query\n\n    Parameters\n    ----------\n    scrape_link: str\n        path to webscrape.sqlite file\n\n    Returns\n    -------\n    coords: sqlite cursor\n        sqlite cursor containing webscrape data\n    \"\"\"\n    cur = cursor(scrape_link)\n    coords = cur.execute(\"SELECT name, long, lat FROM reactors_coordinates\")\n    return coords\n\n\ndef edge_cases():\n    \"\"\" Returns a dictionary of edge cases that fuzzywuzzy is\n    unable to catch. This could be because PRIS database stores\n    reactor names and Webscrape database fetches power plant names,\n    or because PRIS reactor names are abbreviated.\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n    others: dict\n        dictionary of edge cases with \"key=pris_reactor_name, and\n        value=webscrape_plant_name\"\n    \"\"\"\n    pris_edge_cases = {'OHI-': '\u014ci',\n              'ASCO-': 'Asc\u00f3',\n              'ROVNO-': 'Rivne',\n              'SHIN-KORI-': 'Kori',\n              'ANO-': 'Arkansas One',\n              'HANBIT-': 'Yeonggwang',\n              'FERMI-': 'Enrico Fermi',\n              'BALTIC-': 'Kaliningrad',\n              'COOK-': 'Donald C. Cook',\n              'HATCH-': 'Edwin I. Hatch',\n              'HARRIS-': 'Shearon Harris',\n              'SHIN-WOLSONG-': 'Wolseong',\n              'ST. ALBAN-': 'Saint-Alban',\n              'LASALLE-': 'LaSalle County',\n              'SUMMER-': 'Virgil C. Summer',\n              'FARLEY-': 'Joseph M. Farley',\n              'ST. LAURENT ': 'Saint-Laurent',\n              'HADDAM NECK': 'Connecticut Yankee',\n              'HIGASHI DORI-1 (TOHOKU)': 'Higashid\u014dri',\n              }\n    return pris_edge_cases\n\n\ndef sanitize_webscrape_name(name):\n    \"\"\" Sanitizes webscrape powerplant names by removing unwanted\n    strings (listed in blacklist), applying lower case, and deleting\n    trailing whitespace.\n\n    Parameters\n    ----------\n    name: str\n        webscrape plant name\n\n    Returns\n    -------\n    name: str\n        sanitized name for use with fuzzywuzzy\n    \"\"\"\n    blacklist = ['nuclear', 'power',\n                 'plant', 'generating',\n                 'station', 'reactor', 'atomic',\n                 'energy', 'center', 'electric']\n    name = name.lower()\n    for blacklisted in blacklist:\n        name = name.replace(blacklisted, '')\n    name = name.strip()\n    name = ' '.join(name.split())\n    return name\n\n\ndef merge_coordinates(pris_link, scrape_link):\n    \"\"\" Merges webscrape data with pris data performed by string\n    comparison of reactor names from pris and webscrape. Returns\n    updated pris database with coordinates.\n\n    Parameters\n    ----------\n    pris_link: str\n        path to reactors_pris_2016.original.csv file\n    scrape_link: str\n        path to webscrape.sqlite file\n\n    Returns\n    -------\n    pris: pd.DataFrame\n        updated PRIS database with latitude and longitude info\n    \"\"\"\n    others = edge_cases()\n    pris = import_pris(pris_link)\n    coords = import_webscrape_data(scrape_link)\n    for web in coords:\n        for idx, prs in pris.iterrows():\n            webscrape_name = sanitize_webscrape_name(web['name'])\n            pris_name = prs[1].lower()\n            if fuzz.ratio(webscrape_name, pris_name) > 64:\n                prs[13] = web['lat']\n                prs[14] = web['long']\n            else:\n                for other in others.keys():\n                    edge_case_key = other.lower()\n                    edge_case_value = others[other].lower()\n                    if (fuzz.ratio(pris_name, edge_case_key) > 80 and\n                            fuzz.ratio(webscrape_name, edge_case_value) > 75):\n                        prs[13] = web['lat']\n                        prs[14] = web['long']\n    return pris\n\n\ndef save_output(pris):\n    \"\"\" Saves updated PRIS database as 'reactors_pris_2016.csv'\n\n    Parameters\n    ----------\n    pris: pd.DataFrame\n        updated PRIS database with latitude and longitude info\n\n    Returns\n    -------\n\n    \"\"\"\n    pris.to_csv('reactors_pris_2016.csv',\n                index=False,\n                sep=',',\n                )\n\n\ndef main(pris_link, scrape_link):\n    \"\"\" Calls all required functions to merge PRIS and webscrape\n\n    Parameters\n    ----------\n    pris_link: str\n        path to reactors_pris_2016.original.csv file\n    scrape_link: str\n        path to webscrape.sqlite file\n\n    Returns\n    -------\n\n    \"\"\"\n    pris = merge_coordinates(pris_link, scrape_link)\n    save_output(pris)\n\n\nif __name__ == \"__main__\":\n    main(sys.argv[1], sys.argv[2])\n", "53": "from urllib import response\nfrom time import sleep\n\nfrom flask import Flask, jsonify, abort, request, make_response, url_for, session, flash\nfrom flask import render_template, redirect, g\nfrom flask_session import Session\nfrom src.paperTrade.webScraper import Webscrape\nimport pymysql\nfrom src.paperTrade.DTO.profileDTO import Profile\nfrom src.paperTrade.DTO.userDTO import User\nfrom src.paperTrade.DTO.coinDTO import CoinDTO\nimport os\nimport time\nimport datetime\nimport json\n# import MySQLdb\n\n# from flask_mysqldb import MySQL\n\napp = Flask(__name__, static_url_path=\"\")\nsess = Session()\nDB_USERNAME = 'root'\nDB_PASSWORD = 'password'\nDB_NAME = 'paperTrade'\n\np = Profile\nusers = []\nglobal etherium\nglobal bitcoin\nglobal cat\nglobal book\nglobal mv\nglobal cel\nglobal dodge\nglobal Tether\nglobal sand\ndef setProfile( Profile):\n    p.money = Profile.money\n    p.id = Profile.id\n    p.totalcoins = Profile.totalcoins\n\n@app.route('/')\ndef login():\n    return render_template('login.html')\n\n@app.route('/newPassword', methods=['GET', 'POST'])\ndef newPass():\n    newPass=request.form['newpassword']\n    conn = pymysql.connect(host=\"localhost\",\n                           user=DB_USERNAME,\n                           passwd=DB_PASSWORD,\n                           db=DB_NAME,\n                           port=3306)\n    cursor = conn.cursor(pymysql.cursors.DictCursor)\n    sql = \"UPDATE `users` set password =  (`password`) where id = (`id`) VALUES (%s, %s)\"\n    try:\n        cursor.execute(sql,(newPass,p.id))\n\n        conn.commit()\n    except:\n        conn.rollback()\n    conn.close()\n    return render_template('login.html')\n\n\n@app.route('/return', methods=['GET', 'POST'])\ndef returnProfile():\n    id = p.id\n    conn = pymysql.connect(host=\"localhost\",\n                           user=DB_USERNAME,\n                           passwd=DB_PASSWORD,\n                           db=DB_NAME,\n                           port=3306)\n    cursor = conn.cursor(pymysql.cursors.DictCursor)\n\n    cursor.execute('SELECT * FROM profile WHERE id = %s ', id)\n\n    account = cursor.fetchone()\n    profile = Profile(account['id'], account['money'], account['totalcoins'])\n    g.profile = profile\n    cursor.execute('SELECT * FROM users WHERE id = %s ', id)\n\n    account1 = cursor.fetchone()\n    user = User(account1['id'], account1['email'], account1['password'])\n    g.user = user\n\n    return render_template('profile.html')\n\n@app.route('/profile', methods=['GET', 'POST'])\ndef profile():\n    id = p.id\n    conn = pymysql.connect(host=\"localhost\",\n                           user=DB_USERNAME,\n                           passwd=DB_PASSWORD,\n                           db=DB_NAME,\n                           port=3306)\n    cursor = conn.cursor(pymysql.cursors.DictCursor)\n\n    cursor.execute('SELECT * FROM profile WHERE id = %s ', id)\n\n    account = cursor.fetchone()\n    profile = Profile(account['id'], account['money'], account['totalcoins'])\n    g.profile = profile\n    cursor.execute('SELECT * FROM users WHERE id = %s ', id)\n\n    account1 = cursor.fetchone()\n    user = User(account1['id'], account1['email'], account1['password'])\n    g.user = user\n\n    return render_template('profile.html')\n\n@app.route('/home')\ndef homePage():\n    global etherium\n    global bitcoin\n    global cat\n    global book\n    global mv\n    global cel\n    global dodge\n    global Tether\n    global sand\n\n    id = p.id\n    conn = pymysql.connect(host=\"localhost\",\n                           user=DB_USERNAME,\n                           passwd=DB_PASSWORD,\n                           db=DB_NAME,\n                           port=3306)\n    cursor = conn.cursor(pymysql.cursors.DictCursor)\n\n    cursor.execute('SELECT * FROM profile WHERE id = %s ', id)\n\n    account = cursor.fetchone()\n    cursor.execute('SELECT * FROM users WHERE id = %s ', id)\n\n    account1 = cursor.fetchone()\n    profile = Profile(account['id'], account['money'], account['totalcoins'])\n    g.profile = profile\n    email2 = \"\"\n    for i in range(0, len(account1['email'])):\n        if (account1['email'][i] == '@'):\n            email2 = account1['email'][:i]\n    user = User(account['id'], email2, \"hd\")\n    g.user = user\n    coinList = []\n    Coin = bitcoinMonitor()\n    time.sleep(2.5)\n    Coin1 = EteriumMonitor()\n    time.sleep(2.5)\n\n    Coin2 = DodgecoinMonitor()\n    time.sleep(2.5)\n\n    Coin3 = TetherMonitor()\n    time.sleep(2.5)\n\n    Coin4 = CatGirlMonitor()\n    time.sleep(2.5)\n\n    Coin5 = CelsiusMonitor()\n    time.sleep(2.5)\n\n    Coin6 = BitbookMonitor()\n    time.sleep(2.5)\n\n    Coin7 = SandboxMonitor()\n    time.sleep(2.5)\n\n    Coin8 = M7v2Monitor()\n    time.sleep(2.5)\n\n    Coin.type = typeTrim(Coin.type)\n    Coin1.type = typeTrim(Coin1.type)\n    Coin2.type = typeTrim(Coin2.type)\n    Coin3.type = typeTrim(Coin3.type)\n    Coin4.type = typeTrim(Coin4.type)\n    Coin5.type = typeTrim(Coin5.type)\n    Coin6.type = \"BitBook\"\n    Coin7.type = \"The Sandbox\"\n    Coin8.type = \"M7v2\"\n    sand = Coin7.price\n    book = Coin6.price\n    mv = Coin8.price\n    bitcoin = Coin.price\n    etherium = Coin1.price\n    dodge = Coin2.price\n    Tether = Coin3.price\n    cat = Coin4.price\n    cel = Coin5.price\n\n    coinList.append(Coin)\n    coinList.append(Coin1)\n    coinList.append(Coin2)\n    coinList.append(Coin3)\n    coinList.append(Coin4)\n    coinList.append(Coin5)\n    coinList.append(Coin6)\n    coinList.append(Coin7)\n    coinList.append(Coin8)\n    print(\"here\")\n\n    return render_template('home.html')\n\n@app.after_request\ndef after_request(response):\n    response.headers[\"Cache-Control\"] = \"no-cache, no-store, must-revalidate\"\n    return response\n\n@app.route('/purchase', methods=['GET', 'POST'])\ndef purchase():\n\n    if request.method == 'POST':\n        id = p.id\n        money = 0\n        purchaseTotal = request.form['purchaseAmt']\n        cryptoCoin = request.form['coin']\n        conn = pymysql.connect(host=\"localhost\",\n                               user=DB_USERNAME,\n                               passwd=DB_PASSWORD,\n                               db=DB_NAME,\n                               port=3306)\n        cursor = conn.cursor(pymysql.cursors.DictCursor)\n        cursor.execute('SELECT * FROM profile WHERE id = %s ', id)\n        account = cursor.fetchone()\n        profile = Profile(account['id'], account['money'], account['totalcoins'])\n        purchaseTotal = float(purchaseTotal)\n        profile.totalcoins = float(profile.totalcoins)\n        newCoins = float(purchaseTotal+profile.totalcoins)\n        if cryptoCoin == \"Bitcoin\":\n            money = bitcoin\n        if cryptoCoin == \"Ethereum\":\n            money = etherium\n        if cryptoCoin == \"Dodgecoin\":\n            money = dodge\n        if cryptoCoin == \"Tether\":\n            money = Tether\n        if cryptoCoin == \"CatGirl\":\n            money = cat\n        if cryptoCoin == \"Celsius\":\n            money = cel\n        if cryptoCoin == \"Bitbook\":\n            money = book\n        if cryptoCoin == \"Sandbox\":\n            money = sand\n        if cryptoCoin == \"M7v2\":\n            money = mv\n        z = money.replace(\"$\", \"\")\n        m = z.replace(\",\",\"\")\n        moneySpent = float(m)\n        moneySpent = moneySpent * purchaseTotal\n\n        if int(profile.money) - float(moneySpent) < 0:\n            return render_template('Error.html')\n        if float(moneySpent) > int(profile.money):\n            return render_template('Error.html')\n        else:\n            profile.money = float(profile.money - moneySpent)\n            cursor.execute('UPDATE profile set totalcoins = %s where id = %s', (newCoins, id))\n            conn.commit()\n            sql = \"INSERT INTO `crypto` (`id`, `cost`, `type`) VALUES (%s, %s, %s)\"\n\n            cursor.execute(sql, (id, moneySpent, cryptoCoin))\n\n            conn.commit()\n            cursor.execute('UPDATE profile set money = %s where id = %s', (profile.money, id))\n            conn.commit()\n\n\n            cursor.execute('SELECT * FROM users WHERE id = %s ', id)\n            account1 = cursor.fetchone()\n            user = User(account1['id'], account1['email'], account1['password'])\n            profile = Profile(account['id'], account['money'], account['totalcoins'])\n            user = User(id, user.email, user.password)\n            email2 = \"\"\n            for i in range(0, len(user.email)):\n                if (user.email[i] == '@'):\n                    email2 = user.email[:i]\n\n            user.email = email2\n            g.user = user\n            g.profile = profile\n            return render_template('success.html')\n\n\n@app.route('/login', methods=['GET', 'POST'])\ndef home():\n    global etherium\n    global bitcoin\n    global cat\n    global book\n    global mv\n    global cel\n    global dodge\n    global Tether\n    global sand\n    email = request.form['email']\n    password = request.form['password']\n    conn = pymysql.connect(host=\"localhost\",\n                           user=DB_USERNAME,\n                           passwd=DB_PASSWORD,\n                           db=DB_NAME,\n                           port=3306)\n    cursor = conn.cursor(pymysql.cursors.DictCursor)\n\n    cursor.execute('SELECT * FROM users WHERE email = %s AND password = %s', (email, password,))\n    acc = cursor.fetchone()\n    if request.method == 'POST' and acc :\n        g.profile = p\n        email = request.form['email']\n        password = request.form['password']\n        user = User(0, email, password)\n        email2 = \"\"\n        for i in range(0, len(user.email)):\n            if (user.email[i] == '@'):\n                email2 = user.email[:i]\n\n        user.email = email2\n        g.user = user\n\n        conn = pymysql.connect(host=\"localhost\",\n                               user=DB_USERNAME,\n                               passwd=DB_PASSWORD,\n                               db=DB_NAME,\n                               port=3306)\n        cursor = conn.cursor(pymysql.cursors.DictCursor)\n\n        cursor.execute('SELECT * FROM users WHERE email = %s AND password = %s', (email, password,))\n\n        account = cursor.fetchone()\n        cursor.execute('SELECT * FROM profile WHERE id = %s ', (account['id']))\n\n        account1 = cursor.fetchone()\n        prof = Profile(account1['id'], account1['money'], account1['totalcoins'])\n        g.profile = prof\n        setProfile(prof)\n        coinList = []\n        Coin = bitcoinMonitor()\n        time.sleep(2.5)\n        Coin1 = EteriumMonitor()\n        time.sleep(2.5)\n\n\n        Coin2 = DodgecoinMonitor()\n        time.sleep(2.5)\n\n        Coin3 = TetherMonitor()\n        time.sleep(2.5)\n\n        Coin4 = CatGirlMonitor()\n        time.sleep(2.5)\n\n        Coin5 = CelsiusMonitor()\n        time.sleep(2.5)\n\n        Coin6 = BitbookMonitor()\n        time.sleep(2.5)\n\n        Coin7 = SandboxMonitor()\n        time.sleep(2.5)\n\n        Coin8 = M7v2Monitor()\n        time.sleep(2.5)\n\n\n        Coin.type = typeTrim(Coin.type)\n        Coin1.type = typeTrim(Coin1.type)\n        Coin2.type = typeTrim(Coin2.type)\n        Coin3.type = typeTrim(Coin3.type)\n        Coin4.type = typeTrim(Coin4.type)\n        Coin5.type = typeTrim(Coin5.type)\n        Coin6.type = \"BitBook\"\n        Coin7.type = \"The Sandbox\"\n        Coin8.type = \"M7v2\"\n        sand = Coin7.price\n        book = Coin6.price\n        mv = Coin8.price\n        bitcoin = Coin.price\n        etherium = Coin1.price\n        dodge = Coin2.price\n        Tether = Coin3.price\n        cat = Coin4.price\n        cel = Coin5.price\n\n\n\n\n\n\n        coinList.append(Coin)\n        coinList.append(Coin1)\n        coinList.append(Coin2)\n        coinList.append(Coin3)\n        coinList.append(Coin4)\n        coinList.append(Coin5)\n        coinList.append(Coin6)\n        coinList.append(Coin7)\n        coinList.append(Coin8)\n\n\n        if account:\n            session['loggedin'] = True\n            session['email'] = account['email']\n            return render_template(\"home.html\", len=len(coinList), coinList=coinList)\n        else:\n            msg = 'Incorrect username/password!'\n            return render_template('login.html', msg=msg)\n    else:\n        return render_template('login.html', msg='')\n\ndef priceBitcoin():\n    w = Webscrape()\n    price = w.priceBitcoin()\n    return price\ndef priceEthe():\n    w = Webscrape()\n    price = w.priceEterium()\n    return price\ndef priceCat():\n    w = Webscrape()\n    price = w.priceCatGirl()\n    return price\ndef priceCel():\n    w = Webscrape()\n    price = w.priceCelsius()\n    return price\ndef priceMv():\n    w = Webscrape()\n    price = w.priceM7v2()\n    return price\ndef priceThe():\n    w = Webscrape()\n    price = w.priceTether()\n    return price\ndef priceSand():\n    w = Webscrape()\n    price = w.priceSandbox()\n    return price\ndef priceBook():\n    w = Webscrape()\n    price = w.priceBitbook()\n    return price\n\ndef priceDodge():\n    w = Webscrape(0,\"none\",\"none\",0,\"none\")\n    price = w.priceDodgecoin()\n    return price\ndef bitcoinMonitor():\n    w = Webscrape(0,\"none\",\"none\",0,\"none\")\n    x = w.priceTrackBitcoin()\n    coin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n    print(\"here\")\n    return coin\ndef EteriumMonitor():\n    w = Webscrape(0,\"none\",\"none\",0,\"none\")\n    x = w.priceTrackEthereum()\n    coin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n    return coin\ndef DodgecoinMonitor():\n    w = Webscrape(0,\"none\",\"none\",0,\"none\")\n    x = w.priceTrackDodgecoin()\n    coin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n    return coin\ndef TetherMonitor():\n    w = Webscrape(0,\"none\",\"none\",0,\"none\")\n    x = w.priceTrackTether()\n    coin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n    return coin\ndef CatGirlMonitor():\n    w = Webscrape(0,\"none\",\"none\",0,\"none\")\n    x = w.priceTrackCatGirl()\n    coin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n    return coin\ndef CelsiusMonitor():\n    w = Webscrape(0,\"none\",\"none\",0,\"none\")\n    x = w.priceTrackCelsius()\n    coin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n    return coin\ndef BitbookMonitor():\n    w = Webscrape(0,\"none\",\"none\",0,\"none\")\n    x = w.priceTrackBitbook()\n    coin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n    return coin\ndef SandboxMonitor():\n    w = Webscrape(0,\"none\",\"none\",0,\"none\")\n    x = w.priceTrackSandbox()\n    coin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n    return coin\ndef M7v2Monitor():\n    w = Webscrape(0,\"none\",\"none\",0,\"none\")\n    x = w.priceTrackM7v2()\n    coin = CoinDTO(x.price, x.type, x.trendIndicator, x.trendNum, x.source)\n    print(\"final\")\n    return coin\n\ndef typeTrim(type):\n    capitalCount = 0\n    newType = \"\"\n    for i in type:\n        if i.isupper():\n            capitalCount = capitalCount + 1\n        if capitalCount > 1:\n            break\n        newType = newType + i\n    return newType\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    if request.method == 'POST':\n\n        email = request.form['email']\n        password = request.form['password']\n\n        conn = pymysql.connect(host = \"localhost\",\n                               user=DB_USERNAME,\n                               passwd=DB_PASSWORD,\n                               db=DB_NAME,\n                               port=3306)\n        cursor = conn.cursor(pymysql.cursors.DictCursor)\n        cursor.execute('SELECT * FROM users WHERE email = %s AND password = %s', (email, password,))\n        cursor.execute('SELECT * FROM users WHERE email = %s ', email)\n        account1 = cursor.fetchone()\n        if account1:\n\n            return render_template('login.html')\n\n        sql = \"INSERT INTO `users` (`email`, `password`) VALUES (%s, %s)\"\n\n\n\n        cursor.execute(sql, (email, password))\n\n        conn.commit()\n        cursor.execute('SELECT * FROM users WHERE email = %s AND password = %s', (email, password,))\n        account = cursor.fetchone()\n        id = account['id']\n        money = 10000\n        totalcoins = 0\n        profile = Profile(id,money,totalcoins)\n        setProfile(profile)\n        sql2 = \"INSERT INTO `profile` (id,money, totalcoins) VALUES (%s,%s, %s)\"\n        cursor.execute(sql2, (id, money, totalcoins))\n\n        conn.commit()\n        return redirect('/')\n\n    else:\n        return render_template('register.html')\n\n@app.route('/logout', methods=['GET', 'POST'])\ndef logout():\n    if request.method == 'POST':\n\n        flash('You were logged out.')\n\n        return render_template('login.html')\n\n@app.errorhandler(400)\ndef bad_request(error):\n    return make_response(jsonify({'error': 'Bad request'}), 400)\n\n\n@app.errorhandler(404)\ndef not_found(error):\n    return make_response(jsonify({'error': 'Not found'}), 404)\n\nif __name__ == '__main__':\n    app.secret_key = 'superSecretKey'\n    app.config['SESSION_TYPE'] = 'filesystem'\n\n    sess.init_app(app)\n\n    app.debug = True\n    app.run()", "54": "import traceback\nimport urllib\nimport uuid\nimport requests\nimport datetime\nimport SetData\nimport WebScrape\nimport DataBase\nimport ParseData\n\nimport main as Main\n\n\ndef buildFullURL(provider, pageNumber):\n    maxYear = str(datetime.datetime.now().year + 1)\n    baseSiteURL = ParseData.getFromConfig(\"URLs\", \"baseSiteURL\")\n\n    baseURL = baseSiteURL + provider\n    retUrl = baseURL + '?min-rating=0&min-year=1920&max-year=' + maxYear + '&order=title&originals=0&page=' + pageNumber\n    return retUrl\n\n\ndef buildJsonResults(cardBodies, provider):\n    for pt in cardBodies:\n        titleName = str(pt)\n        try:\n            if 'tab-content' in titleName:\n                continue\n            else:\n                stripTitle = titleName.split('>')[3].lstrip().split('<')[0].rstrip()  # Gets the Clean Title. Example: \"This Title\"\n        except Exception:\n            traceback.print_exc()\n            stripTitle = 'No Title Available'\n\n        try:\n            stripYear = titleName.split('>')[6].lstrip().split('<')[0].rstrip()  # Gets the Title Year. Example: \"1996\"\n        except Exception:\n            traceback.print_exc()\n            stripYear = 'No Year Available'\n\n        # Gets the Title Stream Service Link. Example Link Title: \"this-title-name\"\n        if provider == '':  # Netflix\n            try:\n                sRawTitle = str(titleName.split('/')[2].lstrip().split('<')[0].rstrip())\n                sStreamLink, titleType = WebScrape.getStreamServiceLinkAndType(provider, sRawTitle)\n            except Exception:\n                traceback.print_exc()\n                sStreamLink = 'No Link Available'\n                titleType = \"\"\n        elif provider == 'disney-plus/':\n            try:\n                sRawTitle = str(titleName.split('/')[3].lstrip().split('<')[0].rstrip())\n                sStreamLink, titleType = WebScrape.getStreamServiceLinkAndType(provider, sRawTitle)\n            except Exception:\n                traceback.print_exc()\n                sStreamLink = 'No Link Available'\n                titleType = \"\"\n        elif provider == 'hulu/':\n            try:\n                sRawTitle = str(titleName.split('/')[3].lstrip().split('<')[0].rstrip())\n                sStreamLink, titleType = WebScrape.getStreamServiceLinkAndType(provider, sRawTitle)\n            except Exception:\n                traceback.print_exc()\n                sStreamLink = 'No Link Available'\n                titleType = \"\"\n        elif provider == 'hbo-max/':\n            try:\n                sRawTitle = str(titleName.split('/')[3].lstrip().split('<')[0].rstrip())\n                sStreamLink, titleType = WebScrape.getStreamServiceLinkAndType(provider, sRawTitle)\n            except Exception:\n                traceback.print_exc()\n                sStreamLink = 'No Link Available'\n                titleType = \"\"\n\n        elif provider == 'amazon-prime-video/':\n            try:\n                sRawTitle = str(titleName.split('/')[3].lstrip().split('<')[0].rstrip())\n                sStreamLink, titleType = WebScrape.getStreamServiceLinkAndType(provider, sRawTitle)\n            except Exception:\n                traceback.print_exc()\n                sStreamLink = 'No Link Available'\n                titleType = \"\"\n        else:\n            sStreamLink = 'No Link Available'\n            titleType = \"\"\n\n        Main.titleNumber += 1\n        print(str(Main.titleNumber) + '.) ' + SetData.getProviderNames(provider) + ' - ' + stripTitle)\n        if stripTitle != '':\n            if stripYear == '':\n                stripYear = ' '\n            buildDbInsert(stripTitle, stripYear, sStreamLink, provider, titleType)\n        else:\n            print('Title: ' + stripTitle + ', Year: ' + stripYear + ', Link: ' + sStreamLink + ', Provider: ' + provider)\n            print(titleName)\n            print(cardBodies)\n            print(\"buildJsonResults() Catch\")\n\n\ndef buildDbInsert(title, year, link, provider, titleType):\n    service = SetData.getProviderNames(provider)\n    newID = uuid.uuid4()\n    if DataBase.titleExists(title, year):\n        return\n    else:\n        FullAPIresults = buildAPICall(title, year, titleType)\n        dict_parsedAPIforResults = ParseData.parseAPIresults(FullAPIresults, title, titleType)\n        str_Plot = ParseData.parseDictData(dict_parsedAPIforResults, \"overview\")\n        str_TmdbID = ParseData.parseDictData(dict_parsedAPIforResults, 'id')\n        str_PosterURL = buildPosterURL(dict_parsedAPIforResults)\n        DataBase.insertIntoDB(newID, title, year, service, link, str_PosterURL, str_Plot, str_TmdbID)\n        # print(\"buildDbInsert() Catch\")\n\n\ndef buildAPICall(title, year, titleType):\n    try:\n        queryTitle = urllib.parse.quote(title)\n        APIkey = ParseData.getFromConfig(\"API\", \"APIkey\")\n\n        requestURL = 'https://api.themoviedb.org/3/search/' + titleType + '?api_key=' + APIkey + '&query=' + queryTitle + '&year=' + year\n        response = requests.get(requestURL)\n        APIresults = response.json()\n        return APIresults\n    except Exception:\n        traceback.print_exc()\n        print(\"buildAPICall() Catch\")\n\n\ndef buildPosterURL(parsedAPIdict):\n    posterURL = ''\n    try:\n        APIkey = ParseData.getFromConfig(\"API\", \"APIkey\")\n\n        requestURL = 'https://api.themoviedb.org/3/configuration?api_key=' + APIkey\n        response = requests.get(requestURL)\n        APIresults = response.json()\n\n        posterDict = ParseData.parseDictData(APIresults, 'images')\n        posterBaseURL = ParseData.parseDictData(posterDict, 'base_url')\n        posterPath = ParseData.parseDictData(parsedAPIdict, 'poster_path')\n        if posterPath == {}:\n            posterURL = \"Null\"\n        else:\n            posterURL = posterBaseURL + 'original' + posterPath\n    except Exception:\n        traceback.print_exc()\n        print(\"buildPosterURL() Catch\")\n    return posterURL\n", "55": "import requests\nimport re\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport csv\n\nstring = \"https://www.freshersworld.com/jobs-in-bangalore/9999016065?&limit=50&offset={}\"\noffset = {0, 50, 100, 150}\n\n\nclass Jobs:\n    companyName = None\n    designation = None\n    qualification = None\n    location = None\n    link = None\n    lastDate = None\n\n    def __init__(self):\n        self.companyName = []\n        self.designation = []\n        self.qualification = []\n        self.lastDate = []\n        self.location = []\n        self.link = []\n\n    def getBlocks(self):\n        page = requests.get(url)\n        parser = BeautifulSoup(page.content, \"html.parser\")\n        blocks = parser.find('div', id=\"all-jobs-append\")\n        self.getDetails(blocks)\n\n    def getDetails(self, block):\n        for name in block.find_all('span', itemprop=\"name\"):\n            self.companyName.append(name.get_text())\n        for designation in block.find_all('div', class_=\"\"):\n            self.designation.append(designation.get_text())\n        for location in block.find_all('span', class_=\"job-location\"):\n            self.location.append(location.get_text())\n        for qualification in block.find_all('span', class_=\"qualifications\"):\n            self.qualification.append(qualification.get_text())\n        for lastDate in block.find_all('span', class_=\"padding-left-4\"):\n            self.lastDate.append(lastDate.get_text())\n        for link in block.find_all('a',\n                                   attrs={'href': re.compile('^https://'), 'class': re.compile('^view-apply-button')}):\n            self.link.append(link.get('href'))\n        # print(self.companyName)\n\n\nclass Database(Jobs):\n    def putDataIntoDatabase(self, Jobs):\n        jobs = {\n            'Company': Jobs.companyName,\n            'Designation': Jobs.designation,\n            'Qualifications': Jobs.qualification,\n            'Location': Jobs.location,\n            'Last Date to Apply': Jobs.lastDate,\n            'Link': Jobs.link\n        }\n        dataframe = pd.DataFrame.from_dict(jobs, orient='index')\n        dataframe = dataframe.transpose()\n        dataframe.to_csv(r'/home/ananth/WebScrape/Jobs.csv', index=True, header=True)\n\n\njob = Jobs()\ndata = Database()\nfor i in offset:\n    url = string.format(i)\n    job.getBlocks()\n    data.putDataIntoDatabase(job)\nprint(\"Done!\")\n\nprint(\"Search by: \")\nprint('1.Company')\nprint('2.Designation')\nprint('3.Location')\nprint('4.Qualification')\nprint('5.Exit')\nchoice = 0\nwhile int(choice) != 5:\n    choice = input(\"Choice: \")\n\n    if(int(choice) == 1):\n        company = input('Enter Company:')\n        with open(r'/home/ananth/WebScrape/Jobs.csv', 'r') as file:\n            reader = csv.reader(file)\n            for row in reader:\n                if row[1] == company:\n                    print(row)\n    if(int(choice) == 2):\n        designation = input('Enter Designation:')\n        with open(r'/home/ananth/WebScrape/Jobs.csv', 'r') as file:\n            reader = csv.reader(file)\n            for row in reader:\n                if row[2] == designation:\n                    print(row)\n    if(int(choice) == 3):\n        location = input('Enter Location:')\n        with open(r'/home/ananth/WebScrape/Jobs.csv', 'r') as file:\n            reader = csv.reader(file)\n            for row in reader:\n                if row[4] == location:\n                    print(row)\n    if(int(choice) == 4):\n        qualification = input('Enter Qualification:')\n        with open(r'/home/ananth/WebScrape/Jobs.csv', 'r') as file:\n            reader = csv.reader(file)\n            for row in reader:\n                if row[3] == qualification:\n                    print(row)\n", "56": "from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.common.exceptions import NoSuchElementException\nimport pickle\nfrom selenium.common.exceptions import StaleElementReferenceException\nfrom selenium.common.exceptions import TimeoutException\nfrom urllib import robotparser\nimport webscrape.time\n\nclass NYCrawler(object):\n\n    def __init__(self, starter_url):\n        user_agent = \"Amherst College SURF 2018, contact salfeld2018Amherst.edu with any questions.\"\n        opts = Options()\n        opts.add_argument(f\"user-agent={user_agent}\")\n        self.driver = webdriver.Chrome(chrome_options=opts, executable_path='/Applications/chromedriver')\n        self.base_url = starter_url\n        self.driver.get(starter_url)\n        self.links = []\n        webscrape.time.sleep(5)\n        self.log_in()\n\n    def log_in(self):\n        button = self.driver.find_element_by_xpath('//*[@id=\"masthead-cap\"]/div[2]/div[3]/button[2]')\n        try:\n            button.click()\n        except Exception:\n            close = self.driver.find_element_by_xpath('//*[@id=\"closeCross\"]')\n            close.click()\n            button.click()\n\n        username = self.driver.find_element_by_xpath('//*[@id=\"username\"]')\n        username.send_keys(\"trydzews4@gmail.com\")\n        webscrape.time.sleep(3)\n        password = self.driver.find_element_by_xpath('//*[@id=\"password\"]')\n        password.send_keys(\"headcount\")\n        webscrape.time.sleep(3)\n        # remember = self.driver.find_element_by_xpath('//*[@id=\"rememberMe\"]')\n        # remember.click()\n        submit = self.driver.find_element_by_xpath('//*[@id=\"submitButton\"]')\n        submit.click()\n        webscrape.time.sleep(100)\n\n    def get_robo_link(self, link):\n        if \".com/\" in link:\n            robo_link = link.split('com')[0] + \"com/robots.txt\"\n        elif \".org/\" in link:\n            robo_link = link.split('org')[0] + \"org/robots.txt\"\n        elif \".edu/\" in link:\n            robo_link = link.split('edu')[0] + \"edu/robots.txt\"\n        else:\n            robo_link = \"\"\n        return robo_link\n\n    def check_links(self, links):\n        checked_links = []\n        rp = robotparser.RobotFileParser()\n\n        for l in links:\n            rp.set_url(self.get_robo_link(l))\n            rp.read()\n            if rp.can_fetch(\"*\", l):\n                checked_links.append(l)\n        self.links = self.links + checked_links\n        return checked_links\n\n    def get_ny_links(self):\n        links = []\n\n        #section 1\n        try:\n            section1 = self.driver.find_element_by_class_name(\"rank\")\n            ones = section1.find_elements_by_tag_name(\"li\")\n            for o in ones:\n                d = o.find_element_by_tag_name(\"div\")\n                href = str(d.find_element_by_tag_name(\"a\").get_attribute(\"href\"))\n                if \"/photo/\" not in href and \"/video/\" not in href and \"/interactive\" not in href:\n                    if href not in links and href not in self.links:\n                        links.append(href)\n        except NoSuchElementException:\n            print(\"no section one\")\n        print(str(len(links)))\n\n        #section2\n        try:\n            section2 = self.driver.find_element_by_css_selector(\".story-menu.theme-stream.initial-set\")\n            twos = section2.find_elements_by_tag_name(\"li\")\n            for t in twos:\n                d_tag = t.find_element_by_tag_name(\"div\")\n                href = str(d_tag.find_element_by_tag_name(\"a\").get_attribute(\"href\"))\n                if \"/photo/\" not in href and \"/video/\" not in href and \"/interactive\" not in href:\n                    if href not in links and href not in self.links:\n                        links.append(href)\n        except NoSuchElementException:\n            print(\"no section two\")\n        print(str(len(links)))\n\n        #section3\n        try:\n            section3 = self.driver.find_element_by_css_selector(\".story-menu.theme-stream.additional-set\")\n            threes = section3.find_elements_by_tag_name(\"li\")\n            for t in threes:\n                try:\n                    d_tag = t.find_element_by_tag_name(\"div\")\n                    href = str(d_tag.find_element_by_tag_name(\"a\").get_attribute(\"href\"))\n                    if \"/photo/\" not in href and \"/video/\" not in href:\n                        if href not in links and href not in self.links and \"/interactive\" not in href:\n                            links.append(href)\n                except NoSuchElementException:\n                    print(\"paid ad\")\n                except StaleElementReferenceException:\n                    print(\"stale\")\n                    print(t)\n        except NoSuchElementException:\n            print(\"no section three\")\n        print(str(len(links)))\n        return links\n\n    def get_ny_article(self, url):\n        to_return = [None] * 3\n        to_return[0] = url\n        try:\n            self.driver.get(url)\n        except TimeoutException:\n            return to_return\n\n        #try to get title\n        try:\n            title = self.driver.find_element_by_class_name(\"balancedHeadline\")\n            to_return[1] = title.text\n        except NoSuchElementException:\n            try:\n                title=self.driver.find_element_by_id(\"headline\")\n                to_return[1] = title.text\n            except NoSuchElementException:\n                print(\"no title\")\n                print(url)\n                to_return[1] = ''\n\n        #try to get content\n        try:\n            text = ''\n            sections = self.driver.find_elements_by_css_selector(\".css-18sbwfn.StoryBodyCompanionColumn\")\n            for sec in sections:\n                paragraphs = sec.find_elements_by_tag_name(\"p\")\n                for p in paragraphs:\n                    text = text + p.text\n            to_return[2] = text\n        except NoSuchElementException:\n            print(\"no content\")\n            to_return[2] = ''\n        return to_return\n\n    def get_ny_data(self, links):\n        data = []\n        count = 0\n        for link in links:\n            print(count)\n            content = self.get_ny_article(link)\n            if len(content[1]) > 0 and len(content[2]) > 0:\n                data.append(content)\n            webscrape.time.sleep(10)\n            count +=1\n        return data\n\n    def collect_links(self):\n        self.click_button()\n        self.scroll_down()\n        links = self.get_ny_links()\n        print(\"in collect links \" + str(len(links)))\n        return links\n\n\n    def click_button(self):\n        button = self.driver.find_element_by_xpath('//*[@id=\"latest-panel\"]/div[1]/div/div/button')\n        button.click()\n\n    def scroll_down(self):\n        count = 0\n        SCROLL_PAUSE_TIME = 5\n        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n\n        while count < 50:\n            # Scroll down to bottom\n            try:\n                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n\n                # Wait to load page\n                webscrape.time.sleep(SCROLL_PAUSE_TIME)\n\n                # Calculate new scroll height and compare with last scroll height\n                new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n                if new_height == last_height:\n                    print(\"Reached bottom\")\n                    break\n                last_height = new_height\n                print(\"scrolling\")\n                count +=1\n                print(count)\n            except TimeoutException:\n                break\n\n    def one_section(self, url):\n        self.driver.get(url)\n        links = self.collect_links()\n        checked_links = self.check_links(links)\n        data = self.get_ny_data(checked_links)\n        return data\n\n    def start(self):\n        one = self.one_section(\"https://www.nytimes.com/section/us\")\n        two = self.one_section(\"https://www.nytimes.com/section/world?module=SectionsNav&action=click&version=BrowseTree&region=TopBar&contentCollection=World&pgtype=sectionfront\")\n        three = self.one_section(\"https://www.nytimes.com/section/politics?module=SectionsNav&action=click&version=BrowseTree&region=TopBar&contentCollection=Politics&pgtype=sectionfront\")\n        four = self.one_section(\"https://www.nytimes.com/section/business?module=SectionsNav&action=click&version=BrowseTree&region=TopBar&contentCollection=Business&pgtype=sectionfront\")\n        full_data = one + two + three + four\n        return full_data\n\n    #FOR BIG RUN\n\n    def get_master_links(self, url):\n        self.driver.get(url)\n        links = self.collect_links()\n        checked_links = self.check_links(links)\n        return checked_links\n\n    def pickle_links(self):\n        one = self.get_master_links(\"https://www.nytimes.com/section/us\")\n        print(\"length of one is \" + str(len(one)))\n        webscrape.time.sleep(10)\n        two = self.get_master_links(\"https://www.nytimes.com/section/world\")\n        print(\"length of two is \" + str(len(two)))\n        webscrape.time.sleep(10)\n        three = self.get_master_links(\"https://www.nytimes.com/section/politics?module=SectionsNav&action=click&version=BrowseTree&region=TopBar&contentCollection=Politics&pgtype=sectionfront\")\n        print(\"length of three is \" + str(len(three)))\n        webscrape.time.sleep(10)\n        four = self.get_master_links(\"https://www.nytimes.com/section/business?module=SectionsNav&action=click&version=BrowseTree&region=TopBar&contentCollection=Business&pgtype=sectionfront\")\n        print(\"length of four is \" + str(len(four)))\n        full_links = one + two + three + four\n        return full_links\n\n\n\nif __name__ == \"__main__\":\n    # collecting links\n    # url = \"https://www.nytimes.com/?action=click&pgtype=Homepage&module=MastheadLogo&region=TopBar\"\n    # ny = NYCrawler(url)\n    # links = ny.pickle_links()\n    # print(\"length of list is \" + str(len(links)))\n    #\n    # pickle_out = open('./data/master_ny_links.pkl', 'wb')\n    # desc = \"list of links from ny times\"\n    # pickle.dump((links, desc), pickle_out)\n    # pickle_out.close()\n    #\n    # print(\"pickled\")\n    # command = input(\"Press q to quit\")\n    # if command is \"q\":\n    #     ny.driver.quit()\n\n    # collecting the article body and title of each link for a range of links\n    pickle_in = open(\"./data/master_ny_links.pkl\", \"rb\")\n    links, desc = pickle.load(pickle_in)\n    print(len(links))\n\n    indicies = [(100, 200), (200, 300)]\n\n    for i in range(len(links)):\n        start = indicies[i][0]\n        end = indicies[i][1]\n        small_links = links[start: end]\n\n        url = \"https://www.nytimes.com/?action=click&pgtype=Homepage&module=MastheadLogo&region=TopBar\"\n        ny = NYCrawler(url)\n        data = ny.get_ny_data(small_links)\n        print(\"len of articles is \" + str(len(data)))\n        print(data[3])\n\n        pickle_out = open(f\"./data/ny_articles_{start}-{end}.pkl\", \"wb\")\n        desc = f\"url, title, and content for articles with urls from {start} to {end}\"\n        pickle.dump((data, desc), pickle_out)\n        pickle_out.close()\n\n    command = input(\"Press q to quit\")\n    if command is \"q\":\n        ny.driver.quit()\n", "57": "import dotenv\nimport pyodbc\nimport time\nimport os\n\nfrom datetime import date\nfrom openpyxl import load_workbook\n\nfrom WSnagumo import webscrape_nagumo\nfrom WSmercadolivre import webscrape_mercadolivre\n\n\n# Checar tempo de execu\u00e7\u00e3o do programa\nstart_time = time.time()\n\n# Carregar vari\u00e1veis de ambiente\ndotenv.load_dotenv(dotenv.find_dotenv())\n\n# Conex\u00e3o com db do Microsoft Azure\nAZURE_USER = os.getenv('AZURE_USER')\nAZURE_PASSWD = os.getenv('AZURE_PASSWD')\nAZURE_SERVER = os.getenv('AZURE_SERVER')\n\nconnection_data = ('Driver={ODBC Driver 18 for SQL Server};Server=%s;Database=Pre\u00e7os_DB;Uid=%s;Pwd=%s;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30') % (AZURE_SERVER, AZURE_USER, AZURE_PASSWD)\n\nconnection_db = pyodbc.connect(connection_data)\n\nmycursor = connection_db.cursor()   \n\n# Data do dia de hoje em formato dd/mm/yy\ntoday = date.today()\ndata_hoje = today.strftime(\"%Y-%m-%d\")\n\n# Carregar planilha e produtos com openpyxl\narquivo = r\"lista de mercado.xlsx\"\nwb = load_workbook(arquivo)\nws = wb[wb.sheetnames[0]]\nlista_produtos = []\ncoluna_link = ws['A'][1:]\nfor cell in coluna_link:\n    lista_produtos.append(f'{cell.value}')\n\n# Execu\u00e7\u00e3o do webscrapping\nwebscrape_nagumo(lista_produtos, mycursor, data_hoje, ws)\nwebscrape_mercadolivre(lista_produtos, mycursor, data_hoje, ws)\n\n# Fechar planilha excel e conex\u00e3o com db\nwb.save(filename = arquivo)\nwb.close()\nmycursor.close()\nconnection_db.close()\n\n# Print tempo de execu\u00e7\u00e3o do programa\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\n", "58": "from flask import Flask\nfrom webscrape import webscrape_blueprint\n\napp = Flask(__name__)\n\napp.register_blueprint(webscrape_blueprint)\n\nif __name__ == \"__main__\":\n    app.run()", "59": "# Scrapy settings for webscrape project\r\n#\r\n# For simplicity, this file contains only settings considered important or\r\n# commonly used. You can find more settings consulting the documentation:\r\n#\r\n#     https://docs.scrapy.org/en/latest/topics/settings.html\r\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\r\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\r\n\r\nBOT_NAME = 'webscrape'\r\n\r\nSPIDER_MODULES = ['webscrape.spiders']\r\nNEWSPIDER_MODULE = 'webscrape.spiders'\r\n\r\n\r\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\r\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\r\n\r\n# Obey robots.txt rules\r\nROBOTSTXT_OBEY = True\r\n\r\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\r\n#CONCURRENT_REQUESTS = 32\r\n\r\n# Configure a delay for requests for the same website (default: 0)\r\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\r\n# See also autothrottle settings and docs\r\nDOWNLOAD_DELAY = 0.5\r\n# The download delay setting will honor only one of:\r\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\r\n#CONCURRENT_REQUESTS_PER_IP = 16\r\n\r\n# Disable cookies (enabled by default)\r\n#COOKIES_ENABLED = False\r\n\r\n# Disable Telnet Console (enabled by default)\r\n#TELNETCONSOLE_ENABLED = False\r\n\r\n# Override the default request headers:\r\n#DEFAULT_REQUEST_HEADERS = {\r\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\r\n#   'Accept-Language': 'en',\r\n#}\r\n\r\n# Enable or disable spider middlewares\r\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\r\n#SPIDER_MIDDLEWARES = {\r\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\r\n#}\r\n\r\n# Enable or disable downloader middlewares\r\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\r\n#DOWNLOADER_MIDDLEWARES = {\r\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\r\n#}\r\n\r\n# Enable or disable extensions\r\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\r\n#EXTENSIONS = {\r\n#    'scrapy.extensions.telnet.TelnetConsole': None,\r\n#}\r\n\r\n# Configure item pipelines\r\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\r\n#ITEM_PIPELINES = {\r\n#    'webscrape.pipelines.WebscrapePipeline': 300,\r\n#}\r\n\r\n# Enable and configure the AutoThrottle extension (disabled by default)\r\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\r\n#AUTOTHROTTLE_ENABLED = True\r\n# The initial download delay\r\n#AUTOTHROTTLE_START_DELAY = 5\r\n# The maximum download delay to be set in case of high latencies\r\n#AUTOTHROTTLE_MAX_DELAY = 60\r\n# The average number of requests Scrapy should be sending in parallel to\r\n# each remote server\r\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\r\n# Enable showing throttling stats for every response received:\r\n#AUTOTHROTTLE_DEBUG = False\r\n\r\n# Enable and configure HTTP caching (disabled by default)\r\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\r\n#HTTPCACHE_ENABLED = True\r\n#HTTPCACHE_EXPIRATION_SECS = 0\r\n#HTTPCACHE_DIR = 'httpcache'\r\n#HTTPCACHE_IGNORE_HTTP_CODES = []\r\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\r\n", "60": "from django.contrib import admin\r\nfrom django.urls import path\r\nfrom web_scrape import views\r\n\r\n\r\napp_name='web_scrape'\r\n\r\nurlpatterns = [\r\n     #path('', views.index, name='index'),\r\n     \r\n     path('webscrape', views.webscrape, name='webscrape'),\r\n]\r\n", "61": "import requests\nfrom bs4 import BeautifulSoup\nimport random\nimport json\nimport time\nimport os\n\n\n\n#######################################################################################################\n#######################################################################################################\n\n\n\nwebhook_url = str(os.environ.get(\"WEBHOOK_URL\"))\n\n\nnike = [\n    [\"https://www.nike.com/t/air-max-270-bowfin-mens-shoe-0x4rcw/AJ7200-005\", [\"6\", \"6.5\", \"7\", \"9\"]],\n    [\"https://www.nike.com/t/zoom-kd11-basketball-shoe-ZKqRs9\", []],\n    [\"https://www.nike.com/t/vapor-power-2-training-backpack-a0e338\", []],\n    [\"https://www.nike.com/t/gym-club-training-duffel-bag-pvTjnpml/BA5490-652\", []]\n]\n\nadidas = [\n    [\"https://www.adidas.com/us/pod-s3.1-shoes/AQ1059.html\", []],\n    [\"https://www.adidas.com/us/badge-of-sport-cities-tee/DX0403.html\", []],\n    [\"https://www.adidas.com/us/ultimate365-shorts/CE0449.html\", []],\n    [\"https://www.adidas.com/us/daybreak-2-backpack/CK0290.html\", []]\n]\n\nsupreme = [\n    [\"https://www.supremenewyork.com/shop/jackets/zzlaqusmv/zoyc8fzxa\", []],\n    [\"https://www.supremenewyork.com/shop/tops-sweaters/jkpuamv9q/qd87sp6tc\", []],\n    [\"https://www.supremenewyork.com/shop/hats/usr9uodc8/j0mgc2qpb\", []]\n]\n\n\n\n#######################################################################################################\n#######################################################################################################\n\n\n\n\n\n\n\n\nsite_links = {\n    \"nike\": [],\n    \"adidas\": [],\n    \"supreme\": []\n}\nfor nike_link in nike:\n    site_links[\"nike\"].append({\n        \"link\": nike_link[0],\n        \"wanted_sizes\": nike_link[1],\n        \"last_in_stock_sizes\": [],\n        \"last_in_stock\": False\n    })\nfor adidas_link in adidas:\n    site_links[\"adidas\"].append({\n        \"link\": adidas_link[0],\n        \"wanted_sizes\": adidas_link[1],\n        \"last_in_stock_sizes\": [],\n        \"last_in_stock\": False,\n        \"adidas_item_id\": adidas_link[0][-11:-5]\n    })\nfor supreme_link in supreme:\n    site_links[\"supreme\"].append({\n        \"link\": supreme_link[0],\n        \"wanted_sizes\": supreme_link[1],\n        \"last_in_stock_sizes\": [],\n        \"last_in_stock\": False\n    })\n\n\ncookie = str(os.environ.get(\"COOKIE\"))\nuser_agent = str(os.environ.get(\"USER_AGENT\"))\nwebscrape_data = {\n    \"proxies\": [\n        \"178.128.168.88\",\n        \"89.38.144.95\",\n        \"145.239.252.51\",\n        \"178.128.173.108\",\n        \"185.42.221.246\"\n    ],\n    \"headers\": {\n        \"nike\": {\n            \"cookie\": cookie,\n            \"User-Agent\": user_agent\n        },\n        \"adidas\": {\n            \"User-Agent\": user_agent\n        },\n        \"supreme\": {\n            \"User-Agent\": user_agent\n        }\n    },\n    \"adidas_availability_link\": \"https://www.adidas.com/api/products/{0}/availability\"\n}\n\n\n\n\n\n\ndef send_embed(embed_data):\n    global webscrape_data\n\n    data = {\n        \"embeds\": [embed_data]\n    }\n    send_attempts = 0\n    while send_attempts < 3:\n        embed_request = requests.post(webhook_url, json=data)\n        if embed_request.status_code in [204, 200]:\n            break\n        else:\n            print(\"Sending Embed Failed [RETRYING]: {0} because {1}\\n\\nData: {2}\".format(embed_request.status_code, embed_request.reason, data))\n            data = {\n                \"embeds\": [{\n                    \"title\": embed_data[\"title\"],\n                    \"description\": embed_data[\"description\"],\n                    \"url\": embed_data[\"url\"],\n                    \"color\": embed_data[\"color\"]\n                }]\n            }\n            if send_attempts != 0:\n                time.sleep(30)\n            send_attempts += 1\n\n\n\n\n\n\ndef get_nike_link_data(link):\n    global webscrape_data\n\n    request = requests.get(link, headers=webscrape_data[\"headers\"][\"nike\"], proxies=webscrape_data[\"proxies\"])\n    if request.status_code != 200:\n        print(\"Request Failed: {0} because {1}\\n\\nLink: {2}\".format(request.status_code, request.reason, link))\n    else:\n\n        html_text = (request.text)\n        soup = BeautifulSoup(html_text, features=\"lxml\")\n\n        image_element_list = soup.body.find(\"img\", {\"class\": \"css-10f9kvm u-full-width u-full-height css-1436l9y\"})\n        return ({\n            \"item_name\": image_element_list[\"alt\"],\n            \"image_url\": image_element_list[\"src\"]\n        })\n\n\n\ndef get_adidas_link_data(link):\n    global webscrape_data\n\n    request = requests.get(link, headers=webscrape_data[\"headers\"][\"adidas\"], proxies=webscrape_data[\"proxies\"])\n    if request.status_code != 200:\n        print(\"Request Failed: {0} because {1}\\n\\nLink: {2}\".format(request.status_code, request.reason, link))\n    else:\n\n        html_text = (request.text)\n        soup = BeautifulSoup(html_text, features=\"lxml\")\n\n    image_element_list = soup.body.find(\"div\", {\"class\": \"images_container___3KxTB\"}).img\n    return ({\n        \"item_name\": image_element_list[\"alt\"],\n        \"image_url\": image_element_list[\"src\"]\n    })\n\n\n\ndef get_supreme_link_data(link):\n    global webscrape_data\n\n    request = requests.get(link, headers=webscrape_data[\"headers\"][\"supreme\"], proxies=webscrape_data[\"proxies\"])\n    if request.status_code != 200:\n        print(\"Request Failed: {0} because {1}\\n\\nLink: {2}\".format(request.status_code, request.reason, link))\n    else:\n\n        html_text = (request.text)\n        soup = BeautifulSoup(html_text, features=\"lxml\")\n\n        image_element_list = soup.body.find(\"img\", {\"id\": \"img-main\"})\n        return ({\n            \"item_name\": image_element_list[\"alt\"],\n            \"image_url\": image_element_list[\"src\"]\n        })\n\n\n\n\n\ndef get_nike_link_stock(link):\n    global webscrape_data\n    proxy = {\"http\": random.choice(webscrape_data[\"proxies\"])}\n    request = requests.get(link, headers=webscrape_data[\"headers\"][\"nike\"], proxies=proxy)\n    if request.status_code != 200:\n        print(\"Getting Nike Stock Request Failed: {0} because {1}\\n\\nLink: {2}\".format(request.status_code, request.reason, link))\n        return {\n            \"item_in_stock\": False,\n            \"in_stock\": [],\n            \"out_of_stock\": []\n        }\n    else:\n        html_text = (request.text)\n        soup = BeautifulSoup(html_text, features=\"lxml\")\n\n        if soup.body.find(\"button\", {\"aria-label\": \"Add to Cart\"}) == None:\n            return {\n                \"item_in_stock\": False,\n                \"in_stock\": [],\n                \"out_of_stock\": []\n            }\n\n        if soup.body.find(\"div\", {\"name\": \"skuAndSize\"}) == None:\n            return {\n                \"item_in_stock\": True,\n                \"in_stock\": [],\n                \"out_of_stock\": []\n            }\n        element_list = soup.body.find(\"div\", {\"name\": \"skuAndSize\"}).find_all()\n        nike_stock_attributes = {\n            \"in_stock\": [\"data-css-1iiusdt\", \"data-css-lv2huc\", \"data-css-ikkzrh\"],\n            \"out_of_stock\": [\"data-css-yyh50b\", \"data-css-137acxc\", \"data-css-y50moq\"]\n        }\n        stock_list = {\n            \"item_in_stock\": True,\n            \"in_stock\": [],\n            \"out_of_stock\": []\n        }\n        for element in element_list:\n            if str(element).startswith(\"", "62": "# -*- coding: utf-8 -*-\n\n# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'webscrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "63": "from decimal import Decimal\nfrom flask import Blueprint, render_template, request, flash, jsonify, redirect, url_for, app\nfrom flask_login import login_user, login_required, logout_user, current_user\nfrom .models import Note, Selected_item, User, Purchases, Ordered_item\nfrom . import db\nimport json\nfrom werkzeug.security import generate_password_hash, check_password_hash\nimport pandas as pd\nimport os \nfrom flask import send_from_directory\n\ncategories = Blueprint('categories', __name__)\n\ndef clean_name(name):\n    name_header = name.replace('__',' & ')\n    name_header = name_header.replace('_',' ')\n    name_header = name_header.replace(',',', ')\n    name_header = name_header[0].upper() + name_header[1:]\n    return name_header\n\n@categories.route('/fresh-products', methods=['GET', 'POST'])\n@login_required\ndef fresh_products():\n    #if request.method == 'POST':\n        \n    return render_template(\"fresh_products.html\", user=current_user)\n\n#all categories of fresh products\n\n@categories.route('/dairy',methods=['GET','POST'])\n@login_required\ndef dairy():\n    name = 'zuivel'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/apero',methods=['GET','POST'])\n@login_required\ndef apero_products():\n    name = 'aperitief'\n    name_header = clean_name(name)\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Item {product_name} added to cart', category='success')\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/bread-pastry',methods=['GET','POST'])\n@login_required\ndef bread_pastry_products():\n    name = 'brood__patisserie'\n    name_header = clean_name(name)\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Item {product_name} added to cart', category='success')\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/meat',methods=['GET','POST'])\n@login_required\ndef meat_products():\n    name = 'charcuterie'\n    name_header = clean_name(name)\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Item {product_name} added to cart', category='success')\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/fruit-vegetables',methods=['GET','POST'])\n@login_required\ndef fruit_vegetables_products():\n    name = 'groenten__fruit'\n    name_header = clean_name(name)\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Item {product_name} added to cart', category='success')\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/butcher',methods=['GET','POST'])\n@login_required\ndef butcher():\n    name = 'beenhouwerij'\n    name_header = clean_name(name)\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Item {product_name} added to cart', category='success')\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n\n@categories.route('/catering-prepared-meals',methods=['GET','POST'])\n@login_required\ndef catering_prepared_meals_products():\n    name = 'traiteur__bereide_maaltijden'\n    name_header = clean_name(name)\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Item {product_name} added to cart', category='success')\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/vegi-vegan-fresh',methods=['GET','POST'])\n@login_required\ndef vegi_vegan_fresh():\n    name = 'vegetarisch_vegan'\n    name_header = clean_name(name)\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Item {product_name} added to cart', category='success')\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/fish-sushi',methods=['GET','POST'])\n@login_required\ndef fish_sushi_products():\n    name = 'vis__sushi'\n    name_header = clean_name(name)\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Item {product_name} added to cart', category='success')\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n#all categories of groceries\n\n@categories.route('/breakfast',methods=['GET','POST'])\n@login_required\ndef breakfast():\n    name = 'ontbijt'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/world-kitchen',methods=['GET','POST'])\n@login_required\ndef world_kitchen():\n    name = 'wereldkeuken'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/desserts-sugar-flour',methods=['GET','POST'])\n@login_required\ndef desserts_sugar_flour():\n    name = 'desserts,suiker__bloem'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/candy',methods=['GET','POST'])\n@login_required\ndef candy():\n    name = 'snoepgoed'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/chocolate',methods=['GET','POST'])\n@login_required\ndef chocolate():\n    name = 'chocolade'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/chips-apero',methods=['GET','POST'])\n@login_required\ndef chips_apero():\n    name = 'chips__aperitief'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/cookies-pies',methods=['GET','POST'])\n@login_required\ndef cookies_pies():\n    name = 'koeken__taarten'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/rusk-crackers',methods=['GET','POST'])\n@login_required\ndef rusk_crackers():\n    name = 'beschuit__crackers'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/canned',methods=['GET','POST'])\n@login_required\ndef canned():\n    name = 'conserven'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n####categories within drinks###\n\n@categories.route('/water',methods=['GET','POST'])\n@login_required\ndef water():\n    name = 'water'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/fruit-vegetables-drinks',methods=['GET','POST'])\n@login_required\ndef fruit_vegetables_drinks():\n    name = 'vruchten__groentensap'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/syrups',methods=['GET','POST'])\n@login_required\ndef syrups():\n    name = 'siropen'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/milk-plant-based-drinks',methods=['GET','POST'])\n@login_required\ndef milk_plant_based_drinks():\n    name = 'melk__plantaardige_dranken'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/hot-drinks',methods=['GET','POST'])\n@login_required\ndef hot_drinks():\n    name = 'warme_dranken'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/soft-drinks',methods=['GET','POST'])\n@login_required\ndef soft_drinks():\n    name = 'softdrinks'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/beer',methods=['GET','POST'])\n@login_required\ndef beer():\n    name = 'bier'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/apero-strong-drink',methods=['GET','POST'])\n@login_required\ndef apero_strong_drink():\n    name = 'aperitieven__sterke_drank'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/wines',methods=['GET','POST'])\n@login_required\ndef wines():\n    name = 'wijnen'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n###categories within frozen###\n\n@categories.route('/vegi-vegan-frozen',methods=['GET','POST'])\n@login_required\ndef vegi_vegan_frozen():\n    name = 'vegetarisch__vegan'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/ice-desserts',methods=['GET','POST'])\n@login_required\ndef ice_desserts():\n    name = 'ijs__desserten'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/pizza-quiches',methods=['GET','POST'])\n@login_required\ndef pizza_quiches():\n    name = 'pizza__quiches'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/bread-cake',methods=['GET','POST'])\n@login_required\ndef bread_cake():\n    name = 'brood__gebak'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/snacks-apero',methods=['GET','POST'])\n@login_required\ndef snacks_apero():\n    name = 'snacks__aperitiefhapjes'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/starters',methods=['GET','POST'])\n@login_required\ndef starters():\n    name = 'voorgerechten'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/prepared-meals',methods=['GET','POST'])\n@login_required\ndef prepared_meals():\n    name = 'bereide_maaltijden'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/fish',methods=['GET','POST'])\n@login_required\ndef fish():\n    name = 'vis'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/vegetables',methods=['GET','POST'])\n@login_required\ndef vegetables():\n    name = 'groenten'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n###Categories of baby related products###\n\n@categories.route('/milk-drinks',methods=['GET','POST'])\n@login_required\ndef milk_drinks():\n    name = 'melk__dranken'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/baby-food',methods=['GET','POST'])\n@login_required\ndef baby_food():\n    name = 'babyvoeding'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/baby-care',methods=['GET','POST'])\n@login_required\ndef baby_care():\n    name = 'verzorging_baby'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/diapers',methods=['GET','POST'])\n@login_required\ndef diapers():\n    name = 'luiers__luierbroekjes'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/detergent-baby',methods=['GET','POST'])\n@login_required\ndef detergent_baby():\n    name = 'wasmiddelen_baby'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    \n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n###Categories of care and hiegene###\n\n@categories.route('/mouth-hiegene',methods=['GET','POST'])\n@login_required\ndef mouth_hiegene():\n    name = 'mondhygi\u00ebne'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/body',methods=['GET','POST'])\n@login_required\ndef body():\n    name = 'lichaam'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/hair',methods=['GET','POST'])\n@login_required\ndef hair():\n    name = 'haar'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/face',methods=['GET','POST'])\n@login_required\ndef face():\n    name = 'gezicht'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/men',methods=['GET','POST'])\n@login_required\ndef men():\n    name = 'mannen'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/hiegene',methods=['GET','POST'])\n@login_required\ndef hiegene():\n    name = 'hygi\u00ebne'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/pharmacy',methods=['GET','POST'])\n@login_required\ndef pharmacy():\n    name = 'apotheek'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n###Categories of maintenance###\n\n@categories.route('/toiletpaper-papertowels-tissues',methods=['GET','POST'])\n@login_required\ndef toiletpaper_papertowels_tissues():\n    name = 'toiletpapier,keukenpapier__zakdoeken'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/detergents',methods=['GET','POST'])\n@login_required\ndef detergents():\n    name = 'wasmiddelen'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/laundry-care',methods=['GET','POST'])\n@login_required\ndef laundry_care():\n    name = 'verzorging_van_de_was'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/dishwashing-products',methods=['GET','POST'])\n@login_required\ndef dishwashing_products():\n    name = 'Vaatwasproducten'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/cleaning-products',methods=['GET','POST'])\n@login_required\ndef cleaning_products():\n    name = 'schoonmaakproducten'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/airfreshener-refill',methods=['GET','POST'])\n@login_required\ndef air_freshener_refill():\n    name = 'luchtverfrissers__navullingen'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/maintenance-accessoires',methods=['GET','POST'])\n@login_required\ndef maintenance_accessoires():\n    name = 'onderhoudsaccessoires'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/cleaning-accessoires',methods=['GET','POST'])\n@login_required\ndef cleaning_accessoires():\n    name = 'huishoudaccessoires'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n@categories.route('/pesticides',methods=['GET','POST'])\n@login_required\ndef pesticides():\n    name = 'insecticides'\n    name_header = clean_name(name)\n\n    static_directory = os.path.join(os.getcwd(),r'website/static')\n    df_products_info = pd.read_csv(f'{static_directory}/webscrape_info_{name}.csv')\n    products_info = df_products_info.to_dict('records')\n\n    if request.method == 'POST':\n        product_name = request.form.get('product_name')\n        product_img = request.form.get('product_img')\n        product_base_price = request.form.get('product_base_price')\n        product_base_price_type = request.form.get('product_base_price_type')\n        product_big_price = request.form.get('product_big_price')\n        new_product = Selected_item(name=product_name,img=product_img,base_price=product_base_price,base_price_type=product_base_price_type,big_price=product_big_price, user_id=current_user.id)\n        db.session.add(new_product)\n        db.session.commit()\n        if product_name != '':\n            flash(f'Artiekel {product_name} toegevoegd aan mandje', category='success')\n\n    return render_template(\"category.html\",name_header=name_header,products_info=products_info, user=current_user)\n\n### Cart-items page###\n\n@categories.route('/cart-items', methods=['GET', 'POST'])\n@login_required\ndef cart_items():\n    if request.method == 'POST':\n        itemId = request.form.get('id')\n        userId = request.form.get('save_shoppinglist')\n        if itemId:\n            item = Selected_item.query.get(itemId)\n            if item:\n                if item.user_id == current_user.id:\n                    db.session.delete(item)\n                    db.session.commit()\n                    flash(f'{item.name} verwijderd uit mandje')\n        elif userId:\n            user = User.query.get(userId)\n            if user:\n                if user.id == current_user.id:\n                    if current_user.shoppinglist:\n                        new_purchase = Purchases(user_id=current_user.id)\n                        db.session.add(new_purchase)\n                        db.session.commit()\n                        for item in current_user.shoppinglist:\n                            new_ordered_item = Ordered_item(name = item.name,img = item.img, base_price = item.base_price, base_price_type = item.base_price_type, big_price = item.big_price, purchase_id = new_purchase.id)\n                            db.session.add(new_ordered_item)\n                            db.session.delete(item)\n                            db.session.commit()\n                        flash(f\"Winkelmandje succesvol doorgestuurd\", category='success')\n                        return redirect(url_for('views.home'))\n                    else:\n                        flash(f\"Winkelmandje kan niet leeg worden opgeslaan\", category='error')\n    \n    sum_prices = 0\n    for item in current_user.shoppinglist:\n        sum_prices += Decimal(item.base_price[:-1].replace(',','.'))\n    #current_user.total_price = sum_prices  enkel nodig als ik echt in database wil opslaan\n\n    return render_template(\"shopping_list_display.html\",sum_prices=str(sum_prices), user=current_user)\n\n", "64": "import requests\nimport bs4\n\n#url = \"http://quotes.toscrape.com/\"\n\nclass webScrape:\n    def __int__(self, url, text=None,lasttext = None):\n        self.url = url\n        self.text = text\n        self.lasttext = lasttext\n\n    def scrape_to_text(self,page=None):\n        if page is not None:\n            res = requests.get((self.url).format(page))\n        else:\n            res = requests.get(self.url)\n        soup = bs4.BeautifulSoup(res.text,\"lxml\")\n        return soup\n\n    def get_list(self):\n        soup = webScrape.scrape_to_text()\n        lists = soup.select(self.text)\n        temp_list = []\n        for temp in lists:\n            temp_list.append(temp.getText())\n\n        return temp_list\n\n    def scrape_all_pages(self):\n        while True:\n            page = 1\n            soup = webScrape.scrape_to_text(page)\n            temp = webScrape.get_list()\n            if \"No quotes found!\" in soup.select(\".\"+self.lasttext)[1].getText():\n                break\n            page = page + 1\n\n            return set(temp)\n\n\nif __name__ == \"__main__\":\n    # Task1\n    url = \"http://quotes.toscrape.com/\"\n    #ws = webScrape(url,text = \"author\")\n    ws = webScrape()\n    souptext = ws.scrape_to_text()\n    auth_list = ws.get_list()\n    print(set(auth_list))\n", "65": "import Project.WebScrapeCharts as ws\n\n\n# -------------------------------------- TESTING ------------------------------------------\nimport unittest\n\nclass test_WebScrapeCharts(unittest.TestCase):\n\n\n\tdef setup_webscrape(self):\n\t\tself.ws = ws.WebScrapeCharts()\n\t\tself.country_code = 'nl'\n\t\tself.recurrence = 'daily'\n\t\tself.filename = None\n\n\n\tdef check_file(self, filename):\n\t\ttry:\n\t\t\twith open(filename,'r') as rf:\n\t\t\t\tlines = rf.readlines()\n\t\t\t\tcounter = len(lines)\n\t\t\t\treturn counter == 200\n\t\texcept:\n\t\t\treturn False\n\n\tdef set_filename(self):\n\t\tself.filename = 'test_' + self.country_code + '_' + self.recurrence + '.txt'\n\n\n\t### TEST FUNCTIONS\n\n\tdef test_get_charts(self):\n\t\tself.setup_webscrape()\n\t\tself.assertTrue(self.ws.get_charts(self.country_code, self.recurrence))\n\t\tself.assertEqual(self.ws.url,'https://spotifycharts.com/regional/nl/daily/latest')\n\n\tdef test_get_charts_fail(self):\n\t\tself.setup_webscrape()\n\t\tself.country_code = 'jo'\n\t\tself.recurrence = 'daiy'\n\t\tself.assertFalse(self.ws.get_charts(self.country_code, self.recurrence))\n\n\t\n\tdef test_is_empty(self):\n\t\tself.setup_webscrape()\n\t\tself.assertTrue(self.ws.is_empty())\n\t\t# get the charts\n\t\tself.ws.get_charts(self.country_code, self.recurrence)\n\t\tself.assertFalse(self.ws.is_empty())\n\n\t### TEST SAVING -> done Manually as it creates files\n\tdef test_saving_for_testing(self):\n\t\tself.setup_webscrape()\n\t\t# USE DIFFERENT COUNTRY CODE AND RECURRENCE\n\t\tself.country_code = 'au'\n\t\tself.recurrence = 'weekly'\n\t\tself.set_filename()\n\n\t\t# Double check not that the file already exists and this test runs into an error\n\t\tif not self.check_file(self.filename):\n\t\t\tself.assertTrue(self.ws.save_for_testing(self.country_code, self.recurrence))\n\t\t\tself.assertTrue(self.check_file(self.filename))\n\n\tdef test_saving_for_testing_fail(self):\n\t\tself.setup_webscrape()\n\t\tself.country_code = 'ik'\n\t\tself.recurrence = 'day'\n\n\t\tself.assertFalse(self.ws.save_for_testing(self.country_code, self.recurrence))\n\n\n\n\tdef test_load_ids_from_file(self):\n\t\tself.setup_webscrape()\n\t\tself.country_code = 'de'\n\t\tself.recurrence = 'daily'\n\t\tself.set_filename()\n\n\t\tif self.check_file(self.filename):\n\t\t\t# file exists test the loading ids\n\t\t\t# test whether self.ws is empty\n\t\t\tself.assertTrue(self.ws.is_empty())\n\t\t\t# Test if ids are successfully loaded from file\n\t\t\tself.assertTrue(self.ws.load_ids_from_file(self.country_code, self.recurrence))\n\t\t\t# Test if self.ws is not empty\n\t\t\tself.assertFalse(self.ws.is_empty())\n\n\tdef test_load_ids_fail(self):\n\t\tself.setup_webscrape()\n\t\tself.country_code = 'jo'\n\t\tself.recurrence = 'dail'\n\t\tself.set_filename()\n\n\t\tself.assertFalse(self.ws.load_ids_from_file(self.country_code, self.recurrence))\n\n\tdef test_load_ids_is_full_fail(self):\n\t\tself.setup_webscrape()\n\t\tself.country_code = 'us'\n\t\tself.recurrence = 'daily'\n\t\tself.set_filename()\n\t\tself.ws.save_for_testing(self.country_code, self.recurrence)\n\n\t\tself.assertFalse(self.ws.load_ids_from_file(self.country_code, self.recurrence))\n\n\tdef test_set_default(self):\n\t\tself.setup_webscrape()\n\t\tself.assertFalse(self.ws.get_charts('jo', 'daily'))\n\t\tself.assertEqual(self.ws.url, 'https://spotifycharts.com/regional/jo/daily/latest')\n\n\t\tself.ws.set_default()\n\t\tself.assertEqual(self.ws.url, 'https://spotifycharts.com/regional/')\n\n\nif __name__ == '__main__':\n\tunittest.main()\n", "66": "import sqlite3\nimport webscrape\n\n\n#Getting the base price through web scraping\nbij = {}\nud = {}\nbang = {}\nbij = webscrape.bij_price\nud = webscrape.u_price\nbang = webscrape.bang_price\n\ndef price_reduction(investment, profit):\n    ratio = (investment/profit)*100\n    if(ratio>=90):\n        return 7\n    elif(ratio<90 and ratio>=70):\n        return 5\n    elif(ratio<70 and ratio>=50):\n        return 4\n    elif(ratio<50 and ratio>=20):\n        return 2\n    else:\n        return 1\n\ndef calculate_price(ratio, location, crop):\n    if(location=='Bijapur'):\n        base = bij[crop]\n        final = base - ((base*ratio)/100)\n        return final\n    elif(location=='Udupi'):\n        base = ud[crop]\n        final = base - ((base*ratio)/100)\n        return final\n    else:\n        base = bang[crop]\n        final = base - ((base * ratio) / 100)\n        return final\n\n\ndef bijapur():\n    conn = sqlite3.connect('crop.db')\n    c = conn.cursor()\n    c.execute(\"SELECT id, crop, investment, profit FROM bijapur\")\n    for i in c.fetchall():\n        index = i[0]\n        ratio = price_reduction(i[2],i[3])\n        final_price = calculate_price(ratio, 'Bijapur', i[1])\n        c.execute(\"UPDATE bijapur SET price= ? WHERE id= ?\",(final_price,index))\n    conn.commit()\n    conn.close()\n\ndef udupi():\n    conn = sqlite3.connect('crop.db')\n    c = conn.cursor()\n    c.execute(\"SELECT id, crop, investment, profit FROM udupi\")\n    for i in c.fetchall():\n        index = i[0]\n        ratio = price_reduction(i[2],i[3])\n        final_price = calculate_price(ratio, 'Udupi', i[1])\n        c.execute(\"UPDATE udupi SET price= ? WHERE id= ?\", (final_price, index))\n    conn.commit()\n    conn.close()\n\ndef bangalore():\n    conn = sqlite3.connect('crop.db')\n    c = conn.cursor()\n    c.execute(\"SELECT id, crop, investment, profit FROM bangalore\")\n    for i in c.fetchall():\n        index = i[0]\n        ratio = price_reduction(i[2],i[3])\n        final_price = calculate_price(ratio, 'Bangalore rural', i[1])\n        c.execute(\"UPDATE bangalore SET price= ? WHERE id= ?\", (final_price, index))\n    conn.commit()\n    conn.close()\n\n", "67": "from django.db.models.query import QuerySet\nfrom datetime import datetime\n\nclass WebPageQuerySet(QuerySet):\n\n    def webscrape_or_get(self, url):\n        from ..models import WebPage\n\n        resultSet = self.filter(url=url).order_by('-pk')[:1]\n        analysis = resultSet.first()\n\n        # The given url has no cached data\n        if(analysis == None):\n            # perform new webscraping\n            self.webscrape_and_save(url)\n            resultSet = WebPage.objects.filter(url=url).order_by('-pk')[:1]\n\n        # The given url has cached data. However it is outdated and has\n        # not been automatically deleted yet by the regular jobs.\n        # The replace() method is used to avoid problems caused by timezones\n        elif(datetime.now() > analysis.delete_on.replace(tzinfo=None)):\n            # Therefore, delete it and perform new webscraping\n            analysis.delete()\n            # perform new webscraping\n            self.webscrape_and_save(url)\n            resultSet = WebPage.filter(url=url).order_by('-pk')[:1]\n\n\n        return resultSet\n\n    def webscrape_and_save(self, url):\n        from ..models import WebPage, Heading\n        from .webscraping import analyze_page\n\n        # scrapes the url and gets its data\n        analysis_data = analyze_page(url)\n\n        # creates a new database entry for the url from the newly scraped data\n        analysis = WebPage()\n        analysis.url = url\n        analysis.html_version = analysis_data['html_version']\n        analysis.page_title = analysis_data['title']\n\n        analysis.internal_links = analysis_data['internal_links']\n        analysis.external_links = analysis_data['external_links']\n        analysis.inaccessible_links = analysis_data['inaccessible_links']\n        analysis.has_loginform = analysis_data['has_loginform']\n        analysis.save()\n\n        # creates new database entries for the associated headings\n        HEADING_TAGS = ['H1', 'H2', 'H3', 'H4','H5','H6']\n\n        for heading_count, tag in zip(analysis_data['headings'], HEADING_TAGS):\n            heading = Heading()\n            heading.type = tag\n            heading.count = heading_count\n            heading.webpage = analysis\n            heading.save()\n", "68": "from PyQt5 import QtCore, QtGui, uic\nfrom PyQt5.QtWidgets import (\n    QDialog, \n    QApplication, \n    QMainWindow, \n    QPushButton, \n    QVBoxLayout, \n    QCalendarWidget, \n    QSpinBox, \n    QLabel, \n    QDateEdit, \n    QWidget, \n    QProgressBar)\n\nimport pandas as pd\nimport random\nimport os   \nimport sys\n\n# ---- INITIALIZE DIRECTORIES ----\ndir_dataset = 'dataset/'\ndir_config = 'config/'\ndir_current = os.getcwd() # get the current current location\n\ndef resource_path(relative_path):\n    \"\"\" Get absolute path to resource, works for dev and for PyInstaller \"\"\"\n    base_path = getattr(sys, '_MEIPASS', os.path.dirname(os.path.abspath(__file__)))\n    return os.path.join(base_path, relative_path)\n\n# ---- INITIALIZE MATPLOTLIB ----\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\nfrom matplotlib.backends.backend_qt5agg import NavigationToolbar2QT as NavigationToolbar\nfrom matplotlib import pyplot as plt, dates as mdates\n        \nclass DYNARIMA(QMainWindow):\n    def __init__(self):\n        super(DYNARIMA, self).__init__()\n        self.ui = uic.loadUi(resource_path('DESIGN.ui'), self) # load ui file\n        self.prev_lags = self.new_lags = 1 # config_lags highlighter\n        self.dataframe = self.trainset = self.testset = ''\n        \n        # ---- ASSOCIATE ELEMENTS TO VARIABLE ----\n        # buttons\n        self.btn_compile = self.findChild(QPushButton, 'btn_compile')\n        self.btn_forecast = self.findChild(QPushButton, 'btn_forecast')\n        self.btn_webscrape = self.findChild(QPushButton, 'btn_webscrape')\n        \n        self.btn_compile.clicked.connect(self.compile)\n        self.btn_forecast.clicked.connect(self.forecast)\n        self.btn_webscrape.clicked.connect(self.webscrape)\n        \n        # configs\n        self.calendar = self.findChild(QCalendarWidget, 'calendar')\n        self.calendar.selectionChanged.connect(self.calendar_)\n        self.config_integrate = self.findChild(QSpinBox, 'config_integrate')\n        self.config_lags = self.findChild(QSpinBox, 'config_lags')\n        self.config_startdate = self.findChild(QDateEdit, 'config_startdate')\n        \n        # setup startdate\n        self.selected_date = QtCore.QDate.currentDate()\n        self.config_startdate.setDate(self.selected_date)\n        self.calendar.setSelectedDate(self.selected_date)\n        self.config_lags.valueChanged.connect(self.lags)\n        self.config_startdate.dateChanged.connect(self.startdate)\n        self.config_integrate.valueChanged.connect(self.integrate)\n        \n        # informations\n        self.txt_accuracy = self.findChild(QLabel, 'info_txt_accuracy')\n        self.txt_adf = self.findChild(QLabel, 'info_txt_adf')\n        self.txt_aic = self.findChild(QLabel, 'info_txt_aic')\n        self.txt_mae = self.findChild(QLabel, 'info_txt_mae')\n        self.txt_model = self.findChild(QLabel, 'info_txt_model')\n        self.txt_pvalue = self.findChild(QLabel, 'info_txt_pvalue')\n        \n        # matplot qwidgets\n        self.matplot_container = self.findChild(QVBoxLayout, 'matplot_container')\n                \n        # progress bars\n        self.progressbar = self.findChild(QProgressBar, 'progressbar')\n        self.progressbar_text = self.findChild(QLabel, 'progressbar_text')       \n        print('-- LINKED UI AND LOGIC --')\n        \n        # ---- INITIALLY LOCK AND SET TEXT ----\n        # index -1 initial (lock all except webscraper/update button)\n        # index 0 webscrape \n        # index 1 compile\n        # index 2 forecast\n        self.locker(-1)\n        print('-- LOCKED SEQUENCE --')\n        \n        \n        # ---- CREATE PREREQUISITE FILES ----\n        # if not found, create dataset and  folder with configurations \n        self.toScrape = True\n        if not os.path.exists(dir_dataset):\n                os.makedirs(dir_dataset)\n        elif len(os.listdir(dir_dataset)) >= 4:\n            self.btn_webscrape.setText('Compress')\n            self.toScrape = False\n        if not os.path.exists(dir_config):\n            os.makedirs(dir_config)\n        if len(os.listdir(dir_config)) < 2:\n            import json\n            # Google API OAuth2.0\n            client_secrets = json.dumps(\n                {\"web\":{\n                    \"client_id\":\"626625711266-90bhqs8j4vj9cru2jre94cbqamn7e9j8.apps.googleusercontent.com\",\n                    \"project_id\":\"original-bot-295405\",\n                    \"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\n                    \"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"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\",\n                    \"redirect_uris\":[\"http://localhost:8080/\"],\n                    \"javascript_origins\":[\"http://localhost:8080\"]}\n                })\n            # Matplotlib Stylesheet\n            matplotstyle = \"\"\"\n                # Seaborn common parameters\n                # .15 = dark_gray\n                # .8 = light_gray\n                legend.frameon: False\n                legend.numpoints: 1\n                legend.scatterpoints: 1\n                xtick.direction: out\n                ytick.direction: out\n                axes.axisbelow: True\n                font.family: sans-serif\n                grid.linestyle: -\n                lines.solid_capstyle: round\n\n                # Seaborn darkgrid parameters\n                axes.grid: True\n                axes.edgecolor: white\n                axes.linewidth: 0\n                xtick.major.size: 0\n                ytick.major.size: 0\n                xtick.minor.size: 0\n                ytick.minor.size: 0\n\n                # from mplcyberpunk\n                text.color: 0.9\n                axes.labelcolor: 0.9\n                xtick.color: 0.9\n                ytick.color: 0.9\n                grid.color: 2A3459\n\n                # Custom\n                font.sans-serif: Overpass, Helvetica, Helvetica Neue, Arial, Liberation Sans, DejaVu Sans, Bitstream Vera Sans, sans-serif\n                axes.prop_cycle: cycler('color', ['18c0c4', 'f62196', 'A267F5', 'f3907e', 'ffe46b', 'fefeff'])\n                image.cmap: RdPu\n                figure.facecolor: 212946\n                axes.facecolor: 212946\n                savefig.facecolor: 212946\n                \"\"\"\n            # write the files to the config\n            with open(f\"{dir_config}client_secrets.json\", \"w\") as outfile:\n                outfile.write(client_secrets)\n            with open(f\"{dir_config}matplotlib-dark.mplstyle\", \"w\") as outfile:\n                outfile.write(matplotstyle)\n        plt.style.use(dir_config+'matplotlib-dark.mplstyle') # change the matplotlib theme\n        print('-- PREREQUISITES COMPLETE --')    \n        \n    def compile(self):\n        if self.btn_compile.text() == 'Compile':\n            self.progressbar_text.setText('Compiling...')\n            self.progressbar.setValue(0)\n            start = self.config_startdate.date().toString(\"yyyy-MM-dd\") # starting date\n            predict = self.config_lags.value() # number of days to forecast\n            \n            print('-- SPLITTING TRAIN AND TEST DATASETS --')\n            # train date\n            train_fr = pd.to_datetime('Jan 30, 2020').date() # DO NOT TOUCH\n            train_to = (pd.to_datetime(start) + pd.DateOffset(days=-1)).date()\n            # test date\n            test_fr  =  pd.to_datetime(start).date()\n            test_to  =  (test_fr + pd.DateOffset(days=predict-1)).date()\n            # split dates of the dataset\n            self.trainset = self.dataframe.loc[train_fr:train_to].rename(columns={'Cases':'Train'})\n            self.testset  = self.dataframe.loc[test_fr:test_to].rename(columns={'Cases':'Test'})\n            self.testset.Cases = ['Test']\n            \n            print('--- TRAINING DATASET ---')\n            print(self.trainset)\n            print('--- TESTING DATASET ---')\n            print(self.testset)\n            \n            title = f'Train-Test Split [{self.trainset.index[0]} \u2014 {self.testset.index[-1]}]'\n            self.clearLayout(self.matplot_container)\n            df_split = Plotter(title, self.trainset, self.testset)\n            self.matplot_container.addWidget(df_split)\n            \n            self.progressbar_text.setText('Compilation Complete')\n            self.progressbar.setValue(100)\n            self.locker(1)\n        elif self.btn_compile.text() == 'Reset':\n            self.progressbar.setValue(0)\n            self.locker(0)\n            self.progressbar_text.setText('Dataframe Reset')\n    \n    def webscrape(self):\n        self.locker() # lock eveything when this button is clicked\n        if self.toScrape or self.btn_webscrape.text() == 'Update':\n            self.thread = ThreadClass(parent=None, index=0, toScrape=True)\n        else:\n            self.thread = ThreadClass(parent=None, index=0, toScrape=False)\n        # start and configure locker when this thread is done\n        self.thread.progress_signal.connect(self.progress_worker)\n        self.thread.output_signal.connect(self.output_worker)\n        self.thread.start() \n             \n    def forecast(self):\n        self.locker()\n        self.thread = ThreadClass(parent=None, index=2, dataframe=[self.trainset, self.testset, self.config_integrate.value()])\n        self.thread.progress_signal.connect(self.progress_worker)\n        self.thread.output_signal.connect(self.output_worker)\n        self.thread.start() \n        \n    def lags(self):\n        format, date, init = self.reset_highlight()\n        self.new_lags = self.config_lags.value()\n        # reset initial selected position\n        self.calendar.setSelectedDate(date)\n        self.update_highlight()\n        self.prev_lags = self.new_lags\n        \n    def startdate(self):\n        # updates the calendar\n        self.reset_highlight()\n        self.calendar.setSelectedDate(self.config_startdate.date())\n        self.selected_date = self.config_startdate.date()\n        self.update_highlight()\n        \n    def integrate(self):\n        diff = self.config_integrate.value()\n        self.txt_model.setText(f\"Model: {(7, diff, 8)}\")\n        \n    def calendar_(self):\n        # updates the startdate config\n        self.reset_highlight()\n        self.config_startdate.setDate(self.calendar.selectedDate())\n        self.selected_date = self.config_startdate.date()\n        self.update_highlight()\n    \n    def reset_highlight(self):\n        # reset highlights\n        format = QtGui.QTextCharFormat()  \n        date = self.selected_date\n        init = self.calendar.palette().brush(QtGui.QPalette.Base)\n        format.setBackground(init)\n        for i in range(self.prev_lags):\n            self.calendar.setDateTextFormat(date.addDays(i), format)\n        return format, date, init\n    \n    def update_highlight(self):\n        # change the temporary up-date to the number of lags  \n        format = QtGui.QTextCharFormat()  \n        date = self.selected_date\n        active = self.calendar.palette().brush(QtGui.QPalette.LinkVisited)\n        format.setBackground(active)\n        for i in range(self.new_lags):\n            self.calendar.setDateTextFormat(date.addDays(i), format)\n    \n    def locker(self, index=None):\n        # lock everything\n        self.btn_compile.setText('Compile')\n        self.calendar.setDisabled(True)\n        self.btn_compile.setDisabled(True)\n        self.btn_forecast.setDisabled(True)\n        self.btn_webscrape.setDisabled(True)\n        self.config_integrate.setDisabled(True)\n        self.config_lags.setDisabled(True)\n        self.config_startdate.setDisabled(True)\n        \n        self.txt_adf.setText('ADF: 0.0')\n        self.txt_pvalue.setText('P-Value: 0.0')\n        self.txt_aic.setText('AIC: 0.0')\n        self.txt_model.setText(f'Model: {(7, self.config_integrate.value(), 8)}' )\n        self.txt_mae.setText('MAE: 0%')\n        self.txt_accuracy.setText('Accuracy: 0%')\n        \n        if index==-1: # initial webscrape lock\n            self.btn_webscrape.setText('Webscrape')\n            self.btn_webscrape.setDisabled(False)\n        elif index==0 or index==2: # webscrape / forecast finished, what to unlock?\n            self.btn_webscrape.setText('Update')\n            self.calendar.setDisabled(False)\n            self.btn_webscrape.setDisabled(False)\n            self.btn_compile.setDisabled(False)\n            self.config_lags.setDisabled(False)\n            self.config_startdate.setDisabled(False)\n        elif index==1: # compile finished, what to unlock?\n            self.btn_webscrape.setText('Update')\n            self.btn_compile.setText('Reset')\n            self.btn_webscrape.setDisabled(False)\n            self.btn_compile.setDisabled(False)\n            self.btn_forecast.setDisabled(False)\n            self.config_integrate.setDisabled(False)\n\n    # update the progressbar from thread emits\n    def progress_worker(self, counter, title, activate):\n        index = self.sender().index\n        self.progressbar.setValue(counter)\n        self.progressbar_text.setText(title)\n        if activate: # activates locker based on index\n            self.locker(index)\n    \n    # get the dataframe from the worker thread\n    def output_worker(self, data):\n        index = self.sender().index\n        if index==0: # get output of webscrape\n            self.dataframe = data[0]\n            self.calendar.setMaximumDate(data[0].index[-1])\n            self.config_startdate.setMaximumDate(data[0].index[-1])\n            \n            title = f'Webscraped Dataframe [{data[0].index[0]} \u2014 {data[0].index[-1]}]'\n            df_canvas = Plotter(title, data[0]) \n            self.clearLayout(self.matplot_container)\n            self.matplot_container.addWidget(df_canvas) # adding canvas to the layout\n            \n            print('-- CANVAS PLOTTER CREATED --')\n        elif index==2: # get output of forecast\n            adf, pvalue, aic, model, mae, accuracy, df_test, fit_model = data    \n            print(adf, pvalue, aic, model, mae, accuracy)\n\n            self.txt_adf.setText(f'ADF: {round(adf,8)}')\n            self.txt_pvalue.setText(f'P-Value: {round(pvalue,8)}')\n            self.txt_aic.setText(f'AIC: {round(aic,8)}')\n            self.txt_model.setText(f'Model: {model}')\n            self.txt_mae.setText(f'MAE: {round(mae,2)}%')\n            self.txt_accuracy.setText(f'Accuracy: {round(accuracy,2)}%')    \n            \n            plt.rc('font', size=6) # controls default text sizes\n            plt.tight_layout()\n\n            model_diagnostic = FigureCanvas(fit_model.plot_diagnostics(figsize=(30,10)))\n            model_comparison = Plotter(f'Accuracy Analysis', df_test['Test'], df_test['Model'])\n            self.clearLayout(self.matplot_container)\n            self.matplot_container.addWidget(model_comparison) # adding canvas to the layout\n            self.matplot_container.addWidget(model_diagnostic)   \n               \n    def clearLayout(self, layout):\n        if layout is not None:\n            while layout.count():\n                child = layout.takeAt(0)\n                if child.widget() is not None:\n                    child.widget().deleteLater()\n                elif child.layout() is not None:\n                    self.clearLayout(child.layout())\n\nclass ThreadClass(QtCore.QThread): \n    progress_signal = QtCore.pyqtSignal(int, str, bool)\n    output_signal = QtCore.pyqtSignal(list)\n    def __init__(self, parent=None, index=0, toScrape=False, dataframe=[]):\n        super(ThreadClass, self).__init__(parent)\n        self.index = index\n        self.is_running = True\n        self.toScrape = toScrape\n        self.dataframe = dataframe\n        \n    def run(self):\n        if self.index==0: # webscrape\n            if self.toScrape: # if dataset is missing\n                # --AUTHENTICATE--\n                from pydrive.auth import GoogleAuth\n                from pydrive.drive import GoogleDrive\n                # reset progress bar\n                self.progress_signal.emit(0,'Authenticate Google Drive', False)\n                \n                GoogleAuth.DEFAULT_SETTINGS['client_config_file'] = 'config/client_secrets.json'\n                gauth = GoogleAuth()\n                # Try to load saved client credentials\n                gauth.LoadCredentialsFile(\"config/credentials.txt\")\n                if gauth.credentials is None:\n                    gauth.LocalWebserverAuth() # Authenticate if they're not there\n                elif gauth.access_token_expired:\n                    gauth.Refresh() # Refresh them if expired\n                else:\n                    gauth.Authorize() # Initialize the saved creds\n                # Save the current credentials to a file\n                gauth.SaveCredentialsFile(\"config/credentials.txt\")\n                \n                # -- WEBSCRAPE --\n                drive = GoogleDrive(gauth)\n                target_folder = drive.ListFile( # Target DOH folder ID\n                    {'q': \"'1_PhyL7788CLgZ717TklQ_iuMxvvnrrNn' in parents and trashed=false\"}).GetList()\n                webthreads = []\n                progress = 0\n                for idx, dataset in enumerate(target_folder):\n                    # find datasets and put list its information\n                    id = dataset['id'] # file ID \n                    query = 'case information' # look for this\n                    title = dataset['title'].lower() # rename\n                    if (query in title):\n                        print(f'FOUND.. {title} : {idx}')\n                        webthreads.append([title[52:], id, drive])\n                    # update progress bar\n                    progress = progress + int((1/len(target_folder))*100)\n                    self.progress_signal.emit(progress, f'Retrieving Information... {title}', False)     \n                # -- RETRIEVE --\n                progress = 0\n                for thread in webthreads:\n                    title, id, drive = thread\n                    print(f'dumping {thread} for {title}')\n                    # dump file in the console\n                    downloaded = drive.CreateFile({'id': id}) \n                    # splice filename then download file\n                    downloaded.GetContentFile(f'{dir_dataset+title}') \n                    print(f'downloaded {title}')\n                    # update progress bar\n                    progress = progress + int((1/len(webthreads))*100)\n                    self.progress_signal.emit(progress, f'Downloaded... {title}', False) \n                self.progress_signal.emit(0, 'Download Compelete', False) \n            # -- CONCATENATE --\n            # emit a worker signal to the mainthread to reset progressbar\n            self.progress_signal.emit(0,'Compiling...',False)\n            # concatenate every scraped dataset to df\n            folder = os.listdir(dir_dataset) # create folder object \n            # add store and read dataset information to list\n            batch = []\n            progress = 0\n            for csv in folder:\n                if '.csv' in csv:\n                    # update progress bar\n                    progress = progress + int((1/len(folder))*100)\n                    self.progress_signal.emit(progress, f'Reading {csv}', False)\n                    # append csv dump to batch list to concatenate                \n                    batch.append(pd.read_csv(dir_dataset+csv, usecols=['DateRepConf'], parse_dates=['DateRepConf']))\n            # concatenate csv\n            df = pd.concat(batch, ignore_index=True)\n            # check null values\n            df.isnull().values.any()\n            print(df,'\\n')\n            # rename date-confirmed column\n            df.rename(columns={'DateRepConf':'Dates'}, inplace=True) \n            # count distinct-repeating dates\n            df = df.groupby(df.Dates.dt.date).agg('count').rename(columns={'Dates':'Cases'})\n            print(df,'\\n')\n            self.progress_signal.emit(100, f'Compression Successful  [{df.index[0]} \u2014 {df.index[-1]}]', True)\n            self.output_signal.emit([df])\n            \n        elif self.index == 2: # forecast\n            self.progress_signal.emit(0,'Initializing Forecast', False)\n            # Statistic Library: SARIMAX Model\n            from statsmodels.tsa.statespace.sarimax import SARIMAX\n            \n            print(self.dataframe)\n            in_train = self.dataframe[0].reset_index()\n            in_test = self.dataframe[1].reset_index()\n            integration = self.dataframe[2]\n            predict = len(in_test)\n            \n            # check for stationarity (adf and pvalue)\n            print('-- EVALUATING STATIONARITY --')\n            progress = 0\n            check_stationary = 0\n            for i in range(integration):\n                # update progress bar\n                progress = progress + int((1/integration)*100) \n                check_stationary = in_train['Train'].diff().dropna()\n                self.progress_signal.emit(progress, 'Evaluating Stationarity...', False)  \n            # Statistic Library: Augmented Dickey Fuller Function\n            from statsmodels.tsa.stattools import adfuller\n            result_stationary = adfuller(check_stationary)\n            adf = result_stationary[0]\n            pvalue = result_stationary[1]\n            print(f'ADF Statistic: {adf}')\n            print(f'p-value: {pvalue}')\n\n            # get aic scores\n            min_params = (7,integration,8)\n            # generate the sarimax instruction\n            print('-- BUILDING ARIMA OBJECT --')\n            self.progress_signal.emit(25, f'Building Object ARIMA{min_params}...', False)\n            arima = SARIMAX(in_train['Train'], order=min_params, simple_differencing=False)\n            # fitting the model\n            print('-- FITTING THE MODEL --')\n            self.progress_signal.emit(50, f'Fitting The Model... {arima}', False)\n            model = arima.fit(disp=False)\n            aic = model.aic\n            print(model.summary())\n            # forecast the model\n            print('-- GENERATING MODEL FORECAST --')\n            self.progress_signal.emit(75, f'Forecasting the Model... {model}', False)\n            # retrieve forecast values\n            predicted_values = model.get_prediction(end=model.nobs + predict).predicted_mean.round()\n            # create new model column to existing dataframes\n            out_train = in_train.assign(Model=predicted_values[:-predict-1])\n            out_test = in_test.assign(Model=predicted_values[-predict-1:].reset_index(drop=True))\n            # prints the error and accuracy in percent            \n            print('-- SUMMARY --')\n            import numpy as np\n            err = np.subtract(out_test.Test, out_test.Model)\n            abs_err = np.abs(err)\n            total_cases = np.sum(out_test.Test)\n            total_abs_err = np.sum(abs_err)\n            \n            mae = (total_abs_err/total_cases)*100\n            accuracy = 100-(total_abs_err/total_cases)*100\n            print('MAE: {:0.2f}%'.format(mae))\n            print('Forecast Accuracy: {:0.2f}%'.format(accuracy))\n            self.progress_signal.emit(100, f'Forecast Successful...', True)\n            self.output_signal.emit([adf, pvalue, aic, min_params, mae, accuracy, out_test, model])\n    \n    def stop(self):\n        self.is_running = False\n        print('Stopping thread...', self.index)\n        self.terminate()\nclass Plotter(FigureCanvas):\n    # # -- HOW THIS CLASS WORKS --\n    # # a figure instance to plot on\n    # self.figure = plt.figure()\n    # # this is the Canvas Widget that\n    # # displays the 'figure'it takes the\n    # # 'figure' instance as a parameter to __init__\n    # self.canvas = FigureCanvas(self.figure)\n    # # adding canvas to the layout\n    # self.matplot_container.addWidget(self.canvas)\n    \n    def __init__(self, title, *dataframe, parent=None):\n        super(Plotter, self).__init__(parent) \n        self.figure = None\n        plt.close(self.figure)\n        self.title = title\n        \n        # Creating your plot\n        fig, ax = plt.subplots(figsize=(30, 10))\n        \n        # Plot the train and test sets on the same axis ax\n        for obj in dataframe:\n            obj.plot(ax=ax)\n            \n        fmt_month = mdates.MonthLocator() # Minor ticks every month.\n        fmt_year = mdates.YearLocator() # Minor ticks every year.\n        ax.xaxis.set_minor_locator(fmt_month)\n        ax.xaxis.set_minor_formatter(mdates.DateFormatter('%b')) # '%b' to get the names of the month\n        ax.xaxis.set_major_locator(fmt_year)\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n        # fontsize for month labels\n        ax.tick_params(labelsize=6, which='both')\n        # create a second x-axis beneath the first x-axis to show the year in YYYY format\n        sec_xaxis = ax.secondary_xaxis(-0.1)\n        sec_xaxis.xaxis.set_major_locator(fmt_year)\n        sec_xaxis.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n        # Hide the second x-axis spines and ticks\n        sec_xaxis.spines['bottom'].set_visible(False)\n        sec_xaxis.tick_params(length=0, labelsize=6)\n\n        plt.title(title, fontsize=7)\n        plt.grid(which = 'both', linewidth=0.3)\n        plt.xlabel('')\n        plt.legend(loc='upper left') \n        \n        self.figure = fig\n        \n    # def mouseDoubleClickEvent(self, event):\n    #     print('matplot detached')\n    #     plt.show()\n        \n        \ntry:\n    # fixes the windows 11 tasbar icon\n    import ctypes\n    appid = 'mycompany.myproduct.subproduct.version' # arbitrary string\n    ctypes.windll.shell32.SetCurrentProcessExplicitAppUserModelID(appid)\nexcept:\n    print(\"you're probably not on windows\")\nif __name__ == \"__main__\":\n    app = QApplication(sys.argv)\n    win = DYNARIMA()\n    win.setWindowIcon(QtGui.QIcon(resource_path('DYNARIMA.ico')))\n    win.show()\n    sys.exit(app.exec_())\n\n    ", "69": "from __future__ import print_function\nimport requests\nimport lxml\nimport bs4\n\n\"\"\"\n 123-Py-WebScrappingExercises\n\"\"\"\n\nprint(\"123-Py-WebScrappingExercises\")\n\n\"\"\"\n\n\"\"\"\nprint(\"\\n123-Py-WebScrappingExercises\")\nprint(\"- - - - - - - - - - \")\n\n\n# TASK: Use requests library and BeautifulSoup to connect to\n# http://quotes.toscrape.com/ and get the HMTL text from the homepage.\n\nwebScrape = requests.get(\"http://quotes.toscrape.com/\")\n\nprint(\"\\n\\nWeb Scrape\")\nprint(\"- - - - - - - - - - \")\nprint(webScrape)\nprint(webScrape.text)\n\n\nsoupwebScrape = bs4.BeautifulSoup(webScrape.text, \"lxml\")\n\nprint(\"\\n\\nAuthors\")\nprint(\"- - - - - - - - - - \")\nprint(soupwebScrape.select('author'))\n\nprint(soupwebScrape.select(\"author\"))\nproducts = soupwebScrape.select(\"author\")\n\nsoupwebScrape = bs4.BeautifulSoup(webScrape.text, 'lxml')\n\nprint(soupwebScrape.select(\".author\"))\n\nprint(\"\\n\\nAuthors with HTML\")\nprint(\"- - - - - - - - - - \")\nfor index in soupwebScrape.select(\".author\"):\n    print(index)\n\n\nprint(\"\\n\\nAuthors without HTML\")\nprint(\"- - - - - - - - - - \")\nauthors = set()\nfor name in soupwebScrape.select(\".author\"):\n    authors.add(name.text)\n\nfor index in authors:\n    print(index)\n\n\nprint(soupwebScrape.select(\".quote\"))\n\nprint(\"\\n\\nQuotes with HTML\")\nprint(\"- - - - - - - - - - \")\nfor index in soupwebScrape.select(\".quote\"):\n    print(index)\n\n\nprint(\"\\n\\nQuotes\")\nprint(\"- - - - - - - - - - \")\nquotes = set()\nfor name in soupwebScrape.select(\".text\"):\n    quotes.add(name.text)\n\n\nfor index in quotes:\n    print(index)\n\nprint(\"\\n\\nquotes.pop()\")\nprint(\"- - - - - - - - - - \")\nprint(quotes)\n\n\nprint(\"\\n\\noneLiners with HTML\")\nprint(\"- - - - - - - - - - \")\n\noneLiners = set()\nfor name in soupwebScrape.select(\".tag-item\"):\n    oneLiners.add(name.text)\n\n\nfor index in oneLiners:\n    print(index)\n", "70": "import sqlite3\r\nimport pandas as pd\r\ncon = sqlite3.connect(r\"C:\\Users\\VARUN\\Desktop\\flask\\Web Scraper\\Webscrape\\Webscrape\\mydata.db\")\r\n\r\n#con = sqlite3.connect(r\"C:\\Users\\VARUN\\Desktop\\flask\\Web Scraper\\Webscrape\\Webscrape\\mydata.db\")\r\ncur = con.cursor()\r\ncur.execute(\"SELECT * FROM data_tb\")\r\ndata = cur.fetchall()\r\nprint(data[0][0])\r\n#sql_query = pd.read_sql_query ('''\r\n#                               SELECT\r\n#                               *\r\n#                               FROM data_tb\r\n#                              ''', con)\r\n\r\n#df = pd.DataFrame(sql_query, columns = ['Title', 'Author', 'Paragraph'])\r\n#print (df)", "71": "# import Project.UserInterface as ui\n# import Project.WebScrapeCharts as wsc\n# import Project.AudioFeatures as af\n# import Project.DistanceSongs as ds\n# import Project.MinSpanTree as mst\n# import Project.Visualize as vis\n# import Project.KMeans as km\n\nimport UserInterface as ui\nimport WebScrapeCharts as wsc\nimport AudioFeatures as af\nimport DistanceSongs as ds\nimport MinSpanTree as mst\nimport Visualize as vis\nimport Project.KMeans as km\n\nimport time\ntesting = False\n\n# -------------------------------- FUNCTIONS FOR GETTING DATA FROM FILES ----------------------------------------------\ndef check_file(filename):\n    print(f'\\nchecking file:\\t{filename}')\n\n    try:\n        with open(filename,'r') as rf:\n            lines = rf.readlines()\n            counter = len(lines)\n            print(f'contains data:\\t{counter == 200}')\n            return counter == 200\n    except:\n        print(f'*** No such File exists -> Creating File {filename} ***')\n        return False\n\ndef get_ids(cc,rc):\n    filename = f'test_{cc}_{rc}.txt'\n\n    if check_file(filename):\n        # Ids exist in file\n        print(f'load from file:\\t{filename}\\n')\n        webscrape.load_ids_from_file(cc, rc)\n    else:\n        # ids don't exist yet, save them\n        print('no ids in file, need to webscrape for ids')\n        print(f'saving ids for future testing!\\n')\n        webscrape.save_for_testing(cc, rc)\n\n    return webscrape.song_ids_array\n\ndef get_features(cc,rc, song_ids):\n    filename_feat = f'test_{cc}_{rc}_feat.txt'\n    # check if features exist\n    if check_file(filename_feat):\n        # features exist, load them\n        print(f'load from file:\\t{filename_feat}\\n')\n        audio.load_features(filename_feat)\n    else:\n        # Features doesnt exist, save them\n        print('no features in file, need to access spotify for features')\n        print(f'saving features for future testing!\\n')\n        audio.save_features(filename_feat, song_ids)\n    return audio.features\n\n# ----------------------------------------------- RUN MAIN ------------------------------------------------------------\nif __name__ == '__main__':\n    # get the start time\n    start = time.time()\n\n    # Define the Classes to be used\n    user = ui.UserInterface()\n    webscrape = wsc.WebScrapeCharts()\n    audio = af.AudioFeatures()\n    distances = ds.DistanceSongs()\n\n\n    # If testing is True, load ids and features from saved files to speed up time -> (reduces from 40s to about 1.3s)\n    if testing:\n        # possible to change the country code (cc) and recurrence (rc), will create new files for given inputs.\n        cc = 'nl'\n        rc = 'daily'\n        # Get the ids and features. It checks if the files contain values. If yes, it loads the files, if not it webscrapes\n        # and gets the audio features and then saves them for faster running next time\n        song_ids = get_ids(cc,rc)\n        song_features = get_features(cc,rc,song_ids)\n\n        # Set the webscrape song id array and audio features\n        webscrape.song_ids_array = song_ids\n        audio.features = song_features\n\n    else:\n        # Get the user input and find the ids and features from that\n        page_found = False\n\n        while not page_found:\n            user.get_user_input()\n            if webscrape.get_charts(user.country_code, user.recurrence):\n                page_found = True\n            else:\n                print('Collecting Chart from spotifycharts.com')\n                webscrape.set_default()\n                print('Charts dont exist for given country\\nPlease enter a different country!')\n\n        print('Retrieving Audio Features')\n        audio.get_features(webscrape.song_ids_array)\n\n\n\n    # Calculate distances and min spanning tree\n    print('Calculating Song Similarity')\n    distances.find_distances(audio.features)\n\n    print('Computing Minimum Spanning Tree')\n    minspantree = mst.MinSpanTree()\n    minspantree.create_min_span_tree(distances.adj_dict)\n    #print(minspantree.MST_graph.edges())\n\n    print('Computing Clusters')\n    kmeans = km.Kmeans()\n    kmeans.k_means(audio.features)\n\n    print('Visualizing')\n    vis.Visualize(kmeans.clusters, minspantree.MST_graph, kmeans.scaled)\n\n    # get the end time\n    end = time.time()\n    print(f'\\n...finished running in {round(end - start, 3)} seconds')\n", "72": "import csv\nfrom webscraper import webscrape\nfrom machine_learning import clean_string, SentimentClass\nimport time\n\n\nclass StockTask:\n    def __init__(self):\n        self.stock_list = []\n        self.get_stock_list()\n        self.sentiment_class = SentimentClass()\n\n    def get_stock_list(self):\n        with open(\"data/spy_list.csv\", \"r\") as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            for line in csv_reader:\n                self.stock_list.append(line['Symbol'])\n        csv_file.close()\n\n    def start(self):\n        \"\"\"\n        Start the process of webscraping through all the stocks in the stock_list list. Then push the results through\n        to machine learning, and write everything to CSV files.\n        \"\"\"\n        for stock in self.stock_list:\n            # Get the latest tweets\n            tweets = webscrape(stock)\n            for i in range(len(tweets)):\n                tweets[i] = clean_string(tweets[i])\n            self.write_to_csv(stock, tweets, \"_latest\")\n\n            # Get the top tweets\n            tweets = webscrape(stock, top_results=True)\n            for i in range(len(tweets)):\n                tweets[i] = clean_string(tweets[i])\n            self.write_to_csv(stock, tweets, \"_top\")\n            print(f\"Wrote ${stock} to top & latest\")\n            time.sleep(15)\n\n    def write_to_csv(self, ticker: str, tweets, extra_title=\"\"):\n        with open(f\"data/stock_name_csv/{ticker.upper()}{extra_title}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n            csv_writer = csv.writer(csv_file)\n            for tweet in tweets:\n                sentiment = self.sentiment_class.get_sentiment(tweet)\n                if len(sentiment.labels) > 0:  # If the Sentence model is empty/has no prediction, it will not be added\n                    csv_writer.writerow([tweet, sentiment.labels[0].value, sentiment.labels[0].score])\n        csv_file.close()\n\n\nif __name__ == \"__main__\":\n    new_task = StockTask()\n    while True:\n        new_task.start()\n        print(\"FINISHED, sleeping then restarting\")\n        # break\n        time.sleep(2 * 60 * 60)\n", "73": "# -*- mode: python ; coding: utf-8 -*-\n\nblock_cipher = None\n\n\na = Analysis(['NRE_webscrape.py'],\n             pathex=['C:\\\\Users\\\\gwilliams\\\\Documents\\\\GitHub\\\\RME_Rail_Fares\\\\RME_Rail_Fares'],\n             binaries=[],\n             datas=[\n\t\t\t ('C:\\\\Users\\\\gwilliams\\\\Documents\\\\GitHub\\\\RME_Rail_Fares\\\\1_READ_ME_Instructions\\\\Instructions for use.txt','1_READ_ME_Instructions'),\n\t\t\t ('C:\\\\Users\\\\gwilliams\\\\Documents\\\\GitHub\\\\RME_Rail_Fares\\\\2_Route_and_times_metadata\\\\route_and_time_metadata.xlsx','2_Route_and_times_metadata'),\n\t\t\t ('C:\\\\Users\\\\gwilliams\\\\Documents\\\\GitHub\\\\RME_Rail_Fares\\\\3_Data_goes_here\\\\Placeholder for data.txt','3_Data_goes_here'),\n\t\t\t ('C:\\\\Users\\\\gwilliams\\\\Documents\\\\GitHub\\\\RME_Rail_Fares\\\\3_Data_goes_here\\\\appended_data\\\\appended_data_for_intial_run.csv','3_Data_goes_here\\\\appended_data')\n\t\t\t \n\t\t\t ],\n             hiddenimports=[],\n             hookspath=[],\n             runtime_hooks=[],\n             excludes=[],\n             win_no_prefer_redirects=False,\n             win_private_assemblies=False,\n             cipher=block_cipher,\n             noarchive=False)\npyz = PYZ(a.pure, a.zipped_data,\n             cipher=block_cipher)\nexe = EXE(pyz,\n          a.scripts,\n          [],\n          exclude_binaries=True,\n          name='NRE_webscrape',\n          debug=False,\n          bootloader_ignore_signals=False,\n          strip=False,\n          upx=True,\n          console=True )\ncoll = COLLECT(exe,\n               a.binaries,\n               a.zipfiles,\n               a.datas,\n               strip=False,\n               upx=True,\n               upx_exclude=[],\n               name='NRE_webscrape')\n", "74": "from bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\n\ndf = pd.read_csv('salaries_by_college_major.csv')\nchart_list = []\n\nclass Scepedata():\n    def __init__(self,num):\n        self.payscale = f\"https://www.payscale.com/college-salary-report/majors-that-pay-you-back/bachelors/page/{num}\"\n        self.responde = requests.get(self.payscale, headers={\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"})\n        self.respo_txt = self.responde.text\n        self.webscrape = BeautifulSoup(self.respo_txt, \"html.parser\")\n        self.major_name = self.webscrape.find_all(class_=\"csr-col--school-name\")\n        self.total_page_num = self.webscrape.find_all(class_=\"pagination__btn--inner\")\n        self.data_name = self.webscrape.find_all(class_='data-table__title')\n        self.data_value = self.webscrape.find_all(class_=\"data-table__value\")\n\nlist1 = []\nlist2 = []\nlist3 = []\nlist4 = []\nlist5 = []\ndeneme = Scepedata(1)\n\nprint(deneme.total_page_num)\nfor n in deneme.total_page_num:\n    if not n.text == '' or n.text == '\u00e2\u20ac\u00a6':\n        chart_list.append(n.text)\nmax_page = (len(chart_list))\nmax_page_value = int(chart_list[max_page-1])+1\n\nfor pages in range (1,max_page_value):\n    open_web_page = Scepedata(pages)\n    print(open_web_page.data_value)\n    for n in deneme.major_name:\n        if not n.text == 'Major':\n            metin = n.text.split(':')\n            list1.append(metin[1])\n    # for n in deneme.data_name:\n    #     name = n.text\n    #     print(name)\n    counter = 1\n    for n in deneme.data_value:\n        value = n.text\n        # if counter == 1:\n        #     df['Rank']= value\n        if counter == 2:\n            list2.append(value)\n        # if counter == 3:\n        #     df['Type'] = value\n        if counter == 4:\n            list3.append(value)\n        if counter == 5:\n            list4.append(value)\n        # if counter == 6:\n        #     list5.append(value)\n        counter +=1\n        if counter == 7:\n            counter = 1\n\ndf['Undergraduate Major'] = list2\ndf['Starting Median Salary'] = list3\ndf['Mid-Career Median Salary'] = list4\n# df['Meaning Value'] = list5\nprint(df)\nprint(df.head())\n# print(df)\n\n# print(max(chart_list))\n# print(deneme.total_page_num)\n# print(deneme.data_name)\n# print(deneme.data_value)\n", "75": "from bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport re\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\ndf=pd.read_json('News_Category_Dataset_v2.json',lines=True)\ndata=df.drop(['authors','link','short_description','date'],axis=1)\ndata.dropna(inplace=True)\n\nnews_href=['https://edition.cnn.com/','https://www.bbc.com/','https://www.nytimes.com/','https://www.buzzfeednews.com/','https://www.aljazeera.com/news/','https://news.yahoo.com/','https://timesofindia.indiatimes.com/','https://www.theguardian.com/international','https://www.cnbc.com/world/?region=world','https://www.cbsnews.com/','https://www.abc.net.au/news/','https://www.mirror.co.uk/','https://www.express.co.uk/news','https://www.bbc.com/sport']\n\n# web scrape garera lsit ma halne code\nclass webscrape:\n    def __init__(self,link):\n        self.link=link\n    def word_filter(self):\n        all_news=[]\n        for i in range(len(self.link)):\n            source=requests.get(news_href[i]).text\n            soup=BeautifulSoup(source,'lxml')\n            all_link = soup.find_all('a')\n            for link in all_link:\n                new_text=re.sub(\"\\s+\" , \" \",link.text)\n                if len(new_text) >= 50 and len(new_text) <= 150:\n                    if re.search('[a-zA-Z]',new_text) is not None:\n\n                        all_news.append(new_text)\n        return all_news \n    \n    @staticmethod\n    def text_filter(arr):\n        stemmed_news=[]    \n        for i in range(len(arr)):\n            mod_text=re.sub('[^a-zA-Z]',' ',arr[i])\n            mod_text=mod_text.lower()\n            mod_text=mod_text.split(' ')\n            mod_text=filter(None, mod_text)\n            news=[ps.lemmatize(sentence) for sentence in mod_text if sentence not in (stopwords.words('english'))]\n            news=','.join(news)\n            news=news.replace(\",\",\" \")\n            stemmed_news.append(news)\n        return stemmed_news\n                \nwebscrape1=webscrape(news_href)\nall_news = webscrape1.word_filter()\nall_news=list(set(all_news))\ngh=pd.DataFrame(all_news)\n\nps=WordNetLemmatizer()\nstemmed_news=[]\n\nnew_data = X_train=data['headline'].tolist()\nX_train = webscrape.text_filter(new_data)\nX_test = webscrape.text_filter(all_news)\n\ncv=CountVectorizer()\ntrain= cv.fit_transform(X_train)\ntest = cv.transform(X_test)\n\nmb=MultinomialNB(alpha=0.7)\ny_train=df['category']\nmb.fit(train,y_train)\nmb_pred=mb.predict(test)\n\ntr=pd.DataFrame({'title':all_news,'category':mb_pred})\n\nprint(tr[tr['category'] == 'SPORTS'])", "76": "from zipfile import ZipFile\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as ec\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.keys import Keys\nimport time\nfrom selenium.webdriver.common.action_chains import ActionChains\nimport glob\nimport pandas as pd\nimport os\nimport zipfile\nimport webbrowser\nimport matplotlib.pyplot as plt\n\n\n \n#setting up the webdriver and url(*Stretch Goal-Using a webscraper*, *Category 1-Define 3 functions*)\nclass WebScrape():\n    global options\n    options = Options() \n     \n\n    def webdriver_setup():\n        time.sleep(1)\n    \n        options.add_argument('--no-sandbox')\n        options.add_argument('--disable-extensions')\n        options.add_argument('--disable-dev-shm-usage')\n        options.add_argument('--ignore-certificate-errors')\n        options.add_argument('--enable-logging')\n        options.add_experimental_option('detach', True)\n     \n    service = Service(ChromeDriverManager().install())\n    global driver\n    driver  = webdriver.Chrome(service=service, options=options)\n    url = \"http://crashinformationky.org/AdvancedSearch\"   \n    \n    driver.get(url)\n    driver.maximize_window()\n    time.sleep(1)\n    \n#filling  in the date range\n    def date_input():\n        WebDriverWait(driver, 30).until(ec.visibility_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[3]/a\"))).click()\n        time.sleep(1)\n        WebDriverWait(driver, 30).until(ec.visibility_of_element_located((By.XPATH, \"/html/body/div[10]/div/div[3]\"))).click()\n        time.sleep(1)\n        WebDriverWait(driver, 30).until(ec.visibility_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div/div[3]/a\"))).click()\n        time.sleep(1)\n        field = driver.find_element(By.XPATH, \"/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div/div[3]/input\")\n        field.send_keys(Keys.CONTROL + \"a\")\n        field.send_keys(Keys.DELETE)\n        field.send_keys('01/01/2013')\n        field.send_keys(Keys.ENTER)\n        time.sleep(1)\n        range = driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div/div[2]/a')\n        range.click()\n        between = driver.find_element(By.XPATH, '/html/body/div[11]/div/div[9]')\n        between.click()\n        time.sleep(1)\n        WebDriverWait(driver, 30).until(ec.visibility_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div/div[4]/a\"))).click()\n        time.sleep(1)\n        field = driver.find_element(By.XPATH, \"/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div/div[4]/input\")\n        field.send_keys(Keys.CONTROL + \"a\")\n        field.send_keys(Keys.DELETE)\n        field.send_keys('12/31/2021')\n        field.send_keys(Keys.ENTER)\n        time.sleep(1)\n\n#selecting the users county choice\n    def add_county():\n        add_property= driver.find_element(By.XPATH, '//*[@id=\"QueryPanel\"]/div[3]/a')\n        add_property.click()\n        county_name = driver.find_element(By.XPATH, '//*[@id=\"QueryPanel-EntitiesMenu\"]/div/div[4]')\n        county_name.click()\n        time.sleep(1)\n        county_name_select = driver.find_element(By.PARTIAL_LINK_TEXT, 'select')\n        county_name_select.click()\n        time.sleep(1)\n        county_input = driver.find_element(By.ID, 'searchBox')\n        county_input.send_keys(county)\n        time.sleep(1)\n        county_final = driver.find_element(By.XPATH, '//*[@id=\"QueryPanel-cond-2-EditorMenu\"]/div[2]/div')\n        time.sleep(1)\n        county_final.click()\n\n#adding in the roadway number\n    def roadway():\n        add_property= driver.find_element(By.XPATH, '//*[@id=\"QueryPanel\"]/div[3]/a')\n        add_property.click()\n        time.sleep(1)\n        roadway_property = driver.find_element(By.XPATH, '//*[@id=\"QueryPanel-EntitiesMenu\"]/div/div[10]')\n        roadway_property.click()\n        time.sleep(1)\n        starts_with = driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div[3]/div[2]/a')\n        starts_with.click()\n        time.sleep(1)\n        contains =  driver.find_element(By.XPATH, '/html/body/div[11]/div/div[2]')\n        contains.click()\n        time.sleep(1)\n        enter_value = driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[2]/div[2]/div/div[2]/div[2]/div[3]/div[3]/a')\n        enter_value.click()\n        time.sleep(1)\n        actions = ActionChains(driver)\n        actions.send_keys(road)\n        actions.send_keys(Keys.ENTER)\n        actions.perform()\n        time.sleep(1)\n\n#executing the search\ndef get_file():\n    execute= driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[1]/div[1]/div[2]/div[2]')\n    execute.click()\n    time.sleep(35)\n\n#selecting export method\n    driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[2]/ul/li[3]/a').click()\n    time.sleep(25)\n    driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[2]/div/div[3]/div[2]/div/div/div[2]/div/div/div/div/div[5]/div/input').click()\n\n#exporting the file\n    driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[2]/div/div[2]/div/div[3]/div[2]/div/div/div[3]/div/a').click()\n    time.sleep(25)\n\n#finding the file the was downloaded\n    list_of_files= glob.glob(r\"c:\\Users\\kcham\\Downloads\\*\")\n    latest_file = max(list_of_files, key=os.path.getmtime)\n\n#extracting the incident file to a specific location\n    with zipfile.ZipFile(latest_file, mode='r') as archive:\n         archive.extract(\"Incident.txt\", path ='d:projects')\n\n#removing the zip file  \n    os.remove(latest_file)\n\n#converting the file into a data frame (*Cateory 2- reading data from an external file, Stretch goals- using pandas to perform data analysis)\ndef df_convert():\n    global df\n    df= pd.read_csv('d:projects/Incident.txt')\n    os.remove('d:projects/Incident.txt')\n\n#changing date format\n    df['year'] =pd.DatetimeIndex(df['CollisionDate']).year\n\n#selecting columns to use \n    useful_columns = [39,22,23,24,]\n    df = df[df.columns[useful_columns]]\n\n#chaning the data display\n    df = df.groupby(['year']).sum()\n\n\n#displaying data (*Category 3)\ndef display():\n    with open('str.html', 'w') as f:\n        df.to_html(f)\n \n    filename = 'str.html'\n    webbrowser.open_new_tab(filename)\n    plt.figure\n    df.plot.line()\n    plt.show()\n\ndef main():\n    time.sleep(1)\n    print('This program will show crash data for specific roadways in specific counties in Kentucky.\\n')\n    global county \n    county = input('Please select a county from Kentucky\\n')\n    global road\n    road = input('Please provide a valid road number in the county selected above. (ex: shelbyville road would be just 60)\\n')\n   \n    webscrape = WebScrape\n    webscrape.webdriver_setup()\n    webscrape.date_input()\n    webscrape.add_county()\n    webscrape.roadway()\n    get_file()\n    df_convert()\n    display()\n    \n\nif __name__ == \"__main__\":\n    main()", "77": "from flask_script import Manager\nfrom app import app\nfrom app import send_sms, webscrape\nimport time\n\nmanager = Manager(app)\n\n@manager.command\ndef sms():\n    time.sleep(300)\n    send_sms()\n\n@manager.command\ndef scrape():\n    time.sleep(240)\n    webscrape()\n\n@manager.command\ndef both():\n    time.sleep(240)\n    webscrape()\n    time.sleep(60)\n    send_sms()\n\nif __name__ == \"__main__\":\n    manager.run()", "78": "from selenium.webdriver import Chrome\nfrom os import path, system\nfrom webScrape import WebScrape\nfrom time import sleep\ndir = path.dirname(__file__)\nchrome_path = path.join(dir, 'selenium','webdriver','chromedriver.exe')\nwebS = WebScrape()\nurl = 'https://bongo.cat/'\nwith Chrome(chrome_path,options=webS.optionsChrome(True)) as driver:\n    driver.get(url)\n    elmnt = driver.find_element_by_tag_name('html')\n    while True:\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('9')\n        sleep(0.5)\n        elmnt.send_keys('0')\n        sleep(0.5)\n        elmnt.send_keys('0')\n        sleep(0.5)\n        elmnt.send_keys('3')\n        sleep(0.5)\n        elmnt.send_keys('3')\n        sleep(0.5)\n        elmnt.send_keys('2')\n        sleep(0.5)\n        elmnt.send_keys('2')\n        sleep(0.5)\n        elmnt.send_keys('2')\n        sleep(0.5)\n        elmnt.send_keys('5')\n        sleep(0.5)\n        elmnt.send_keys('5')\n        sleep(0.5)\n        elmnt.send_keys('5')\n        sleep(0.5)\n        elmnt.send_keys('5')\n        sleep(0.5)\n        elmnt.send_keys('5')\n        sleep(0.5)", "79": "import webscrape\n\nprint(webscrape.wtrlvl1)\nprint(webscrape.wtrlvl2)\nprint(webscrape.date1)\nprint(webscrape.date2)", "80": "# bar graphs for washington fisheries data\n\nimport numpy as np\nimport csv\nimport os\nimport matplotlib.pyplot as plt\n\n## paths\n# windows\ntable_path = \"C:\\\\Users\\\\Sean.McFall\\\\Documents\\\\SH\\\\fisheries-webscrape\\\\tables\\\\\"\nfig_path = \"C:\\\\Users\\\\Sean.McFall\\\\Documents\\\\SH\\\\fisheries-webscrape\\\\figures\\\\\"\n# linux\n#table_path = \"/home/smcfall/Documents/fisheries-webscrape/tables/\"\n#fig_path = \"/home/smcfall/Documents/fisheries-webscrape/figures/\"\n\nif not os.path.exists(fig_path):\n    os.makedirs(fig_path)\n\ntables = os.listdir(table_path)\n\ndef makeBar(position, data, x_labels, y_title, fill_color, line_bf):\n    ax = fig.add_subplot(position)\n    ax.bar(x_axis, data, width, color=fill_color, align='center')\n    labels = plt.xticks(x_axis, x_labels, rotation='vertical', ha='center')\n    plt.grid(True)\n    ax.set_ylabel(y_title, fontweight='bold')\n    plt.xlim([0,len(x_axis)+1])\n\n\n    if line_bf == 1:\n\n        # best fit of data\n        (mu, sigma) = norm.fit(data)\n\n        # the histogram of the data\n        #bins = hist(datos, 60, normed=1, facecolor='green', alpha=0.75)\n\n        # add a 'best fit' line\n        deg = round(max(x_axis)/3)\n\n        if deg % 2 == 0:\n            deg += 1\n\n        coefficients = np.polyfit(x_axis, data, deg)\n        polynomial = np.poly1d(coefficients)\n        xs = np.linspace(min(data), max(data), 1000)\n        ys = polynomial(xs)\n\n        plt.plot(xs,ys)\n\n    axes = plt.gca()\n    axes.set_ylim(min(data),max(data))\n\ndef createFig(table_name):\n    global fig\n    global x_axis\n    global width\n    #open csv\n\n    # get list of csv files\n    # iterate over list of files\n\n    # read the csv\n    res = csv.reader(open(table_path + table_name), delimiter = ',')\n\n    # ang = anglers, ws = wild steelhead, h = hatchery steelhed, rel = released\n    date, num_ang, hrsPerWS, wsCaught, hrsPerHS, hsCaught, hrs_fished = [], [], [], [], [], [], []\n\n    # populate lists from csv\n    for col in res:\n        date.append(col[1])\n        num_ang.append(int(col[2]))\n        hrsPerWS.append(float(col[3]))\n        wsCaught.append(int(col[4]))\n        hrsPerHS.append(float(col[5]))\n        hsCaught.append(int(col[6]))\n        hrs_fished.append(float(col[11]))\n\n        river_name = col[0]\n\n    #\n    # bar plot creation\n    #\n\n    width = 0.8\n    x_axis = range(1,len(date)+1)\n    bar_color = '#455A64'  # slate\n    empty_list = ['']*len(date)\n\n    # set size of the figure area\n\n    fig = plt.figure(figsize=(15, 10))\n    title = river_name + ' 2014/2015'\n    title.upper()\n    fig.suptitle(title.upper(), fontsize=16, fontweight='bold')\n    # position, attributes, title\n\n    # six plots\n    num_ang_ax = makeBar(322,num_ang, empty_list,'Number of Anglers',bar_color,0)\n    hrs_fished_ax = makeBar(321,hrs_fished,empty_list,'Hours Fished',bar_color,0)\n    hrsPerWS_ax = makeBar(323,hrsPerWS,empty_list,'Hours Per \\n Wild Steelhead','#FF5252',0)\n    ws_caught_ax = makeBar(324,wsCaught,empty_list,'Wild Steelhead Caught','#FF5252',0)\n    hrs_per_ws_ax = makeBar(325,hrsPerHS,date,'Hours Per \\n Hatchery Steelhead','#8bc34a',0)\n    hs_caught_ax = makeBar(326,hsCaught,date,'Hatchery Steelhead Caught','#8bc34a',0)\n\n    # gives the x-axis labels enough room\n    fig.tight_layout()\n    plt.subplots_adjust(top=.92)\n    #plt.savefig(fig_path + 'test.png', bbox_inches=\"tight\")\n    #plt.show()\n\n    # save the figure\n    plt.savefig(fig_path + table_name[:3], bbox_inches='tight')\n\nfor table in tables:\n    if table[-3:] == 'csv':\n        createFig(table)\n", "81": "# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'webscrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "82": "from analyse import CheckUrl\nfrom get import GetRequest\nfrom top_words import GetWords\nfrom barchart import BarChart\n\n\nclass Web:\n    def WebScrape(self):\n        ''' This function prompts the user to analyse any website of their choice '''\n\n        while True:\n            ask_to_scrape = input('Would you like to scrape a webiste (y/n) ')\n            if ask_to_scrape == 'n':\n                print('Thanks for analyzing! come back again')\n                break\n                \n            elif ask_to_scrape == 'y':\n                url= CheckUrl.ValidUrl()\n                if url:\n                    request_url = GetRequest().request(url)\n                    if request_url:\n                        pull = GetWords().pull_data(request_url)\n                        print (f'The top word is: {pull[0][0]}')\n                        bar = BarChart().plot_chart(pull)\n                        print(bar)\n                else:\n                    print('The website url is not valid ')\n                \n            else:\n                print (\" 'y' means yes, 'n' means no \")\n\nif __name__ == '__main__':\n    Web().WebScrape()\n    \n\n\n\n\n", "83": "#Import all dependencies to webscrape\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\n#Scrape stuff\nclass WebScrape():\n\n    def __init__(self):\n        pass\n\n    def read_parsons():\n\n        \"\"\"\n        Reads in Parson's code written by compute.py to use as variable\n        \"\"\"\n\n        #So that variable can be accessed elsewhere in the file\n        global parsons_code \n\n        with open(\"parson.txt\", \"r\") as parsons_file:\n            parsons_code = parsons_file.read()\n\n    def scrape_musipedia():\n\n        \"\"\"\n        - Uses Selenium & ChromeDriver to extract information about top 3 most likely songs\n        - Requires scraping html\n        - Relies on Musipedia not being updated\n        \"\"\"\n\n        #Path details\n        path_of_driver = \"/Users/adityatatwawadi/Downloads/chromedriver\"\n        driver = webdriver.Chrome(executable_path = path_of_driver)\n\n        #Access musipedia's website\n        website_url = \"https://www.musipedia.org/melodic_contour.html\"\n        driver.get(website_url)\n        #print(driver.title)\n\n        #Enters in the Parson's code to search engine\n        driver.find_element(By.NAME, 'tx_mpsearch_pi1[pc]').send_keys(parsons_code)\n\n        #Submits & accesses database\n        click_button = driver.find_element(By.NAME, 'tx_mpsearch_pi1[submit_button]')\n        click_button.click()\n\n        #Scrolls down to top 3 identified songs\n        driver.execute_script(\"window.scrollBy(0,400)\")\n\n        #Reframes & takes a screenshot of top3 recommendtaions\n        S = lambda X: driver.execute_script('return document.body.parentNode.scroll' +X)\n        driver.set_window_size(S('Width'), S('Height'))\n        driver.find_element_by_tag_name('body').screenshot(\"musipedia_recommendations.png\")\n\n        #Exits automated chrome\n        driver.quit()\n\nWebScrape.read_parsons()\nWebScrape.scrape_musipedia()", "84": "#Import all dependencies to webscrape\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\n#Scrape stuff\nclass WebScrape():\n\n    def __init__(self):\n        pass\n\n    def read_parsons():\n\n        \"\"\"\n        Reads in Parson's code written by compute.py to use as variable\n        \"\"\"\n\n        #So that variable can be accessed elsewhere in the file\n        global parsons_code \n\n        with open(\"parson.txt\", \"r\") as parsons_file:\n            parsons_code = parsons_file.read()\n\n    def scrape_musipedia():\n\n        \"\"\"\n        - Uses Selenium & ChromeDriver to extract information about top 3 most likely songs\n        - Requires scraping html\n        - Relies on Musipedia not being updated\n        \"\"\"\n\n        #Path details\n        path_of_driver = \"/Users/adityatatwawadi/Downloads/chromedriver\"\n        driver = webdriver.Chrome(executable_path = path_of_driver)\n\n        #Access musipedia's website\n        website_url = \"https://www.musipedia.org/melodic_contour.html\"\n        driver.get(website_url)\n        #print(driver.title)\n\n        #Enters in the Parson's code to search engine\n        driver.find_element(By.NAME, 'tx_mpsearch_pi1[pc]').send_keys(parsons_code)\n\n        #Submits & accesses database\n        click_button = driver.find_element(By.NAME, 'tx_mpsearch_pi1[submit_button]')\n        click_button.click()\n\n        #Scrolls down to top 3 identified songs\n        driver.execute_script(\"window.scrollBy(0,400)\")\n\n        #Reframes & takes a screenshot of top3 recommendtaions\n        S = lambda X: driver.execute_script('return document.body.parentNode.scroll' +X)\n        driver.set_window_size(S('Width'), S('Height'))\n        driver.find_element_by_tag_name('body').screenshot(\"musipedia_recommendations.png\")\n\n        #Exits automated chrome\n        driver.quit()\n\nWebScrape.read_parsons()\nWebScrape.scrape_musipedia()", "85": "# Work with Python 3.6\nimport discord\nimport json\nimport WebScrape\nimport GenerateGraph\nfrom discord import File\n\nwith open('Secret.json') as json_file:  \n    data = json.load(json_file)\n    TOKEN = data['discordKey']\n\nclient = discord.Client()\n\n@client.event\nasync def on_message(message):\n    # we do not want the bot to reply to itself\n    if message.author == client.user:\n        return\n\n    if message.content.startswith('!hello'):\n        msg = 'Hello {0.author.mention}'.format(message)\n        # await message.channel.send(msg)\n        testFile = File('options.csv')\n        await message.channel.send(file = testFile)\n\n    elif message.content.startswith('!recordanewmixtape'):\n        msg = 'No {0.author.mention}, you do it'.format(message)\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!tickers'):\n        cmd = message.content\n        cmd = cmd[9:]\n        print('tickers command ticker: ' + cmd)\n        try:\n            tickers = WebScrape.printTickers(cmd)\n            msg = ''\n            for tick in tickers:\n                msg = msg + tick + '\\n'\n        except:\n            msg = 'Not a valid option\\nLook at !listOptions'\n            \n        await message.channel.send(msg)\n\n    elif message.content.startswith('!setOption'):\n        cmd = message.content\n        cmdParts = cmd.split()\n        WebScrape.setOption(cmdParts[1], cmdParts[2])\n        msg = '{0.author.mention} new option added '.format(message) + cmdParts[2]\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!removeOption'):\n        cmd = message.content\n        cmdParts = cmd.split()\n        WebScrape.removeOption(cmdParts[1])\n        msg = '{0.author.mention} option was removed '.format(message) + cmdParts[1]\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!listOptions'):\n        options = WebScrape.getOptions()\n        msg = ''\n        if options == 'There are no current options setup use !setOptions to set a new option':\n            msg = options\n        else:\n            for opt in options:\n                msg = msg + opt + '\\n'\n        await message.channel.send(msg)\n\n    elif message.content.startswith('!stock'):\n        cmd = message.content\n        cmdParts = cmd.split()\n        GenerateGraph.makeGraph(cmdParts[1])\n        graphFile = File('stock_information.html')\n        await message.channel.send(file = graphFile)\n\n\n    elif message.content.startswith('!help'):\n        msg = 'Remember to seperate each part of the command with a space\\n'\n        msg = msg + '!setOption  \\n'\n        msg = msg + '!removeOption \\n'\n        msg = msg + '!listOptions\\n'\n        msg = msg + '!tickers \\n'\n        msg = msg + '!stock \\n'\n        await message.channel.send(msg)\n\n@client.event\nasync def on_ready():\n    print('Logged in as')\n    print(client.user.name)\n    print(client.user.id)\n    print('------')\n\nclient.run(TOKEN)", "86": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props[\"last_date_amazon\"] = '2020-01-01'\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n            column_names=df.columns.to_list(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n            destination_table=\"yankee_candle_reviews_agita\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "87": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props[\"last_date_amazon\"] = '2020-01-01'\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n            column_names=df.columns.to_list(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n            destination_table=\"yankee_candle_reviews_agita\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "88": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props[\"last_date_amazon\"] = '2020-01-01'# <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n            column_names=df.columns.tolist(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n            destination_table=\"yankee_candle_reviews_titi\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "89": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props[\"last_date_amazon\"] = '2020-01-01'# <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n            column_names=df.columns.tolist(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n            destination_table=\"yankee_candle_reviews_titi\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "90": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props[\"last_date_amazon\"] = '2020-01-01' # <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n            column_names=df.columns.to_list(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n            destination_table=\"yankee_candle_reviews_dimira2\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "91": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props[\"last_date_amazon\"] = '2020-01-01' # <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n            column_names=df.columns.to_list(), # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n            destination_table=\"yankee_candle_reviews_dimira2\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "92": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props[\"last_date_amazon\"] = '2020-01-01'\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        # Essentially, we are removing the last list element from date_result (if date_result list is longer than rev_result).\n        # This has some degree of error in it, but is chosen as an approach for simplicity and illustrative purposes.\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a cloud Trino database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values,\n            column_names=df.columns.to_list(),\n            destination_table=\"yankee_candle_reviews\"\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in table yankee_candle_reviews.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "93": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props[\"last_date_amazon\"] = '2020-01-01'\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        # Essentially, we are removing the last list element from date_result (if date_result list is longer than rev_result).\n        # This has some degree of error in it, but is chosen as an approach for simplicity and illustrative purposes.\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a cloud Trino database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values,\n            column_names=df.columns.to_list(),\n            destination_table=\"yankee_candle_reviews\"\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in table yankee_candle_reviews.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)", "94": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props[\"last_date_amazon\"] = '2020-01-01'\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        # Essentially, we are removing the last list element from date_result (if date_result list is longer than rev_result).\n        # This has some degree of error in it, but is chosen as an approach for simplicity and illustrative purposes.\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a cloud Trino database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values,\n            column_names=df.columns.to_list(),\n            destination_table=\"yankee_candle_reviews\"\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in table yankee_candle_reviews.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)", "95": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props['last_date_amazon'] = '2020-01-01'\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values,\n            column_names=df.columns.to_list(),\n            destination_table= \"yankee_candle_reviews_avramov\"\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "96": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props['last_date_amazon'] = '2020-01-01'\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values,\n            column_names=df.columns.to_list(),\n            destination_table= \"yankee_candle_reviews_avramov\"\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "97": "from bs4 import BeautifulSoup\nimport requests\nimport re\nimport json\n\nWIKI_PREFIX = 'https://en.wikipedia.org'\n\n# query1=input('Enter a Wiki query: ').replace(' ','_')\n# query2=''\n# url = f'https://en.wikipedia.org/wiki/{query1}'\n\n\ndef webscrape(url): #takes wiki url and returns set of all the links\n    source = requests.get(url).text\n    soup = BeautifulSoup(source,'html.parser')\n    # print(soup.prettify())\n\n    links = soup.find_all('a',href = True) #filter html for href links\n    # print(links)\n    final_links=[]\n    for link in links: #extract links with regex\n        linkregex = re.compile(r'href=\"(\\w|/)+\"')\n        final_link = linkregex.search(str(link))\n        if final_link is not None:\n            # print(final_link.group())\n            final_links.append(f'{WIKI_PREFIX}{final_link.group()[6:-1]}')\n    final_links = list(set(final_links)) #rid duplicates\n    final_links.remove(f'{WIKI_PREFIX}/wiki/Main_Page') #rid unnecessary link\n    # print(final_links,len(final_links))\n    return final_links\n\n# with open('wikidata.json') as jsonfile:\n#     jsondata = json.load(jsonfile)\n# print(len(webscrape(f'{WIKI_PREFIX}/wiki/stalin')),len(jsondata))\n\n# print(webscrape('https://en.wikipedia.org/wiki/stalin') == \\\n# webscrape('https://en.wikipedia.org/wiki/Joseph_Stalin'))", "98": "import os\nimport pandas as pd\nimport numpy as np\nimport sqlalchemy\nfrom sqlalchemy.ext.automap import automap_base\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import create_engine\nfrom flask import Flask, jsonify, render_template\nimport sqlalchemy\nfrom flask import Flask, render_template, redirect\nfrom flask_pymongo import PyMongo\nimport renewable_scrape\nimport json\n\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\n#################################################\n# Database Setup\n#################################################\nengine = create_engine(\"sqlite:///project.sqlite\")\n\n\n# reflect an existing database into a new model\nBase = automap_base()\n# reflect the tables\nBase.prepare(engine, reflect=True)\n\n# Save reference to the table\nDataset = Base.classes.dataset\n\n\n#################################################\n# Flask Setup\n#################################################\napp = Flask(__name__)\n\nSITE_ROOT = os.path.realpath(os.path.dirname(__file__))\njson_url = os.path.join(SITE_ROOT, \"templates\", \"USA.geojson\")\nheatmapdata = json.load(open(json_url))\nprint(heatmapdata)\n\n#################################################\n# Flask Routes\n#################################################\n\n\n@app.route(\"/\") \ndef welcome():\n    return render_template(\"index.html\")\n\n\n@app.route(\"/hydro\")\ndef hydro():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"hydro.html\")\n\n@app.route(\"/wind\")\ndef wind():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"wind.html\")\n\n\n@app.route(\"/heatmap\")\ndef heatmap():\n    \n    return render_template(\"heatmap.html\")\n\n@app.route(\"/solar\")\ndef solar():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"solar.html\")\n\n@app.route(\"/location\")\ndef location():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"location.html\")\n\n@app.route(\"/webscrape\")\ndef webscrape_sunburst():\n    data =  json.load(open(\"my_renewables.json\",\"r\")) \n    return render_template(\"webscrape.html\",r_last_refresh=data[\"last_scrape\"],renewable_title_0=data[\"articles \"][0],renewable_link_0=data[\"links\"][0],renewable_title_1=data[\"articles \"][1],renewable_link_1=data[\"links\"][2], renewable_title_2 = data[\"articles \"][2],renewable_link_2=data[\"links\"][4],renewable_title_3=data[\"articles \"][3],renewable_link_3=data[\"links\"][6])\n\n@app.route(\"/scrape\")\ndef scrape():\n    renewable_scrape.renewable_scrape()\n    return redirect(\"/webscrape\")\n\n@app.route(\"/api/heatmap\")\ndef heatmapgeojson():\n    return jsonify(data = heatmapdata)\n\n\n@app.route(\"/data\")\ndef data():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"data.html\")\n\n\nif __name__ == '__main__':\n    app.run(debug=True)\n", "99": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Sat Jul 10 07:39:22 2021\r\n\r\n\"\"\"\r\n\r\nimport discord\r\nfrom discord.ext import commands\r\nfrom discord.ext.commands.cooldowns import BucketType\r\nfrom .webscraper import WebScraper\r\nimport nest_asyncio\r\nimport asyncio\r\nimport random\r\nnest_asyncio.apply()\r\n\r\nclass GeneralCommands(commands.Cog):\r\n    def __init__(self,client):\r\n        self.client = client\r\n        \r\n    @commands.command(name = 'conq', aliases = ['conqueror']) #add rune images + explanations later\r\n    async def displayConq(self,ctx):\r\n        file = discord.File('images/conq.png', filename = 'conq.png')\r\n        await ctx.send(file=file)\r\n        await ctx.send('\\'Standard\\' runes are unboxed. Runes with yellow boxes are alternative choices.')\r\n        \r\n    @commands.command(name = 'elec', aliases = ['electrocute'])\r\n    async def displayElec(self,ctx):\r\n        file = discord.File('images/elec.png', filename = 'elec.png')\r\n        await ctx.send(file=file)\r\n        await ctx.send('\\'Standard\\' runes are unboxed. Runes with yellow boxes are alternative choices.')\r\n        \r\n    @commands.command(name = 'lethality', aliases = ['leth','assassin'])\r\n    async def displayLethality(self,ctx):\r\n        embed1 = discord.Embed(\r\n            title = 'Lethality items',\r\n            description = 'A pool of lethality items you should buy on lethality Talon. Note that items from this list and the bruiser list can both be in the same build and that there is no definitive order to build them.',\r\n            color = discord.Color.blue()\r\n        )\r\n        embed1.add_field(name = '\u2605Prowler\u2019s Claw', \r\n                         value = 'Its dash allows you to gapclose and gives a 15% damage increase against targets. Ideally, you use the active before you proc your passive.\\\r\n This item should be bought if you\\'re dealing with mobility targets or want extra damage in one-shotting. It is also a good item to build from behind.',\r\n                         inline = False)\r\n        embed1.add_field(name = '\u2605Duskblade of Draktharr', \r\n                         value = 'Gives bonus damage and a slow on AA and invisiblity. Equal in stats to Prowler\\'s, its active is not as good but gives you higher potential for multi-kills.\\\r\n Generally, you want to utilize the invisibility from the item to reposition. Use FOG to your advantage if you can. Currently Prowler\\'s serves as a much better option.',\r\n                         inline = False)\r\n        embed1.add_field(name = '\u2605Eclipse', \r\n                         value = 'A mythic that allows you to duel better. Its passive and shield makes it ideal to take against bruisers and tanks. Overall, it is still sub-par to Prowler\\'s\\\r\n due to its poorer waveclear. You can use the movespeed you get from its active to dodge abilities or reposition. Build it if you want\\\r\n lethality while also having good dueling.',\r\n                         inline = False)\r\n        embed1.set_footer(text = 'Page 1 of 3')\r\n        \r\n        embed2 = discord.Embed(\r\n            title = 'Lethality items',\r\n            color = discord.Color.blue()\r\n        )\r\n        embed2.add_field(name = 'Edge of Night', \r\n                         value = 'Grants a reusable spell shield that\\'s good against teams with CC or TF/Nocturne. Build this if you want damage, surviability, or want to deal with CC.',\r\n                         inline = False)\r\n        embed2.add_field(name = 'Serpent\u2019s Fang', \r\n                         value = 'Anti-shield item, AA + abilities apply 50% shield debuff on all affected targets for 3 seconds. Effective against all shields except magic shields like Morgana\u2019s.\\\r\n Due to its low cost, it\\'s one of the best lethality items to rush even if there are no shields on the enemy team.' ,\r\n                         inline = False)\r\n        embed2.add_field(name = 'Umbral Glaive', \r\n                         value = 'Disables and reveals wards nearby you, deal 3x damage to wards. Cheap and grants decent lethality. Build this if you want vision control, though Serpent\\'s and oracle lenses is usually a better alternative.',\r\n                         inline = False)\r\n        embed2.add_field(name = 'Youmuu\u2019s Ghostblade', \r\n                         value = 'Grants lethality, 20% movement speed buff active, and out-of-combat movement speed. It is not a very good item due to its cost; however, you can build this if you want faster rotations or to gapclose faster.',\r\n                         inline = False)\r\n        embed2.set_footer(text = 'Page 2 of 3')\r\n        \r\n        embed3 = discord.Embed(\r\n                title = 'Lethality items',\r\n                color = discord.Color.blue()\r\n        )\r\n        embed3.add_field(name = 'Serylda\u2019s Grudge', \r\n                         value = 'Grants +30% armor pen and haste. Abilities apply 30% slow on all enemies for 1 second. Provides strong pen and gap close against mobility targets. Build this if you\\'re dealing with enemies stacking armor.',\r\n                         inline = False)\r\n        embed3.add_field(name = 'Lord Dominiks\\' Regards', \r\n                         value = 'Grants +35% armor pen and bonus (0-15)% physical damage based upon maximum health difference.\\\r\n Build this if the enemy has a substantial health difference over you, you need armor pen, and you don\\'t need the haste of Serylda\\'s.', \r\n                         inline = False)\r\n        embed3.add_field(name = 'Black Cleaver', \r\n                         value = 'Applies %armor reduction and grants bonus movement speed when dealing damage. Build this if you want AH, survivability, and damage or if you\\'re dealing with armor stacking teams since the armor shred helps your team as well.',\r\n                         inline = False)\r\n        embed3.add_field(name = 'Executioner\u2019s Calling', \r\n                         value = 'Grants grievous wounds. Build this when dealing with healing champions like Sylas, Yuumi, or Soraka. This is usually bought after your mythic or second item, depending on how impactful\\\r\n the enemy healing is.',\r\n                         inline = False)\r\n        embed3.add_field(name = 'Maw of Malmortius',\r\n                         value = 'Grants AD and MR. Gives a lifeline against magic damage. Generally inefficient for what it does, this is only really an acceptable option into 3+ heavy ap damage champs.',\r\n                         inline = False)\r\n        embed3.set_footer(text = 'Page 3 of 3')\r\n        \r\n        contents = [embed1, embed2, embed3]\r\n        cur_page = 1\r\n        message = await ctx.send(embed = contents[cur_page - 1])\r\n        await message.add_reaction('1\ufe0f\u20e3')\r\n        await message.add_reaction('2\ufe0f\u20e3')\r\n        await message.add_reaction('3\ufe0f\u20e3')\r\n        await message.add_reaction('\\U0001F5D1')\r\n        def check(reaction, user):\r\n            return user == ctx.author and str(reaction.emoji) in {'1\ufe0f\u20e3', '2\ufe0f\u20e3', '3\ufe0f\u20e3', '\\U0001F5D1'} and reaction.message.id == message.id\r\n            # Only the sender can interact with the message. Also checks that the message is the same one being reacted on\r\n        while True:\r\n            try:\r\n                reaction, user = await self.client.wait_for(\"reaction_add\", timeout=300, check=check) \r\n                #waits for a reaction, first argument is the reaction and second is the user, times out after 5 minutes\r\n                if str(reaction.emoji) == '1\ufe0f\u20e3' and cur_page != 1:\r\n                    cur_page = 1\r\n                    await message.edit(embed = contents[cur_page - 1])\r\n                    await message.remove_reaction(reaction, user)\r\n                elif str(reaction.emoji) == '2\ufe0f\u20e3' and cur_page != 2:\r\n                    cur_page = 2\r\n                    await message.edit(embed = contents[cur_page - 1])\r\n                    await message.remove_reaction(reaction, user)\r\n                elif str(reaction.emoji) == '3\ufe0f\u20e3' and cur_page != 3:\r\n                    cur_page = 3\r\n                    await message.edit(embed = contents[cur_page - 1])\r\n                    await message.remove_reaction(reaction, user)\r\n                elif str(reaction.emoji) == '\\U0001F5D1':\r\n                    await message.delete()\r\n                    break\r\n                else:\r\n                    await message.remove_reaction(reaction, user)\r\n                    \r\n            except asyncio.TimeoutError:\r\n                await message.clear_reactions()\r\n                break\r\n        \r\n    @commands.command(name = 'bruiser')\r\n    async def displayBruiser(self,ctx):\r\n        embed1 = discord.Embed(\r\n            title = 'Bruiser items',\r\n            description = 'A pool of bruiser mythic items you should buy on bruiser Talon. Note that items from this list and the bruiser list can both be in the same build and that there is no definitive order to build them.',\r\n            color = discord.Color.blue()\r\n        )\r\n        embed1.add_field(name = '\u2605Goredrinker', \r\n                         value = 'The go-to mythic for bruiser Talon. It gives you CDR, damage, HP, and waveclear. ALWAYS build ironspike whip first for the waveclear. This gives all the essential stats for bruiser Talon.', \r\n                         inline = False)\r\n        embed1.add_field(name = '\u2605Prowler\u2019s Claw', \r\n                         value = 'Its dash allows you to gapclose and gives a 15% damage increase against targets. However,\\\r\n it has worse waveclear than Goredrinker. Ideally, use the active before you proc your passive. Build this mythic if you\\'re against a squishy team or you want higher one-shot potential.',\r\n                         inline = False)\r\n        embed1.add_field(name = '\u2605Eclipse', \r\n                         value = 'A mythic that allows you to duel better. Its passive and shield makes it ideal to take against bruisers and tanks. Overall, it is sub-par to Prowler\\'s\\\r\n due to its poorer waveclear. You can use the movespeed you get from its active to dodge abilities or reposition. Build it if you want\\\r\n lethality while also having good dueling. Note that your waveclear is even worse than if you bought Eclipse on lethality Talon.',\r\n                         inline = False)\r\n        embed1.set_footer(text = 'Page 1 of 3')\r\n        \r\n        embed2 = discord.Embed(\r\n            title = 'Bruiser items',\r\n            color = discord.Color.blue()\r\n        )\r\n        embed2.add_field(name = 'Black Cleaver', \r\n                         value = 'Applies %armor reduction and grants bonus movement speed when dealing damage. Build this if you want AH, survivability, and damage or if you\\'re dealing with armor stacking teams since the armor shred helps your team as well.', \r\n                         inline = False)\r\n        embed2.add_field(name = 'Sterak\\'s Gage', \r\n                         value = 'Grants %max health healing and %health shield. Build this if you want\\\r\n higher survivability in teamfights. It\\'s a decent item to buy if you\\'re fed since it\\'ll be much harder to kill you.', \r\n                         inline = False)\r\n        embed2.add_field(name = 'Bramble Vest',\r\n                         value = 'Grants 30 armor and grievous wounds. It is not optimal to finish into Thornmail. Build this if you\\'re against healing champs, usually after Goredrinker. If you chose a lethality mythic, build execution\\'s calling\\\r\n instead.', inline = False)\r\n        embed2.add_field(name = 'Death\\'s Dance', \r\n                         value = 'Delays 35% of all physical damage over 3 seconds into true damage. Build this if you\\'re against\\\r\n a lot of AD champions or need to deal with AD burst.', \r\n                         inline = False)\r\n        embed2.add_field(name = 'Ravenous Hydra',\r\n                         value = 'Grants AD and Omnivamp. Gives a cleave passive that\u2019s unparalleled in it\u2019s waveclear power. Build this if you want stronger splitting power or sustain.', \r\n                         inline = False)\r\n        embed2.set_footer(text = 'Page 2 of 3')\r\n        \r\n        embed3 = discord.Embed(\r\n                title = 'Bruiser items',\r\n                color = discord.Color.blue()\r\n        )\r\n        embed3.add_field(name = 'Serylda\\'s Grudge', \r\n                         value = 'Grants +30% armor pen and haste. Abilities apply 30% slow on all enemies for 1 second. Provides\\\r\n strong pen and gap close against mobility targets. Build this if you\\'re dealing with enemies stacking armor.',inline = False)\r\n        embed3.add_field(name = 'Lord Dominiks\\' Regards', \r\n                         value = 'Grants +35% armor pen and bonus (0-15)% physical damage based upon maximum health difference.\\\r\n Build this if the enemy has a substantial health difference over you, you need armor pen, and you don\\'t need the haste of Serylda\\'s.', inline = False)\r\n        embed3.add_field(name = 'Maw of Malmortius',\r\n                         value = 'Grants AD and MR. Gives a lifeline against magic damage. Generally inefficient for what it does, this is only really an acceptable option into 3+ heavy ap damage champs.',\r\n                         inline = False)\r\n        embed3.add_field(name = 'Edge of Night', \r\n                         value = 'Grants a reusable spell shield that\\'s good against teams with CC or TF/Nocturne. Build this if you want damage, surviability, or want to deal with CC.',inline = False)\r\n        embed3.add_field(name = 'Serpent\u2019s Fang', \r\n                         value = 'Anti-shield item, AA + abilities apply 50% shield debuff on all affected targets for 3 seconds. Effective against all shields except magic shields like Morgana\u2019s.\\\r\n Due to its low cost, it\\'s one of the best lethality items to rush even if there are no shields on the enemy team.', inline = False)\r\n        embed3.set_footer(text = 'Page 3 of 3')\r\n        \r\n        contents = [embed1, embed2, embed3]\r\n        cur_page = 1\r\n        message = await ctx.send(embed = contents[cur_page - 1])\r\n        await message.add_reaction('1\ufe0f\u20e3')\r\n        await message.add_reaction('2\ufe0f\u20e3')\r\n        await message.add_reaction('3\ufe0f\u20e3')\r\n        await message.add_reaction('\\U0001F5D1')\r\n        def check(reaction, user):\r\n            return user == ctx.author and str(reaction.emoji) in {'1\ufe0f\u20e3', '2\ufe0f\u20e3', '3\ufe0f\u20e3', '\\U0001F5D1'} and reaction.message.id == message.id\r\n            # Only the sender can interact with the message. Also checks that the message is the same one being reacted on\r\n        while True:\r\n            try:\r\n                reaction, user = await self.client.wait_for(\"reaction_add\", timeout=300, check=check) \r\n                #waits for a reaction, first argument is the reaction and second is the user, times out after 5 minutes\r\n                if str(reaction.emoji) == '1\ufe0f\u20e3' and cur_page != 1:\r\n                    cur_page = 1\r\n                    await message.edit(embed = contents[cur_page - 1])\r\n                    await message.remove_reaction(reaction, user)\r\n                elif str(reaction.emoji) == '2\ufe0f\u20e3' and cur_page != 2:\r\n                    cur_page = 2\r\n                    await message.edit(embed = contents[cur_page - 1])\r\n                    await message.remove_reaction(reaction, user)\r\n                elif str(reaction.emoji) == '3\ufe0f\u20e3' and cur_page != 3:\r\n                    cur_page = 3\r\n                    await message.edit(embed = contents[cur_page - 1])\r\n                    await message.remove_reaction(reaction, user)\r\n                elif str(reaction.emoji) == '\\U0001F5D1':\r\n                    await message.delete()\r\n                    break\r\n                else:\r\n                    await message.remove_reaction(reaction, user)\r\n                    \r\n            except asyncio.TimeoutError:\r\n                await message.clear_reactions()\r\n                break\r\n        \r\n    @commands.command(name = 'combos', aliases = ['combo'])\r\n    async def displayCombos(self,ctx):\r\n        await ctx.send('')\r\n        \r\n    @commands.command(name = 'skills', aliases = ['ability','skill'])\r\n    async def displayAbility(self,ctx):\r\n        await ctx.send('With Elec and Conq, you should always be going W max into Q max. 3 in W and then Q max is also viable for Conq.')\r\n        \r\n    @commands.command(name = 'starter', aliases = ['starting','start','starts'])\r\n    async def displayStart(self,ctx):\r\n        embed = discord.Embed(\r\n                title = 'Starting items',\r\n                color = discord.Color.blue()\r\n        )\r\n        embed.add_field(name = 'Longsword + refillable',\r\n                        value = 'The gold standard. This works against every matchup.',\r\n                        inline = False)\r\n        embed.add_field(name = 'Doran\\'s Blade + red pot',\r\n                        value = 'This is very good start for bruiser Talon. It is good against melee matchups as well as poke matchups. Less damage than LS but more sustain.',\r\n                        inline = False)\r\n        embed.add_field(name = 'Doran\\'s Shield + red pot',\r\n                        value = 'Start this if you expect the poke to be very heavy. Examples being ADC matchups like Quinn or Lucian.',\r\n                        inline = False)\r\n        embed.add_field(name = 'Corrupting pot',\r\n                        value = 'Viable, but less so than the above mentioned. Generally you would start this if you expect to use a lot of mana and to be poked a lot.',\r\n                        inline = False)\r\n        await ctx.send(embed = embed)\r\n        \r\n    @commands.command(name = 'quote', aliases = ['quotes'])\r\n    async def quote(self,ctx):\r\n        quoteList = ['Live and die by the blade.','Pathetic!','There\\'s nowhere to hide.','Let\\'s finish this quickly.',\r\n                     'Don\\'t cross me.','On the razor\\'s edge.','Your allegiances mean nothing to me.','I never compromise.',\r\n                     'Only fools pledge life to honor.','They won\\'t survive.']\r\n        await ctx.send(random.choice(quoteList))\r\n        \r\n    @commands.command(name = 'boots', aliases = ['boot','shoe','shoes'])\r\n    async def displayBoots(self,ctx):\r\n        embed = discord.Embed(\r\n                title = 'Boots',\r\n                color = discord.Color.blue()\r\n        )\r\n        embed.add_field(name = 'Plated Steelcaps',\r\n                        value = 'Provides armor. Build this if you\\'re against an AD comp or need more armor against AD champs.',\r\n                        inline = False)\r\n        embed.add_field(name = 'Mercury\u2019s Treads',\r\n                        value = 'Provides tenacity and MR. Build this if you\\'re against burst mages and need the extra surviability or if you\\'re against CC comps.',\r\n                        inline = False)\r\n        embed.add_field(name = 'Ionian Boots of Lucidity',\r\n                        value = 'Almost always built on lethality because they don\u2019t require resistances. You build Ionian Boots on bruiser if neither Steelcaps or Treads are applicable in the current match.',\r\n                        inline = False)\r\n        await ctx.send(embed = embed)\r\n        \r\n    '''\r\n    BucketType.default for a global basis.\r\n    BucketType.user for a per-user basis.\r\n    BucketType.guild for a per-server basis.\r\n    BucketType.channel for a per-channel basis.\r\n    '''\r\n    \r\n    @commands.cooldown(1,5,commands.BucketType.guild) #1 use per 5 seconds at all times <-- prevents multiple requests a second\r\n    @commands.command(name = 'stats', aliases = ['winrate','pickrate','banrate'])\r\n    async def statsv2(self,ctx,*,args = ''):\r\n        if not args:\r\n            webScrape = WebScraper('https://u.gg/lol/champions/talon/build')\r\n            await ctx.send(webScrape.parse())\r\n            return\r\n        args = args.lower()\r\n        argsList = args.split()\r\n        \r\n        rankSet = {'plat','platinum','dia','diamond'}\r\n        regionSet = {'kr','na','eu','euw','br','eune','eun','jp','lan','las',\r\n                                   'oce','ru','tr'}\r\n        roleSet = {'top','jg','jungle','mid','middle','bot','adc','supp','support'}\r\n        \r\n        '''\r\n        Permutations: 3 options to pick: Rank, region, role; Order matters\r\n        If 1 argument -> 3 pick 1 = 3 possibilities \r\n        If 2 arguments -> 3 pick 2 = 6 possibilities\r\n        If 3 arguments -> 3 pick 3 = 6 possibilities\r\n        '''\r\n        \r\n        if len(argsList) == 1: #1 argument\r\n            if argsList[0] in rankSet: #only argument is a rank, then we return the plat+ or diamond+ of the site\r\n                if argsList[0] in {'plat','platinum'}:\r\n                    webScrape = WebScraper('https://u.gg/lol/champions/talon/build')\r\n                else:\r\n                    webScrape = WebScraper('https://u.gg/lol/champions/talon/build?rank=diamond_plus','Diamond+')\r\n            elif argsList[0] in regionSet: #only argument is a region, then we return that region for plat+\r\n                region = regionUrl(argsList[0])\r\n                webScrape = WebScraper(region,'',argsList[0])\r\n            elif argsList[0] in roleSet: #only argument is a role, then we return that role for plat+\r\n                role = roleUrl(argsList[0])\r\n                webScrape = WebScraper('https://u.gg/lol/champions/talon/build' + '?' + role,'','',argsList[0])\r\n            else:\r\n                return\r\n                \r\n        elif len(argsList) == 2: #2 arguments\r\n            \r\n            if argsList[0] in rankSet and argsList[1] in regionSet:\r\n                region = regionUrl(argsList[1])\r\n                if argsList[0] in {'plat','platinum'}:\r\n                    webScrape = WebScraper(region,'',argsList[1])\r\n                elif argsList[0] in {'dia','diamond'}:\r\n                    webScrape = WebScraper(region + '&rank=diamond_plus','Diamond+',argsList[1])\r\n    \r\n            elif argsList[0] in rankSet and argsList[1] in roleSet: #@Talon Bot stats diamond top\r\n                role = roleUrl(argsList[1])\r\n                if argsList[0] in {'plat','platinum'}:\r\n                    webScrape = WebScraper('https://u.gg/lol/champions/talon/build' + '?' + role,'','',argsList[1])\r\n                else:\r\n                    webScrape = WebScraper('https://u.gg/lol/champions/talon/build' + '?' + role + '&rank=diamond_plus','Diamond+','',argsList[1])\r\n                    \r\n            elif argsList[0] in regionSet and argsList[1] in rankSet:\r\n                region = regionUrl(argsList[0])\r\n                if argsList[1] in {'plat','platinum'}:\r\n                    webScrape = WebScraper(region,'',argsList[0])\r\n                else:\r\n                    webScrape = WebScraper(region + '&rank=diamond_plus','Diamond+',argsList[0])\r\n                    \r\n            elif argsList[0] in regionSet and argsList[1] in roleSet: #@Talon Bot stats na top\r\n                region = regionUrl(argsList[0])\r\n                role = roleUrl(argsList[1])\r\n                webScrape = WebScraper(region + '&' + role,'',argsList[0],argsList[1])\r\n                \r\n            elif argsList[0] in roleSet and argsList[1] in regionSet:\r\n                region = regionUrl(argsList[1])\r\n                role = roleUrl(argsList[0])\r\n                webScrape = WebScraper(region + '&' + role,'',argsList[1],argsList[0])\r\n                \r\n            elif argsList[0] in roleSet and argsList[1] in rankSet:\r\n                role = roleUrl(argsList[0])\r\n                if argsList[1] in {'plat','platinum'}:\r\n                    webScrape = WebScraper('https://u.gg/lol/champions/talon/build' + '?' + role,'','',argsList[0])\r\n                else:\r\n                    webScrape = WebScraper('https://u.gg/lol/champions/talon/build' + '?' + role + '&rank=diamond_plus','Diamond+','',argsList[0])\r\n            \r\n            else:\r\n                return\r\n                    \r\n        elif len(argsList) == 3: #3 arguments\r\n            if argsList[0] in rankSet and argsList[1] in regionSet and argsList[2] in roleSet:\r\n                role = roleUrl(argsList[2])\r\n                region = regionUrl(argsList[1])\r\n                if argsList[0] in {'plat','platinum'}:\r\n                    webScrape = WebScraper(region + '&' + role,'',argsList[1],argsList[2])\r\n                else:\r\n                    webScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[1],argsList[2])\r\n                    \r\n            elif argsList[0] in rankSet and argsList[1] in roleSet and argsList[2] in regionSet:\r\n                role = roleUrl(argsList[1])\r\n                region = regionUrl(argsList[2])\r\n                if argsList[0] in {'plat','platinum'}:\r\n                    webScrape = WebScraper(region + '&' + role,'',argsList[2],argsList[1])\r\n                else:\r\n                    webScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[2],argsList[1])\r\n                    \r\n            elif argsList[0] in regionSet and argsList[1] in rankSet and argsList[2] in roleSet:\r\n                role = roleUrl(argsList[2])\r\n                region = regionUrl(argsList[0])\r\n                if argsList[1] in {'plat','platinum'}:\r\n                    webScrape = WebScraper(region + '&' + role,'',argsList[0],argsList[2])\r\n                else:\r\n                    webScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[0],argsList[2])\r\n                    \r\n            elif argsList[0] in regionSet and argsList[1] in roleSet and argsList[2] in rankSet:\r\n                role = roleUrl(argsList[1])\r\n                region = regionUrl(argsList[0])\r\n                if argsList[2] in {'plat','platinum'}:\r\n                    webScrape = WebScraper(region + '&' + role,'',argsList[0],argsList[1])\r\n                else:\r\n                    webScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[0],argsList[1])\r\n                    \r\n            elif argsList[0] in roleSet and argsList[1] in rankSet and argsList[2] in regionSet:\r\n                role = roleUrl(argsList[0])\r\n                region = regionUrl(argsList[2])\r\n                if argsList[1] in {'plat','platinum'}:\r\n                    webScrape = WebScraper(region + '&' + role,'',argsList[2],argsList[0])\r\n                else:\r\n                    webScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[2],argsList[0])\r\n                    \r\n            elif argsList[0] in roleSet and argsList[1] in regionSet and argsList[2] in rankSet:\r\n                role = roleUrl(argsList[0])\r\n                region = regionUrl(argsList[1])\r\n                if argsList[2] in {'plat','platinum'}:\r\n                    webScrape = WebScraper(region + '&' + role,'',argsList[1],argsList[0])\r\n                else:\r\n                    webScrape = WebScraper(region + '&' + role + '&rank=diamond_plus','Diamond+',argsList[1],argsList[0])\r\n            \r\n            else: #argument not in a set\r\n                return    \r\n                \r\n        else: #too many arguments\r\n            return\r\n        await ctx.send(webScrape.parse())\r\n\r\n#functions here\r\ndef regionUrl(args):\r\n    if args == 'kr':\r\n        return 'https://u.gg/lol/champions/talon/build?region=kr'\r\n    elif args == 'na':\r\n        return 'https://u.gg/lol/champions/talon/build?region=na1'\r\n    elif args in {'eu','euw'}:\r\n        return 'https://u.gg/lol/champions/talon/build?region=euw1'\r\n    elif args == 'br':\r\n        return 'https://u.gg/lol/champions/talon/build?region=br1'\r\n    elif args in {'eune','eun'}:\r\n        return 'https://u.gg/lol/champions/talon/build?region=eun1'\r\n    elif args == 'jp':\r\n        return 'https://u.gg/lol/champions/talon/build?region=jp1'\r\n    elif args == 'lan':\r\n        return 'https://u.gg/lol/champions/talon/build?region=la1'\r\n    elif args == 'las':\r\n        return 'https://u.gg/lol/champions/talon/build?region=la2'\r\n    elif args == 'oce':\r\n        return 'https://u.gg/lol/champions/talon/build?region=oc1'\r\n    elif args == 'ru':\r\n        return 'https://u.gg/lol/champions/talon/build?region=ru'\r\n    elif args == 'tr':\r\n        return 'https://u.gg/lol/champions/talon/build?region=tr1'\r\n    \r\ndef roleUrl(args):\r\n    if args == 'top':\r\n        return 'role=top'\r\n    elif args in {'jg','jungle'}:\r\n        return 'role=jungle'\r\n    elif args in {'mid','middle'}:\r\n        return 'role=middle'\r\n    elif args in {'adc','bot'}:\r\n        return 'role=adc'\r\n    elif args in {'supp','support'}:\r\n        return 'role=support'\r\n    \r\ndef setup(client):\r\n    client.add_cog(GeneralCommands(client))", "100": "import Webscrape\nimport streamlit as st\nfrom docx import Document\n\nst.set_page_config(page_title = \"In-text Citation Generator\", page_icon= \":book:\")\n\nst.write(\"\"\"   \n# In-text Citation Generator\n\nInsert your document with PubMed links to convert them to in-text citations\n\n\"\"\")\n\nwith st.form(key = \"form1\"):\n    document = st.text_area(\"Insert document text\", height = 400)\n    delimiter = st.text_input(\"Insert delimiter\", help = \"Set the border you wish to surround your PubMed links, by default it will be []\" ,placeholder = \"[]\")\n    if st.form_submit_button(\"Process document\", help = \"After entering your document, you can press this button to create a cited file\"):\n        if delimiter == '':\n            delimiter = \"[]\"\n        cited_document = Webscrape.document_citation(document, delimiter)\n        #st.write(cited_document)\n\n\n\nwith st.form(key = \"form2\"):\n    uploaded_file = st.file_uploader(\"Upload document or text file\", type=['.txt', '.docx'])\n    delimiter = st.text_input(\"Insert delimiter\", help = \"Set the border you wish to surround your PubMed links, by default it will be []\" ,placeholder = \"[]\")\n    if st.form_submit_button(\"Process file\", help = \"Process the file you uploaded\") and uploaded_file is not None:\n        if uploaded_file.type == \"text/plain\":\n            #st.text(str(uploaded_file.read(),\"utf-8\"))\n            document = str(uploaded_file.read(),\"utf-8\")\n        elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n            document = Document(uploaded_file)\n            document = ''.join([str(paragraph.text) for paragraph in document.paragraphs])\n        if delimiter == '':\n            delimiter = \"[]\"\n        cited_document = Webscrape.document_citation(document, delimiter)\n        #st.write(cited_document)\n\n\ntry:\n    st.write(cited_document)\n    file_name = st.text_input(\"File name\", help = \"Set your file_name\")\n    st.download_button (label = \"Download Cited Document\", data = cited_document, file_name = file_name)\nexcept:\n    pass", "101": "from edtools import *\nfrom edtools.Scrape import WebScrape\nimport sys\n\n\n# Extend core\nclass DownloadWeb(WebScrape.WebScrape):\n\t# def __init__(self, root = \"C:/xampp/htdocs/webscrape\", folder = \"\", srfolder = \"\" ):\n\t# self.root = root\n\t# self.folder = folder\n\t# self.srcFolder = srcFolder\n\n\tpass\n\n\n# Extend core\nclass BrowseWeb(WebScrape.browseWeb):\n\t# def __init__(self, root = \"C:/xampp/htdocs/webscrape\", folder = \"\", srfolder = \"\" ):\n\t# self.root = root\n\t# self.folder = folder\n\t# self.srcFolder = srcFolder\n\n\tpass\n\n\ndef scrape( url, folder, rewriteIMG=1, rewriteFiles=1, downloadFiles = 1, createFolders = 1, createExt = 0  ):\n\n\n\tdef initScrape():\n\n\t\tapp = DownloadWeb( url = url, folder=folder, rewriteIMG=rewriteIMG, rewriteFiles=rewriteFiles, downloadFiles = downloadFiles, createFolders = createFolders, createExt = createExt )\n\n\t\t# -------------------------------------------------------\n\t\t# Scraping Procces\n\t\t# -------------------------------------------------------\n\n\n\t\t#app.getHtmlCode(url) # Get html code from web\n\t\tapp.loadCodeHtml(url)  # load html code from HDD\n\n\t\tapp.loadResourcesFromJson() # load list of resources from file\n\t\tapp.fixFoldersByReferers() # fix folders tree\n\t\tapp.saveResources() # download requests\n\n\t\tapp.makeCopyBackup() # create backups of files that need to be changed\n\t\tapp.beautyWeb() # beautifying files\n\t\tapp.saveHtml() # save index.html\n\n\t\tapp.fixResourcesChanges() # Fix resources errors and make conversions\n\t\tapp.convertWEbToLocal() # Convert web to local relative paths\n\n\n\t\t# -------------------------------------------------------\n\t\t# END Scraping Procces\n\t\t# -------------------------------------------------------\n\n\tinitScrape()\n\n\ndef browser( url, folder, rwi, rwf, dF, cf, ce ):\n\n\n\tappd = DownloadWeb( url = url, folder = folder, rewriteIMG = rwi, rewriteFiles = rwf, downloadFiles = dF, createFolders = cf, createExt = ce  )\n\n\tappb = BrowseWeb( folder = folder ) # Launching Selenium and BrowserMobProxy\n\tappb.setUrl( url ) # Just set url but not execute\n\n\tdef custom():\n\n\t\trequestsList = appb.getRequests()\n\n\t\tappd.setCode(appb.getCode(), url)\n\t\tappd.setResources(requestsList)\n\n\n\n\t\t# custom code from downloadweb( class )\n\n\t\t#appd.saveResources()\n\t\t# app.convertWEbToLocal()\n\t\t# appd.saveHtml()\n\n\n\twhile True:\n\n\t\tdata = input(\"--> \")\n\n\t\tif data in \" \\n\":\n\t\t\tpass\n\n\t\telif data == \"9\":\n\t\t\tcustom()\n\n\t\telif data in \"close exit out finish stop\" or data == \"0\":\n\t\t\tappb.exit()\n\t\t\tbreak\n\n\t\telif \"exec\" in data:\n\t\t\tdata = data.replace(\"exec \", \"\")\n\t\t\ttry: exec(data)\n\t\t\texcept: print(\"Unexpected error:\", sys.exc_info()[0])\n\n\t\telse:\n\t\t\tappb.command(data)\n\n\n\n\ndef init( url, folder, rwi, rwf, system, downloadFiles, createFolders, createExt  ):\n\n\n\tdef pInit():\n\t\tprint(\"\\n\" * 2 + \"=\" * 80 + \"\\n\" + \"\\n STARTED\\n\\n\" + \"=\" * 80 + \"\\n\")\n\n\tdef kInfo():\n\t\tprint( \"\\n 0 ) stop \\n\\n 1 ) start \\n 2 ) getrequests \\n 9 ) custom code \\n\\n\\n\" + \"=\" * 80 + \"\\n\")\n\n\tdef pEnd():\n\t\tprint(\"\\n\" * 5 + \"=\" * 80 + \"\\n\" + \"\\n ENDED\\n\\n\" + \"=\" * 80 + \"\\n\")\n\n\tdef start():\n\n\t\tif system == \"scrape\":\n\t\t\tscrape(url, folder, rwi, rwf, downloadFiles, createFolders, createExt )\n\n\t\tif system == \"browser\":\n\t\t\tbrowser( url, folder, rwi, rwf, downloadFiles, createFolders, createExt )\n\n\n\tpInit()\n\n\tkInfo()\n\n\tstart()\n\n\tpEnd()\n\n\n\n\n\n# -------------------------------------------------------\n#\n#    ( rwi = 1 )    -> 1 Rewrite images, 0 don't rewrite\n#    ( rwf = 1 )    -> 1 Rewrite files, 0 don't rewrite\n#    ( system = \"scrape\" ) -> scrape with just BeautifulSoup\n#    ( system = \"browser\" ) -> scrape with Selenium, BrowserMobProxy and BeautifulSoup\n#    ( folder )     -> folder project\n#\n# -------------------------------------------------------\n\n\nsystem = \"scrape\"\n#system = \"browser\"\n\n\nrwi = 0\nrwf = 0\ndownloadFiles = 1       # Just for test purpose\ncreateExt = 1           # Create a extension by typefile\ncreateFolders =  0      # If folder not exist create it by type file\n\n\nfolder = \"stripe\"\nurl = \"https://stripe.com\"\n\n#folder = \"nytimes\"\n#url = \"https://nytimes.com\"\n#url = \"https://google.com\"\n\n\n'''\nThis app works with the files to work properly:\n\tlogs/resources.json\n\tindexOriginal.html\n\t\nThose file are generate by system = \"browser\"\n'''\n\ninit( url, folder, rwi, rwf, system, downloadFiles, createFolders, createExt )\n\n\n\n", "102": "# selenium \nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver \nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\nimport json\n\nclass WebData:\n  def __init__(self, url: str, gpp_type: str, workout_dates: list = [], params: str = ''):\n    self.url = url\n    self.gpp_type = gpp_type\n    self.workout_dates = workout_dates\n    self.params = params\n    self.wods_json = self.webscrape_data_to_json\n\n  # Replace \\n and * in  and \\t code with empty string\n  @staticmethod\n  def replace_chars(s: str) -> str:\n      s = s.replace('\\n', '').replace('*', '').replace('\\t', '')\n      return s   \n\n  # Pulls data from websites using selenium--SSLP is static tables, DEUCE is dynamic\n  # Printing the static SSLP to csv for parsing\n  # DEUCE ultimately needs to be jsonified and saved to db\n  def webscrape_data_to_csv(self) -> bool:\n    bool_csv_created = False\n    driver = webdriver.Chrome(options=self.get_chrome_options())\n    driver.get(self.url)\n    elem_to_find = \"proggy\"\n    try:\n      nlp_elements = driver.find_elements(By.CLASS_NAME, elem_to_find)\n      # df.append deprecated so using tmp list of dataframes to append then concat \n      tmp = []      \n      for nlp_phase in nlp_elements:\n          nlp_phase_innerHTML = self.replace_chars(nlp_phase.get_attribute('innerHTML'))\n          tmp.append(pd.read_html(\"\" + nlp_phase_innerHTML + \"\")[0])\n      nlp_df = pd.concat(tmp, ignore_index=True)\n      nlp_df.to_csv('sslp.csv', encoding='utf-8', index=False)\n      bool_csv_created = True\n      #print(calc_sslp_ph1())\n    finally:\n        driver.quit()   \n        return bool_csv_created\n    \n  def webscrape_data_to_json(self) -> str:\n    if self.gpp_type == 'DEUCE':\n      return self.webscrape_deuce_data_to_json()\n    else: # PushJerk\n      return self.webscrape_pj_data_to_json()\n\n  def webscrape_deuce_data_to_json(self) -> str:\n    json_formatted_str = ''\n    driver = webdriver.Chrome(options=self.get_chrome_options())\n    # TODO: get this working for singleton the loop it => for wod_date in workout_dates:\n    wod_date = self.workout_dates[0]\n    driver.get(self.url+wod_date)\n    popup_xpath = '/html/body/div[3]/div/img'\n    try:\n        popup = driver.find_element_by_xpath(popup_xpath)\n        if popup.is_displayed:\n          popup.click() # Closes the popup\n    except Exception: #NoSuchElementException:\n      # no popup\n      pass\n    else: \n      inner_html = '\\n\\t\\t\\t11/29/21 WOD\\n\\t\\t\\tDEUCE ATHLETICS GPP\\nComplete 4 rounds for quality of:\\n8 Barbell Strict Press (3x1x)\\n8 Single Kettlebell Lateral Lunge\\nThen, AMRAP 12\\n1,2,3,\u2026,\u221e\\nFront Squat (135/95)\\nDB Renegade Row (40/20)\\n**Every 2 min, 1 7th Street Corner Run\\nDEUCE GARAGE GPP\\n5-5-5-5-5\\nPendlay Row\\nThen, complete 3 rounds for quality of:\\n10 Single Arm Bent Over row (ea)\\n10-12 Parralette Push Ups\\n10 Hollow Body Lat Pulls\u00a0\\nThen, AMRAP8\\n6 Chest to Bar Pull Ups\\n8 HSPU\\n48 Double unders\\n\\t\\t'\n      inner_html = self.replace_chars(inner_html)\n      df_wod = pd.read_html('' + inner_html + '')\n      print(df_wod)\n      wod_link_xpath = '/html/body/div[1]/main/center/article/div/p/a'\n      wod_link = driver.find_element_by_xpath(wod_link_xpath)\n      ActionChains(driver).move_to_element(wod_link).click(wod_link).perform()\n      wod_element = WebDriverWait(driver, 10).until(\n          EC.presence_of_element_located((By.CLASS_NAME, \"wod_block\"))\n      )\n      wod_innerHTML = wod_element.get_attribute('innerHTML')\n      df_wod = pd.read_html(\"\" + wod_innerHTML + \"\")\n      wod_json_str = df_wod.to_json(orient='records')\n      obj_data = json.loads(wod_json_str)\n      json_formatted_str += json.dumps(obj_data, indent=4) \n    finally:\n      driver.quit()  \n      return json_formatted_str    \n\n  def webscrape_pj_data_to_json(self) -> str:\n    wod_list = []\n    try:\n        driver = webdriver.Chrome(chrome_options=self.get_chrome_options())\n        driver.get(self.url + self.gpp_type + self.params)\n        source_code = driver.page_source\n        soup = BeautifulSoup(source_code,'lxml')\n        entry_block = soup.find_all('div', class_='entry-title')\n        for entries in entry_block:\n          pj_entry_url = entries.find('a')\n          wod_list.append(pj_entry_url)\n        # for w in wods:\n        #     pubDate = w.find('pubDate').text\n        #     encoded_content = w.find('encoded')\n        #     for child in encoded_content.children:\n        #         content = str(child)\n        #     wod = {\n        #         'pubDate': pubDate,\n        #         'content': content,\n        #     }\n        #     wod_list.append(wod)\n    except Exception as e:\n        print('The scraping job failed. See exception: ')\n        print(e)\n    return json.dumps(wod_list)    \n\n  def get_chrome_options():\n    options = webdriver.ChromeOptions()\n    options.add_argument('--ignore-certificate-errors')\n    options.add_argument('--incognito')\n    options.add_argument('--headless')\n    return options    ", "103": "from selenium.webdriver import Chrome\nfrom os import path, system\nfrom webScrape import WebScrape\nfrom time import sleep\ndir = path.dirname(__file__)\nchrome_path = path.join(dir, 'selenium','webdriver','chromedriver.exe')\nwebS = WebScrape()\nurl = 'https://bongo.cat/'\nwith Chrome(chrome_path,options=webS.optionsChrome(True)) as driver:\n    driver.get(url)\n    elmnt = driver.find_element_by_tag_name('html')\n    while True:\n        elmnt.send_keys('u')\n        sleep(0.5)\n        elmnt.send_keys('u')\n        sleep(0.5)\n        elmnt.send_keys('o')\n        sleep(0.5)\n        elmnt.send_keys('p')\n        sleep(0.5)\n        elmnt.send_keys('p')\n        sleep(0.5)\n        elmnt.send_keys('p')\n        sleep(0.5)\n        elmnt.send_keys('e')\n        sleep(0.5)\n        elmnt.send_keys('e')\n        sleep(0.5)\n        elmnt.send_keys('w')\n        sleep(0.5)\n        elmnt.send_keys('w')\n        sleep(0.5)\n        elmnt.send_keys('w')\n        sleep(0.5)\n        elmnt.send_keys('t')\n        sleep(0.5)\n        elmnt.send_keys('t')\n        sleep(0.5)\n        elmnt.send_keys('t')\n        sleep(0.5)\n        elmnt.send_keys('t')\n        sleep(0.5)\n        elmnt.send_keys('t')\n        sleep(0.5)", "104": "\"\"\"\nDjango settings for webScrape project.\n\nGenerated by 'django-admin startproject' using Django 4.0.6.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.0/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/4.0/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/4.0/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-=kz&bl&1cz^(e=n)1u#xcy_=ryr2mewr665av4)pt1#vpx9-%m'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'drf_yasg',\n    'rest_framework_swagger',\n    'rest_framework',\n    'webScrape',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'webScrape.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'webScrape.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/4.0/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/4.0/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/4.0/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/4.0/howto/static-files/\n\nSTATIC_URL = 'static/'\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/4.0/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n", "105": "# Import dependencies\nfrom flask import Flask, render_template, request\nimport os\nfrom Scripts import eplWebscrape\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.route('/copies', methods=['POST'])\ndef copies():\n    if request.method=='POST':\n        os.popen('cp rankings/DefenderRank.csv rankings/DefenderRank\\ copy.csv')\n        os.popen('cp rankings/ForwardRank.csv rankings/ForwardRank\\ copy.csv')\n        os.popen('cp rankings/GoalieRank.csv rankings/GoalieRank\\ copy.csv')\n        os.popen('cp rankings/MidfielderRank.csv rankings/MidfielderRank\\ copy.csv')\n        return render_template('copies.html')\n\n@app.route('/update', methods=['POST'])\ndef runScript():\n    if request.method=='POST':\n        eplWebscrape.webscrape()\n        return render_template('update.html')\n\n@app.route('/defenders')\ndef defenders():\n    return render_template('defenders.html')\n\n@app.route('/goalies')\ndef goalies():\n    return render_template('goalies.html')\n\n@app.route('/midfielders')\ndef midfielders():\n    return render_template('midfielders.html')\n\n@app.route('/forwards')\ndef forwards():\n    return render_template('forwards.html')\n\n\nif __name__ == '__main__':\n    app.run\n", "106": "import airflow.utils.dates\n\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.email_operator import EmailOperator\nfrom airflow.models import Variable\n\n# import sys\n# sys.path.append('/home/speng/Desktop/projects/spca-cat-adoption')\nfrom webscraping import webscrape\nfrom data_processing import check_cats\nfrom data_processing import clean_up\n\nstart = DummyOperator(task_id='start') \n\ndag = DAG(\n    dag_id=\"scrape-spca\",\n    start_date=airflow.utils.dates.days_ago(3),\n    max_active_runs=1,\n    schedule_interval=\"@hourly\"\n)\n\n\nemail= EmailOperator(\n       task_id='email',\n       to=Variable.get('receiver_email'),\n       subject='Cat Adoption Updates',\n       html_content=\"{{ task_instance.xcom_pull(task_ids='check_for_new_cats', key='new_cats') }}\",\n       dag=dag\n)\n\nscrape_eastbay_spca = PythonOperator(\n    task_id='scrape_eastbay_spca',\n    python_callable=webscrape.run_eastbay_spca_scraper,\n    dag=dag\n)\n\nscrape_sf_spca = PythonOperator(\n    task_id='scrape_sf_spca',\n    python_callable=webscrape.run_sf_spca_scraper,\n    dag=dag\n)\n\nscrape_jellysplace = PythonOperator(\n    task_id='scrape_jellys_place',\n    python_callable=webscrape.run_jellys_place_scraper,\n    dag=dag\n)\n\ncheck_for_new_cats = PythonOperator(\n    task_id='check_for_new_cats',\n    python_callable=check_cats.check_for_new_cats,\n    trigger_rule=\"none_failed\",\n    dag=dag\n)\n\nclean_up = PythonOperator(\n    task_id='clean_up',\n    python_callable=clean_up.run_clean_up,\n    dag=dag\n)\n\n\nstart >> [scrape_sf_spca, scrape_eastbay_spca, scrape_jellysplace]\n[scrape_eastbay_spca, scrape_sf_spca, scrape_jellysplace] >> check_for_new_cats >> email >> clean_up\n ", "107": "import sys\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\nimport subprocess\nimport time\nimport re\n\ndef webscrape_conncected(issueState, issueList, keywords, status, topic):\n    files = []\n    print('Number of {} issues to be explored : {}'.format(issueState, len(issueList)))\n\n    for keyword in keywords:\n        if status == 'test':\n            files.append(open('issues/test/{}/{}/All-{}-Issues-Having-{}-in-{}.txt'.format(topic, issueState, issueState, keyword, framework), 'w+', encoding='utf-8'))\n        else:\n            files.append(open('issues/{}/{}/All-{}-Issues-Having-{}-in-{}.txt'.format(topic, issueState, issueState, keyword.replace('\\\\','').replace('/','-'), framework), 'w+', encoding='utf-8'))\n\n    for issue in issueList:\n        with open(issue, 'r', encoding='utf-8') as f:\n            #parsing the html file corresponding to the issue\n            soup = BeautifulSoup(f.read(), 'html5lib')\n\n            #getting the first post which will correspond to the issue's description\n            # issueDesc = soup.find('td', class_=\"d-block comment-body markdown-body js-comment-body\")\n\n            #extracting the issue's ID from the bug report\n            issueIdMatch = re.search('\\\\d+', issue)\n            issueId = issue[issueIdMatch.start() : issueIdMatch.end()]\n\n            for i, keyword in enumerate(keywords):\n                #check if the html has the keyword\n                issueTitle = soup.find('span', class_=\"js-issue-title markdown-title\")\n                issueDescs = soup.findAll('td', class_=\"d-block comment-body markdown-body js-comment-body\")\n                issueDesc = '\\n'.join(issueDescs)\n                issue_title_desc = issueTitle + '\\n' + issueDesc\n                stringsContainingTheKeyword = issue_title_desc.findAll(text=re.compile(keyword, re.I))\n                NumOfRepeat = len(stringsContainingTheKeyword)\n                if (NumOfRepeat > 0):\n                    issueInfo = {\n                    'ID' : issueId,\n                    # 'Description' : issueDesc,\n                    'Num of Repeat': NumOfRepeat\n                    }\n                    files[i].write('ID: {}\\tNum of Repeat: {}\\n'.format(issueInfo['ID'], issueInfo['Num of Repeat']))\n\n\ndef webscrape_separated(issueState, issueList, keywords_set, status, topic):\n    files = []\n    print('Number of {} issues to be explored : {}'.format(issueState, len(issueList)))\n\n    for keyword_set in keywords_set:\n        keyword = '-'.join(keyword_set)\n        if status == 'test':\n            files.append(open('issues/test/{}/{}/All-{}-Issues-Having-{}-in-{}.txt'.format(topic, issueState, issueState, keyword, framework), 'w+', encoding='utf-8'))\n        else:\n            files.append(open('issues/{}/{}/All-{}-Issues-Having-{}-in-{}.txt'.format(topic, issueState, issueState, keyword.replace('\\\\','').replace('/','-'), framework), 'w+', encoding='utf-8'))\n\n    for issue in issueList:\n        with open(issue, 'r', encoding='utf-8') as f:\n            #parsing the html file corresponding to the issue\n            soup = BeautifulSoup(f.read(), 'html5lib')\n\n            #getting the first post which will correspond to the issue's description\n            # issueDesc = soup.find('td', class_=\"d-block comment-body markdown-body js-comment-body\")\n\n            #extracting the issue's ID from the bug report\n            issueIdMatch = re.search('\\\\d+', issue)\n            issueId = issue[issueIdMatch.start() : issueIdMatch.end()]\n\n            for i, keyword_set in enumerate(keywords_set):\n                #check if the html has the keyword\n                NumOfRepeats = ''\n                issueTitle = soup.find('span', class_=\"js-issue-title markdown-title\")\n                issueDescs = soup.findAll('td', class_=\"d-block comment-body markdown-body js-comment-body\")\n                issueDesc = '\\n'.join(issueDescs)\n                issue_title_desc = issueTitle + '\\n' + issueDesc\n                for keyword_index, keyword in enumerate(keyword_set):\n                    stringsContainingTheKeyword = issue_title_desc.findAll(text=re.compile(keyword, re.I))\n                    NumOfRepeat = len(stringsContainingTheKeyword)\n                    if (NumOfRepeat == 0):\n                        break\n                    \n                    if keyword_index == len(keyword_set) - 1:\n                        NumOfRepeats += str(NumOfRepeat)\n                        issueInfo = {\n                            'ID' : issueId,\n                            # 'Description' : issueDesc,\n                            'Num of Repeats': NumOfRepeats\n                        }\n                        files[i].write('ID: {}\\tNum of Repeats: {}\\n'.format(issueInfo['ID'], issueInfo['Num of Repeats']))\n\n                    else:\n                        NumOfRepeats += str(NumOfRepeat) + ','\n\n\ndef writeResults(issueState, issuesInfo, keyword):\n    try:\n        f = open('All-{}-Issues-Having-{}-in-{}.txt'.format(issueState, keyword, framework), 'w+', encoding='utf-8')\n        for issueInfo in issuesInfo:\n            f.write('ID: {}\\tNum of Repeat: {}\\tDescription: {}\\n'.format(issueInfo['ID'], issueInfo['Num of Repeat'], issueInfo['Description']))\n    except:\n        print('Something went wrong while writing the results')\n    finally:\n        f.close()\n\n\nissueStates = ['open', 'closed']\n\nframeworkOrg = {\n    'tensorflow' : 'tensorflow',\n}\n\nframework = sys.argv[1]\n\nif framework not in frameworkOrg.keys():\n    raise Exception(\"Please enter a valid framework in the command line\")\n\nissueState = sys.argv[2]\n\nstatus = sys.argv[3]\n\ntopic = sys.argv[4]\n\nstart = time.time()\n\nissues = []\n\nkeywords = []\n\n\nif issueState == 'open' and status == 'real':\n    issues = ['../polygot_in_dl_frameworks/polygot_in_dl_frameworks/{}/open/{}'.format(framework, f) for f in os.listdir('../polygot_in_dl_frameworks/polygot_in_dl_frameworks/{}/open'.format(framework))]\n    # issues = ['{}/open/{}'.format(framework, f) for f in os.listdir('{}/open'.format(framework))]\n\nelif issueState == 'closed' and status == 'real':\n    issues = ['../polygot_in_dl_frameworks/polygot_in_dl_frameworks/{}/closed/{}'.format(framework, f) for f in os.listdir('../polygot_in_dl_frameworks/polygot_in_dl_frameworks/{}/closed'.format(framework))]\n    # issues = ['{}/closed/{}'.format(framework, f) for f in os.listdir('{}/closed'.format(framework))]\n\nelif issueState == 'open' and status == 'test':\n    issues = ['../Test/{}/open/{}'.format(framework, f) for f in os.listdir('../Test/{}/open'.format(framework))]\n\nelif issueState == 'closed' and stauts == 'test':\n    issues = ['../Test/{}/closed/{}'.format(framework, f) for f in os.listdir('../Test/{}/closed'.format(framework))]\n\nelse:\n    raise Exception(\"Please enter valid inputs in the command line\")\n\n# security_keywords = ['exception', 'crash', 'security', 'token', 'secret', 'TODO', 'password', 'vulnerable', 'hash', 'HMAC', 'MD5', 'SHA-1', 'SHA-2']#attacker, \n# performance_keywords = ['performance', 'efficiency', 'efficient', 'fast', 'speed', 'slow', 'memory usage', 'improve', 'memory leak', 'optimize']\n# performance_accuracy_regression = [['accuracy', 'decreas(e|ed)'], ['accuracy', 'degrad(e|ed)'], ['worse', 'accuracy'], ['accuracy', 'dro(p|pped|pping)']]\nperformance_regression_keywrods = ['accuracy decreas(e|ed)', 'accuracy degrad(e|ed)', 'worse in accuracy', 'accuracy dro(p|pped|pping)', 'memory increas(ed|e|ing)', 'memory usage increase(ed|e|ing)', 'computation time increase(ed|e|ing)', 'got slow', 'performance regression']\n\nif topic == \"performance_accuracy_regression\":\n    webscrape_separated(issueState, issues, performance_accuracy_regression, status, topic)\nelif topic == \"prediction\":\n    webscrape_separated(issueState, issues, None, status, topic)\nelif topic == \"both\":\n    webscrape_separated(issueState, issues, performance_accuracy_regression, status, 'performance_accuracy_regression')\n    webscrape_separated(issueState, issues, None, status, 'prediction')\nelse:\n    raise Exception(\"Wrong topic\")\n\ntimeTaken = time.time() - start\n\nprint('Time taken: {}'.format(timeTaken))\n", "108": "from flask import Flask, request\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\napp = Flask(__name__)\n\n@app.route('/webscrape', methods=['POST'])\ndef webscrape():\n    if request.method == 'POST':\n        driver = webdriver.Chrome(executable_path='./chromedriver.exe')\n        options = webdriver.ChromeOptions()\n        options.add_argument('--enable-javascript')\n        options.add_argument('headless')\n        \n        url = request.get_json()\n        driver.get(url)\n\n        title = driver.find_element(By.CLASS_NAME, 'product-name').get_attribute('textContent')\n        price = driver.find_element(By.CLASS_NAME, 'regPrice').get_attribute('textContent')\n        desc = driver.find_element(By.CLASS_NAME, 'description-text').get_attribute('textContent')\n        \n        return {'items': [url, title, price, desc]}\n    \n    else:\n        return \"Error\"\n\nif __name__=='__main__':\n    app.run(debug=True)", "109": "from django.conf.urls import patterns, include, url\n\nfrom django.contrib import admin\nadmin.autodiscover()\n\nurlpatterns = patterns('',\n    # Examples:\n    # url(r'^$', 'websnoop.views.home', name='home'),\n    # url(r'^blog/', include('blog.urls')),\n\n    url(r'^admin/', include(admin.site.urls)),\n    url(r'^$', 'websnoop.webforms.showFinderForm'),\n    url(r'^snoop/getpages/$', 'webscrape.views.getPages'),\n    url(r'^snoop/$', 'webscrape.views.getInputs'),\n    url(r'^snoop/getinput/$', 'webscrape.views.getInputs'),\n)\n", "110": "#!/usr/bin/env python\r\n\r\nfrom distutils.core import setup\r\n\r\nsetup(name='webscrape',\r\n      version='0.1',\r\n      description='Thematic analysis of scientific literature: tools for web scraping',\r\n      url='http://github.com/LucaDiStasio/literature_analysis/webscrape',\r\n      author='Luca Di Stasio',\r\n      author_email='luca.distasio@gmail.com',\r\n      license='GNU GPL',\r\n      packages=['webscrape'],\r\n      install_requires=[\r\n          'mechanize',\r\n          'urllib',\r\n          'urllib2',\r\n          'cookielib',\r\n          'BeautifulSoup',\r\n          'random',\r\n      ],\r\n      zip_safe=False,\r\n      keywords = [\"scraping\", \"web scraping\"],\r\n      classifiers = [\r\n        \"Programming Language :: Python\",\r\n        \"Programming Language :: Python :: 2\",\r\n        \"Development Status :: 0.1\",\r\n        \"Environment :: Other Environment\",\r\n        \"Intended Audience :: Developers\",\r\n        \"License :: OSI Approved :: GNU Library or Lesser General Public License (LGPL)\",\r\n        \"Operating System :: Windows\",\r\n        \"Topic :: Web Scraping :: Libraries :: Python Modules\",\r\n        \"Topic :: Text Processing :: Linguistic\",\r\n        ],\r\n      long_description = \"\"\"\\\r\nWeb scraping tools\r\n-------------------------------------\r\n\r\nThis version requires Python 2 or later.\r\n\"\"\")", "111": "from selenium.webdriver import Chrome\nfrom os import path, system\nfrom webScrape import WebScrape\nfrom time import sleep\ndir = path.dirname(__file__)\nchrome_path = path.join(dir, 'selenium','webdriver','chromedriver.exe')\nwebS = WebScrape()\nurl = 'https://bongo.cat/'\nwith Chrome(chrome_path,options=webS.optionsChrome(True)) as driver:\n    driver.get(url)\n    elmnt = driver.find_element_by_tag_name('html')\n    while True:\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.6)\n\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.6)\n\n        elmnt.send_keys('0')\n        sleep(0.5)\n        elmnt.send_keys('3')\n        sleep(0.5)\n        elmnt.send_keys('5')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.6)\n\n        elmnt.send_keys('8')\n        sleep(0.5)\n        elmnt.send_keys('8')\n        sleep(0.5)\n        elmnt.send_keys('8')\n        sleep(0.5)\n        elmnt.send_keys('8')\n        sleep(0.5)\n        elmnt.send_keys('8')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.6)\n\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('5')\n        sleep(0.5)\n        elmnt.send_keys('5')\n        sleep(0.5)\n        elmnt.send_keys('7')\n        sleep(0.5)\n        elmnt.send_keys('5')\n        sleep(0.6)\n\n        elmnt.send_keys('0')\n        sleep(0.5)", "112": "# apps modules\nfrom chatbot_app.modules.status_news import StatusNews\nfrom chatbot_app.modules.dist2hospital import Dist2Hospital\nfrom chatbot_app.modules.diagnosis import Diagnosis\nfrom chatbot_app.modules.webscrape import Webscrape\nfrom chatbot_app.modules.dialogflow_msg import Server\n\n\n#### FOR GLOBAL STATUS - FOR INFECTION STATUS INTENT ####\n\nclass Feature(Server):\n    \n    def __init__(self, request):\n        self.sn = StatusNews(request)\n        self.d2h = Dist2Hospital(request)\n        self.dgs = Diagnosis(request)\n        self.wbs = Webscrape()\n        super().__init__(request)\n\n        self.intent = super().rcvIntent()\n\n    def main(self):\n        # --------------------------#\n        # INFECTION STATUS INTENT   #\n        # --------------------------#\n        if self.intent == \"infection-status-covid\":\n            return self.sn.infectionStatus()\n\n        # --------------------------#\n        # HEADLINE NEWS INTENT      #\n        # --------------------------#\n        if self.intent == \"latest-news-covid\":\n            return self.sn.headlineNews()\n\n        # --------------------------#\n        # Distance to Hospital      #\n        # --------------------------#\n        if self.intent == \"nearest-hospital-covid\" or self.intent == \"treatment-covid.yes.address\":\n            return self.d2h.dist2hospital()\n\n        # --------------------------#\n        # DIAGNOSIS INTENT          #\n        # --------------------------#\n        if self.intent == \"diagnosis-covid\":\n            return self.dgs.diagnosis()\n\n        # --------------------------#\n        # SYNC  INTENT              #\n        # --------------------------#\n        if self.intent == \"sync\":\n            try:\n                self.wbs.statusScrapper()\n                self.wbs.newsScrapper()\n                self.dgs.updateResponses()\n                self.text1 = \"Sync/update completed.\"\n            except:\n                self.text1=\"Error occurred. Contact admin to debug.\"\n            finally:\n                return super().sendMsg()\n\n      \n        \n", "113": "import sqlite3\nimport webscrape\n\n\n#Getting the base price through web scraping\nbij = {}\nud = {}\nbang = {}\nbij = webscrape.bij_price\nud = webscrape.u_price\nbang = webscrape.bang_price\n\ndef price_reduction(investment, profit):\n    ratio = (investment/profit)*100\n    if(ratio>=90):\n        return 7\n    elif(ratio<90 and ratio>=70):\n        return 5\n    elif(ratio<70 and ratio>=50):\n        return 4\n    elif(ratio<50 and ratio>=20):\n        return 2\n    else:\n        return 1\n\ndef calculate_price(ratio, location, crop):\n    if(location=='Bijapur'):\n        base = bij[crop]\n        final = base - ((base*ratio)/100)\n        return final\n    elif(location=='Udupi'):\n        base = ud[crop]\n        final = base - ((base*ratio)/100)\n        return final\n    else:\n        base = bang[crop]\n        final = base - ((base * ratio) / 100)\n        return final\n\n\ndef bijapur():\n    conn = sqlite3.connect('crop.db')\n    c = conn.cursor()\n    c.execute(\"SELECT id, crop, investment, profit FROM bijapur\")\n    for i in c.fetchall():\n        index = i[0]\n        ratio = price_reduction(i[2],i[3])\n        final_price = calculate_price(ratio, 'Bijapur', i[1])\n        c.execute(\"UPDATE bijapur SET price= ? WHERE id= ?\",(final_price,index))\n    conn.commit()\n    conn.close()\n\ndef udupi():\n    conn = sqlite3.connect('crop.db')\n    c = conn.cursor()\n    c.execute(\"SELECT id, crop, investment, profit FROM udupi\")\n    for i in c.fetchall():\n        index = i[0]\n        ratio = price_reduction(i[2],i[3])\n        final_price = calculate_price(ratio, 'Udupi', i[1])\n        c.execute(\"UPDATE udupi SET price= ? WHERE id= ?\", (final_price, index))\n    conn.commit()\n    conn.close()\n\ndef bangalore():\n    conn = sqlite3.connect('crop.db')\n    c = conn.cursor()\n    c.execute(\"SELECT id, crop, investment, profit FROM bangalore\")\n    for i in c.fetchall():\n        index = i[0]\n        ratio = price_reduction(i[2],i[3])\n        final_price = calculate_price(ratio, 'Bangalore rural', i[1])\n        c.execute(\"UPDATE bangalore SET price= ? WHERE id= ?\", (final_price, index))\n    conn.commit()\n    conn.close()\n\n", "114": "import numpy as np\r\nimport cv2\r\nimport tkinter \r\nimport PIL.Image\r\nimport PIL.ImageTk\r\nimport makeup\r\nimport imutils\r\nfrom imutils import face_utils\r\nfrom tkinter import *\r\nimport os\r\nimport webscrapeTopRated\r\nimport webbrowser\r\nimport twilioSMS\r\nfrom tkinter.filedialog import askopenfilename\r\n\r\n\r\n####################\r\n\r\ndef loadSwatches(data):\r\n    images = []\r\n    dirpath = os.path.dirname(os.path.realpath(__file__)).replace(\"\\\\\", \"/\")\r\n    pathToFeature = dirpath + \"/Colors/%s\" %(data.currentFeature)\r\n    files = os.listdir(pathToFeature)\r\n    for image in files:\r\n        path = pathToFeature + \"/\" + image\r\n        images += [path]\r\n    swatches = []\r\n    for pic in range(len(images)):\r\n        photo = cv2.imread(images[pic])\r\n        photo = cv2.cvtColor(photo, cv2.COLOR_BGR2RGB)\r\n        photo = imutils.resize(photo, width = 36, inter = cv2.INTER_CUBIC)\r\n        photo2 = PIL.ImageTk.PhotoImage(image = PIL.Image.fromarray(photo))\r\n        photoData = [photo2, pic*36 + data.scrollX, photo]\r\n        swatches += [photoData]\r\n    return swatches\r\n\r\ndef loadTopProducts(feature, data):\r\n    dirpath = os.path.dirname(os.path.realpath(__file__)).replace(\"\\\\\", \"/\")\r\n    pathToFeature = dirpath + \"/Top Products/%s\" %(feature)\r\n    files = os.listdir(pathToFeature)\r\n    images = []\r\n    for image in files:                                \r\n        path = pathToFeature + \"/\" + image\r\n        images += [path]\r\n    products = []\r\n    for pic in range(len(images)):\r\n        photo = cv2.imread(images[pic])\r\n        photo = cv2.cvtColor(photo, cv2.COLOR_BGR2RGB)\r\n        photo = imutils.resize(photo, width = 100, inter = cv2.INTER_CUBIC)\r\n        photo2 = PIL.ImageTk.PhotoImage(image = PIL.Image.fromarray(photo))\r\n        photoData = [photo2, pic*100 + data.scrollY, photo]\r\n        products += [photoData]\r\n    return products\r\n\r\ndef convertPhoto(data):\r\n    img = cv2.cvtColor(data.image, cv2.COLOR_BGR2RGB)\r\n    img = imutils.resize(img, width = 500, inter = cv2.INTER_CUBIC)\r\n    photo = PIL.ImageTk.PhotoImage(image = PIL.Image.fromarray(img))\r\n    return photo \r\n\r\ndef makeover(data):\r\n    image = cv2.imread(data.imagePath)\r\n    for product in data.products:\r\n        data.objects[product].img = image\r\n    for product in data.products:\r\n        image = data.objects[product].editPhoto()\r\n    return image\r\n\r\ndef drawSwatches(canvas, data):\r\n    for i in range(len(data.swatches)):\r\n        placement = i * 36\r\n        canvas.create_image(placement + data.scrollX, data.height - 36, image = data.swatches[i][0], anchor = tkinter.NW)\r\n\r\ndef drawTopProducts(canvas, data):\r\n    for i in range(len(data.topProducts)):\r\n        placement = i*100\r\n        canvas.create_image(600, placement + data.scrollY, image = data.topProducts[i][0], anchor = tkinter.NW)\r\n\r\ndef init(data): \r\n    print(data.imagePath)\r\n    data.image = cv2.imread(data.imagePath)\r\n    data.photo = convertPhoto(data)\r\n    data.dimX, data.dimY, nc = data.img.shape\r\n    data.products = [\"lips\", \"shadow\", \"liner\", \"brows\", \"contour\", \"highlight\", \"blush\"] \r\n    data.currentFeature = data.products[0]\r\n    data.targets = {\"lips\" : webscrapeTopRated.getTopProducts(\"lips\"), \r\n    \"shadow\" : webscrapeTopRated.getTopProducts(\"shadow\"), \r\n    \"liner\" : webscrapeTopRated.getTopProducts(\"liner\"), \r\n    \"brows\" : webscrapeTopRated.getTopProducts(\"brows\"), \r\n    \"contour\" : webscrapeTopRated.getTopProducts(\"contour\"), \r\n    \"highlight\" : webscrapeTopRated.getTopProducts(\"highlight\"), \r\n    \"blush\" : webscrapeTopRated.getTopProducts(\"blush\")}\r\n    # data.edited = dict()\r\n    data.scrollX = 0\r\n    data.scrollY = 0\r\n    data.swatches = loadSwatches(data)\r\n    data.topProducts = {\"lips\" : loadTopProducts(\"lips\", data), \r\n    \"shadow\" : loadTopProducts(\"shadow\", data), \r\n    \"liner\" : loadTopProducts(\"liner\", data), \r\n    \"brows\" : loadTopProducts(\"brows\", data), \r\n    \"contour\" : loadTopProducts(\"contour\", data), \r\n    \"highlight\" : loadTopProducts(\"highlight\", data), \r\n    \"blush\" : loadTopProducts(\"blush\", data)}\r\n    data.lips = makeup.Lips(data.image, \"lips\", 0.1, None)\r\n    data.shadow = makeup.Shadow(data.image, \"shadow\", 0.1, None)\r\n    data.liner = makeup.Liner(data.image, \"liner\", 0.6, None)\r\n    data.brows = makeup.Brows(data.image, \"brows\", 0.1, None)\r\n    data.contour = makeup.Contour(data.image, \"contour\", 0.1, None)\r\n    data.highlight = makeup.Highlight(data.image, \"highlight\", 0.1, None)\r\n    data.blush = makeup.Blush(data.image, \"blush\", 0.1, None)\r\n    data.objects = {\"lips\" : data.lips, \r\n    \"shadow\" : data.shadow, \r\n    \"liner\" : data.liner, \r\n    \"brows\" : data.brows, \r\n    \"contour\" : data.contour, \r\n    \"highlight\" : data.highlight, \r\n    \"blush\" : data.blush}\r\n    data.popup = False \r\n    data.input = \"\"\r\n    print(\"Done\")\r\n\r\ndef mousePressed(event, data):\r\n    if 0 <= event.x <= 500 and data.height-36 <= event.y <= data.height:\r\n        index = (event.x - data.scrollX)//36\r\n        currentImage = data.swatches[index][2]\r\n        currentColor = currentImage[17, 17]\r\n        (r, g, b) = (int(currentColor[0]), int(currentColor[1]), int(currentColor[2]))\r\n        data.objects[data.currentFeature].color = (b, g, r)\r\n    for j in range(0, data.height, data.height//len(data.products)):\r\n        if 500 <= event.x <= 600:\r\n            if j <= event.y <= j + data.height//len(data.products):\r\n                data.currentFeature = data.products[j//(data.height//len(data.products))]\r\n                data.scrollX = 0\r\n                data.scrollY = 0\r\n                event.x = 0\r\n                event.y = 0\r\n                print(data.currentFeature)\r\n    if 15 <= event.x <= 105 and data.height - 165 <= event.y <= data.height - 120:\r\n        if data.objects[data.currentFeature].alphaMax >= data.objects[data.currentFeature].alpha:\r\n            data.objects[data.currentFeature].alpha += 0.025\r\n            print(data.objects[data.currentFeature].alpha)    \r\n    if 15 <= event.x <= 105 and data.height - 105 <= event.y <= data.height - 60:\r\n        if data.objects[data.currentFeature].alphaMin < data.objects[data.currentFeature].alpha:\r\n            data.objects[data.currentFeature].alpha -= 0.025\r\n            print(data.objects[data.currentFeature].alpha)\r\n    if 600 < event.x <= data.width:\r\n        index = ((event.y - data.scrollY) - 50)//100\r\n        link = \"http://sephora.com\" + data.targets[data.currentFeature][index]\r\n        webbrowser.open(link)\r\n    if 380 <= event.x <= 470 and data.height - 105 <= event.y <= data.height - 60:\r\n        data.popup = True\r\n    if 310 <= event.x <= 360 and 200 <= event.y <= 250:\r\n        path = os.path.dirname(data.imagePath) + \"finalImage.jpg\"\r\n        cv2.imwrite(path, data.image)\r\n        try:\r\n            twilioSMS.getPhotoSendMessage(path, int(data.input))\r\n            data.input = \"\"\r\n            data.popup = False\r\n        except:\r\n            data.popup = False\r\n    data.image = makeover(data)\r\n    data.photo = convertPhoto(data)\r\n    data.swatches = loadSwatches(data)\r\n\r\n\r\ndef keyPressed(event, data):\r\n    if event.keysym == \"Left\":\r\n        data.scrollX += 10\r\n        print(data.scrollX)\r\n    if event.keysym == \"Right\":\r\n        data.scrollX -= 10\r\n        print(data.scrollX)\r\n    if event.keysym == \"Up\":\r\n        data.scrollY += 10\r\n        print(data.scrollY)\r\n    if event.keysym == \"Down\":\r\n        data.scrollY -= 10\r\n        print(data.scrollY)\r\n    if data.popup == True and str(event.char) in \"1234567890\":\r\n        data.input = str(data.input) \r\n        data.input += str(event.char)\r\n        data.input = int(data.input)\r\n    if data.popup == True and event.keysym == \"BackSpace\":\r\n        data.input = str(data.input) \r\n        if len(data.input) > 0:\r\n            data.input = data.input[0:-1]\r\n            data.input = int(data.input)\r\n        else: \r\n            data.input = \"\"\r\n\r\n\r\ndef redrawAll(canvas, data):\r\n    canvas.create_image(0, 0, image = data.photo, anchor = tkinter.NW)\r\n    for i in range(len(data.swatches)):\r\n        placement = i * 36\r\n        canvas.create_image(placement + data.scrollX, data.height - 36, image = data.swatches[i][0], anchor = tkinter.NW)\r\n    canvas.create_rectangle(600, 0, data.width, data.height, fill = \"white\", width = 0)\r\n    for i in range(len(data.topProducts[data.currentFeature])):\r\n        placement = i*100 + 50\r\n        canvas.create_image(600, placement + data.scrollY, image = data.topProducts[data.currentFeature][i][0], anchor = tkinter.NW)\r\n    for j in range(0, data.height, round(data.height/len(data.products))):\r\n        index = j//(data.height//len(data.products))\r\n        canvas.create_rectangle(500, j, 600, j+100, fill = \"white\", width = 10) \r\n        canvas.create_text(550, j + 50, text = \"%s\" %(data.products[index]), fill = \"Black\", font = \"Arial 15\")\r\n    canvas.create_rectangle(605, 0, data.width, 50, fill = \"white\", width = 0)\r\n    canvas.create_text(612, 0, text = \"Bestsellers\", fill = \"Black\", font = \"Arial 15\", anchor = tkinter.NW)\r\n    canvas.create_text(610, 20, text = \"on Sephora\", fill = \"Black\", font = \"Arial 15\", anchor = tkinter.NW)\r\n    canvas.create_rectangle(15, data.height - 165, 105, data.height - 120, fill = \"white\", width = 5)\r\n    canvas.create_text(60, data.height - 143, text = \"%s\" %(\"sharpen\"), fill = \"Black\", font = \"Arial 15\")\r\n    canvas.create_rectangle(15, data.height - 105, 105, data.height - 60, fill = \"white\", width = 5)\r\n    canvas.create_text(60, data.height - 83, text = \"%s\" %(\"blend\"), fill = \"Black\", font = \"Arial 15\")\r\n    canvas.create_rectangle(380, data.height - 105, 470, data.height - 60, fill = \"white\", width = 5)\r\n    canvas.create_text(425, data.height - 83, text = \"%s\" %(\"text me!\"), fill = \"Black\", font = \"Arial 15\")\r\n    if data.popup == True:\r\n        canvas.create_rectangle(200, 200, 300, 250, fill = \"white\", width = 5)\r\n        canvas.create_text(250, 225, text = \"%s\" %(str(data.input)), fill = \"Black\", font = \"Arial 10\")\r\n        canvas.create_rectangle(310, 200, 360, 250, fill = \"white\", width = 5)\r\n        canvas.create_text(335, 225, text = \"Done!\", fill = \"Black\")\r\n\r\n    # for i in range(len(data.swatches)):\r\n    #     xPlacement = i*36 + 18\r\n    #     canvas.create_image(xPlacement, data.height - 18, \\\r\n    #         image = \"C:/Users/sanam/Documents/CMU/Year 1/Semester 2/15-112 Fundamentals of Programming/Term Project/Colors/%s\" \\\r\n    #         %(data.swatches[i]))\r\n    \r\n\r\ndef run(width=300, height=300):\r\n    def redrawAllWrapper(canvas, data):\r\n        canvas.delete(ALL)\r\n        canvas.create_rectangle(0, 0, data.width, data.height,\r\n                                fill='white', width=0)\r\n        redrawAll(canvas, data)\r\n        canvas.update()    \r\n\r\n    def mousePressedWrapper(event, canvas, data):\r\n        mousePressed(event, data)\r\n        redrawAllWrapper(canvas, data)\r\n\r\n    def keyPressedWrapper(event, canvas, data):\r\n        keyPressed(event, data)\r\n        redrawAllWrapper(canvas, data)\r\n\r\n    # Set up data and call init\r\n    class Struct(object): pass\r\n    data = Struct()\r\n    data.width = width\r\n    data.height = height\r\n    root = tkinter.Tk()\r\n\r\n    data.imagePath = askopenfilename()\r\n    data.img = cv2.imread(data.imagePath)\r\n    data.img = cv2.cvtColor(data.img, cv2.COLOR_BGR2RGB)\r\n    data.img = imutils.resize(data.img, width = 500, inter = cv2.INTER_CUBIC)\r\n\r\n    init(data)\r\n    # create the root and the canvas\r\n    canvas = Canvas(root, width=data.width, height=data.height)\r\n    canvas.pack()\r\n\r\n    data.photo = PIL.ImageTk.PhotoImage(image = PIL.Image.fromarray(data.img))\r\n    canvas.create_image(0, 0, image = data.photo, anchor = tkinter.NW)\r\n\r\n    # set up events\r\n    root.bind(\"\", lambda event:\r\n                            mousePressedWrapper(event, canvas, data))\r\n    root.bind(\"\", lambda event:\r\n                            keyPressedWrapper(event, canvas, data))\r\n    redrawAll(canvas, data)\r\n    print(\"Done\")\r\n    # and launch the app\r\n    root.mainloop()  # blocks until window is closed\r\n\r\nrun(710, 600) ", "115": "import requests\r\nfrom bs4 import BeautifulSoup\r\nimport time\r\n\r\ndef webscrape():\r\n    with open(\"web_scrape.txt\", 'w')as wc:\r\n        for i in range(0,50):\r\n            try:\r\n                resp = requests.get('http://3.95.249.159:8000/random_company').text\r\n            except:\r\n                print('cURL failed')\r\n            else:\r\n                soup = BeautifulSoup(resp, 'html.parser')\r\n                for li in soup.find_all('li'):\r\n                    if(\"Name: \" in li.text):\r\n                        wc.write(str(li.text))\r\n                        wc.write('\\n')\r\n                    if(\"Purpose: \" in li.text):\r\n                        wc.write(str(li.text))\r\n                        wc.write('\\n\\n')\r\n            time.sleep(0.1)\r\n\r\nif __name__ == \"__main__\":\r\n    webscrape()\r\n", "116": "from tableauscraper import TableauScraper as TS\nfrom fuzzywuzzy import process, fuzz\nimport pandas as pd\nimport os\nimport json\n\n# url = \"https://public.tableau.com/views/COVID-19CasesandDeathsinthePhilippines_15866705872710/Cases?%3AshowVizHome=no\"\n\n# ts = TS()\n# ts.loads(url)\n\n# ws = (ts.getWorksheet(\"C_Table\")).data\n\n# Data Cleaning\n# ws = ws.drop(ws[ws['ProvinceCity Clean-value'] == \"%null%\"].index)\n# clean_columns = ws[[\"ProvinceCity Clean-value\", \"RegionRes Clean-value\", \"Measure Names-alias\", \"Measure Values-alias\"]]\n# clean_columns.columns = ['Municipality', 'Region', 'Measures', 'Value']\n\n# # Pivot Table Contents\n# final_df = clean_columns.pivot_table(index=['Region', 'Municipality'],\n#                           columns=['Measures'],\n#                           values='Value',\n#                           aggfunc=lambda x: ' '.join(x))\n\n# Export to CSV\n# final_df.to_csv('covidData.csv', index=True)\n\n\n# Adding COVID-19 data to geoJson files\ndef getMunicipalityList():\n    # Import from existing CSV\n    filepath = os.getcwd() + \"\\\\backend\\\\webscrape\"\n    df = pd.read_csv(filepath + \"\\\\covidData.csv\")\n    fmuni = df['Municipality'].tolist()\n   \n    fullpath = os.getcwd() + \"\\\\geojson\"\n    dir_list = os.listdir(fullpath)\n    \n    count = 0\n    for file in dir_list:\n        file = (f\"{fullpath}\\\\{file}\")\n        print(file)\n        with open(file, \"r\") as jsonFile:\n            data = json.load(jsonFile)\n            for i in range(0, len(data['features'])):\n                pr = str()\n                if \"ph18-\" in file or \"ph0\" in file:\n                    pr = data['features'][i]['properties']['name'].upper() \n                else:\n                    pr = data['features'][i]['properties']['province'].upper()\n                \n                match = process.extractOne(pr, fmuni, scorer=fuzz.token_sort_ratio)\n                cases = df.loc[df['Municipality'] == match[0], 'Active Cases'].iloc[0]\n\n                data['features'][i]['properties']['cases'] = cases \n\n                print(data['features'][i]['properties']['cases'])\n                \n        with open(file, \"w\") as jsonFile:\n            json.dump(data, jsonFile)\n\n\ngetMunicipalityList()\n\n# Cleaning geoJson files\n# def cleanJson():\n### For initial cleaning and formatting of geojson files only\n# Clean files #1-17\n    # for i in range(1, 18):\n    #     filename = f\"backend/webscrape/geojson/provinces-region-ph{i}.json\"\n    #     keys = [\"Shape_Leng\", \"Shape_Area\", \"ADM2_PCODE\", \"ADM2_REF\", \"ADM2ALT2EN\",\n    #             \"ADM1_PCODE\", \"ADM0_EN\", \"ADM2ALT1EN\", \"ADM0_PCODE\", \"date\", \"validOn\"]\n\n    #     with open(filename, \"r\") as jsonFile:\n    #         data = json.load(jsonFile)\n\n    #         for i in range(0, len(data['features'])):\n    #             for key in keys:\n    #                 data['features'][i]['properties'].pop(key)\n    #             data['features'][i]['properties']['province'] = data['features'][i]['properties'].pop(\n    #                 \"ADM2_EN\")\n    #             data['features'][i]['properties']['region'] = data['features'][i]['properties'].pop(\n    #                 \"ADM1_EN\")\n    #             data['features'][i]['properties']['province'] = data['features'][i]['properties']['province'].upper()\n    #             # print(data['features'][i]['properties'])\n    #             # geo_muni.append(data['features'][i]['properties']['province'])\n\n    #     with open(filename, \"w\") as jsonFile:\n    #         json.dump(data, jsonFile)\n\n# Clean file #18\n    # for j in range(1, 5):\n    #     filename = f\"backend/webscrape/geojson/provinces-region-ph18-{j}.json\"\n    #     if j != 1:\n    #         keys = [\"Shape_Leng\", \"Shape_Area\", \"ADM3_PCODE\", \"ADM3_REF\", \"ADM3ALT1EN\",\n    #                 \"ADM3ALT2EN\", \"ADM2_PCODE\", \"ADM1_PCODE\", \"ADM0_EN\", \"ADM0_PCODE\", \"date\", \"validOn\"]\n    #     else:\n    #         # filename = f\"backend/webscrape/geojson/provinces-region-ph18.json\"\n    #         keys = [\"Shape_Leng\", \"Shape_Area\", \"ADM2_PCODE\", \"ADM2_REF\", \"ADM2ALT2EN\",\n    #                 \"ADM1_PCODE\", \"ADM0_EN\", \"ADM2ALT1EN\", \"ADM0_PCODE\", \"date\", \"validOn\"]\n\n    #     with open(filename, \"r\") as jsonFile:\n    #         data = json.load(jsonFile)\n\n    #         for i in range(0, len(data['features'])):\n    #             for key in keys:\n    #                 data['features'][i]['properties'].pop(key)\n\n    #             data['features'][i]['properties']['province'] = data['features'][i]['properties'].pop(\n    #                 \"ADM2_EN\")\n    #             data['features'][i]['properties']['province'] = data['features'][i]['properties']['province'].upper()\n    #             data['features'][i]['properties']['region'] = data['features'][i]['properties'].pop(\n    #                 \"ADM1_EN\")\n    #             if j != 1:\n    #                 data['features'][i]['properties']['name'] = data['features'][i]['properties'].pop(\n    #                     \"ADM3_EN\")\n    #                 data['features'][i]['properties']['name'] = data['features'][i]['properties']['name'].upper()\n    #                 # geo_muni.append(data['features'][i]['properties']['name'])\n    #             # else:\n    #                 # geo_muni.append(data['features'][i]['properties']['province'])\n\n    #     with open(filename, \"w\") as jsonFile:\n    #         json.dump(data, jsonFile)\n\n# Clean file #19\n    # filename = f\"backend/webscrape/geojson/provinces-region-ph19.json\"\n    # keys = [\"Shape_Leng\", \"Shape_Area\", \"ADM3_PCODE\", \"ADM3_REF\", \"ADM3ALT1EN\",\n    #         \"ADM3ALT2EN\", \"ADM2_PCODE\", \"ADM1_PCODE\", \"ADM0_EN\", \"ADM0_PCODE\", \"date\", \"validOn\"]\n\n    # with open(filename, \"r\") as jsonFile:\n    #         data = json.load(jsonFile)\n\n    #         for i in range(0, len(data['features'])):\n    #             for key in keys:\n    #                 data['features'][i]['properties'].pop(key)\n\n    #             data['features'][i]['properties']['province'] = data['features'][i]['properties'].pop(\n    #                 \"ADM2_EN\")\n    #             data['features'][i]['properties']['province'] = data['features'][i]['properties']['province'].upper()\n    #             data['features'][i]['properties']['region'] = data['features'][i]['properties'].pop(\n    #                 \"ADM1_EN\")\n                \n    #             data['features'][i]['properties']['name'] = data['features'][i]['properties'].pop(\n    #                 \"ADM3_EN\")\n    #             data['features'][i]['properties']['name'] = data['features'][i]['properties']['name'].upper()\n    #                 # geo_muni.append(data['features'][i]['properties']['name'])\n    #             # else:\n    #                 # geo_muni.append(data['features'][i]['properties']['province'])\n\n    # with open(filename, \"w\") as jsonFile:\n    #     json.dump(data, jsonFile)\n\n# cleanJson()", "117": "import appg.webscrape as appg\nimport click\nimport sqlite3\n\n\n@click.command()\n@click.option('--dbname', default='appg_data.db',  help='SQLite database name')\ndef webscrape(dbname):\n    click.echo('Using SQLite3 database: {}'.format(dbname))\n    conn = sqlite3.connect(dbname)\n    appg.scraper(conn=conn, exists='replace')", "118": "from allLinks import allLinks\nfrom webscrapeLinksClean import webscrape\nfrom combining import get_avg, combineData\n\nif __name__ == \"__main__\":\n    # getting all the links from the all EV page\n    allLinks()\n    # scrape the useful info from the links and clean the data\n    webscrape()\n    # put the avg from every merk + handelsbenaming\n    get_avg()\n    # combine RDW and evCompare\n    combineData()\n", "119": "import config\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport urllib.request\nfrom urllib.request import Request, urlopen\nfrom api.advertising import settings\nimport numpy as np\n\n################\n### LOAD VARIABLES ###\n\nclass config:\n  domain, ranks = config.API()\n  ip, port, retry, timeout = config.Server()\n  download = config.download()\n  name, alias, game_client = config.Game()\n\nclass info:\n  name = config.name\n  alias = config.alias\n\n################\n### WEBSCRAPE FOR LEADER BOARDS ###\n  \ndef topPlayers():\n  topplayers = pd.read_html(config.ranks)[0]\n  topplayers.index += 1\n  Lists = topplayers.head(100)\n  Players = Lists[0]\n  Level = Lists[1]\n  Names = Players.values\n  Levels = Level.values\n  return Names, Levels\n\n###################################\n### WEBSCRAPE FOR DOWNLOAD LINK ###\n  \ndef download():\n#  Scrapes website for download\n#  download = config.download\n#  req = Request(download, headers={'User-Agent': 'Mozilla/5.0'})\n#  parser = 'lxml'  \n#  resp = urllib.request.urlopen(req)\n#  soup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n#  for link in soup.find_all('a', href=True):\n#    client = link['href']\n#    if client.endswith('msi'):\n  client = config.download()\n  return client\n\n\n##################################\n### WEBSCRAPE FOR PLAYER COUNT ###\n\ndef players():\n  try:\n    Players = pd.read_html('https://eosource.net/characters.php?ip=game.fallen-evolution.com&port=8078')[0]\n    Players.index += 1\n    Lists = Players.head(100) #Only show x Items\n    People = Lists.Name\n    People = sorted(People)\n    print (People)\n    Names = People.values\n    plist = '\\n '.join(Names) #List of Players\n    pnum = len(Names) #Count of players\n    return plist, pnum\n  except:\n    Table = pd.read_html('http://www.apollo-games.com/SLN/sln.php/onlinelist?server=host:server1.fallen-evolution.com:8078')[0]\n    Table.index += 1\n    List2 = Table.head(100) #Only show x Items\n    People_Names = List2.Name\n    Names = People_Names.values\n    plist = '\\n '.join(Names) #List of Players\n    pnum = len(Names) #Count of players\n    return plist, pnum\n\n####################\n### LINK ADVERTS ###\n\ndef Adverts():\n  advert = settings.Ads()\n  return advert", "120": "\"To use multiprossing pool, this name main guard is needed.\"\n\"I do not understand the reason.\"\nif __name__ == \"__main__\":\n    import dkfinance_modeller.utility.webscrape as webscrape\n\n    f = open(\"data/template_skat_positiv_liste.csv\", \"r\")\n    isiner = []\n    for i, line in enumerate(f):\n        if i == 0:\n            continue\n        isiner.append(line.strip(\"\\n\"))\n    f.close()\n\n    out = open(\"skat_positiv_liste_info.csv\", \"w\")\n    out.write(\"ISIN;Navn;Index;\u00c5OP;Replication;Domicil\\n\")\n    infoer = webscrape.f\u00e5_etf_info(isiner, 4)\n    for info in infoer:\n        if info[\"succes\"]:\n            out.write(\n                f\"{info['isin']};{info['navn']};{info['indeks']};\"\n                f\"{info['\u00e5op']};{info['replication']};{info['domicile']}\\n\"\n            )\n        else:\n            print(info)\n    out.close()\n    f = open(\"data/template_nordnet_liste.csv\", \"r\")\n    isiner = []\n    skat = {}\n    for i, line in enumerate(f):\n        if i == 0:\n            continue\n        isiner.append(line.strip(\"\\n\").split(\";\")[1])\n        skat[line.strip(\"\\n\").split(\";\")[1]] = line.strip(\"\\n\").split(\";\")[0]\n    f.close()\n    out = open(\"nordnet_liste_info.csv\", \"w\")\n    out.write(\"ISIN;Navn;Index;\u00c5OP;Replication;Domicil;Beskatning\\n\")\n    infoer = webscrape.f\u00e5_etf_info(isiner, 4)\n    for info in infoer:\n        if info[\"succes\"]:\n            out.write(\n                f\"{info['isin']};{info['navn']};{info['indeks']};\"\n                f\"{info['\u00e5op']};{info['replication']};{info['domicile']};{skat[str(info['isin'])]}\\n\"\n            )\n        else:\n            print(info)\n    out.close()\n", "121": "from time import perf_counter\n\nfrom src import airport_dict\nfrom src.file_manager import input, output_excel, add_prices\nfrom src.scraper.azul import azul\nfrom src.scraper.gol import gol\nfrom src.scraper.latam import latam\nfrom src.util import util_get_logger, post_scraping\n\nLOGGER = util_get_logger.get_logger(__name__)\n\n\ndef webscrape_azul(list_airports_dict: list[dict]):\n    azul.get_flights(list_airports_dict.copy(), 1)\n    azul.get_flights(list_airports_dict.copy(), 15)\n    azul.get_flights(list_airports_dict.copy(), 30)\n\n\ndef webscrape_gol(list_airports):\n    flight_list = gol.get_flights(list_airports, 1)\n    add_prices.insert_price(flight_list, 1)\n    flight_list = gol.get_flights(list_airports, 15)\n    add_prices.insert_price(flight_list, 15)\n    flight_list = gol.get_flights(list_airports, 30)\n    output_excel.write_file(flight_list)\n\n\ndef webscrape_latam(list_airports):\n    flight_list = latam.get_flights(list_airports, 1)\n    add_prices.insert_price(flight_list, 1)\n    flight_list = latam.get_flights(list_airports, 15)\n    add_prices.insert_price(flight_list, 15)\n    flight_list = latam.get_flights(list_airports, 30)\n    output_excel.write_file(flight_list)\n\n\ndef main():\n    list_airports = input.get_airport_list()\n    list_airports_dict = airport_dict.get_airport_dict_list(list_airports)\n    webscrape_azul(list_airports_dict)\n    webscrape_gol(list_airports_dict)\n    webscrape_latam(list_airports_dict)\n    post_scraping.create_csv_backup()\n    post_scraping.delete_old_backup()\n    post_scraping.delete_old_log()\n\n\nif __name__ == \"__main__\":\n    start = perf_counter()\n    main()\n    end = perf_counter()\n    LOGGER.info(\"Script ran succesfully.\")\n    LOGGER.info(f\"Elapsed time: {(end - start):.2f} seconds.\")\n", "122": "#### code to pull old 7 digit sourceforge trackers and hopefully replace them (or add)\n## new git trackers\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\n\n################################\n\"\"\"\nThis function gets passed a url, and sends a request to the server at that url\nthen the module 'beautifulsoup' finds all tables in the HTML with a \"class\" of \"ticket-list\"\nthis happens to only be one single table in our case, and there is only one result in the table\nthe result is the number of the git tracker, it is a link to that tracker, so then this \nfunction puts that link into the \"links\" object.  It then returns the text of that link. (not the actual link)\n\"\"\"\ndef webscrape(url):\n    r= requests.get(url)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    \n    tables = soup.find_all(\"table\", {\"class\": \"ticket-list\"})\n    for table in tables:\n        links = table.tbody.a\n        return links.text\n        \n################################\n\"\"\"\nthis creates a list, and iterates through the .obo file, pulls out the 7-digit SF xref,\nand puts them all into a list 'SFtrack'\n\"\"\"\n\n# this specifies which ontology to look for these xrefs (must be .OBO format)\nname = \"/Users/austinmeier/Documents/jaiswal/git/plant-ontology/plant-ontology.obo\"\n\nOBO = open(name)\n\nSFtrack=[]\n\n### Loop to pull out all the OBO_SF_PO numbers, put them in a list \"SFtrack\"\n\nfor line in OBO:\n  \n    if not line.startswith('xref: OBO_SF_PO:'):\n        continue\n    else:\n        #the line contains a new line character '\\n' that gets pulled with the tracker number if not stripped()\n        line=line.strip()\n        SFxref= line.split(':')\n        \n        #the line contains 2 colons, and must be split.(':'), this leaves a list of strings, and we want the 3rd string, AKA [2]\n        SFtrack.append(SFxref[2])\n       \n\"\"\"  \nloop through the previously generated list \"SFtrack\", add the OBO_SF_PO number to the end of the url\nexecute the function webscrape() on that URL that is unique for each SF tracker\nwebscrape returns the git tracker, and this loop creates a dictionary (lookup{}) with SF:git trackers       \n\"\"\" \n\nlookup ={}\n\nfor SF in SFtrack:\n    \n    url = \"https://sourceforge.net/p/obo/plant-ontology-po-term-requests/search/?q=import_id%3A\" +SF\n    git=webscrape(url)\n    \n    lookup.update({SF:git})\n    \n\n\"\"\"\nnow export the dictionary to a CSV\nnote: the search on sourceforge didn't return any results for a few, and lol had to manually add the remainder\n\"\"\"\nwriter = csv.writer(open('obo_SF_trackerV2.csv', 'wb'))\nfor key, value in lookup.items():\n   writer.writerow([key, value])\n\n\n", "123": "from django.apps import AppConfig\n\n\nclass WebscrapeConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'webscrape'\n", "124": "import requests\nfrom bs4 import BeautifulSoup\nfrom pyltp import SentenceSplitter\n\nclass WebScrape(object):\n    def __init__(self, word, url):\n        self.url = url\n        self.word = word\n\n    def web_parse(self):\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n        req = requests.get(url=self.url, headers=headers)\n\n        # \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n        if req.status_code == 200:\n            soup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n            return soup\n        return None\n\n\n    def get_gloss(self):\n        soup = self.web_parse()\n        if soup:\n            lis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n            if lis:\n                for li in lis('li'):\n                    if '", "125": "from flask import Flask, redirect, url_for, request, redirect\nfrom flask_restful import Resource, Api\nfrom webScrape import WebScrape\nimport json\n\napp = Flask(__name__)\napi = Api(app)\n\nclass HelloWorld(Resource):\n    def get(self):\n        webscraper = WebScrape()\n        webscraper.run()\n        with open('../JSONWebServer/internships.json') as f:\n            data = json.load(f)\n        return data\n\napi.add_resource(HelloWorld, '/')\n\nif __name__ == '__main__':\n    app.run(debug=True)", "126": "# https://realpython.com/beautiful-soup-web-scraper-python/\n# https://www.youtube.com/watch?v=m-koIYWCaIo\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport datetime\n\n\nheaders = {'User Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36'}\ndatalist = []\n\ndef webscrape(pages):\n    URL = f'https://www.capitoltrades.com/trades?page={pages}&pageSize=25'\n    # \"https://app.capitoltrades.com/trades?page={pages}&pageSize=100\"\n    r = requests.get(URL, timeout=(2, 20), headers=headers)\n    soup = BeautifulSoup(r.text, 'html.parser')\n\n    print()\n    print(f'Status Code:  {r.status_code}')                                         # Check connection to webpage\n    print(f'Page Title: {soup.title.text}')                                         # Check it is the right webpage by displaying title\n    # trades = soup.find_all('tr', {'class': 'p-selectable-row ng-star-inserted'})    \n    trades = soup.find_all('tr', {'class': 'q-tr'})                                 # Find HTML Element by class name\n    print(len(trades))                                                              # Print number of times ^that^ element is on page \n\n    for item in trades:\n        try:\n            print()\n            fullname = item.find('span', {'class': 'firstName'}).text + \" \" + item.find('span', {'class': 'lastName'}).text\n            print(fullname)\n            party = item.find('span', {'class': 'party'}).text\n            print(party)\n            state = item.find('span', {'class': 'us-state-compact'}).text\n            print(state)\n            date = item.find('span', {'class': 'format--date-iso'}).find('time')['title']\n            print(date)\n        except:\n            pass\n\ndone = False\nwhile done == False:\n    print()\n    print(\"WHAT WOULD YOU LIKE TO DO?\")\n    print('1: Search by keyword (ex: \"Nancy Pelosi\")')\n    print('2: See cumulative trades by party')\n    print('3: Extra option')\n    answer = input('Please enter your action number: ')\n\n    if answer == \"1\":\n        done = True\n        print()\n        search = input(\"What keyword would you like to search? \")\n        length = int(input(\"How many entries of 100 would you like to search?\"))\n        # for x in range(length):\n            # webscrape(search, length)\n    elif answer == \"2\":\n        done = True\n        pass\n    elif answer == \"3\":\n        done = True\n        pass\n    else:\n        print(\"-------------------------------------------\")\n        print(\"Your input was not valid, please try again.\")\n        print(\"-------------------------------------------\")\n        done = False\n\n# check basics\nwebscrape(10)", "127": "from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import TimeoutException\nfrom urllib import robotparser\nimport pickle\nimport webscrape.time\n\nclass huffCrawler(object):\n\n    def __init__(self, starter_url):\n        user_agent = \"Amherst College SURF 2018, contact salfeld2018Amherst.edu with any questions.\"\n        opts = Options()\n        opts.add_argument(f\"user-agent={user_agent}\")\n        self.driver = webdriver.Chrome(chrome_options=opts, executable_path='/Applications/chromedriver')\n        self.base_url = starter_url\n        self.driver.get(starter_url)\n        self.links = []\n\n    def get_robo_link(self, link):\n        if \".com/\" in link:\n            robo_link = link.split('com')[0] + \"com/robots.txt\"\n        elif \".org/\" in link:\n            robo_link = link.split('org')[0] + \"org/robots.txt\"\n        elif \".edu/\" in link:\n            robo_link = link.split('edu')[0] + \"edu/robots.txt\"\n        else:\n            robo_link = \"\"\n        return robo_link\n\n    def check_links(self, links):\n        checked_links = []\n        rp = robotparser.RobotFileParser()\n\n        for l in links:\n            rp.set_url(self.get_robo_link(l))\n            rp.read()\n            if rp.can_fetch(\"*\", l):\n                checked_links.append(l)\n        self.links = self.links + checked_links\n        return checked_links\n\n\n    def get_huff_links(self):\n        links = []\n        cards = self.driver.find_elements_by_class_name(\"card__details\")\n        for card in cards:\n            try:\n                label = card.find_element_by_class_name(\"card__label\").text\n                print(label)\n                if label != \"HUFFPOST PERSONAL\" and label != \"VIDEOS\" and label != \"COMEDY\" and label != \"ENTERTAINMENT\" and label != \"OPINION\" and label != \"STYLE & BEAUTY\":\n                    article = card.find_element_by_class_name(\"card__headline\")\n                    a_tag = article.find_element_by_tag_name(\"a\")\n                    href = str(a_tag.get_attribute(\"href\"))\n                    if \"huffingtonpost\" in href and href not in self.links:\n                        links.append(href)\n            except NoSuchElementException:\n                article = card.find_element_by_class_name(\"card__headline\")\n                a_tag = article.find_element_by_tag_name(\"a\")\n                href = str(a_tag.get_attribute(\"href\"))\n                if \"huffingtonpost\" in href and href not in self.links:\n                    links.append(href)\n        return links\n\n    def get_huff_article(self, link):\n        to_return = [None] * 3\n        to_return[0] = link\n        try:\n            self.driver.get(link)\n        except TimeoutException:\n            return to_return\n\n        #get title\n        try:\n            header = self.driver.find_element_by_class_name(\"headline__title\")\n            to_return[1] = header.text\n            print(\"header \" + header.text)\n        except NoSuchElementException:\n            to_return[1] = ''\n            print(\"WHYYYYYY\")\n\n        #get content\n        try:\n            text = ''\n            body = self.driver.find_element_by_id(\"entry-text\")\n            d_tags = body.find_elements_by_tag_name(\"div\")\n            for div in d_tags:\n                try:\n                    p = div.find_element_by_tag_name(\"p\")\n                    if len(p.text) > 0:\n                        text = text + p.text\n                except NoSuchElementException:\n                     continue\n            to_return[2] = text\n        except NoSuchElementException:\n            print(\"NOOOOO\")\n            to_return[2] = ''\n        return to_return\n\n    def get_huff_data(self, links):\n        data = []\n        count = 0\n        for link in links:\n            print(count)\n            content = self.get_huff_article(link)\n            if len(content[1]) > 0 and len(content[2]) > 0:\n                data.append(content)\n            count += 1\n            webscrape.time.sleep(7)\n        return data\n\n    def one_section(self, url):\n        self.driver.get(url)\n        links = self.collect_links(url)\n        print(len(links))\n\n        checked_links = self.check_links(links)\n        print(len(checked_links))\n\n        data = self.get_huff_data(checked_links)\n        print(len(data))\n        print(data[1])\n        return data\n\n    def collect_links(self,url):\n        links = []\n        one = self.get_huff_links()\n        for a in one:\n            if a not in self.links:\n                links.append(a)\n\n        for i in range(2, 20):\n            webscrape.time.sleep(7)\n            l = url + (f\"?page={i}\")\n            try:\n                self.driver.get(l)\n            except Exception:\n                break\n            list = self.get_huff_links()\n            for link in list:\n                if link not in self.links:\n                    links.append(link)\n        return links\n\n\n    def start(self):\n        one = self.one_section(\"https://www.huffingtonpost.com/section/us-news\")\n        two = self.one_section(\"https://www.huffingtonpost.com/section/world-news\")\n        three = self.one_section(\"https://www.huffingtonpost.com/section/business\")\n        four = self.one_section(\"https://www.huffingtonpost.com/section/politics\")\n        full_data = one + two + three + four\n        print(len(full_data))\n        return full_data\n\n    def get_master_links(self, url):\n        self.driver.get(url)\n        links = self.collect_links(url)\n        checked_links = self.check_links(links)\n        return checked_links\n\n    def pickle_links(self):\n        one = self.get_master_links(\"https://www.huffingtonpost.com/section/us-news\")\n        print(\"length of one is \" + str(len(one)))\n        p_one = open(\"./data/huff_us_links.pkl\", \"wb\")\n        desc1 = \"huff us links\"\n        pickle.dump((one, desc1), p_one)\n        p_one.close()\n\n        webscrape.time.sleep(10)\n        two = self.get_master_links(\"https://www.huffingtonpost.com/section/world-news\")\n        print(\"length of two is \" + str(len(two)))\n        p_two = open(\"./data/huff_world_links.pkl\", \"wb\")\n        desc2 = \"huff world links\"\n        pickle.dump((two, desc2), p_two)\n        p_two.close()\n\n        webscrape.time.sleep(10)\n        three = self.get_master_links(\"https://www.huffingtonpost.com/section/business\")\n        print(\"length of three is \" + str(len(three)))\n        p_three = open(\"./data/huff_buisness_links.pkl\", \"wb\")\n        desc3 = \"huff buisness links\"\n        pickle.dump((three, desc3), p_three)\n        p_three.close()\n\n        webscrape.time.sleep(10)\n        four = self.get_master_links(\"https://www.huffingtonpost.com/section/politics\")\n        print(\"length of four is \" + str(len(four)))\n        p_four = open(\"./data/huff_politics_links.pkl\", \"wb\")\n        desc4 = \"huff politics links\"\n        pickle.dump((four, desc4), p_four)\n        p_four.close()\n\n        full_links = one + two + three + four\n        return full_links\n\n\nif __name__ == '__main__':\n    #collecting the article body and title of each link for a range of links\n    pickle_in = open(\"./data/master_huff_set.pkl\", \"rb\")\n    links, desc = pickle.load(pickle_in)\n\n    indicies = [(700, len(links))]\n\n    for i in range(len(indicies)):\n        start = indicies[i][0]\n        print(type(start))\n        end = indicies[i][1]\n        small_links = links[start: end]\n\n        url = \"https://www.huffingtonpost.com\"\n        huff = huffCrawler(url)\n        data = huff.get_huff_data(small_links)\n        print(\"len of articles is \" + str(len(data)))\n        print(data[3])\n\n        pickle_out = open(f\"./data/huff_articles_{start}-{end}.pkl\", \"wb\")\n        desc = f\"url, title, and content for articles with urls from {start} to {end}\"\n        pickle.dump((data, desc), pickle_out)\n        pickle_out.close()\n\n    #collecting links\n    # url = \"https://www.huffingtonpost.com\"\n    # huff = huffCrawler(url)\n    # links = huff.pickle_links()\n    # print(\"length of list is \" + str(len(links)))\n    #\n    # pickle_out = open('./data/master_huff_links.pkl', 'wb')\n    # desc = \"list of links from huff times\"\n    # pickle.dump((links, desc), pickle_out)\n    # pickle_out.close()\n    #\n    # print(\"pickled\")\n\n\n    command = input(\"Press q to quit\")\n    if command is \"q\":\n        huff.driver.quit()", "128": "#Web Scraper\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\n\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\"debuggerAddress\", \"localhost:8989\")\n\n\nurl = 'https://www.youtube.com/c/NikolajMitrofanov'\ndriver = webdriver.Chrome(chrome_options=options, executable_path=r\"C:/Users/clib_/Desktop/UNIVERSE/PROGRAMMING/PYTHON/GIT-WEBSCRAPE/webscrape_clicker/chromedriver/chromedriver.exe\")\ndriver.get(url)\n\n\nelement = WebDriverWait(driver, 30).until(\n    EC.presence_of_element_located((By.XPATH, '//*[@id=\"items\"]/ytd-grid-video-renderer[5]'))\n)\n\n#using ActionChains instead to click\nwebdriver.ActionChains(driver).move_to_element(element).click(element).perform()\n", "129": "import pandas as pd\r\nfrom bs4 import BeautifulSoup\r\nimport requests\r\nimport time\r\nimport datetime\r\nimport csv\r\nimport smtplib\r\nimport os\r\n\r\n# CREATE FILE FOR PRICE LOG\r\n\r\n# header = ['Item_Name', 'Price', 'Date_added']\r\n# with open('Amazon_webscrape.csv', 'w', newline='', encoding='UTF8') as f:\r\n#     writer = csv.writer(f)\r\n#     writer.writerow(header)\r\n\r\nURL = 'https://www.amazon.com/WDIRARA-Womens-Cartoon-Sleeve-Multicolored/dp/B08SQ5D232/ref=sr_1_19_sspa?crid=JUGEMB8LZ6U&keywords=cow%2Bshirt&qid=1663717496&sprefix=cow%2Bshir%2Caps%2C197&sr=8-19-spons&smid=A34CQ7S73A4TP4&th=1&psc=1'\r\n\r\ndef record_price():\r\n\r\n    headers = {\r\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",\r\n        \"Accept-Encoding\": \"gzip, deflate\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\r\n        \"DNT\": \"1\", \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"\r\n    }\r\n\r\n    page = requests.get(URL, headers=headers)\r\n    soup1 = BeautifulSoup(page.content, \"html.parser\")\r\n    soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\r\n    title = soup2.find(id = 'productTitle').get_text().strip()\r\n    price = float(soup2.find(class_ = 'a-offscreen').get_text().strip()[1:])\r\n    date = datetime.date.today()\r\n    data = [title, price, date]\r\n\r\n    with open('Amazon_webscrape.csv', 'a+', newline='', encoding='UTF8') as f:\r\n        writer = csv.writer(f)\r\n        writer.writerow(data)\r\n\r\n        df = pd.read_csv(r'C:\\Users\\zio_p\\PycharmProjects\\pythonProject\\AtA Web Scraping (Amazon)\\Amazon_webscrape.csv')\r\n        print(df)\r\n        return price\r\n\r\ndef send_mail():\r\n    server = smtplib.SMTP_SSL('smtp.gmail.com',465)\r\n    server.ehlo()\r\n    # server.starttls()\r\n    server.ehlo()\r\n    server.login('francesco.tintori93@gmail.com', os.environ(\"PASSCODE\"))\r\n\r\n    subject = \"Price Drop!\"\r\n    body = f\"The item you have been tracking at {URL} has dropped in price!\"\r\n\r\n    msg = f\"Subject: {subject}\\n\\n{body}\"\r\n\r\n    server.sendmail(\r\n        'francesco.tintori93@gmail.com',\r\n        'francesco.tintori93@gmail.com',\r\n        msg\r\n    )\r\n\r\nwhile (True):\r\n    if record_price() < 20:\r\n        send_mail()\r\n    time.sleep(3600)\r\n", "130": "# -*- mode: python ; coding: utf-8 -*-\n\nblock_cipher = None\n\n\na = Analysis(['NRE_webscrape.py'],\n             pathex=['C:\\\\Users\\\\gwilliams\\\\Documents\\\\GitHub\\\\RME_Rail_Fares\\\\RME_Rail_Fares'],\n             binaries=[],\n             datas=[\n\t\t\t ('C:\\\\Users\\\\gwilliams\\\\Documents\\\\GitHub\\\\RME_Rail_Fares\\\\1_READ_ME_Instructions\\\\Instructions for use.txt','1_READ_ME_Instructions'),\n\t\t\t ('C:\\\\Users\\\\gwilliams\\\\Documents\\\\GitHub\\\\RME_Rail_Fares\\\\2_Route_and_times_metadata\\\\route_and_time_metadata.xlsx','2_Route_and_times_metadata'),\n\t\t\t ('C:\\\\Users\\\\gwilliams\\\\Documents\\\\GitHub\\\\RME_Rail_Fares\\\\3_Data_goes_here\\\\Placeholder for data.txt','3_Data_goes_here'),\n\t\t\t ('C:\\\\Users\\\\gwilliams\\\\Documents\\\\GitHub\\\\RME_Rail_Fares\\\\3_Data_goes_here\\\\appended_data\\\\appended_data_for_intial_run.csv','3_Data_goes_here\\\\appended_data')\n\t\t\t \n\t\t\t ],\n             hiddenimports=[],\n             hookspath=[],\n             runtime_hooks=[],\n             excludes=[],\n             win_no_prefer_redirects=False,\n             win_private_assemblies=False,\n             cipher=block_cipher,\n             noarchive=False)\npyz = PYZ(a.pure, a.zipped_data,\n             cipher=block_cipher)\nexe = EXE(pyz,\n          a.scripts,\n          [],\n          exclude_binaries=True,\n          name='NRE_webscrape',\n          debug=False,\n          bootloader_ignore_signals=False,\n          strip=False,\n          upx=True,\n          console=True )\ncoll = COLLECT(exe,\n               a.binaries,\n               a.zipfiles,\n               a.datas,\n               strip=False,\n               upx=True,\n               upx_exclude=[],\n               name='NRE_webscrape')\n", "131": "# Import libraries\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options as ChromeOptions\nimport time\n\n\ndef webscrape_vienna(path_download_dir, path_chromedriver):\n    \"\"\"\n    Function to download files via webscrapping, using selenium and google's\n    chromedriver, which can be downloaded here: https://chromedriver.chromium.org/\n\n    How to webscrape:\n    0. Download all required packages (incl. selenium and chromedriver)\n    1. Open the url with the download links\n    2. right cklick on the first download link and select 'inspect'\n    3. right click again on the higlighted entry in firefox developer mode and choose 'copy path' -> 'xpath'\n    4. do the same for the last download link\n\n    Args:\n    - url_page (str): address of webpage where files should be downloaded\n    - path_download_dir (str): path to dir in which downloads should be saved\n    - path_driver (str): path to folder in which geckodriver is stored\n    - path_xpath1(str): xpath of file where download should start with\n    - path_xpath2 (str): xpath of file where download should stop (all download links in between will be downloaded)\n    - sleep_time (int): on some pages it takes some time to load all download links; here, a timer can be set to wait\n\n    Returns:\n    -\n\n    Last update: 21/06/21. By Felix.\n\n    \"\"\"\n    print('Starting to download')\n\n    # Set options for webdriver\n    options = ChromeOptions()\n    options.add_argument(\"--headless\")\n    prefs = {\"profile.default_content_settings.popups\": 0,\n             \"download.default_directory\": path_download_dir,\n             \"directory_upgrade\": True}\n    options.add_experimental_option(\"prefs\", prefs)\n\n    # Run webdriver from executable path of your choice\n    driver = webdriver.Chrome(executable_path=path_chromedriver, options=options)\n\n    # -----------------------------------------------------------------------------\n\n    # Get web page\n    # time.sleep(1)\n    i = 0\n    # loop through columns\n    for col in range(78, 136 + 1):\n        if col < 100:\n            str_col = '0' + str(col)\n        else:\n            str_col = str(col)\n        for row in range(62, 107 + 1):\n            # loop through rows\n            if col < 100:\n                str_row = '0' + str(row)\n            else:\n                str_row = str(row)\n            part_a = 'https://www.wien.gv.at/ma41datenviewer/downloads/ma41/geodaten/lod2_gml/'\n            part_b = '_lod2_gml.zip'\n            path = part_a + str_col + str_row + part_b\n            driver.get(path)\n            time.sleep(1)\n        time.sleep(20)\n\n        print('Downloaded col: ', col)\n\n    # Sleep for 5s to ensure that everythings loaded properly\n    time.sleep(200)\n    driver.close()\n\n    # -----------------------------------------------------------------------------\n    print('-------------')\n    print('Download sucessfull!')\n    print('Files downloaded to ' + path_download_dir)\n\n\ndef main():\n\n    # Paths for webscrapper\n    driver_dir = \"/Users/Felix Wagner/Documents/0_program/0_webscraper/chromedriver\"\n    # Path of output dir\n    download_dir = \"/Drive/Downloads/Vienna\"\n\n    # Start to webscrape\n    webscrape_vienna(download_dir, driver_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "132": "from selenium.webdriver import Chrome\nfrom os import path, system\nfrom webScrape import WebScrape\nfrom time import sleep\ndir = path.dirname(__file__)\nchrome_path = path.join(dir, 'selenium','webdriver','chromedriver.exe')\nwebS = WebScrape()\nurl = 'https://bongo.cat/'\nwith Chrome(chrome_path,options=webS.optionsChrome(True)) as driver:\n    driver.get(url)\n    elmnt = driver.find_element_by_tag_name('html')\n    while True:\n        sleep(0.5)\n        elmnt.send_keys('4')\n        sleep(0.5)\n        elmnt.send_keys('6')\n        sleep(0.5)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.8)\n        elmnt.send_keys('4')\n        sleep(0.5)\n        elmnt.send_keys('6')\n        sleep(0.5)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.8)\n        elmnt.send_keys('4')\n        sleep(0.5)\n        elmnt.send_keys('6')\n        sleep(0.5)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.8)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('8')\n        sleep(0.8)\n    system('pause')", "133": "from selenium.webdriver import Chrome\nfrom os import path, system\nfrom webScrape import WebScrape\nfrom time import sleep\ndir = path.dirname(__file__)\nchrome_path = path.join(dir, 'selenium','webdriver','chromedriver.exe')\nwebS = WebScrape()\nurl = 'https://bongo.cat/'\nwith Chrome(chrome_path,options=webS.optionsChrome(True)) as driver:\n    driver.get(url)\n    elmnt = driver.find_element_by_tag_name('html')\n    while True:\n        sleep(0.5)\n        elmnt.send_keys('4')\n        sleep(0.5)\n        elmnt.send_keys('6')\n        sleep(0.5)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.8)\n        elmnt.send_keys('4')\n        sleep(0.5)\n        elmnt.send_keys('6')\n        sleep(0.5)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.8)\n        elmnt.send_keys('4')\n        sleep(0.5)\n        elmnt.send_keys('6')\n        sleep(0.5)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.8)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('9')\n        sleep(0.4)\n        elmnt.send_keys('8')\n        sleep(0.8)\n    system('pause')", "134": "from ast import And\r\nimport json\r\nimport gc\r\nfrom config import websiteLink2020\r\nfrom config import websiteLink2021\r\nfrom config import websiteLink2022\r\nfrom config import websiteLink2023\r\nfrom config import websiteLink2024\r\nfrom config import apiLink\r\nfrom datetime import date, datetime\r\nfrom bs4 import BeautifulSoup\r\nimport requests\r\nfrom dataclasses import dataclass\r\nimport csv\r\nfrom flask import Flask, render_template, request\r\n\r\nclass Holiday:\r\n    def __init__(self, name, date):\r\n        self.name = name\r\n        self.date = date\r\n    def __str__ (self):\r\n        return self.name + ' ('+ self.date +')'\r\n\r\nclass HolidayEncoder(json.JSONEncoder):\r\n    def default(self, o):\r\n        return o.__dict__\r\n    \r\n\r\nclass HolidayList:\r\n   def __init__(self):\r\n       self.holidayList = []\r\n\r\ndef getHTML(url):\r\n    response = requests.get(url)\r\n    return response.text\r\n\r\n    \r\ndef initialJSON():    \r\n    #opens json file \r\n    f = open('holidays.json')\r\n    # returns JSON object as \r\n    # a dictionary\r\n    data = json.loads(f.read())\r\n  \r\n    # Iterating through the json\r\n    # list\r\n    global holidayList\r\n    holidayList = []\r\n    for i in data['holidays']:\r\n        holidays = Holiday(i['name'], i['date'])\r\n        holidayList.append(holidays)\r\n\r\n    # Closing file\r\n    f.close()\r\n\r\ndef fromJSON():    \r\n    #opens json file \r\n    f = open('holidayFiles.json')\r\n    # returns JSON object as \r\n    # a dictionary\r\n    data = json.loads(f.read())\r\n  \r\n    # Iterating through the json\r\n    # list\r\n    global holidayList\r\n    holidayList = []\r\n    for i in data:\r\n        holidays = Holiday(i['name'], i['date'])\r\n        holidayList.append(holidays)\r\n\r\n    # Closing file\r\n    f.close()\r\n\r\ndef writeJSON():\r\n    #open('holidayFiles.json', 'w').close()\r\n    with open('holidayFiles.json', 'w', encoding='utf-8') as f:\r\n        json.dump(holidayList, f, ensure_ascii=False, indent=4, cls=HolidayEncoder)\r\n\r\ndef webScrape(link, year):\r\n\r\n    url = getHTML(link)\r\n    soup = BeautifulSoup(url, 'html.parser')\r\n\r\n    out = [[td.text.strip() for td in tr.select('th, td')] for tr in soup.select('tr[data-mask]')]\r\n\r\n    with open('file.csv', 'w', newline='') as f_out:\r\n        writer=csv.writer(f_out)\r\n        writer.writerows(out)\r\n\r\n    with open('file.csv', 'r') as file:\r\n        reader = csv.reader(file)\r\n        for row in reader:\r\n            csvDate = year + ' ' + row[0]\r\n            date = str(datetime.strptime(csvDate , '%Y %b %d').date())\r\n            holidayCSV = Holiday(row[2], date)\r\n            holidayList.append(holidayCSV)\r\n        \r\n   \r\n        \r\ndef removeDuplicates():\r\n    global holidayList\r\n    holidayList = [*set(holidayList)]\r\n\r\ndef findHoliday(HolidayName, Date):\r\n    sameNameList = [x for x in holidayList if x.name == HolidayName]\r\n    sameBothList = [y for y in sameNameList if y.date == Date]\r\n    i = 0\r\n    while i != len(sameBothList):\r\n        return sameBothList[i]\r\n        # Find Holiday in holidayList\r\n        # Return Holiday\r\n\r\ndef addHoliday():\r\n    print(\"Add a Holiday\")\r\n    holidayInput = input(\"Holiday:\")\r\n    dateInput = input(\"Date (YYYY-MM-DD):\")\r\n    try:\r\n        dateTest = datetime.strptime(dateInput, '%Y-%m-%d')\r\n    except ValueError:\r\n        print(\"Incorrect Date Format Try Again\")\r\n        return addHoliday()\r\n    holidayUser = Holiday(holidayInput, dateInput)\r\n    holidayList.append(holidayUser)\r\n    print(\"Added \" + holidayInput)\r\n\r\ndef removeHoliday():\r\n    print(\"Remove a Holiday\")\r\n    holidayInput = input(\"Holiday:\")\r\n    dateInput = input(\"Date (YYYY-MM-DD):\")\r\n    try:\r\n        dateTest = datetime.strptime(dateInput, '%Y-%m-%d')\r\n    except ValueError:\r\n        print(\"Incorrect Date Format Try Again\")\r\n        return removeHoliday()\r\n    try:\r\n        holidayList.remove(findHoliday(holidayInput, dateInput))\r\n    except ValueError:\r\n        print(\"Incorrect Holiday Name\")\r\n        return removeHoliday()\r\n    print(holidayInput + \" Removed\")\r\n\r\ndef viewHoliday(yearUser, weekNumUser):\r\n    if weekNumUser == '':\r\n        todayDate = date.today()\r\n        weekNumUser = str(todayDate.isocalendar().week)\r\n        print(weekNumUser)\r\n    \r\n    viewHolidayList = []\r\n    for x in holidayList:\r\n        if yearUser == str(datetime.strptime(x.date, '%Y-%m-%d').date().isocalendar().year):\r\n            if weekNumUser == str(datetime.strptime(x.date, '%Y-%m-%d').date().isocalendar().week):\r\n                viewHolidayList.append(x.name + ' (' + x.date + ')')\r\n    print(*viewHolidayList, sep = \"\\n\")\r\n    return viewHolidayList      \r\n\r\n\r\n \r\n    \r\n\r\ninitialJSON()\r\nwriteJSON()\r\n\r\nwebScrape(websiteLink2020, str(2020))\r\nwebScrape(websiteLink2021, str(2021))\r\nwebScrape(websiteLink2022, str(2022))\r\nwebScrape(websiteLink2023, str(2023))\r\nwebScrape(websiteLink2024, str(2024))\r\nwriteJSON()\r\nremoveDuplicates()\r\n\r\nexitCount = 0\r\nsaveCount = 0\r\nwhile exitCount == 0:\r\n    print(\"Holiday Management\")\r\n    print(\"There are \" + str(len(holidayList)) + \" holidays stored in the system\")\r\n    print(\"--------------------\")\r\n    print(\"Holiday Menu\")\r\n    print(\"1. Add a Holiday\")\r\n    print(\"2. Remove a Holiday\")\r\n    print(\"3. View Holidays\")\r\n    print(\"4. Save Changes\")\r\n    print(\"5. Exit\")\r\n    selection = input(\"Enter you choice:\")\r\n    if selection == '1':\r\n        addHoliday()\r\n    if selection == '2':\r\n        removeHoliday()\r\n    if selection == '3':\r\n        print(\"View Holidays\")\r\n        yearUser = input(\"Which Year:\")\r\n        weekNumUser = input(\"Which Week? (#1-52, leave blank for current week):\")\r\n        viewHoliday(yearUser, weekNumUser)\r\n    if selection == '4':\r\n        saveChoice = input('Are you sure you want to save your changes? y or n')\r\n        saveCount = 1\r\n        if saveChoice == 'y':\r\n            print(\"Changes Saved\")\r\n            saveCount = 1\r\n            open('holidayFiles.json', 'w').close()\r\n            writeJSON()\r\n        if saveChoice == 'm':\r\n            print(\"Changes Not Saved\")\r\n            \r\n    if selection == '5':\r\n        if saveCount == 1:\r\n            exitChoice = input(\"Are you sure you want to exit? y or n\")\r\n            if exitChoice == 'y':\r\n                    exitCount = 1\r\n                    print(\"Goodbye\")\r\n        else:\r\n            exitChoice = input(\"Changes are unsaved and will be lost. Want to exit?  y or n\")\r\n            if exitChoice == 'y':\r\n                exitCount = 1\r\n                print(\"Goodbye\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n \r\n   \r\n    \r\n\r\n", "135": "\"\"\"\nNV module for scraping and processing text from https://leg.state.nv.us\n\n# Status, as of January 1, 2022\n\nCurrent Coverage (Stable):\n    [X] Committee Hearings (PDF Transcript Links) (2011 - 2021)\n\nPlanned Coverage:\n    [ ] Floor Speeches\n\n# NV Work Flow\n\nStateLegiscraper has two classes for each state module: Scrape and Process\n\n    Scrape includes 1 function that scrapes PDF transcripts present at a list of weblinks\n    This class provides user with raw data saved on a local or mounted drive\n\n    Process includes 2 functions that processes raw data (PDF Transcripts)\n    generated by Scrape class functions.\n    This class provides useres with Python objects ready to work with popular\n    NLP packages (e.g., nltk, SpaCy)\n\nCLASS Scrape\n\n    - nv_scrape_pdf\n\nCLASS Process\n\n    - nv_pdf_to_text\n    - nv_text_clean\n\n\"\"\"\n\nimport json\nimport os\nimport re\nimport time\nimport urllib.request\n\n#import string\n#from urllib.parse import urljoin\n\nimport pdfplumber\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\n\n\nclass Scrape:\n    \"\"\"\n    Scrape functions for Nevada State Legislature website\n    \"\"\"\n\n    def nv_scrape_pdf(webscrape_links, dir_chrome_webdriver, dir_save):\n        \"\"\"\n        Webscrape function for Nevada State Legislature Website.\n\n        Parameters\n        ----------\n        webscrape_links : List\n            List of direct link(s) to NV committee webpage.\n            see assets/weblinks/nv_weblinks.py for lists organized by chamber and committee\n        dir_chrome_webdriver : String\n            Local directory that contains the appropriate Chrome Webdriver.\n        dir_save : String\n            Local directory to save PDFs.\n\n        Returns\n        -------\n        All PDF files found on the webscrape_links, saved on local dir_save.\n\n        \"\"\"\n\n        if not isinstance(webscrape_links, list):\n            raise ValueError(\"webscrape_links must be a list\")\n        else:\n            pass\n\n        if not os.path.exists(dir_chrome_webdriver):\n            raise ValueError(\"Chrome Webdriver not found\")\n        else:\n            pass\n\n        if not os.path.exists(dir_save):\n            raise ValueError(\"Save directory not found\")\n        else:\n            pass\n\n        for link_index in range(len(webscrape_links)):\n\n            service = Service(dir_chrome_webdriver)\n            options = webdriver.ChromeOptions()\n            # Chrome runs headless, \n            # comment out \"options.add_argument('headless')\"\n            # to see the action\n            options.add_argument('headless')\n            driver = webdriver.Chrome(service=service, options=options)\n\n            time.sleep(5)\n            driver.get(webscrape_links[link_index])\n            time.sleep(5)\n\n            arrow01 = driver.find_element_by_id('divCommitteePageMeetings')\n            arrow01.click()\n            time.sleep(5)\n\n            arrow02 = driver.find_element_by_id('divMeetings')\n            arrow02.click()\n\n            url = driver.page_source\n            regex_pattern = r'https.*Minutes.*\\.pdf'\n            lines = url.split()\n            meeting_regex = re.compile(regex_pattern)\n            all_files = []\n\n            for line in lines:\n                hit = meeting_regex.findall(line)\n                if hit:\n                    all_files.extend(hit)\n\n            for filename in all_files:\n                print(filename)\n\n            folder_location = dir_save\n\n            for link in all_files:\n                filename = os.path.join(\n                    folder_location, \"_\".join(link.split('/')[4:]))\n                urllib.request.urlretrieve(link, filename)\n            time.sleep(15)\n\n            driver.close()\n\n\nclass Process:\n    \"\"\"\n    Process functions for PDF transcripts scraped from Nevada State Legislature website\n    \"\"\"\n\n    def nv_pdf_to_text(dir_load, nv_json_name):\n        \"\"\"\n        Convert all PDFs to a dictionary and then saved locally as a JSON file.\n\n        Parameters\n        ----------\n        dir_load : String\n            Local location of the directory holding PDFs.\n        nv_json_name : String\n            JSON file name, include full local path.\n\n        Returns\n        -------\n        A single JSON file, can be loaded as dictionary to work with.\n\n        \"\"\"\n        directory = dir_load\n        n = 0\n        committee = {}\n\n        file_list = sorted(os.listdir(directory))\n        del file_list[0]\n\n        for file in file_list:\n            filename = directory + file\n            all_text = ''\n            with pdfplumber.open(filename) as pdf:\n                for pdf_page in pdf.pages:\n                    single_page_text = pdf_page.extract_text()\n                    all_text = all_text + '\\n' + single_page_text\n                    committee[n] = all_text\n            n = n + 1\n\n        with open(nv_json_name, 'w') as file:\n            json.dump(committee, file, ensure_ascii=False)\n\n    def nv_text_clean(nv_json_path, trim=None):\n        \"\"\"\n        Loads JSON into environment as dictionary\n        Preprocesses the raw PDF export from previously generated json\n        Optional: Trims transcript to exclude list of those present\n        and signature page/list of exhibits\n\n        Parameters\n        ----------\n        nv_json_path : String\n            Local path of nv_json generated by nv_pdf_to_text.\n        trim: True/Default(None)\n            Provides option to trim transcript to spoken section and transcriber notes\n\n        Returns\n        -------\n        Cleaned dictionary that excludes PDF formatting and (optional) front and back end\n\n        \"\"\"\n\n        file_path = open(nv_json_path,)\n        data = json.load(file_path)\n\n        if trim:\n            for key in data:\n                if isinstance(data[key], str):\n                    transcript = data[key]\n                    start_location = re.search(\n                        r\"(CHAIR.*[A-z]\\:|Chair.*[A-z]\\:)\", transcript).start()\n                    # Starts transcript from when Chair first speaks\n                    transcript = transcript[start_location:]\n                    # Removes signature page after submission (RESPECTFULLY\n                    # SUBMITTED)\n                    end_location = re.search(\n                        r\"(Respectfully\\sSUBMITTED\\:|RESPECTFULLY\\sSUBMITTED\\:|RESPECTFULLY\\sSUBMITTED)\",\n                        transcript)\n                    if end_location:\n                        end_location_num = end_location.start()\n                        transcript = transcript[:end_location_num]\n                    transcript = re.sub(\n                        r\"Page\\s[0-9]{1,}\", \"\", transcript)  # Removes page number\n                    transcript = re.sub(r\"\\n\", \"\", transcript)\n                    transcript = transcript.strip()\n                    transcript = \" \".join(transcript.split())\n                    data[key] = transcript\n                else:\n                    print(\"Incompatible File\")\n\n            return data\n\n        else:\n            for key in data:\n                if isinstance(data[key], str):\n                    transcript = data[key]\n                    transcript = re.sub(r\"Page\\s[0-9]{1,}\", \"\", transcript)\n                    transcript = re.sub(r\"\\n\", \"\", transcript)\n                    transcript = transcript.strip()\n                    transcript = \" \".join(transcript.split())\n                    data[key] = transcript\n                else:\n                    print(\"Incompatible File\")\n\n            return data\n", "136": "from data_extraction.webscrape import webscrape\nfrom topic_relationships.keywords import get_keywords\n\n# this program runs the keyword finder script on the scraped pdfs\n\ndef build_mapper():\n    mapper = {}\n    txtfiles, headers = webscrape(\"http://www.eecs70.org/\")\n    for i in range(len(txtfiles)):\n        with open(txtfiles[i], encoding=\"utf8\") as f:\n            text = f.read().replace('\\n', '')\n            keywords = get_keywords(text)\n            mapper[headers[i]] = keywords\n    return mapper\n\nglobal mapper = build_mapper()\n\n\nif __name__ == \"__main__\":\n    print(build_mapper())\n", "137": "#Import dependencies\nimport os\nimport pandas as pd\nimport numpy as np\nimport sqlalchemy\nfrom sqlalchemy.ext.automap import automap_base\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import create_engine\nfrom flask import Flask, jsonify, render_template\nimport sqlalchemy\nfrom flask import Flask, render_template, redirect\nfrom flask_pymongo import PyMongo\nimport renewable_scrape\nimport json\n\n\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\nengine = create_engine(\"sqlite:///project.sqlite\")\n\n\n# reflect an existing database into a new model\nBase = automap_base()\n# reflect the tables\nBase.prepare(engine, reflect=True)\n\n# Save reference to the table\nDataset = Base.classes.dataset\n\n#Create Flask App\napp = Flask(__name__)\n\nSITE_ROOT = os.path.realpath(os.path.dirname(__file__))\njson_url = os.path.join(SITE_ROOT, \"templates\", \"USA.geojson\")\nheatmapdata = json.load(open(json_url))\n\n#Connect to Mongo DB\napp.config[\"MONGO_URI\"] = \"mongodb://localhost:27017/renewables\"\nmongo = PyMongo(app)\n\n@app.route(\"/\") \ndef welcome():\n\n    return render_template(\"index.html\")\n\n#Create route for scrape\n@app.route(\"/scrape\")\ndef scrape():\n    # Run the scrape function\n    renewable_data = renewable_scrape.renewable_scrape()\n\n    # Update the Mongo database using update and upsert=True\n    mongo.db.renewables.replace_one({}, renewable_data, upsert=True)\n    return redirect(\"/webscrape_sunburst\")\n\n#Create route for hydro-electric energy production\n@app.route(\"/hydro\")\ndef hydro():\n\n    return render_template(\"hydro.html\")\n\n#Create route for wind energy production\n@app.route(\"/wind\")\ndef wind():\n\n    return render_template(\"wind.html\")\n\n#Create route for heatmap\n@app.route(\"/heatmap\")\ndef heatmap():\n\n    return render_template(\"heatmap.html\")\n\n#Create route for solar energy production\n@app.route(\"/solar\")\ndef solar():\n\n    return render_template(\"solar.html\")\n\n#Create route for location\n@app.route(\"/location\")\ndef location():\n\n    return render_template(\"location.html\")\n\n#Create route for webscrape and sunburst\n@app.route(\"/webscrape_sunburst\")\ndef webscrape_sunburst():\n\n    #Take one instance from the Mongo DB\n    data = mongo.db.renewables.find_one()\n\n    return render_template(\"Webscrape_sunburst.html\",r_last_refresh=data[\"renewable_refresh\"],renewable_title_0=data[\"renewable_titles\"][0],renewable_link_0=data[\"renewable_links\"][0],renewable_title_1=data[\"renewable_titles\"][1],renewable_link_1=data[\"renewable_links\"][2], renewable_title_2 = data[\"renewable_titles\"][2],renewable_link_2=data[\"renewable_links\"][4],renewable_title_3=data[\"renewable_titles\"][3],renewable_link_3=data[\"renewable_links\"][6])\n\n#Create route for heatmap\n@app.route(\"/api/heatmap\")\ndef heatmapgeojson():\n    return jsonify(data = heatmapdata)\n\n\n@app.route(\"/data\")\ndef data():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"data.html\")\n\n\nif __name__ == '__main__':\n    app.run(debug=True)\n", "138": "from flask import Flask, request, jsonify, render_template ,make_response\nimport numpy as np\nimport requests\nfrom datetime import datetime\nfrom datetime import date\nimport csv\nimport pandas as pd\nimport flask_excel as excel\nimport os\nimport re\nfrom bs4 import BeautifulSoup\ndef flip_scrap(url):\n    user_agent = {'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'}\n    page = requests.get(url,headers=user_agent)\n    soup=BeautifulSoup(page.content,\"lxml\")\n    loc=soup.find_all('div',{'class':\"_3pLy-c row\"})\n    return loc\ndef amazon_scrap(url):\n    user_agent = {'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'}\n    page = requests.get(url,headers=user_agent)\n    soup=BeautifulSoup(page.content,\"lxml\")\n    location=soup.find_all(\"div\", {\"class\":\"sg-col sg-col-4-of-12 sg-col-8-of-16 sg-col-12-of-20\"})\n    return location\ndef download(mobile_name,df):\n    response =make_response(df.to_csv())\n    response.headers[\"Content-Disposition\"] = \"attachment; filename=\"+mobile_name+ '.csv'\n    return response\napp = Flask(__name__)\n@app.route('/')\n\ndef home():\n    return render_template('index1.html')\n@app.route('/price',methods=['GET'])\ndef price():\n    return render_template('price.html')\n    \n@app.route('/single_pred',methods=['GET'])\ndef single_pred():\n    return render_template('single_pred.html')\ndef flipkart_webscrape(mobile_name,scrap_pages):\n    fliploc=[]\n    for j in range(scrap_pages):\n        URL = \"https://www.flipkart.com/search?q=\"+mobile_name+\"&page=\"\n        flip_location = flip_scrap(f'{URL}{str(j)}')\n        while(len(flip_location)==0):\n            flip_location = flip_scrap(f'{URL}{str(j)}')\n        fliploc.append(flip_location)\n    flipkart=[]\n    for flip_product in fliploc:\n        for flip_product_src in flip_product:\n            flip_specification=flip_product_src.find(\"li\",{\"class\":\"rgWa7D\"}).text\n            flip_mobile = flip_product_src.find('div',{\"class\":\"_4rR01T\"}).text\n            flip_mobilename=flip_mobile.split('(')[0]\n            if 'Power Bank ' in  flip_mobilename:\n                continue\n            try:  \n                flip_mobcolor=flip_mobile.split('(')[1].split(',')[0]\n                flip_ram=re.search(r'\\d+',(flip_specification.split('|')[0])).group()\n                flip_storage=re.search(r'\\d+',(flip_specification.split('|')[1])).group()\n            \n            except(IndexError,AttributeError):\n                continue\n            try:\n                flip_price=flip_product_src.find(\"div\", {\"class\":\"_3tbKJL\"}).text.split(\"\u20b9\")[1].strip()\n            except(IndexError,AttributeError):\n                continue\n            fill_flip=[]\n            fill_flip.append(flip_mobilename.upper().strip())\n            fill_flip.append(flip_mobcolor.upper().strip())\n            fill_flip.append(flip_ram+'GB'.strip())\n            fill_flip.append(flip_storage+'GB'.strip())\n            fill_flip.append(int(flip_price.replace(',','').strip()))\n            flipkart.append(fill_flip)\n            df_flipkart = pd.DataFrame(flipkart)\n            df_flipkart.columns = [\"Mobile\",\"Colour\",\"Ram\",\"Storage\",\"Flipkart_Price\"]\n            df_flipkart.drop_duplicates(inplace=True)\n            flipkart_datframe=df_flipkart\n    return flipkart_datframe\n@app.route('/scrap',methods =[\"GET\", \"POST\"])\ndef scrap():\n    if request.method == \"POST\":\n        mobile_name = request.form.get(\"mobile_name\")\n        scrap_pages=request.form.get('scrap_pages')\n        scrap_pages=int(scrap_pages)\n        \n    df_flipkart=flipkart_webscrape(mobile_name,scrap_pages)\n    #print(df_flipkart)\n    #myData = list(flipkart_datframe.values)\n    return render_template('index1.html',column_names=df_flipkart.columns.values, row_data=list(df_flipkart.values.tolist()), zip=zip)\ndef amazon_webscrape(amazon_mobile_name,amazon_scrap_pages):\n    loc=[]\n    for j in range(amazon_scrap_pages):\n        URL = \"https://www.amazon.in/s?k=\" + amazon_mobile_name.lower() + \"&page=\"\n        location = amazon_scrap(f'{URL}{str(j)}')\n        while(len(location)==0):\n            location = amazon_scrap(f'{URL}{str(j)}')\n        loc.append(location)\n    amazon=[]\n    for product_name in loc:\n        for product_src in product_name:\n            \n            specification_amazon=product_src.find(\"span\", {\"class\":\"a-size-medium a-color-base a-text-normal\"}).text.split(\")\")[0]\n            mobile_name=specification_amazon.split('(')[0].strip()\n            if amazon_mobile_name.upper() not in mobile_name.upper() :\n                continue\n            try:\n                ram_spec=re.search(r'\\d+',specification_amazon.split('(')[1].split(',')[1]).group()\n                storage_spec=re.search(r'\\d+',specification_amazon.split('(')[1].split(',')[2].replace('Storage','ROM').strip()).group()\n                mob_color=specification_amazon.split('(')[1].split(',')[0]\n            except(IndexError,AttributeError):\n                continue\n            try:\n                amazon_price = product_src.find(\"span\", {\"class\":\"a-offscreen\"}).text.replace(\"\u20b9\",\"\")\n            except(AttributeError):\n                continue\n            amazon_mobile=[]\n            amazon_mobile.append(mobile_name.upper().strip())\n            amazon_mobile.append(mob_color.upper().strip())\n            amazon_mobile.append(str(ram_spec)+'GB'.strip())\n            amazon_mobile.append(str(storage_spec)+'GB'.strip())\n            amazon_mobile.append(int(amazon_price.replace(',','').strip()))\n            amazon.append(amazon_mobile)\n            df_amazon = pd.DataFrame(amazon)\n            df_amazon.columns = [\"Mobile\",\"Colour\",\"Ram\",\"Storage\",\"Amazon_Price\"]\n            df_amazon.drop_duplicates(inplace=True)\n            amazon_dataframe=df_amazon\n    return amazon_dataframe\n@app.route('/amazon',methods =[\"GET\", \"POST\"])\ndef amazon():\n    if request.method == \"POST\":\n        amazon_mobile_name = request.form.get(\"amazon_mobile\")\n        amazon_scrap_pages=request.form.get('amazon_scrap_pages')\n        amazon_scrap_pages=int(amazon_scrap_pages)\n    df_amazon=amazon_webscrape(amazon_mobile_name,amazon_scrap_pages)\n    return render_template('index1.html',column_names=df_amazon.columns.values, row_data=list(df_amazon.values.tolist()), zip=zip)\n@app.route('/compare',methods =[\"GET\", \"POST\"])\ndef compare():\n    if request.method == \"POST\":\n        online_mobile = request.form.get(\"mobile_name\")\n        online_scrap_pages=request.form.get('mobile_pages')\n        df_amazon=amazon_webscrape(online_mobile,int(online_scrap_pages))\n        df_flipkart=flipkart_webscrape(online_mobile,int(online_scrap_pages))\n    result = pd.merge(df_amazon, df_flipkart,how='inner')\n    result[\"Price_Difference\"] = abs(result[\"Flipkart_Price\"]-result[\"Amazon_Price\"])\n    #print(result)\n    #result.to_csv(\"iqoo.csv\", index=False)\n    return render_template('price.html',column_names=result.columns.values, row_data=list(result.values.tolist()), zip=zip)\n@app.route('/single',methods =[\"GET\", \"POST\"])\ndef single():\n    if request.method == \"POST\":\n        single_mobile = request.form.get(\"single_mobile_name\")\n        single_color=request.form.get('single_mobile_color')\n        single_ram=request.form.get('single_mobile_ram')+'GB'\n        single_storage=request.form.get('single_mobile_storage')+'GB'\n        # int_features = [str(x) for x in request.form.values()]\n        df = pd.read_csv(\"vivo.csv\")\n        # final_features = [np.array(int_features)]\n        # boolean_series = df['Mobile','Color','Ram','Storage'].isin(final_features)\n        # filtered_df = df[boolean_series]\n        df2 = df[(df['Mobile'] == single_mobile.upper()) & (df['Colour'] == single_color.upper()) & (df['Ram'] == single_ram) & (df['Storage'] == single_storage)]\n        res_pric_flipcart = df2['Flipkart_Price'].to_string(index=False)\n        res_pric_amazon = df2['Amazon_Price'].to_string(index=False)\n        #return render_template('single_pred.html',result={res_pric_amazon})\n        if(int(res_pric_flipcart) == int(res_pric_amazon )):\n             return render_template('single_pred.html', text_result='You can prefer both Amazon or Flipkart', result='Amazon Price : '+(res_pric_amazon),result1='Flipkart Price :'+(res_pric_flipcart))\n        if (int(res_pric_flipcart) > int(res_pric_amazon )):\n            return render_template('single_pred.html', text_result='You can prefer  Amazon ', result='Amazon Price : '+(res_pric_amazon),result1='Flipkart Price :'+(res_pric_flipcart))\n        else:\n            return render_template('single_pred.html', text_result='You can prefer Flipkart ', result='Amazon Price : '+(res_pric_amazon),result1='Flipkart Price :'+(res_pric_flipcart))\n\n    print(res_pric_flipcart)  \n    print(res_pric_amazon)  \n\n    #print(final_features)\n\n        \n        #print(data_xls)\nif __name__ == \"__main__\":\n    app.run(debug=True)", "139": "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport selenium\nimport urllib.parse\nimport time\nimport os\nfrom video_class import YtVideo\ndriver = \"\"\n#if os.environ.get(\"DEBUG_YOUTUBE_TUI\") != \"True\":\n#    options.headless = True\ntry:\n    options = webdriver.firefox.options.Options()\n    options.headless = True\n    driver = webdriver.Firefox(options=options)\nexcept selenium.common.exceptions.WebDriverException:\n    try:\n        options = webdriver.chrome.options.Options()\n        options.headless = True\n        driver = webdriver.Chrome(options=options)\n\n    except selenium.common.exceptions.WebDriverException:\n        try:\n            driver = webdriver.Safari()\n        except:\n            try:\n                options = webdriver.ie.options.Options()\n                options.headless = True\n                driver= webdriver.Ie(options=options)\n            except selenium.common.exceptions.WebDriverException:\n                print(\"No usable browser found.\")\n\n\ndef request(url):\n    driver.get(url)\n\n\ndef webscrape(search: bool):\n    titles_array = []\n    urls_array = []\n    creators_array = []\n\n    # get creators\n    creators = driver.find_elements_by_xpath(\n        \"//a[@class='yt-simple-endpoint style-scope yt-formatted-string']\")\n    i = 0\n    for creator in creators:\n        i += 1\n        if search == True:\n            #print(\"we're searching\")\n            if (i % 2) != 0:\n                # print(creator.text)\n                creators_array.append(creator.text)\n        else:\n            # print(creator.text)\n            creators_array.append(creator.text)\n\n    urls = \"\"\n\n    # get urls and titles\n    if not search:\n        urls = driver.find_elements_by_id(\"video-title-link\")\n    else:\n        urls = driver.find_elements_by_id(\"video-title\")\n    for url in urls:\n        urls_array.append(url.get_attribute(\"href\"))\n        titles_array.append(url.get_attribute(\"title\"))\n\n    # for some reason, the first index of the array, should be the last, so i remove it and append it immediatly\n    titles_array.append(titles_array.pop(0))\n    creators_array.append(creators_array.pop(0))\n    urls_array.append(urls_array.pop(0))\n\n    # Generate return value\n    return_value = []\n    for i in range(len(titles_array)):\n        if i >= 20:\n            break\n        try:\n            return_value.append(\n                YtVideo(titles_array[i - 1], creators_array[i - 1], urls_array[i - 1]))\n        except:\n            break\n\n    return return_value\n\n\ndef get_main_page():\n\n    request('https://www.youtube.com/')\n\n    return webscrape(False)\n    #driver.\n\ndef search(text):\n    request(\"https://www.youtube.com/results?search_query=\" +\n            urllib.parse.quote(text))\n\n    WebDriverWait(driver, 10).until(\n        EC.element_to_be_clickable(\n            (By.XPATH, \"//*[@id=\\\"logo-icon-container\\\"]\"))\n    )\n    try:\n        driver.find_element_by_xpath(\n            \"/html/body/ytd-app/div/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-background-promo-renderer\")\n    except:\n        time.sleep(0.3)\n        return webscrape(True)\n    else:\n        return \"error\"\nprint(get_main_page()[2].title)\ndef close_driver():\n    driver.quit()\n", "140": "#BACKGROUND\n#This is a python code that is used to webscrape details from a vendor such as flipkart. \n#The code works for Flipkart and did not work for Amazon.\n#Idea behind the code was to find the actual realtime Ecommerce market valuation for a specific query\n#The number of reviews give a minimum idea about the people who bought the product, which can be multipled with the price to find the guaranteed sales figure.\n#Post this, with a thumbrule to identify what % of the actual buyers are reviwers we can guestimmate the overall sales turnover\n\n#CODE LOGIC\n#Items webscraped are product name, product price, number of reviews based on padding and XPath values\n#The number of pages for the search query is given in a loop to webscrape all the queries\n#The output gets saved as an excel file for furthe analysis\n\n#ISSUES\n#The code has complexity if we have a lot of data and it will take some time to run\n#Other developers can let me know how to reduce the complexity\n\nfrom bs4 import BeautifulSoup # Importing the BeautifulSoup package\nimport requests # Importing requests library\nimport csv # Import CSV library\nimport pandas as pd # Import Pandas\n\n#The dummy URL below holds the URL link for flipkart where we need to webscrape the details\ndummyurl= \"https://www.flipkart.com/beauty-and-grooming/body-face-skin-care/pr?sid=g9b%2Cema&q=coconut+oil&otracker=categorytree&p%5B%5D=facets.rating%255B%255D%3D1%25E2%2598%2585%2B%2526%2Babove\"\nmaster = []\ndescriptions = []\n\n# The number of pages in the search query is dynamic. Hence it has to be hardcoded in the below for loop definition\n# Example: the below for range has loop from 1st page to 41st page\n\nfor i in range(1,41):\n    print(i) #Just print to see if the code is working\n    req = requests.get(dummyurl + \"&page=\" + str(i)) # Requesting the content of the URL\n    content = req.content # Getting the content\n    soup = BeautifulSoup(content,'html.parser') # Here we need to specify the content variable and the parser which is the HTML parser\n    # So now soup is a variable of the BeautifulSoup object of our parsed HTML\n    desc = soup.find_all('div' , class_='_4ddWXP') # Extracting the descriptions from the website using the find method - grabbing the div tag which has the classname _3wU53n\n    # So now this returns all the div tags with the classname of _3wU53n\n    # As class is a special keyword in python we have to use the class_ keyword and pass the arguments here\n    print(len(desc))\n    for j in range(len(desc)):\n               descriptions.append(desc[j].text) # We can even access the child tags with dot access.\n    master.append(descriptions)\n# Example.csv gets created in the current working directory\nwith open ('Example.csv','w',newline = '',encoding='UTF-8') as csvfile:\n    my_writer = csv.writer(csvfile, delimiter = ' ')\n    my_writer.writerows(descriptions)\n    \n    #END OF CODE\n    \n", "141": "# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'webscrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "142": "import sys \nimport requests\nimport json\nfrom bs4 import BeautifulSoup #install python-beautifulsoup\n\n\n#grab all elements in container holding desired list\ndef webscrape(URL, string):\n\tpstring = processCD(string)\n\tpage = requests.get(URL)\n\tsoup = BeautifulSoup(page.content, 'html.parser')\n\tresults = soup.find(id=pstring[0])\n\tresults = results.find_all(pstring[1], class_=pstring[2])\n\treturn results\n\n#process html into dictionary, strip all useless html details \ndef parseContainer(containerList, string):\n\tpstring = processCED(string)\n\tlst = []\n\tfor elem in containerList:\n\t\tobj = {}\n\t\tfor i, cat in enumerate(pstring[1]):\n\t\t\titem = elem.find(pstring[0][i],class_=cat)\n\t\t\tif item == None: continue;\n\t\t\titem = (elem.find(pstring[0][i],class_=cat)).text\n\t\t\tobj[cat] = item.strip()\n\t\tif obj:\n\t\t\tlst.append(obj.copy())\n\treturn lst, {\"categories\": pstring[1]}\n\n#process container details\ndef processCD(string):\n\tpstring = string.split(\",\")\n\treturn pstring\n\n#process container entry details\ndef processCED(string):\n\tpstring = string.split(\"],[\")\n\tfor i, elem in enumerate(pstring):\n\t\tpstring[i] = elem.replace(\"[\",\"\").replace(\"]\",\"\")\n\t\tpstring[i] = pstring[i].split(\",\")\n\treturn pstring\n\ndef main():\n\t#url = \"https://www.monster.com/jobs/search/?q=Software-Developer&where=Seattle\"\n\t#cd = \"SearchResults,section,card-content\"\n\t#ced = \"[h2,div,div],[title,company,location]\"\n\turl = sys.argv[1]\n\tcd = sys.argv[2]\n\tced = sys.argv[3]\n\tresults = webscrape(url,cd)\n\tobjects, categories = parseContainer(results,ced)\n\tjsonCategories = json.dumps(categories)\n\tjsonObject = json.dumps(objects)\n\tprint(jsonCategories)\n\tprint(jsonObject)\n\nif __name__ == \"__main__\":\n    main()", "143": "#python3 zmq_reply.py\nimport sys \nimport requests\nimport json\nfrom bs4 import BeautifulSoup\nimport zmq\n\n\n#grab all elements in container holding desired list\ndef webscrape(URL, string):\n\tpstring = processCD(string)\n\tpage = requests.get(URL)\n\tsoup = BeautifulSoup(page.content, 'html.parser')\n\tresults = soup.find(id=pstring[0])\n\tresults = results.find_all(pstring[1], class_=pstring[2])\n\treturn results\n\n#process html into dictionary, strip all useless html details \ndef parseContainer(containerList, string):\n\tpstring = processCED(string)\n\tlst = []\n\tfor elem in containerList:\n\t\tobj = {}\n\t\tfor i, cat in enumerate(pstring[1]):\n\t\t\titem = elem.find(pstring[0][i],class_=cat)\n\t\t\tif item == None: continue;\n\t\t\titem = (elem.find(pstring[0][i],class_=cat)).text\n\t\t\tobj[cat] = item.strip()\n\t\tif obj:\n\t\t\tlst.append(obj.copy())\n\treturn lst, {\"categories\": pstring[1]}\n\n#process container details\ndef processCD(string):\n\tpstring = string.split(\",\")\n\treturn pstring\n\n#process container entry details\ndef processCED(string):\n\tpstring = string.split(\"],[\")\n\tfor i, elem in enumerate(pstring):\n\t\tpstring[i] = elem.replace(\"[\",\"\").replace(\"]\",\"\")\n\t\tpstring[i] = pstring[i].split(\",\")\n\treturn pstring\n\n#TODO: add URL and search criteria\ndef main():\n\t#url = \"https://www.monster.com/jobs/search/?q=Software-Developer&where=Seattle\"\n\t#cd = \"SearchResults,section,card-content\"\n\t#ced = \"[h2,div,div],[title,company,location]\"\n\turl = sys.argv[1]\n\tcd = sys.argv[2]\n\tced = sys.argv[3]\n\tresults = webscrape(url,cd)\n\tobjects, categories = parseContainer(results,ced)\n\tjsonCategories = json.dumps(categories)\n\tjsonObject = json.dumps(objects)\n\tprint(jsonCategories)\n\tprint(jsonObject)\n\nif __name__ == \"__main__\":\n    main()", "144": "import sqlite3\r\nimport webscrape\r\n\r\n\r\n#Getting the base price through web scraping\r\nbij = {}\r\nud = {}\r\nbang = {}\r\nbij = webscrape.bij_price\r\nud = webscrape.u_price\r\nbang = webscrape.bang_price\r\n\r\ndef price_reduction(investment, profit):\r\n    investment = int(investment)\r\n    profit = int(profit)\r\n    ratio = (investment/profit)*100\r\n    if(ratio>=90):\r\n        return 7\r\n    elif(ratio<90 and ratio>=70):\r\n        return 5\r\n    elif(ratio<70 and ratio>=50):\r\n        return 4\r\n    elif(ratio<50 and ratio>=20):\r\n        return 2\r\n    else:\r\n        return 1\r\n\r\ndef calculate_price(ratio, location, crop):\r\n    if(location=='Bijapur'):\r\n        base = bij[crop]\r\n        final = base - ((base*ratio)/100)\r\n        return final\r\n    elif(location=='Udupi'):\r\n        base = ud[crop]\r\n        final = base - ((base*ratio)/100)\r\n        return final\r\n    else:\r\n        base = bang[crop]\r\n        final = base - ((base * ratio) / 100)\r\n        return final\r\n\r\n\r\ndef bijapur():\r\n    conn = sqlite3.connect('crop.db')\r\n    c = conn.cursor()\r\n    c.execute(\"SELECT id, crop, investment, profit FROM bijapur\")\r\n    for i in c.fetchall():\r\n        index = i[0]\r\n        ratio = price_reduction(i[2],i[3])\r\n        final_price = calculate_price(ratio, 'Bijapur', i[1])\r\n        c.execute(\"UPDATE bijapur SET price= ? WHERE id= ?\",(final_price,index))\r\n    conn.commit()\r\n    conn.close()\r\n\r\ndef udupi():\r\n    conn = sqlite3.connect('crop.db')\r\n    c = conn.cursor()\r\n    c.execute(\"SELECT id, crop, investment, profit FROM udupi\")\r\n    for i in c.fetchall():\r\n        index = i[0]\r\n        ratio = price_reduction(i[2],i[3])\r\n        final_price = calculate_price(ratio, 'Udupi', i[1])\r\n        c.execute(\"UPDATE udupi SET price= ? WHERE id= ?\", (final_price, index))\r\n    conn.commit()\r\n    conn.close()\r\n\r\ndef bangalore():\r\n    conn = sqlite3.connect('crop.db')\r\n    c = conn.cursor()\r\n    c.execute(\"SELECT id, crop, investment, profit FROM bangalore\")\r\n    for i in c.fetchall():\r\n        index = i[0]\r\n        ratio = price_reduction(i[2],i[3])\r\n        final_price = calculate_price(ratio, 'Bangalore rural', i[1])\r\n        c.execute(\"UPDATE bangalore SET price= ? WHERE id= ?\", (final_price, index))\r\n    conn.commit()\r\n    conn.close()\r\n\r\n", "145": "from bs4 import BeautifulSoup\nfrom textmagic.rest import TextmagicRestClient\nimport requests, json, urllib2\n\nclient = TextmagicRestClient(\"yradsmikham\", \"VlY7PjdtNSZkiiyZn6U1GX0KV7jLTl\")\n\n# inputs\n# enter phone number, specify dates and campground id\nphone_number = \"\" # must include country code\ncampground_id = \"232472\" # example correponds to Yosemite Upper Pines Campground\nstart_date = \"2019-01-28\"\nend_date = \"2019-01-30\"\n\n#232472 Indian Cove JTree\n\n# specify url\nurl = \"https://www.recreation.gov/api/camps/availability/campground/\"+campground_id+\"?start_date=\"+start_date+\"T00%3A00%3A00.000Z&end_date=\"+end_date+\"T00%3A00%3A00.000Z\"\n\ndef webscrape(url):\n    # query website and return html\n    data = urllib2.urlopen(url)\n\n    # parse the html using beautiful soup and store in variable \n    soup = str(BeautifulSoup(data, 'html.parser'))\n    campsite_json = json.loads(soup)\n\n    # iterate through each campsite to check for availabilities\n    available_campsites = []\n    for campsite in campsite_json['campsites']:\n        days = campsite_json['campsites'][campsite][\"availabilities\"]\n        availabilities = []\n        for key,value in days.items():\n            if value != None:\n                availabilities.append(value)\n        if not availabilities:\n            #print(\"List is empty.\")\n            pass\n        else:\n            #print(availabilities)\n            #print(all(dawgie == 'Open' or 'Available' for dawgie in availabilities))\n            if all(dawgie == 'Open' or 'Available' for dawgie in availabilities):\n                #print(\"THERE IS AN AVAILABLE CAMPSITE!\")\n                available_campsites.append(campsite)\n            else:\n                #print(\"NO CAMPSITES AVAILABLE.\")\n                continue\n        #print(\"------------------------------------------------------------------------\")\n    #print(\"AVAILABLE CAMPSITES:\" + str(available_campsites))\n    return available_campsites\n\nresult = webscrape(url)\n#print(result[:5])\ndef send_notifcation(url, list_of_available_campsites):\n# send text notification\n    if len(result) != 0:\n        client.messages.create(phones=phone_number, text=\"There are campsites available at \" + campground_id + \". The following campsites are available: \" +  str(result[:5]))\n    else:\n        webscrape(url)\n        send_notifcation(url, list_of_available_campsites)\n\n# notes:\n# multiprocessing/threaded?\n# possibly kick off another script that will log in and adds to cart?\n", "146": "import flask\nfrom flask import request, jsonify\nfrom webscrape_turnover import get_teams_turnover\nfrom webscrape_point_diff import get_teams_point_diff\nfrom webscrape_OYG import get_teams_OYG\nfrom teams import teams\n\napp = flask.Flask(__name__)\napp.config['DEBUG'] = True\n\n# Will return the winner with the better team stats\ndef get_winner(tracker, team1, team2):\n    if(tracker[team1] > tracker[team2]):\n        return team1\n    else:\n        return team2\n\n# A route to return the predicted winner of both teams given\n@app.route('/api/v1/winner', methods=['GET'])\ndef api_all():\n    # Get both parameters given\n    team1 = request.args['team1']\n    team2 = request.args['team2']\n\n    # Convert the team name into the correct format above\n    team1 = teams[team1]\n    team2 = teams[team2]\n\n    # Create dictionary of both teams to keep track of values\n    tracker = {\n        team1: 0,\n        team2: 0\n    }\n\n    # Call the web scraper function and return stats for selected team\n    tracker[get_teams_turnover(team1, team2)] += 1\n    tracker[get_teams_point_diff(team1, team2)] += 1\n    tracker[get_teams_OYG(team1, team2)] += 1\n\n    # Check to see who is the winner with better stats\n    result = get_winner(tracker, team1, team2)\n\n    # Convert into a json object after getting correct key again\n    result = list(teams.keys())[list(teams.values()).index(result)]\n    winner = {'result': result}\n\n    # Return as a json package\n    return jsonify(winner)\n\n# Run the Server App API\napp.run()", "147": "from flask import Flask, render_template\nfrom web_scrape import webscrape\nfrom web_scrape import barchart\n\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef index():\n\twebscrape()\n\tbarchart()\n\treturn render_template(\"index.html\")\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)", "148": "\"\"\"\nBase settings to build other settings files upon.\n\"\"\"\n\nimport environ\n\nROOT_DIR = (\n    environ.Path(__file__) - 3\n)  # (webscrape/config/settings/base.py - 3 = webscrape/)\nAPPS_DIR = ROOT_DIR.path(\"webscrape\")\n\nenv = environ.Env()\n\nREAD_DOT_ENV_FILE = env.bool(\"DJANGO_READ_DOT_ENV_FILE\", default=False)\nif READ_DOT_ENV_FILE:\n    # OS environment variables take precedence over variables from .env\n    env.read_env(str(ROOT_DIR.path(\".env\")))\n\n# GENERAL\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#debug\nDEBUG = env.bool(\"DJANGO_DEBUG\", False)\n# Local time zone. Choices are\n# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name\n# though not all of them may be available with every OS.\n# In Windows, this must be set to your system time zone.\nTIME_ZONE = \"UTC\"\n# https://docs.djangoproject.com/en/dev/ref/settings/#language-code\nLANGUAGE_CODE = \"en-us\"\n# https://docs.djangoproject.com/en/dev/ref/settings/#site-id\nSITE_ID = 1\n# https://docs.djangoproject.com/en/dev/ref/settings/#use-i18n\nUSE_I18N = True\n# https://docs.djangoproject.com/en/dev/ref/settings/#use-l10n\nUSE_L10N = True\n# https://docs.djangoproject.com/en/dev/ref/settings/#use-tz\nUSE_TZ = True\n# https://docs.djangoproject.com/en/dev/ref/settings/#locale-paths\nLOCALE_PATHS = [ROOT_DIR.path(\"locale\")]\n\n# DATABASES\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#databases\n\n# DATABASES = {\n#     \"default\": env.db(\"DATABASE_URL\", default=\"postgres:///webscrape\")\n# }\n# DATABASES[\"default\"][\"ATOMIC_REQUESTS\"] = True\n\nDATABASES = {\n    'default': {\n        'NAME': 'messaging',\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'USER': 'messaging',\n        'PASSWORD': 'messaging',\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'ATOMIC_REQUESTS': True\n    }\n}\n\n# URLS\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#root-urlconf\nROOT_URLCONF = \"config.urls\"\n# https://docs.djangoproject.com/en/dev/ref/settings/#wsgi-application\nWSGI_APPLICATION = \"config.wsgi.application\"\n\n# APPS\n# ------------------------------------------------------------------------------\nDJANGO_APPS = [\n    \"django.contrib.auth\",\n    \"django.contrib.contenttypes\",\n    \"django.contrib.sessions\",\n    \"django.contrib.sites\",\n    \"django.contrib.messages\",\n    \"django.contrib.staticfiles\",\n    # \"django.contrib.humanize\", # Handy template tags\n    \"django.contrib.admin\",\n]\nTHIRD_PARTY_APPS = [\n    \"rest_framework\",\n]\n\nLOCAL_APPS = [\n    \"webscrape.application.apps.ApplicationConfig\",\n    # Your stuff: custom apps go here\n]\n# https://docs.djangoproject.com/en/dev/ref/settings/#installed-apps\nINSTALLED_APPS = DJANGO_APPS + THIRD_PARTY_APPS + LOCAL_APPS\n\n# MIGRATIONS\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#migration-modules\nMIGRATION_MODULES = {\"sites\": \"webscrape.contrib.sites.migrations\"}\n\n# AUTHENTICATION\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#authentication-backends\nAUTHENTICATION_BACKENDS = [\n    \"django.contrib.auth.backends.ModelBackend\",\n    \"allauth.account.auth_backends.AuthenticationBackend\",\n]\n# https://docs.djangoproject.com/en/dev/ref/settings/#auth-user-model\n# AUTH_USER_MODEL = \"users.User\"\n# https://docs.djangoproject.com/en/dev/ref/settings/#login-redirect-url\n# LOGIN_REDIRECT_URL = \"users:redirect\"\n# https://docs.djangoproject.com/en/dev/ref/settings/#login-url\n# LOGIN_URL = \"account_login\"\n\n# PASSWORDS\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#password-hashers\nPASSWORD_HASHERS = [\n    # https://docs.djangoproject.com/en/dev/topics/auth/passwords/#using-argon2-with-django\n    \"django.contrib.auth.hashers.Argon2PasswordHasher\",\n    \"django.contrib.auth.hashers.PBKDF2PasswordHasher\",\n    \"django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher\",\n    \"django.contrib.auth.hashers.BCryptSHA256PasswordHasher\",\n]\n# https://docs.djangoproject.com/en/dev/ref/settings/#auth-password-validators\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.UserAttributeSimilarityValidator\"\n    },\n    {\"NAME\": \"django.contrib.auth.password_validation.MinimumLengthValidator\"},\n    {\"NAME\": \"django.contrib.auth.password_validation.CommonPasswordValidator\"},\n    {\"NAME\": \"django.contrib.auth.password_validation.NumericPasswordValidator\"},\n]\n\n# MIDDLEWARE\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#middleware\nMIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.locale.LocaleMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n]\n\n# STATIC\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#static-root\nSTATIC_ROOT = str(ROOT_DIR(\"staticfiles\"))\n# https://docs.djangoproject.com/en/dev/ref/settings/#static-url\nSTATIC_URL = \"/static/\"\n# https://docs.djangoproject.com/en/dev/ref/contrib/staticfiles/#std:setting-STATICFILES_DIRS\nSTATICFILES_DIRS = [str(APPS_DIR.path(\"static\"))]\n# https://docs.djangoproject.com/en/dev/ref/contrib/staticfiles/#staticfiles-finders\nSTATICFILES_FINDERS = [\n    \"django.contrib.staticfiles.finders.FileSystemFinder\",\n    \"django.contrib.staticfiles.finders.AppDirectoriesFinder\",\n]\n\n# MEDIA\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#media-root\nMEDIA_ROOT = str(APPS_DIR(\"media\"))\n# https://docs.djangoproject.com/en/dev/ref/settings/#media-url\nMEDIA_URL = \"/media/\"\n\n# TEMPLATES\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#templates\nTEMPLATES = [\n    {\n        # https://docs.djangoproject.com/en/dev/ref/settings/#std:setting-TEMPLATES-BACKEND\n        \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n        # https://docs.djangoproject.com/en/dev/ref/settings/#template-dirs\n        \"DIRS\": [str(APPS_DIR.path(\"templates\"))],\n        \"OPTIONS\": {\n            # https://docs.djangoproject.com/en/dev/ref/settings/#template-loaders\n            # https://docs.djangoproject.com/en/dev/ref/templates/api/#loader-types\n            \"loaders\": [\n                \"django.template.loaders.filesystem.Loader\",\n                \"django.template.loaders.app_directories.Loader\",\n            ],\n            # https://docs.djangoproject.com/en/dev/ref/settings/#template-context-processors\n            \"context_processors\": [\n                \"django.template.context_processors.debug\",\n                \"django.template.context_processors.request\",\n                \"django.contrib.auth.context_processors.auth\",\n                \"django.template.context_processors.i18n\",\n                \"django.template.context_processors.media\",\n                \"django.template.context_processors.static\",\n                \"django.template.context_processors.tz\",\n                \"django.contrib.messages.context_processors.messages\",\n                \"webscrape.utils.context_processors.settings_context\",\n            ],\n        },\n    }\n]\n# http://django-crispy-forms.readthedocs.io/en/latest/install.html#template-packs\nCRISPY_TEMPLATE_PACK = \"bootstrap4\"\n\n# FIXTURES\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#fixture-dirs\nFIXTURE_DIRS = (str(APPS_DIR.path(\"fixtures\")),)\n\n# SECURITY\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#session-cookie-httponly\nSESSION_COOKIE_HTTPONLY = True\n# https://docs.djangoproject.com/en/dev/ref/settings/#csrf-cookie-httponly\nCSRF_COOKIE_HTTPONLY = True\n# https://docs.djangoproject.com/en/dev/ref/settings/#secure-browser-xss-filter\nSECURE_BROWSER_XSS_FILTER = True\n# https://docs.djangoproject.com/en/dev/ref/settings/#x-frame-options\nX_FRAME_OPTIONS = \"DENY\"\n\n# EMAIL\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#email-backend\nEMAIL_BACKEND = env(\n    \"DJANGO_EMAIL_BACKEND\", default=\"django.core.mail.backends.smtp.EmailBackend\"\n)\n# https://docs.djangoproject.com/en/2.2/ref/settings/#email-timeout\nEMAIL_TIMEOUT = 5\n\n# ADMIN\n# ------------------------------------------------------------------------------\n# Django Admin URL.\nADMIN_URL = \"admin/\"\n# https://docs.djangoproject.com/en/dev/ref/settings/#admins\nADMINS = [(\"\"\"Sukant Priyadarshi\"\"\", \"sukant1994@gmail.com\")]\n# https://docs.djangoproject.com/en/dev/ref/settings/#managers\nMANAGERS = ADMINS\n\n# LOGGING\n# ------------------------------------------------------------------------------\n# https://docs.djangoproject.com/en/dev/ref/settings/#logging\n# See https://docs.djangoproject.com/en/dev/topics/logging for\n# more details on how to customize your logging configuration.\nLOGGING = {\n    \"version\": 1,\n    \"disable_existing_loggers\": False,\n    \"formatters\": {\n        \"verbose\": {\n            \"format\": \"%(levelname)s %(asctime)s %(module)s \"\n            \"%(process)d %(thread)d %(message)s\"\n        }\n    },\n    \"handlers\": {\n        \"console\": {\n            \"level\": \"DEBUG\",\n            \"class\": \"logging.StreamHandler\",\n            \"formatter\": \"verbose\",\n        }\n    },\n    \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n}\n\n\n# django-allauth\n# ------------------------------------------------------------------------------\nACCOUNT_ALLOW_REGISTRATION = env.bool(\"DJANGO_ACCOUNT_ALLOW_REGISTRATION\", True)\n# https://django-allauth.readthedocs.io/en/latest/configuration.html\nACCOUNT_AUTHENTICATION_METHOD = \"username\"\n# https://django-allauth.readthedocs.io/en/latest/configuration.html\nACCOUNT_EMAIL_REQUIRED = True\n# https://django-allauth.readthedocs.io/en/latest/configuration.html\nACCOUNT_EMAIL_VERIFICATION = \"mandatory\"\n# https://django-allauth.readthedocs.io/en/latest/configuration.html\n# ACCOUNT_ADAPTER = \"webscrape.users.adapters.AccountAdapter\"\n# https://django-allauth.readthedocs.io/en/latest/configuration.html\n# SOCIALACCOUNT_ADAPTER = \"webscrape.users.adapters.SocialAccountAdapter\"\n\n\n# Your stuff...\n# ------------------------------------------------------------------------------\n", "149": "# webscrape documentation build configuration file, created by\n# sphinx-quickstart.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport os\nimport sys\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n# sys.path.insert(0, os.path.abspath('.'))\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = []\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix of source filenames.\nsource_suffix = \".rst\"\n\n# The encoding of source files.\n# source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# General information about the project.\nproject = \"webscrape\"\ncopyright = \"\"\"2019, Sukant Priyadarshi\"\"\"\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \"0.1\"\n# The full version, including alpha/beta/rc tags.\nrelease = \"0.1\"\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = ''\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\"_build\"]\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n# default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \"default\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \" v documentation\".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a  tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n# html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"webscrapedoc\"\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    # 'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    # 'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n    (\n        \"index\",\n        \"webscrape.tex\",\n        \"webscrape Documentation\",\n        \"\"\"Sukant Priyadarshi\"\"\",\n        \"manual\",\n    )\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\n        \"index\",\n        \"webscrape\",\n        \"webscrape Documentation\",\n        [\"\"\"Sukant Priyadarshi\"\"\"],\n        1,\n    )\n]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False\n\n\n# -- Options for Texinfo output ------------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        \"index\",\n        \"webscrape\",\n        \"webscrape Documentation\",\n        \"\"\"Sukant Priyadarshi\"\"\",\n        \"webscrape\",\n        \"\"\"Behold My Awesome Project!\"\"\",\n        \"Miscellaneous\",\n    )\n]\n\n# Documents to append as an appendix to all manuals.\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n# texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n# texinfo_show_urls = 'footnote'\n", "150": "import unittest\nfrom batterydataextractor.scrape import RSCWebScraper\n\n\nclass TestRSCScraper(unittest.TestCase):\n\n    rsc_scraper = RSCWebScraper()\n\n    def test_get_doi_rsc(self):\n        \"\"\"\n        Test if the list of dois can be found with the RSCWebScraper\n        \"\"\"\n        dois = self.rsc_scraper.get_doi(query=\"battery materials\", page=3)\n        length = len(dois)\n\n        self.assertEqual(length, 25)\n\n    def test_els_scraper(self):\n        # As Elsevier web-scraper requires API key, we have no tests here.\n        # See `examples/webscrape/elsevier.py` for more details.\n        pass\n\n    def test_spr_scraper(self):\n        # As Springer web-scraper requires API key, we have no tests here.\n        # See `examples/webscrape/spr.py` for more details.\n        pass\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "151": "from django import forms\nfrom django.forms import ModelForm\nfrom .models import WebScrape, UserLogin\n\n\nclass WebscrapeForm(ModelForm):\n    class Meta:\n        model = WebScrape\n        fields = ('category', 'url', 'date', 'price', 'imageUrl', 'profit')\n\n        widgets = {\n            'category': forms.Select(attrs={'class': 'form-control'}),\n            'url': forms.TextInput(attrs={'class': 'form-control'}),\n            'date': forms.TextInput(attrs={'class': 'form-control'}),\n            'price': forms.TextInput(attrs={'class': 'form-control'}),\n            'imageUrl': forms.TextInput(attrs={'class': 'form-control'}),\n            'profit': forms.TextInput(attrs={'class': 'form-control'}),\n        }\n\n\nclass UserLoginForm(ModelForm):\n    class Meta:\n        model = UserLogin\n        fields = ('first_name', 'last_name', 'email')\n\n        widgets = {\n            'first_name': forms.TextInput(attrs={'class': 'form-control'}),\n            'last_name': forms.TextInput(attrs={'class': 'form-control'}),\n            'email': forms.TextInput(attrs={'class': 'form-control'}),\n        }", "152": "from convertPDF.webScrape.pdf2xml import *\nfrom convertPDF.webScrape.xml2txt import *\n\nbulkXml2Txt('./../charts/xmls', './../charts/temptxt')", "153": "from dotenv import load_dotenv\nimport mysql.connector, os, csv\nimport webscrapeHallOfFame, webscrapeCareerLeaders, webscrapeTopIndividualPerf, webscrapePlayerCareerStats\nfrom datetime import datetime\nfrom dateutil import relativedelta\n\n\ndef connect_to_SQL():\n    load_dotenv()\n    conn = mysql.connector.connect(user=os.getenv(\"USERNAME\"), password=os.getenv(\"PASSWORD\"),\n                                   host='127.0.0.1')\n    cursor = conn.cursor()\n    return cursor, conn\n\n\ndef createBaseballDB(cursor, db_name):\n    cursor.execute('DROP DATABASE IF EXISTS ' + db_name)\n    cursor.execute('CREATE DATABASE ' + db_name)\n    cursor.execute('USE ' + db_name)\n\n\ndef createTable(cursor, fields, table_name):\n    cursor.execute('DROP TABLE IF EXISTS {}'.format(table_name))\n    columns = ','.join(['{} {}'.format(key, fields[key]) for key in fields.keys()])\n    cursor.execute('CREATE TABLE {} ({})'.format(table_name, columns))\n\n\ndef loadPlayerNamesTable(cursor, player_names_dict, table_name):\n    for player in player_names_dict:\n        cursor.execute('INSERT INTO {} VALUES (\"{}\",\"{}\")'.format(table_name, player_names_dict[player], player))\n\n\ndef loadHOFTable(cursor, hall_of_fame_dict, table_name):\n    for player in hall_of_fame_dict:\n        cursor.execute('INSERT INTO {} VALUES (\"{}\", \"{}\")'.format(table_name, player, hall_of_fame_dict[player]))\n\n\ndef loadPlayerBiosTable(cursor, bios_dict, table_name):\n    for player in bios_dict:\n        debut_date, final_game, bats, throws, career_length, excess_months, birth_state, birth_country = bios_dict[\n            player]\n        cursor.execute(\n            'INSERT INTO {} VALUES (\"{}\", \"{}\", \"{}\", \"{}\",\"{}\", {},{}, \"{}\", \"{}\")'.format(table_name, player,\n                                                                                            debut_date, final_game,\n                                                                                            bats, throws,\n                                                                                            career_length,\n                                                                                            excess_months, birth_state,\n                                                                                            birth_country))\n\n\ndef loadCareerStatsTables(cursor, career_stats, table_name):\n    for player in career_stats:\n        stats_string = ','.join(career_stats[player])\n        sql = f'INSERT INTO {table_name} VALUES (\"{player}\",' + stats_string + ')'  # string slicing to remove extra comma and append a parenthesis to the sql command\n        cursor.execute(sql)\n\n\ndef createDBFields():\n    name_fields = {\n        'PlayerID': 'VARCHAR(100)',\n        'PlayerName': 'VARCHAR(100)'\n    }\n    player_bio_fields = {\n        'PlayerID': 'VARCHAR(100)',\n        'debutDate': 'DATE',\n        'finalGameDate': 'DATE',\n        'bats': 'CHAR(1)',\n        'throws': 'CHAR(1)',\n        'CareerLength_Years': 'INT',\n        'MonthsExtra': 'INT',\n        'birthState': 'VARCHAR(100)',\n        'birthCountry': 'VARCHAR(100)'\n\n    }\n    hall_of_fame_fields = {\n        'PlayerID': 'VARCHAR(25)',\n        'YearOfInduction': 'Year'\n    }\n\n    career_batting_stats = {\n        # Career Batting Table Columns\n        'PlayerID': 'VARCHAR(100)',\n        'Games': 'INT',\n        'AtBats': 'INT',\n        'Runs': 'INT',\n        'Hits': 'INT',\n        'Doubles': 'INT',\n        'Triples': 'INT',\n        'Homeruns': 'INT',\n        'RBI': 'INT',\n        'Walks': 'INT',\n        'IntentionalWalks': 'INT',\n        'Strikeouts': 'INT',\n        'HitByPitch': 'INT',\n        'Sacrifice_Hits': 'INT',\n        'Sacrifice_Flies': 'INT',\n        'XI': 'INT',\n        'ROE': 'INT',\n        'GroundedIntoDoublePlays': 'INT',\n        'StolenBases': 'INT',\n        'CaughtStealing': 'INT',\n        'BattingAVG': 'FLOAT',\n        'On_BasePercent': 'FLOAT',\n        'SluggingPercent': 'FLOAT',\n        'BFW': 'FLOAT'\n    }\n\n    career_pitching_stats = {\n        # Career Pitching Table Columns\n        'PlayerID': 'VARCHAR(100)',\n        'Games': 'INT',\n        'GamesStarted': 'INT',\n        'CompleteGames': 'INT',\n        'Shutouts': 'INT',\n        'GamesFinished': 'INT',\n        'Saves': 'INT',\n        'InningsPitched': 'INT',\n        'Hits': 'INT',\n        'BFP': 'INT',\n        'Homeruns': 'INT',\n        'Runs': 'INT',\n        'EarnedRuns': 'INT',\n        'Walks': 'INT',\n        'IntentionalWalks': 'INT',\n        'Strikeouts': 'INT',\n        'SacrificeHits': 'INT',\n        'SacrificeFlies': 'INT',\n        'WildPitches': 'INT',\n        'HitByPitch': 'INT',\n        'Balks': 'INT',\n        'Doubles': 'INT',\n        'Triples': 'INT',\n        'GroundedIntoDoublePlays': 'INT',\n        'ROE': 'INT',\n        'Wins': 'INT',\n        'Losses': 'INT',\n        'ERA': 'FLOAT',\n        'RunSupport': 'FLOAT',\n        'PW': 'FLOAT'\n    }\n    single_game_batting = {\n        'playerID': 'VARCHAR(100)',\n        'CaughtStealing': 'INT',\n        'ConsecutiveGameHitStreaks': ''\n    }\n\n    return name_fields, player_bio_fields, hall_of_fame_fields, career_batting_stats, career_pitching_stats\n\n\ndef loadBaseballData():\n    \"\"\"\n    This function calls all of our webscraping python files which loads various sources from retrosheet into CSVs for database loading\n    \"\"\"\n    webscrapeHallOfFame.main()\n    webscrapeTopIndividualPerf.main()\n    webscrapeCareerLeaders.main()\n\n\ndef webscrapeCareerStatsForEachPlayer(playerNameDictionary):\n    \"\"\"\n    Opens batting/pitching files for adding career statistics\n    This function calls our separate webscrape file which goes to each individual players url and scrapes either their pitching record, fielding record or both\n    Any errors in formatting such as players missing fields or players that did not have certain stats were skipped over in the webscraping process\n    We made sure not to abuse the webscraping of retrosheet by making sure only making calls out to the server 10 times per minute.\n\n    I ran this particular function on my raspberry pi and it took roughly 35 hours total to webscrape all of the necessary data\n\n    \"\"\"\n    # TODO Better String Handling for Individual Player Career Stat Lines\n    batting_file = open('playerinformation/batting_stats.csv', 'a')\n    pitching_file = open('playerinformation/pitching_stats.csv', 'a')\n    player_dict_len = len(playerNameDictionary.keys())\n    current_index = 0\n    player_list = list(playerNameDictionary.keys())\n    for player in player_list:\n        try:\n            webscrapePlayerCareerStats.webscrapeCareerStats(player, playerNameDictionary, batting_file,\n                                                            pitching_file)\n            current_index += 1\n            print('Current Completion Level: {}/{}'.format(current_index, player_dict_len))\n        except ValueError:\n            # Players with invalid career line formats would be skipped, such as not having enough columns for data\n            continue\n\n\ndef getDataDirectories(folder_name):\n    directories = []\n    for root, dirs, files in os.walk(folder_name):\n        for file in files:\n            if file.endswith('.csv'):\n                directories.append(os.path.join(root, file))\n    return directories\n\n\ndef convertDate(date):\n    format_string = \"%m/%d/%Y\"\n\n    try:\n        d = datetime.strptime(date, format_string)\n    except ValueError:\n        # if there is no date we return 0000-01-01 to denote no date\n        return '0000-01-01', 0\n\n    return d, d.year\n\n\ndef getPlayerNamesDictionary(filename):\n    playerNameDictionary = {}\n    with open(filename) as file:\n        file.readline()\n        player_info = csv.reader(file)\n        for line in player_info:\n            line = [element.strip('\"') for element in line]\n            playerID, playerName = line[0], line[3] + ' ' + line[1]\n            playerNameDictionary[playerName] = playerID\n\n    return playerNameDictionary\n\n\ndef getPlayerBiosDictionary(filename):\n    playerBioDictionary = {}\n    with open(filename) as file:\n        headers = [header.strip() for header in file.readline().split(',')]\n        bats_index = headers.index('BATS')\n        throws_index = headers.index('THROWS')\n        birth_state_index = headers.index('BIRTH STATE')\n        birth_country_index = headers.index('BIRTH COUNTRY')\n\n        debut_date_index = headers.index('PLAY DEBUT')\n        final_game_index = headers.index('PLAY LASTGAME')\n        player_info = csv.reader(file)\n        for line in player_info:\n            line = [element.strip('\"') for element in line]\n            playerID, debut_date, final_game, bats, throws = line[0], line[debut_date_index], line[final_game_index], \\\n                                                             line[bats_index], line[throws_index]\n\n            birth_state, birth_country = line[birth_state_index], line[birth_country_index]\n\n            \"\"\"New Column for Career Length in Years\"\"\"\n            debut_date, debut_date_year = convertDate(debut_date)\n            final_game, final_game_year = convertDate(final_game)\n\n            career_length_in_years = final_game_year - debut_date_year\n            if debut_date != '0000-01-01' and final_game != '0000-01-01':\n                date_diff = relativedelta.relativedelta(final_game, debut_date)\n                years, months, days = date_diff.years, date_diff.months, date_diff.days\n                # if months is greater than or equal to one we want to include this as a year of playing since they at least started the season\n                if months >= 1:\n                    career_length_in_years += 1\n                output_format_date = \"%Y-%m-%d\"\n                debut_date_str, final_game_str = datetime.strftime(debut_date, output_format_date), datetime.strftime(\n                    final_game, output_format_date)\n\n            playerBioDictionary[playerID] = [debut_date_str, final_game_str, bats, throws, career_length_in_years,\n                                             months, birth_state, birth_country]\n    return playerBioDictionary\n\n\ndef getHallOfFamePlayersDictionary(filename, playerNameDictionary, ):\n    hall_of_fame_dictionary = {}\n    with open(filename) as file:\n        file.readline()\n        hall_of_fame_info = csv.reader(file)\n        for line in hall_of_fame_info:\n            player_name, year_inducted = line[0], line[1]\n            if player_name in playerNameDictionary:\n                player_id = playerNameDictionary[player_name]\n                hall_of_fame_dictionary[player_id] = year_inducted\n    return hall_of_fame_dictionary\n\n\ndef loadAllTimeLeaders(filedirectories, playerDictionary, player_position, cursor):\n    for filename in filedirectories:\n        with open(filename) as file:\n            headers = file.readline()\n            headers = headers.replace(\"\\n\", \"\")\n            categories = headers.split(\",\")\n            categories[0] = categories[0].replace(\" \", \"\")\n            player_name_header = categories[0]\n            category = categories[1]\n            if category == \"G\":\n                category_list = categories[1]\n            else:\n                category_list = categories[1:]\n\n            fields = \"playerID VARCHAR(255), \" + player_name_header + \" VARCHAR(255)\"\n            for value in category_list:\n                value = value.replace(\"/\", \"Per\")\n                value = value.replace(\"%\", \"Percentage\")\n                fields = fields + \", \" + value + \" FLOAT\"\n            table_name = player_position + category\n            cursor.execute('DROP TABLE IF EXISTS {}'.format(table_name))\n            cursor.execute('CREATE TABLE {} ({})'.format(table_name, fields))\n            all_time_stats = csv.reader(file)\n            for line in all_time_stats:\n                name = line[0]\n                if name in playerDictionary:\n                    value_command = 'INSERT INTO {} VALUES (\"{}\", \"{}\"'.format(table_name, playerDictionary[name], name)\n                for values in line[1:]:\n                    if category == \"G\":\n                        value_command += ', \"{}\"'.format(values)\n                        break\n                    value_command += ', \"{}\"'.format(values)\n                value_command += ')'\n                cursor.execute(value_command)\n\n\ndef getCareerStatsForPlayersDictionary(filename):\n    career_stats_dict = {}\n    with open(filename) as file:\n        file.readline()\n        career_stats = csv.reader(file)\n        for line in career_stats:\n            career_stats_dict[line[0]] = line[1:]\n    return career_stats_dict\n\n\ndef addColumns(cursor, column, datatype, tbl_name, player_bio_dict, career_stats_dict):\n    cursor.execute(f\"ALTER TABLE {tbl_name} ADD {column} {datatype}\")\n    cursor.execute(f\"ALTER TABLE {tbl_name} ADD careerLength INT\")\n\n    for player in player_bio_dict:\n        try:\n            games = career_stats_dict[player][0]\n            career_length = player_bio_dict[player][4]\n            if float(career_length) == 0:\n                cursor.execute(f\"UPDATE {tbl_name} SET {column} = 0 WHERE playerID ='{player}'\")\n                cursor.execute(f\"UPDATE {tbl_name} SET careerLength = 0 WHERE playerID ='{player}'\")\n                continue\n            games_per_year = float(games) / float(career_length)\n            cursor.execute(f\"UPDATE {tbl_name} SET {column} = {round(games_per_year, 2)} WHERE PlayerID ='{player}'\")\n            cursor.execute(f\"UPDATE {tbl_name} SET careerLength = {career_length} WHERE PlayerID ='{player}'\")\n            # UPDATE table_name SET column1 = value1, column2 = value2, ...WHERE condition;\n        except KeyError:\n            continue\n\n\ndef main():\n    # loadBaseballData()  # function calls webscraping py files, will take a little while to complete\n\n    cursor, conn = connect_to_SQL()\n    createBaseballDB(cursor, \"baseballStats_db\")\n    name_fields, player_bio_fields, hall_of_fame_fields, \\\n    career_batting_stats_fields, career_pitching_stats_fields = createDBFields()\n\n    # Player Names Table\n    createTable(cursor, name_fields, 'PlayerNames')\n    player_names_dict = getPlayerNamesDictionary('playerinformation/playerBios.csv')\n    loadPlayerNamesTable(cursor, player_names_dict, 'PlayerNames')\n    print(\"PlayerNames Table Loaded...\")\n\n    # Player Bios Table\n    createTable(cursor, player_bio_fields, 'PlayerBios')\n    player_bio_dict = getPlayerBiosDictionary('playerinformation/playerBios.csv')\n    loadPlayerBiosTable(cursor, player_bio_dict, 'PlayerBios')\n    print(\"PlayerBios Table Loaded...\")\n\n    # Hall Of Fame Table\n    createTable(cursor, hall_of_fame_fields, 'HallOfFame')\n    hall_of_fame_dict = getHallOfFamePlayersDictionary('awards/The Hall of Fame.csv', player_names_dict)\n    loadHOFTable(cursor, hall_of_fame_dict, 'HallOfFame')\n    print(\"HallOfFame Table Loaded...\")\n\n    # All Time Batting Leaders Table\n    all_time_batting_dirs = sorted(getDataDirectories('battingstats/careerleaders/'))\n    batting_string = \"Batting\"\n    loadAllTimeLeaders(all_time_batting_dirs, player_names_dict, batting_string, cursor)\n    print(\"AllTimeBattingLeaders Tables Loaded...\")\n\n    # All Time Pitching Leaders Table\n    all_time_pitching_dirs = sorted(getDataDirectories('pitchingstats/careerleaders/'))\n    pitching_string = \"Pitching\"\n    loadAllTimeLeaders(all_time_pitching_dirs, player_names_dict, pitching_string, cursor)\n    print(\"AllTimePitchingLeaders Table Loaded...\")\n\n    # Career Batting Stats for All Players Table\n    # webscrapeCareerStatsForEachPlayer(player_names_dict)\n    createTable(cursor, career_batting_stats_fields, 'CareerBattingStats')\n    career_batting_stats = getCareerStatsForPlayersDictionary('playerinformation/batting_stats.csv')\n    loadCareerStatsTables(cursor, career_batting_stats, 'CareerBattingStats')\n    print(\"CareerBattingStats Table Loaded...\")\n\n    # Career Pitching Stats for All Players Table\n    createTable(cursor, career_pitching_stats_fields, 'CareerPitchingStats')\n    career_pitching_stats = getCareerStatsForPlayersDictionary('playerinformation/pitching_stats.csv')\n    loadCareerStatsTables(cursor, career_pitching_stats, 'CareerPitchingStats')\n    print(\"CareerPitchingStats Table Loaded...\")\n\n    addColumns(cursor, 'AVGGamesPerYear', 'FLOAT', 'CareerBattingStats', player_bio_dict, career_batting_stats)\n    addColumns(cursor, 'AVGGamesPerYear', 'FLOAT', 'CareerPitchingStats', player_bio_dict, career_pitching_stats)\n    conn.commit()\n\n\nif __name__ == '__main__':\n    main()\n", "154": "from requests_html import HTMLSession\nimport gc\n\n\ndef webscrape(ticker: str, top_results=False):\n    session = HTMLSession()\n    url = f\"https://twitter.com/search?q=%24{ticker}%20lang%3Aen&src=typed_query\"\n    if not top_results:\n        url += \"&f=live\"\n\n    r = session.get(url)\n    r.html.render(sleep=1, keep_page=True, scrolldown=1)\n    session.close()\n\n    result = []\n    tweets = r.html.find(\"article\")  # A tweet object currently is the whole tweet textbox, including username/handle\n\n    # Loop through each tweet and extract ONLY the text that is the actual tweet, not the handles or anything else\n    for tweet in tweets:\n        textbox = tweet.find(\"div\")\n        for div in textbox:\n            if \"lang\" in div.attrs:  # The  with the \"lang\" class is the actual tweet, other  elements are\n                # irrelevant\n                result.append(div.text)\n    # Returns a list of  tweets\n    return result\n\n\nif __name__ == \"__main__\":\n    test = webscrape(\"IFF\")\n    print(test)\n\n", "155": "import webscrape\r\nimport string\r\n\r\n\r\ndef main():\r\n\r\n    base_url = 'https://eoddata.com/stocklist/OTCBB/'\r\n\r\n    alphabet_string = string.ascii_uppercase\r\n    alphabet_list = list(alphabet_string)\r\n    data_set = {}\r\n\r\n    for i in alphabet_list:\r\n        url = base_url + i + '.htm'\r\n        page = webscrape.LinkParser(url)\r\n        data_set[i] = page.gather_page_data()\r\n\r\n    return data_set\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    print(main())\r\n", "156": "from datetime import datetime, timedelta\nimport pickle\nimport os\n\nimport boto3\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport pytest_mock\nimport requests\nimport sqlite3\n\n# import moto\nfrom nba_bbref_webscrape import *\n## Testing transformation functions from utils.py with custom csv + pickle object fixtures with edge cases\n\n# mock s3 / mock ses\n@pytest.fixture(scope=\"function\")\ndef aws_credentials():\n    \"\"\"Mocked AWS Credentials for moto.\"\"\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\"\n    os.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\"\n    os.environ[\"AWS_SESSION_TOKEN\"] = \"testing\"\n    os.environ[\"USER_EMAIL\"] = \"jyablonski9@gmail.com\"\n\n\n@pytest.fixture(scope=\"session\")\ndef setup_database():\n    \"\"\" Fixture to set up an empty in-memory database \"\"\"\n    conn = sqlite3.connect(\":memory:\")\n    yield conn\n\n\n@pytest.fixture(scope=\"session\")\ndef player_stats_data():\n    \"\"\"\n    Fixture to load player stats data from a csv file for testing.\n    \"\"\"\n    fname = os.path.join(\n        os.path.dirname(__file__), \"fixture_csvs/player_stats_data.csv\"\n    )\n    stats = pd.read_csv(fname)\n    stats = get_player_stats_transformed(stats)\n    return stats\n\n\n@pytest.fixture(scope=\"session\")\ndef boxscores_data():\n    \"\"\"\n    Fixture to load boxscores data from a csv file for testing.\n    \"\"\"\n    fname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/boxscores_data.csv\")\n    df = pd.read_csv(fname)\n    day = (datetime.now() - timedelta(1)).day\n    month = (datetime.now() - timedelta(1)).month\n    year = (datetime.now() - timedelta(1)).year\n    season_type = \"Regular Season\"\n    df = get_boxscores_transformed(df)\n    return df\n\n\n@pytest.fixture(scope=\"session\")\ndef opp_stats_data():\n    \"\"\"\n    Fixture to load team opponent stats data from a csv file for testing.\n    \"\"\"\n    fname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/opp_stats_data.csv\")\n    df = pd.read_csv(fname)\n    df = get_opp_stats_transformed(df)\n    return df\n\n\n@pytest.fixture(scope=\"session\")\ndef injuries_data():\n    \"\"\"\n    Fixture to load injuries data from a csv file for testing.\n    \"\"\"\n    fname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/injuries_data.csv\")\n    df = pd.read_csv(fname)\n    df = get_injuries_transformed(df)\n    return df\n\n\n@pytest.fixture(scope=\"session\")\ndef transactions_data():\n    \"\"\"\n    Fixture to load transactions data from a csv file for testing.\n    \"\"\"\n    fname = os.path.join(\n        os.path.dirname(__file__), \"fixture_csvs/transactions_data.csv\"\n    )\n    transactions = pd.read_csv(fname)\n    transactions = get_transactions_transformed(transactions)\n    return transactions\n\n\n@pytest.fixture(scope=\"session\")\ndef advanced_stats_data():\n    \"\"\"\n    Fixture to load team advanced stats data from a csv file for testing.\n    \"\"\"\n    fname = os.path.join(\n        os.path.dirname(__file__), \"fixture_csvs/advanced_stats_data.csv\"\n    )\n    df = pd.read_csv(fname)\n    df = get_advanced_stats_transformed(df)\n    return df\n\n\n@pytest.fixture(scope=\"session\")\ndef shooting_stats_data():\n    \"\"\"\n    Fixture to load shooting stats data from a csv file for testing.\n    \"\"\"\n    fname = os.path.join(\n        os.path.dirname(__file__), \"fixture_csvs/shooting_stats_data.csv\"\n    )\n    shooting_stats = pd.read_csv(fname)\n    shooting_stats = get_shooting_stats_transformed(shooting_stats)\n    return shooting_stats\n\n\n# has to be pickle bc odds data can be returned in list of 1 or 2 objects\n@pytest.fixture(scope=\"session\")\ndef odds_data():\n    \"\"\"\n    Fixture to load odds data from a csv file for testing.\n    \"\"\"\n    fname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/odds_data\")\n    with open(fname, \"rb\") as fp:\n        df = pickle.load(fp)\n    day = (datetime.now() - timedelta(1)).day\n    month = (datetime.now() - timedelta(1)).month\n    year = (datetime.now() - timedelta(1)).year\n    df = get_odds_transformed(df)\n    return df\n\n\n@pytest.fixture(scope=\"session\")\ndef pbp_transformed_data():\n    \"\"\"\n    Fixture to load boxscores data from a csv file for PBP Transform testing.\n    \"\"\"\n    fname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/pbp_data.csv\")\n    boxscores = pd.read_csv(fname, parse_dates=[\"date\"])\n    pbp_transformed = get_pbp_data_transformed(boxscores)\n    return pbp_transformed\n\n\n@pytest.fixture(scope=\"session\")\ndef logs_data():\n    \"\"\"\n    Fixture to load dummy error logs for testing\n    \"\"\"\n    df = pd.DataFrame({\"errors\": \"Test... Failure\"})\n    return df\n\n\n@pytest.fixture()\ndef schedule_data(mocker):\n    \"\"\"\n    Fixture to load schedule data from an html file for testing.\n    *** THIS WORKS FOR ANY REQUESTS.GET MOCKING IN THE FUTURE ***\n    \"\"\"\n    fname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/schedule.html\")\n    with open(fname, \"rb\") as fp:\n        mock_content = fp.read()\n\n    # IT WORKS\n    # you have to first patch the requests.get response, and subsequently the return value of requests.get(url).content\n    mocker.patch(\"nba_bbref_webscrape.schedule_functions.requests.get\").return_value.content = mock_content\n\n    schedule = schedule_scraper(\"2022\", [\"february\", \"march\"])\n    return schedule\n\n\n@pytest.fixture()\ndef reddit_comments_data(mocker):\n    \"\"\"\n    Fixture to load reddit_comments data from a csv file for testing.\n    \"\"\"\n    fname = os.path.join(\n        os.path.dirname(__file__), \"fixture_csvs/reddit_comments_data.csv\"\n    )\n    with open(fname, \"rb\") as fp:\n        reddit_comments_fixture = pd.read_csv(\n            fname, index_col=0\n        )  # literally fuck indexes\n\n    # mock a whole bunch of praw OOP gahbage\n    mocker.patch(\"nba_bbref_webscrape.reddit_functions.praw.Reddit\").return_value = 1\n    mocker.patch(\"nba_bbref_webscrape.reddit_functions.praw.Reddit\").return_value.submission = 1\n    mocker.patch(\n        \"nba_bbref_webscrape.reddit_functions.praw.Reddit\"\n    ).return_value.submission.comments.list().return_value = 1\n    mocker.patch(\"nba_bbref_webscrape.reddit_functions.pd.DataFrame\").return_value = reddit_comments_fixture\n\n    reddit_comments_data = get_reddit_comments([\"fake\", \"test\"])\n    return reddit_comments_data\n\n\n@pytest.fixture()\ndef twitter_stats_data(mocker):\n    fname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/nba_tweets.csv\")\n    twitter_csv = pd.read_csv(fname)\n\n    df = mocker.patch(\n        \"nba_bbref_webscrape.twitter_functions.pd.read_csv\"\n    )  # mock the return value for the csv to use my fixture\n    df.return_value = twitter_csv\n\n    twint_mock = mocker.patch(\n        \"nba_bbref_webscrape.twitter_functions.twint.run.Search\"\n    )  # mock the twitter scrape so it doesnt run\n    twint_mock.return_value = 1\n    twitter_data = scrape_tweets(\"nba\")\n    return twitter_data\n\n\n@pytest.fixture(scope=\"session\")\ndef clean_player_names_data():\n    df = pd.DataFrame(\n        {\n            \"player\": [\n                \"Marcus Morris Sr.\",\n                \"Kelly Oubre Jr.\",\n                \"Gary Payton II\",\n                \"Robert Williams III\",\n                \"Lonnie Walker IV\",\n            ]\n        }\n    )\n    df = clean_player_names(df)\n    return df\n\n\n##### NEW TESTS\n\n# @pytest.fixture()\n# def player_stats_html_get(mocker):\n#     fname = os.path.join(os.path.dirname(__file__), \"fixture_csvs/stats_html.html\")\n#     with open(fname, \"rb\") as fp:\n#         html_data = fp.read()\n\n#     html = mocker.patch(\"utils.requests.get\")\n#     html.content = PropertyMock(return_value = html_data)\n#     html.return_value = html_data\n\n#     stats_html_get = get_player_stats_data()\n#     return stats_html_get\n\n# @pytest.fixture(scope=\"session\")\n# def schedule_upsert_data_1():\n#     \"\"\"\n#     Fixture to load team advanced stats data from a csv file for testing.\n#     \"\"\"\n#     fname = os.path.join(\n#         os.path.dirname(__file__), \"fixture_csvs/schedule_upsert_data.csv\"\n#     )\n#     df = pd.read_csv(fname, nrows = 2)\n#     return df\n\n# @pytest.fixture(scope=\"session\")\n# def schedule_upsert_data_2():\n#     \"\"\"\n#     Fixture to load team advanced stats data from a csv file for testing.\n#     \"\"\"\n#     fname = os.path.join(\n#         os.path.dirname(__file__), \"fixture_csvs/schedule_upsert_data.csv\"\n#     )\n#     df = pd.read_csv(fname, skiprows = [1, 2])\n#     return df\n", "157": "from django.shortcuts import render\nfrom django.http import HttpResponse\nfrom .script import webScrape\n\n# Create your views here.\ndef index(request):\n    context = {}\n    return render(request, 'core/index.html', context)\n\ndef about(request):\n    context = {}\n    return render(request, 'core/about.html', context)\n\ndef results(request):\n    if request.method == \"POST\":\n        query = \"\"\n        request.session['player_name'] = request.POST['player_name']\n        query += \" \" + request.POST['player_name']\n\n        query += \" \" + request.POST['card_keyword']\n        request.session['keyword'] = request.POST['card_keyword']\n\n        query += \" \" + request.POST['card_brand']\n        request.session['brand'] = request.POST['card_brand']\n\n        query += \" \" + request.POST['card_year']\n        request.session['year'] = request.POST['card_year']\n        \n        request.session['exclude'] = request.POST['exclude']\n        if request.POST['exclude']:\n            for term in request.POST['exclude'].split():\n                query += \" -\" + term\n\n        request.session['card_psa'] = request.POST.get('card_psa', '')\n        psa = request.POST.get('card_psa', '')\n        if (psa != ''):\n            query += \" \" + psa\n        \n        query = query.replace(\"  \", \" \")\n        query = query.replace(\" \", \"+\")\n        data = webScrape(query)\n        #print(data)\n        if data:\n            request.session['img_url'] = data[0]\n            request.session['avg_price'] = data[1]\n            request.session['ebay_listings'] = data[2]\n        else:\n            request.session['img_url'] = ''\n            request.session['avg_price'] = ''\n            request.session['ebay_listings'] = ''\n\n    context = {}\n    #print(query)\n    return render(request, 'core/results.html', context)", "158": "import matplotlib.pyplot as plt\nimport random\nimport webscrape\nimport alpaca\nimport td_ameritrade\n\n\n# Chad Mode\ndef chad(names, counts):\n    #print(31231)\n    #print(names)\n    # print(len(names))\n    for i in range(len(names)):\n        alpaca.create_order(names[i], random.randint(2, 10))\n        #print(\"hi\")\n    # print(3213213)\n\n# Gambler Mode\ndef gambler(names, platform_mode = \"alpaca\"):\n    random_idx = random.randint(0, len(names))\n    max_quantity = 0\n    if platform_mode == \"alpaca\":\n        alpaca.create_order(names[random_idx], 1000)\n    if platform_mode == \"TD\":\n        REFRESH_TOEKN=input(\"REFRESH_TOEKN\")\n        CONSUMER_KEY=input(\"CONSUMER_KEY\")\n        ACCOUNT_ID=input(\"ACCOUNT_ID\")\n        td_ameritrade.buyyyy(names)\n\n# Danger Mode\ndef danger():\n    gambler(\"TD\")\n    pass\n\n\n\ntop_tickers = webscrape.ticker_count()\n# print(top_tickers)\nshow_graph = input(\"Want to see today's pretty graph? Y or N\")\nif show_graph == \"Y\":\n    final_ticker, final_count = webscrape.show_pretty_graph(top_tickers)\ntotal_sum = sum(final_count)\nprint(final_ticker)\n# while True:\nmode = input(\"Enter your mode on how to lose money:\\n1. Chad Mode (enter 1)\\n2. Gambler Mode (enter 2)\\n3. DANGER MODE (enter 3)\")\n\nif mode == '1':\n    chad(final_ticker, final_count)\nif mode == '2':\n    gambler(final_ticker, final_count)\nif mode == '3':\n    danger(final_ticker, final_count)\n\n\nmode = input(\"Enter your mode on how to lose money:\\n1. Chad Mode (enter 1)\\n2. Gambler Mode (enter 2)\\n3. DANGER MODE (enter 3)\")\n\nif mode == '1':\n    chad(final_ticker, final_count)\nif mode == '2':\n    gambler(final_ticker, final_count)\nif mode == '3':\n    danger(final_ticker, final_count)\n\n\nmode = input(\"Enter your mode on how to lose money:\\n1. Chad Mode (enter 1)\\n2. Gambler Mode (enter 2)\\n3. DANGER MODE (enter 3)\")\n\nif mode == '1':\n    chad(final_ticker, final_count)\nif mode == '2':\n    gambler(final_ticker, final_count)\nif mode == '3':\n    danger(final_ticker, final_count)\n\n", "159": "# -*- coding: utf-8 -*-\n\n# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://doc.scrapy.org/en/latest/topics/settings.html\n#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n#Export as CSV Feed\nFEED_FORMAT = \"csv\"\nFEED_URI = \"bananarepublic.csv\"\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://doc.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'webscrape.pipelines.WebscrapePipeline': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://doc.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "160": "from flask import Flask\r\nfrom flask import render_template,request\r\nimport t5_textsumm as t5\r\nimport livemint as lm\r\napp = Flask(__name__)\r\n\r\n@app.route(\"/\",methods=[\"GET\",\"POST\"])\r\ndef index():\r\n    if request.method == \"GET\":\r\n        return render_template('index.html')\r\n    if request.method == \"POST\":\r\n        if request.form['submit_button'] == \"webscrape\":\r\n            lm.webscrape()\r\n            return render_template('index.html')\r\n        elif request.form['submit_button'] == \"generate_summary\":\r\n            summary_list = t5.generate_summary()\r\n            return render_template('index.html',contacts = summary_list)\r\n        else:\r\n            pass\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    app.run(debug=True)", "161": "\"\"\"\nWebscrape mountain statistics from JollyTurns\n\n1. Request all mountain data\n2. Format/ preprocess data\n3. Save data to CSV\n\"\"\"\n\nimport os\nimport time\nimport warnings\nfrom datetime import date\n\nimport numpy as np\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom tqdm import tqdm\n\nfrom src.webscrape_trails import WebscrapeTrails\n\nwarnings.filterwarnings(\"ignore\")\n\n\nclass MakeMountainDF:\n    def __init__(self):\n\n        self.CURRENT_DIRECTORY = os.getcwd()\n\n        self.browser_options = webdriver.ChromeOptions()\n        self.browser_options.add_argument(\"--no-sandbox\")\n        self.browser_options.add_argument(\"--headless\")\n        self.browser_options.add_argument(\"--disable-gpu\")\n\n        self.browser = webdriver.Chrome(options=self.browser_options)\n\n        # 2020 ticket prices, fetched manually\n        self.dict_resort_prices = {\n            \"Alpine Meadows\": 169,\n            \"Arapahoe Basin\": 109,\n            \"Aspen Snowmass\": 179,\n            \"Bald Mountain\": 145,\n            \"Beaver Creek\": 209,\n            \"Copper\": 119,\n            \"Crested Butte\": 129,\n            \"Diamond Peak\": 104,\n            \"Eldora\": 140,\n            \"Jackson Hole\": 165,\n            \"Loveland\": 89,\n            \"Monarch\": 94,\n            \"Steamboat\": 199,\n            \"Taos\": 110,\n            \"Telluride\": 149,\n            \"Vail\": 209,\n            \"Winter Park\": 139,\n            \"Wolf Creek\": 76,\n        }\n\n    def get_mountain_data(self, URL: str) -> pd.core.frame.DataFrame:\n        \"\"\"\n        Inputs:\n            URL from URLs (str)\n        Outputs:\n            Pandas DataFrame of ski resort information\n        \"\"\"\n\n        self.browser.get(URL)\n\n        time.sleep(3)\n\n        soup = BeautifulSoup(self.browser.page_source, \"html.parser\")\n\n        # JollyTurns parsing (runs breakdown)\n        X_runs = soup.select(\n            \"resort-glance div.row div.col-xs-12 div.row.text-left.statistics.ng-scope span.ng-binding\"\n        )\n        lst_runs = [run.text for run in X_runs]\n        lst_runs = [run.replace(\" ski runs: \", \"\") for run in lst_runs]\n        df_ski_runs = pd.DataFrame({\"Runs\": lst_runs[0::2], \"total\": lst_runs[1::2]})\n        df_ski_runs = df_ski_runs.set_index(\"Runs\").T.reset_index(drop=True)\n\n        # JollyTurns parsing (Chairlifts / total runs)\n        X_lifts = soup.select(\"div.content-in-circle\")\n        lst_lifts = [lift.text.lstrip() for lift in X_lifts]\n        df_lifts = pd.DataFrame({\"Lifts\": lst_lifts[0]}, index=[0])\n\n        # JollyTurns parsing (Elevations)\n        X_elevations = soup.select(\"resort-glance div.row div.col-xs-12 table tr td\")\n        lst_elevations = [\n            elevation.text for elevation in X_elevations if \"Lift\" not in elevation.text\n        ]\n        lst_elevations = [\n            elevation.replace(\" \\xa0\", \"\") for elevation in lst_elevations\n        ]\n        lst_elevations = [elevation.replace(\" ft\", \"\") for elevation in lst_elevations]\n        lst_elevations = [elevation.replace(\":\", \"\") for elevation in lst_elevations]\n\n        df_elevations = pd.DataFrame(\n            {\"Elevation\": lst_elevations[0::2], \"Total\": lst_elevations[1::2]}\n        )\n        df_elevations = df_elevations.set_index(\"Elevation\").T.reset_index(drop=True)\n\n        # Combine total runs, total lifts, and elevation data\n        df_ski = pd.concat([df_ski_runs, df_lifts, df_elevations], axis=1)\n\n        df_ski[\"URL\"] = URL\n\n        return df_ski\n\n    def format_mountain_data_frame_values(\n        self, df: pd.core.frame.DataFrame\n    ) -> pd.core.frame.DataFrame:\n        \"\"\"\n        Pivot DataFrame, and format values\n\n        Input\n            df: Pandas DataFrame\n\n        Output\n            Formatted Pandas DataFrame\n        \"\"\"\n\n        lst_columns = [\n            \"Top\",\n            \"Base\",\n            \"Lifts\",\n            \"Vertical rise\",\n            \"black\",\n            \"blue\",\n            \"double black\",\n            \"green\",\n            \"terrain park\",\n        ]\n\n        df[lst_columns] = df[lst_columns].fillna(0)\n\n        df[lst_columns] = df[lst_columns].astype(\"int\")\n\n        return df\n\n    def save_mountain_data(\n        self, df: pd.core.frame.DataFrame\n    ) -> pd.core.frame.DataFrame:\n        \"\"\"\n        Save formatted mountain data to Parquet file\n        \"\"\"\n\n        current_date = date.today().strftime(\"%Y%m%d\")\n\n        df.to_parquet(\n            f\"{self.CURRENT_DIRECTORY}/data/mountain_data_{current_date}.parquet\",\n            index=False,\n        )\n\n\nif __name__ == \"__main__\":\n\n    mountain = MakeMountainDF()\n\n    ws = WebscrapeTrails()\n\n    # Request mountain data from all resorts\n    lst_mountain_data = [mountain.get_mountain_data(URL=url) for url in tqdm(ws.URLs)]\n\n    # Combine mountain data\n    df_mountain = pd.concat(lst_mountain_data).reset_index(drop=True)\n\n    df_mountain = ws.rename_resorts(df=df_mountain)\n\n    # Fill prices\n    df_mountain[\"Price\"] = df_mountain[\"Resort\"].map(mountain.dict_resort_prices)\n\n    # Convert column data types\n    df_mountain = mountain.format_mountain_data_frame_values(df=df_mountain)\n\n    # Convert total runs to percentage of total runs per resort\n    lst_run_types = [\"black\", \"blue\", \"double black\", \"green\", \"terrain park\"]\n    df_mountain[\"Total Runs\"] = df_mountain[lst_run_types].sum(axis=1)\n    df_mountain[lst_run_types] = (\n        df_mountain[lst_run_types]\n        .div(df_mountain[\"Total Runs\"], axis=0)\n        .mul(100)\n        .round()\n        .astype(int)\n    )\n\n    # Rename columns\n    df_mountain.rename(\n        columns={\n            \"Vertical rise\": \"Vertical Rise (ft)\",\n            \"black\": \"Percent Blacks\",\n            \"blue\": \"Percent Blues\",\n            \"double black\": \"Percent Double Blacks\",\n            \"green\": \"Percent Greens\",\n            \"terrain park\": \"Percent Terrain Parks\",\n        },\n        inplace=True,\n    )\n\n    # Save data\n    # mountain.save_mountain_data(df=df_mountain)\n", "162": "import WebScrape\nimport main as M\n\n\ndef changeProviders(currentProvider):\n    if currentProvider in M.providerList:\n        M.providerList.remove(currentProvider)\n        if M.providerList:\n            nextProvider = M.providerList[0]\n            pageNumber = '0'\n            WebScrape.loopThroughScrapePages(nextProvider, pageNumber)\n        else:\n            return\n\n\ndef getProviderNames(service):\n    if service == '':\n        service = 'Netflix'\n\n    elif service == 'disney-plus/':\n        service = 'Disney Plus'\n\n    elif service == 'hulu/':\n        service = 'Hulu'\n\n    elif service == 'hbo-max/':\n        service = 'HBO Max'\n\n    elif service == 'amazon-prime-video/':\n        service = 'Amazon Prime'\n    return service\n\n\ndef containsAny(sArg, charSet):\n    # Check whether sequence str contains ANY of the items in set.\n    return 1 in [c in sArg for c in charSet]\n", "163": "from webscrape import scrape\nimport urllib.parse\nimport urllib.request\n\nclass Uniprot():\n    '''\n    TODO: Add class and method documentation\n    '''\n    def __init__(self,gene_name=None,gene_entry=None,organism=\"HUMAN\"):\n        self.__gene_entry = gene_entry\n        self.__gene_name = gene_name\n        self.__organism = organism\n\n\n    def protein_entries(self):\n        data = urllib.parse.urlencode({\n            'from': 'GENENAME',\n            'to': 'ID',\n            'format': 'list',\n            'query': self.__gene_name,\n            }).encode('utf-8')\n\n        req = urllib.request.Request('https://www.uniprot.org/uploadlists/', data)\n        with urllib.request.urlopen(req) as f:\n           response = f.read()\n        return [r for r in response.decode('utf-8').split() if self.__organism in r]\n\n    def protein_function(self):\n        '''TODO: Override this webscrape with uniprot API support'''\n        items = scrape('https://www.uniprot.org/uniprot/'+self.__gene_entry,'span')\n        return str(items[46])[22:str(items[46]).find('", "164": "# -*- mode: python ; coding: utf-8 -*-\n\nblock_cipher = None\n\n\na = Analysis(['Webscrape.py'],\n             pathex=['C:\\\\Users\\\\albcy\\\\PycharmProjects\\\\untitled3\\\\venv\\\\Webscrape'],\n             binaries=[],\n             datas=[],\n             hiddenimports=[],\n             hookspath=[],\n             runtime_hooks=[],\n             excludes=[],\n             win_no_prefer_redirects=False,\n             win_private_assemblies=False,\n             cipher=block_cipher,\n             noarchive=False)\npyz = PYZ(a.pure, a.zipped_data,\n             cipher=block_cipher)\nexe = EXE(pyz,\n          a.scripts,\n          a.binaries,\n          a.zipfiles,\n          a.datas,\n          [],\n          name='Webscrape',\n          debug=False,\n          bootloader_ignore_signals=False,\n          strip=False,\n          upx=True,\n          upx_exclude=[],\n          runtime_tmpdir=None,\n          console=False )\n", "165": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"National Weather Service(NWS) website scraping \nfor river guage observations and forecasts.\nThis module will create local database of readings and forecasts.\nA single database will contain the readings from any guages of interest.\nKeys are the guage name + date of webscrape.\n    Data is a dict of webscrape times in UTC. \n        Webscrape times contain a dict of NWS reported times in UTC.\n            Data is guagename and details, \n                    observed/forecast tag, \n                    NWS:UTC timestamp for value.\n\"\"\"\n\nfrom pathlib import Path\nfrom time import sleep\n# from pupdb.core import PupDB\n\n# from datetime import *\nfrom dateutil.parser import *\nfrom pytz import utc as pytzutc\n\n\n# used to standardize string formats across modules\nfrom time_strings import LOCAL_CURRENT_YEAR, UTC_NOW, LOCAL_TODAY\nfrom time_strings import timefstring\n\nfrom loguru import logger\n# from tabulate import tabulate\n\n\nLOGGING_LEVEL = \"INFO\"\nRUNTIME_NAME = Path(__file__).name\nfrom core_logging_setup import defineLoggers\n\nTS_LABEL_STR = \"timestamp\" # for use in generating data and indexing result\n\nfrom WebScrapeTools import retrieve_cleaned_html\nfrom RiverGuages import RIVER_MONITORING_POINTS, RIVER_GUAGES, GUAGE_NAME_KEY\nfrom data_2_csv import write_csv\n\n# National Weather Service does not put the YEAR into the tabular data of their website\n# we must declare the current year here.\n\n\n@logger.catch\ndef get_NWS_web_data(site):\n    \"\"\"Retrieve data from National Weather Service website. Provided 'site' variable\n    should point to a page that provides tabular river data. This function returns\n    the contents of the site and a timestamp of the actual website scraping event.\n    \"\"\"\n    start_time = UTC_NOW()\n    clean_soup = retrieve_cleaned_html(site)\n    finish_time = UTC_NOW()\n    elapsed_time = finish_time - start_time\n    guage_id = clean_soup.h1[\"id\"]\n    guage_string = clean_soup.h1.string\n    nws_class = clean_soup.find(class_=\"obs_fores\")\n    nws_obsfores_contents = nws_class.contents\n    return (nws_obsfores_contents, guage_id, guage_string, start_time, elapsed_time)\n\n\n@logger.catch\ndef FixDate(s, currentyear, time_zone=pytzutc):\n    \"\"\"Split date from time and add timezone label.\n    Unfortunately, NWS chose not to include the year. \n    This will be problematic when forecast dates are into the next year.\n    If Observation dates are in December, Forecast dates must be checked \n    and fixed for roll over into next year.\n    \"\"\"\n    # TODO check and fix end of year forecast dates\n\n    p = parse(s) # parse is returning a timezone-naive datetime obj \n    # additionally, parse is appending the current year into the object\n    aware = p.replace(tzinfo=time_zone)\n\n    return timefstring(aware)\n\n\n@logger.catch\ndef sort_and_label_data(web_data, guage_details, time):\n    \"\"\"Returns a list of dicts containing relevant data from webscrape.\n    NWS results are in UTC timezone for both observations and forecasts.\n    Results are stored to CSV file with all timestamps in UTC.\n    \"\"\"\n\n    readings = []\n    LCY = LOCAL_CURRENT_YEAR()\n    guage_id, elev, milemarker, _ = guage_details\n    relevant_labels = [TS_LABEL_STR, \"level\", \"flow\"]\n    for i, item in enumerate(web_data):\n        if i >= 1:  # zeroth item is an empty list\n            # locate the name of this section (observed / forecast)\n            section = item.find(class_=\"data_name\").contents[0]\n            sect_name = section.split()[0]\n            row_dict = {}\n            # extract all readings from this section\n            section_data_list = item.find_all(class_=\"names_infos\")\n            # organize the readings and add details\n            for i, data in enumerate(section_data_list):\n                element = data.contents[0]\n                pointer = i % 3  # each reading contains 3 unique data points\n                label = relevant_labels[pointer]\n                if pointer == 0:  # this is the element for date/time\n                    element = FixDate(element, LCY )\n                row_dict[label] = element\n                if pointer == 2:  # end of this reading\n                    row_dict[\"guage\"] = guage_id\n                    row_dict[\"scrape time\"] = time\n                    row_dict[\"elevation\"] = elev\n                    row_dict[\"milemarker\"] = milemarker\n                    row_dict[\"type\"] = sect_name             \n                    readings.append(row_dict)  # add to the compilation\n                    # reset the dict for next reading\n                    row_dict={}\n\n    return readings\n\n\n@logger.catch\ndef generate_keys_based_on_timestamp(web_list):\n    \"\"\"Take a list of dicts and return a key for each dict in list.\n    \"\"\"\n    keys = []\n\n    for itm in web_list:\n        cdt = itm[TS_LABEL_STR]\n        key = f\"{cdt}\"\n        keys.append(key)\n    return keys\n\n\n@logger.catch\ndef decode_database_key(s):\n    \"\"\"Extract Guage_id, Reading_type, Datestr form provided keystring.\n    \"\"\"\n    lst = s.split(\"-\")\n    gid = lst[0]\n    rdng = lst[1]\n    dstr = lst[2]\n    return (gid, rdng, dstr)\n\n\n@logger.catch\ndef Scrape_NWS_site(site):\n    \"\"\"Return a dictionary of guage readings from supplied XML tabular text site.\n    \"\"\"\n    site_url = site[\"guage_URL\"]\n    # get the data for this guage\n    (\n        raw_data,\n        guage_id,\n        friendly_name,\n        scrape_start_time,\n        duration_of_scrape,\n    ) = get_NWS_web_data(site_url)\n    \n    tot_secs = duration_of_scrape.total_seconds()\n    logger.info(f\"Time to process website: {tot_secs} seconds.\")\n    logger.info(f\"Webscrape started at: {scrape_start_time}\")\n    # TODO verify webscraping success\n    guage_data = (guage_id, site[\"guage_elevation\"], site[\"milemarker\"], friendly_name)\n    ValuableData_listOfDicts = sort_and_label_data(raw_data, guage_data, timefstring(scrape_start_time))\n    # TODO verify successful conversion of data\n    database_keys = generate_keys_based_on_timestamp(ValuableData_listOfDicts)\n    # TODO compare length of keys_list to length of data_list for validity\n    database_dict = dict(zip(database_keys, ValuableData_listOfDicts))\n    # TODO compare length of database to data_list to verify all items included\n    return (database_dict, ValuableData_listOfDicts)\n\n\n@logger.catch\ndef display_tabulardata(datalist_of_dicts):\n    \"\"\"create a new file based on the time of scrape\n    \"\"\"\n    #print(tabulate(datalist_of_dicts, headers=\"keys\"))    \n\n\n@logger.catch\ndef save_results_to_storage(list_of_dicts):\n    \"\"\"save unique readings and forecasts to longterm storage.\n    list_of_dicts is expected to contain one dict for each reading/forecast\n    \"\"\"\n    fname = list_of_dicts[0]['scrape time']\n    logger.info(f'Creating CSV filename: {fname}')\n    # TODO organize storage as a tree of directories: YEAR/MONTH/DAY/xx:xx:xx\n    write_csv(list_of_dicts, filename=fname)\n\n\n@logger.catch\ndef update_web_scrape_results():\n    \"\"\"Update filesystem CSV records for latest website scrapes.\n    \"\"\"\n    for guage in RIVER_GUAGES:\n        details = RIVER_MONITORING_POINTS[guage]\n        logger.info(details)\n        dbd, vdl = Scrape_NWS_site(details)\n        display_tabulardata(vdl)\n        save_results_to_storage(vdl)\n        sleep(1) # guarantee at least 1 second difference in webscrapes timestamps\n        # TODO verify webscraping success\n        count = len(dbd)\n        fnme = details[GUAGE_NAME_KEY]\n        logger.info(f\"Total values retrieved: {count} from {fnme}\")\n\n    return True\n\n\n@logger.catch\ndef Main():\n    \"\"\"Update webscrape files.\n    \"\"\"\n    defineLoggers(LOGGING_LEVEL, RUNTIME_NAME)\n    logger.info(\"Program Start.\")\n    today = LOCAL_TODAY() \n    logger.info(f\"Today is: {today}\")\n    \n    update_web_scrape_results()\n\n    return True\n\n\nif __name__ == \"__main__\":\n    Main()\n", "166": "from django.contrib import admin\r\nfrom django.urls import path\r\nfrom web_scrape import views\r\n\r\n\r\napp_name='web_scrape'\r\n\r\nurlpatterns = [\r\n     path('webscrape', views.webscrape, name='webscrape'),\r\n]\r\n", "167": "from lxml import html\nimport requests\n\nclass Webscrape:\n    kjorProg = True\n\n    @staticmethod\n    def webScrape():\n        nettSide = requests.get('https://www.akademika.no/computer-organization-and-architecture-global-edition/william-stallings/9781292096858')\n        tree = html.fromstring(nettSide.content)\n        return tree.xpath('//*[@id=\"node-21225459\"]/div[2]/div[2]/div[1]/span/text()')\n\n\n", "168": "\"\"\"\nfolding@home stats webscraper.\nUses requests-html library to load the Javascript to get the statistics.\nLook at the bottom to see an example of how to use the code.\n\"\"\"\n\nfrom requests_html import HTMLSession\n\n\n# Main webscraping method. Takes in URL of the statistics page\ndef webscrape(url):\n    session = HTMLSession()\n\n    # Start the session, run the javascript and get the HTML\n    r = session.get(url)\n    r.html.render()\n    donor_info = r.html.find(\"#content\", first=True)\n\n    # Split the data into a list. Now every even index (0, 2, 4, ...) should contain a label while every odd index,\n    # (1, 3, 5, ...) should contain the actual data. This will make it easy to work with the list.\n    donor_info_list = donor_info.text.split(\"\\n\")\n\n    # This turns the first item in the list from \"Donor: \" into two items, \"Donor:\" and \".\n    # This is done to keep with the theme of every odd and every even index being a label or data.\n    donor_info_list[0] = donor_info_list[0].split(\" \")[1]\n    donor_info_list.insert(0, \"Donor\")\n\n    # End the session in case the script is run again\n    session = None\n\n    # Return the list\n    return donor_info_list[0:14]\n\n\n# Testing area\nif __name__ == \"__main__\":\n    # Example usage:\n    data = webscrape(\"https://stats.foldingathome.org/donor/1437\")\n    print(data)\n\n    import time\n\n    for i in range(5):\n        time.sleep(1)\n        data = webscrape(\"https://stats.foldingathome.org/donor/1437\")\n        print(data)\n", "169": "# This is a sample Python script.\n\n# Press \u2303R to execute it or replace it with your code.\n# Press Double \u21e7 to search everywhere for classes, files, tool windows, actions, and settings.\n\n# THIS PROGRAM IS ONLY FOR DAZBEE YOUTUBE. (FAN MADE LOL)\n\n# this will check the number of views, likes, and comments.\n# display them as a graph (prob top 20 or something)\n# as y: view, likes, and comments\n# as x: title of the video and if click them, open the link\n# at start: ask the user what graph they want to see.\n# three buttons : views, likes, and comments\n\n# IF POSSIBLE!\n# Or, how many video the creator made by genre\n# (COVERS, collabs, original songs, or special songs)\n# (Also, DAZVillege, too)\n# and display the Namuwiki link\n\n# The main purpose of the program is creating graphs with pandas, matplotlib, and other libraries.\n# THUS, I might not make other setting that not related in it\n# until the program display the graphes(views, likes, comments)\n# * for view, the program will also mention the total view *\n\n# What I need for the graph:\n# Title: Top # (View, Likes) for (Youtube Creators)\n# Y-axis: (# of Views, Likes) - prob bar graph\n#         - Start point: lowest number of views, likes, or comments - 1000, 100, or 50\n# X-axis: (Title of the videos)\n#         If possible, if click the title, go to the link\n\n# Before the graph:\n# ask the user what kind of graph they want to see\n# show three options: Views, Likes, and Comments\n# And one more button (ENTER)\n\n\nimport sqlite3\nimport requests\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nimport threading\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom pylab import *\nfrom collections import namedtuple\n\nimport SaveSQL\nimport GraphCreator\nimport WebScrape\n\ndef print_hi(name):\n    # Use a breakpoint in the code line below to debug your script.\n    print(f'Hi, {name}')  # Press \u2318F8 to toggle the breakpoint.\n\n\n# Press the green button in the gutter to run the script.\nif __name__ == '__main__':\n    a = WebScrape.Webscrape()\n    a.readVideoList()\n    a.sql()\n    a.gr()\n\n# See PyCharm help at https://www.jetbrains.com/help/pycharm/\n", "170": "\"\"\"\nWSGI config for webScrape project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.0/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webScrape.settings')\n\napplication = get_wsgi_application()\n", "171": "\"\"\"\nASGI config for webScrape project.\n\nIt exposes the ASGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.0/howto/deployment/asgi/\n\"\"\"\n\nimport os\n\nfrom django.core.asgi import get_asgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webScrape.settings')\n\napplication = get_asgi_application()\n", "172": "import boto3\ni = 0\ncar = ['audi','acura','bmw','bugatti','cadillac','dodge','ferrari','ford','gmc','honda','hyundai','infiniti','jeep','kia','lexus','lincoln','mazda','mercedes','mitsubishi','nissan','pontiac','porsche','ram','subaru','tesla','toyota','volkswagen']\n\ndef upload():\n    s3 = boto3.resource('s3')\n    bucket = s3.Bucket('de-car-times')\n    s3.Object('de-car-times', f\"0:60-times/{''.join(car[i])}.csv\").put(Body=open(f\"/Users/kordellewalker/Documents/GitHub/DEProjects/Webscrape-DWH/Resultss/{''.join(car[i])}.csv\", 'rb'))\n\n# Uploads all \"car.csv\" files\nwhile i < 26:\n    i += 1\n    upload()\n\n#Uploads only \"Master_table.csv\" file\ns3 = boto3.resource('s3')\nbucket = s3.Bucket('de-car-times')\ns3.Object('de-car-times', \"master_table.csv\").put(Body=open(f\"/Users/kordellewalker/Documents/GitHub/DEProjects/Webscrape-DWH/Resultss/master_table.csv\", 'rb'))", "173": "## This is a piece of a script that helps process data from wine.com and put it into a database table.\n \n page_response = requests.get(page_link, headers=headers, timeout=5)\n    # here, we fetch the content from the url, using the requests library\n    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n    #we use the html parser to parse the url content and store it in a variable.\n    textContent = []\n    # %%\n    # %% [markdown]\n    # # Build and Store Webscrape\n    # I am storing webscraped data in a webscrape table. This isn't an automated scrape, but I still want to put it in there. Also, since this is manually matched\n    # this data can be used in the future to train record matching algos.\n    #\n    if page_content.find(class_='icon icon-screwcap prodAttr_icon prodAttr_icon-screwcap'):\n        closure = 'Screwcap'\n    else:\n        closure = ''\n\n    if page_content.find(class_='icon icon-glass-red prodAttr_icon prodAttr_icon-redWine'):\n        category = 'Red'\n    elif page_content.find(class_='icon icon-glass-white prodAttr_icon prodAttr_icon-whiteWine'):\n        category = 'White'\n    elif page_content.find(class_='icon icon-champagne prodAttr_icon prodAttr_icon-champagne'):\n        category = 'Sparkling'\n    elif page_content.find(class_='icon icon-glass-white prodAttr_icon prodAttr_icon-roseWine'):\n        category = 'Rose'\n    else:\n        category = ''\n\n    try:\n        price = float(re.findall('[0-9.]+', page_content.find(class_='productPrice').text)[0].strip())\n    except AttributeError:\n        price = 0\n    \n    try:\n        msrp = round_up(float(re.findall('[0-9.]+', page_content.find(class_='productPrice_price-regWhole').text)[0].strip()))\n    except AttributeError:\n        msrp = price\n\n    try:\n        vintage = re.findall('[0-9]+', page_content.find(class_='pipName').text)[0].strip()\n    except IndexError:\n        vintage = 0\n\n    # %%\n    insert_webscrape = (\"INSERT INTO webscrapes \"\n                        \"(retail_systemID, is_match, best_match, _web_scraper_order, web_scraper_start_url, scrape_sourceID, sku, product, product_href, title, name, vintage, producer, brand, price, msrp, region, subregion, description, size, alcohol, closure, category, varietal) \"\n                        \"VALUES (%(retail_systemID)s, %(is_match)s, %(best_match)s, %(_web_scraper_order)s, %(web_scraper_start_url)s, %(scrape_sourceID)s, %(sku)s, %(product)s, %(product_href)s, %(title)s, %(name)s, %(vintage)s, %(producer)s, %(brand)s, %(price)s, %(msrp)s, %(region)s, %(subregion)s, %(description)s, %(size)s, %(alcohol)s, %(closure)s, %(category)s, %(varietal)s)\")\n\n    data = {\n        'retail_systemID': system_id,\n        'is_match': 1,\n        'best_match': 1,\n        '_web_scraper_order': system_id,\n        'web_scraper_start_url': page_link,\n        'scrape_sourceID': 7,\n        'sku':  page_content.find(attrs={'name':'productID'})[\"content\"],\n        'product': page_content.find(class_='pipName').text,\n        'product_href': page_link,\n        'title': page_content.find(class_='pipName').text,\n        'name':  re.findall('[A-z ]+', page_content.find(class_='pipName').text)[0].strip(),\n        'vintage': vintage,\n        'producer': page_content.find(class_='pipWinery_headlineLink').text,\n        'brand': page_content.find(class_='pipWinery_headlineLink').text,\n        'price': price,\n        'msrp': msrp,\n        'region': page_content.find(attrs={'name':'productVarietal'})[\"content\"],\n        'subregion':  page_content.find(attrs={'name':'productRegion'})[\"content\"],\n        'description':  page_content.find(class_='viewMoreModule_text').text,\n        'size': page_content.find(class_='prodAlcoholVolume_text').text,\n        'alcohol': page_content.find(class_='prodAlcoholPercent_percent').text,\n        'closure':  closure,\n        'category': category,\n        'varietal': page_content.find(attrs={'name':'productVarietal'})[\"content\"]  \n    }\n\n    cursor.execute(insert_webscrape, data)\n    conn.commit()\n\n    # %% [markdown]\n    # ## Store Reviews and Build Description\n    # \n    # We store reviews in the Reviews table, and use them to create an HTML description to post.\n\n    # %%\n    # Start the description so we can append reviews if any\n    item_content =\"\"\n    item_content = '' + page_content.find(class_='viewMoreModule_text').text\n\n    reviews = page_content.select('div.pipProfessionalReviews_list')\n    # If there are reviews, add a header to the content\n    if reviews:\n        item_content = item_content + 'Tasting Notes'\n    else:\n        item_content = item_content + ''\n\n    for review in reviews:\n        # Writing the HTML is failing, I am sure because I need to escape some things, but it isn't really needed so I am skipping it.\n        insert_review = (\"INSERT INTO wine_reviews \"\n                    \"(source, initials, rating, review_detail, wine_itemID, wine_systemSKU, url) \"\n                    \"VALUES (%(source)s, %(initials)s, %(rating)s, %(review_detail)s, %(wine_itemID)s, %(wine_systemSKU)s, %(url)s)\")\n        data = {\n            'source': review.select('div[class=\"pipProfessionalReviews_authorName\"]')[0].text,\n            'initials': review.select('span[class=\"wineRatings_initials\"]')[0].text,\n            'rating': int(review.select('span[class=\"wineRatings_rating\"]')[0].text),\n            'review_detail': review.select('div[class=\"pipProfessionalReviews_review\"]')[0].text,\n            'wine_itemID': int(item_id),\n            'wine_systemSKU': system_id,\n            'url': page_link\n            \n        }\n        item_content = item_content + \"\" + review.select('span[class=\"wineRatings_rating\"]')[0].text + \" \" + review.select('div[class=\"pipProfessionalReviews_authorName\"]')[0].text + \"\"\n        item_content = item_content + review.select('div[class=\"pipProfessionalReviews_review\"]')[0].text + \"\"\n        cursor.execute(insert_review, data)\n        conn.commit()\n\n", "174": "## This is a piece of a script that helps process data from wine.com and put it into a database table.\n \n page_response = requests.get(page_link, headers=headers, timeout=5)\n    # here, we fetch the content from the url, using the requests library\n    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n    #we use the html parser to parse the url content and store it in a variable.\n    textContent = []\n    # %%\n    # %% [markdown]\n    # # Build and Store Webscrape\n    # I am storing webscraped data in a webscrape table. This isn't an automated scrape, but I still want to put it in there. Also, since this is manually matched\n    # this data can be used in the future to train record matching algos.\n    #\n    if page_content.find(class_='icon icon-screwcap prodAttr_icon prodAttr_icon-screwcap'):\n        closure = 'Screwcap'\n    else:\n        closure = ''\n\n    if page_content.find(class_='icon icon-glass-red prodAttr_icon prodAttr_icon-redWine'):\n        category = 'Red'\n    elif page_content.find(class_='icon icon-glass-white prodAttr_icon prodAttr_icon-whiteWine'):\n        category = 'White'\n    elif page_content.find(class_='icon icon-champagne prodAttr_icon prodAttr_icon-champagne'):\n        category = 'Sparkling'\n    elif page_content.find(class_='icon icon-glass-white prodAttr_icon prodAttr_icon-roseWine'):\n        category = 'Rose'\n    else:\n        category = ''\n\n    try:\n        price = float(re.findall('[0-9.]+', page_content.find(class_='productPrice').text)[0].strip())\n    except AttributeError:\n        price = 0\n    \n    try:\n        msrp = round_up(float(re.findall('[0-9.]+', page_content.find(class_='productPrice_price-regWhole').text)[0].strip()))\n    except AttributeError:\n        msrp = price\n\n    try:\n        vintage = re.findall('[0-9]+', page_content.find(class_='pipName').text)[0].strip()\n    except IndexError:\n        vintage = 0\n\n    # %%\n    insert_webscrape = (\"INSERT INTO webscrapes \"\n                        \"(retail_systemID, is_match, best_match, _web_scraper_order, web_scraper_start_url, scrape_sourceID, sku, product, product_href, title, name, vintage, producer, brand, price, msrp, region, subregion, description, size, alcohol, closure, category, varietal) \"\n                        \"VALUES (%(retail_systemID)s, %(is_match)s, %(best_match)s, %(_web_scraper_order)s, %(web_scraper_start_url)s, %(scrape_sourceID)s, %(sku)s, %(product)s, %(product_href)s, %(title)s, %(name)s, %(vintage)s, %(producer)s, %(brand)s, %(price)s, %(msrp)s, %(region)s, %(subregion)s, %(description)s, %(size)s, %(alcohol)s, %(closure)s, %(category)s, %(varietal)s)\")\n\n    data = {\n        'retail_systemID': system_id,\n        'is_match': 1,\n        'best_match': 1,\n        '_web_scraper_order': system_id,\n        'web_scraper_start_url': page_link,\n        'scrape_sourceID': 7,\n        'sku':  page_content.find(attrs={'name':'productID'})[\"content\"],\n        'product': page_content.find(class_='pipName').text,\n        'product_href': page_link,\n        'title': page_content.find(class_='pipName').text,\n        'name':  re.findall('[A-z ]+', page_content.find(class_='pipName').text)[0].strip(),\n        'vintage': vintage,\n        'producer': page_content.find(class_='pipWinery_headlineLink').text,\n        'brand': page_content.find(class_='pipWinery_headlineLink').text,\n        'price': price,\n        'msrp': msrp,\n        'region': page_content.find(attrs={'name':'productVarietal'})[\"content\"],\n        'subregion':  page_content.find(attrs={'name':'productRegion'})[\"content\"],\n        'description':  page_content.find(class_='viewMoreModule_text').text,\n        'size': page_content.find(class_='prodAlcoholVolume_text').text,\n        'alcohol': page_content.find(class_='prodAlcoholPercent_percent').text,\n        'closure':  closure,\n        'category': category,\n        'varietal': page_content.find(attrs={'name':'productVarietal'})[\"content\"]  \n    }\n\n    cursor.execute(insert_webscrape, data)\n    conn.commit()\n\n    # %% [markdown]\n    # ## Store Reviews and Build Description\n    # \n    # We store reviews in the Reviews table, and use them to create an HTML description to post.\n\n    # %%\n    # Start the description so we can append reviews if any\n    item_content =\"\"\n    item_content = '' + page_content.find(class_='viewMoreModule_text').text\n\n    reviews = page_content.select('div.pipProfessionalReviews_list')\n    # If there are reviews, add a header to the content\n    if reviews:\n        item_content = item_content + 'Tasting Notes'\n    else:\n        item_content = item_content + ''\n\n    for review in reviews:\n        # Writing the HTML is failing, I am sure because I need to escape some things, but it isn't really needed so I am skipping it.\n        insert_review = (\"INSERT INTO wine_reviews \"\n                    \"(source, initials, rating, review_detail, wine_itemID, wine_systemSKU, url) \"\n                    \"VALUES (%(source)s, %(initials)s, %(rating)s, %(review_detail)s, %(wine_itemID)s, %(wine_systemSKU)s, %(url)s)\")\n        data = {\n            'source': review.select('div[class=\"pipProfessionalReviews_authorName\"]')[0].text,\n            'initials': review.select('span[class=\"wineRatings_initials\"]')[0].text,\n            'rating': int(review.select('span[class=\"wineRatings_rating\"]')[0].text),\n            'review_detail': review.select('div[class=\"pipProfessionalReviews_review\"]')[0].text,\n            'wine_itemID': int(item_id),\n            'wine_systemSKU': system_id,\n            'url': page_link\n            \n        }\n        item_content = item_content + \"\" + review.select('span[class=\"wineRatings_rating\"]')[0].text + \" \" + review.select('div[class=\"pipProfessionalReviews_authorName\"]')[0].text + \"\"\n        item_content = item_content + review.select('div[class=\"pipProfessionalReviews_review\"]')[0].text + \"\"\n        cursor.execute(insert_review, data)\n        conn.commit()\n\n", "175": "import scrapy\nfrom items import WebscrapeItem\n\nclass TechSpider(scrapy.Spider):\n    name = 'tech'\n    allowed_domains = ['techcrunch.com']\n    start_urls = ['http://techcrunch.com/']\n\n    def parse(self, response):\n        c=response.css('.content a::attr(href)').getall()\n        for i in c:\n            i=response.urljoin(i)\n            \n            yield scrapy.Request(url=i,callback=self.parse_content)\n\n\n\n    def parse_content(self,response):\n\n        item = WebscrapeItem()\n        Title=response.css('h1.article__title::text').extract()\n        Author=response.css('div.article__byline a::text').extract()\n        post=response.css('div.article-content ::text').extract()\n            \n        item['title']= Title\n        item['author']=Author\n        item['post']=post\n            \n        yield item\n         \n", "176": "import os\nimport pandas as pd\nimport numpy as np\nimport sqlalchemy\nfrom sqlalchemy.ext.automap import automap_base\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import create_engine\nfrom flask import Flask, jsonify, render_template\nimport sqlalchemy\nfrom flask import Flask, render_template, redirect\nfrom flask_pymongo import PyMongo\nimport renewable_scrape\nimport json\n\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\n#################################################\n# Database Setup\n#################################################\nengine = create_engine(\"sqlite:///project.sqlite\")\n\n\n# reflect an existing database into a new model\nBase = automap_base()\n# reflect the tables\nBase.prepare(engine, reflect=True)\n\n# Save reference to the table\nDataset = Base.classes.dataset\n\n\n#################################################\n# Flask Setup\n#################################################\napp = Flask(__name__)\n\nSITE_ROOT = os.path.realpath(os.path.dirname(__file__))\njson_url = os.path.join(SITE_ROOT, \"templates\", \"USA.geojson\")\nheatmapdata = json.load(open(json_url))\nprint(heatmapdata)\n\n#################################################\n# Flask Routes\n#################################################\n\n\n@app.route(\"/\") \ndef welcome():\n    return render_template(\"index.html\")\n\n\n@app.route(\"/hydro\")\ndef hydro():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"hydro.html\")\n\n@app.route(\"/wind\")\ndef wind():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"wind.html\")\n\n\n@app.route(\"/heatmap\")\ndef heatmap():\n    \n    return render_template(\"heatmap.html\")\n\n@app.route(\"/solar\")\ndef solar():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"solar.html\")\n\n@app.route(\"/location\")\ndef location():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"location.html\")\n\n@app.route(\"/webscrape\")\ndef webscrape():\n    data =  json.load(open(\"my_renewables.json\",\"r\")) \n    return render_template(\"webscrape.html\",r_last_refresh=data[\"last_scrape\"],renewable_title_0=data[\"articles \"][0],renewable_link_0=data[\"links\"][0],renewable_title_1=data[\"articles \"][1],renewable_link_1=data[\"links\"][2], renewable_title_2 = data[\"articles \"][2],renewable_link_2=data[\"links\"][4],renewable_title_3=data[\"articles \"][3],renewable_link_3=data[\"links\"][6])\n\n@app.route(\"/scrape\")\ndef scrape():\n    renewable_scrape.renewable_scrape()\n    return redirect(\"/webscrape\")\n\n@app.route(\"/api/heatmap\")\ndef heatmapgeojson():\n    return jsonify(data = heatmapdata)\n\n\n@app.route(\"/data\")\ndef data():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"data.html\")\n\n\nif __name__ == '__main__':\n    app.run(debug=True)\n", "177": "from flask import Flask, render_template, jsonify, request\nfrom flask_cors import CORS, cross_origin\n\nimport json\nimport os\nfrom crawler import *\n\n\n#https://medium.com/@dtkatz/3-ways-to-fix-the-cors-error-and-how-access-control-allow-origin-works-d97d55946d9\n\napp = Flask('Website Timer')\nCORS(app)\napp.config['CORS_HEADERS'] = 'Content-Type'\n\n#make a dictionary of dictionaries\n#d[name][website] = [time1, time2, ...]\n\nd = {}\nwebsites = {}\n\n@app.route('/')\ndef hello_world():\n    return render_template('popup.html')\n\n\n#get -> return data to extension\n#post -> get data from extension\n@app.route('/test', methods = ['GET'])\ndef testfunc():\n\n    print(\"testfunc\")\n    message = {'flask': 'Hello World'}\n    print('sending hello world to extension')\n    return jsonify(message)\n\n\n\n\n@app.route('/saveTime', methods=['POST'])\n@cross_origin()\ndef saveTime():\n\n    print('saving time')\n    data = request.get_json()\n    print(data)\n    if data == {}:\n        return jsonify({'flask': 'nothing received'})\n\n    name = data['username']\n    time = data['time']\n    website = data['website']\n    print(name, time, website)\n    print(d)\n\n\n    #store stats of website into server\n    if website not in websites.keys():\n        websites[website] = webscrape(website)\n\n    #store user data into server\n    if name in d.keys():\n        if website in d[name].keys():\n            d[name][website].append(time)\n        else:\n            d[name][website] = [time]\n    else:\n        d[name] = {}\n        d[name][website] = [time]\n\n\n    #save_file()\n    with open(\"data.json\", \"w\") as f:\n        json.dump(d, f)\n    message = {'flask': 'Time Saved!'}\n    print(d)\n    return jsonify(message)\n\n@app.route('/delTime', methods=['POST'])\n@cross_origin()\ndef deleteTime():\n    print('saving time')\n    data = request.get_json()\n    print(data)\n    if data == {}:\n        return jsonify({'flask': 'nothing received'})\n\n    name = data['username']\n    website = data['website']\n    print(name, website)\n    print(d)\n\n    #store stats of website into server\n    if website not in websites.keys():\n        websites[website] = webscrape(website)\n\n    if name in d.keys():\n        if website in d[name].keys():\n            del d[name][website]\n\n    #save_file()\n    with open(\"data.json\", \"w\") as f:\n        json.dump(d, f)\n    message = {'flask': 'Time Deleted!'}\n    print(d)\n    return jsonify(message)\n\n@app.route('/showTime', methods=['GET', 'POST'])\n@cross_origin()\ndef showTime():\n    print(\"sending Times\")\n    data = request.get_json()\n    print(data)\n    if data == {}:\n        return jsonify({'flask': 'nothing received'})\n    name = data['username']\n    website = data['website']\n    print(name, website)\n    print(d)\n\n    #store stats of website into server\n    if website not in websites.keys():\n        websites[website] = webscrape(website)\n\n    try:\n        temp = sorted(d[name][website])[:10]\n    except KeyError:\n        return jsonify({'flask': 'nothing received'})\n\n    return jsonify({\"time\": temp})\n\n@app.route('/showWords', methods=['GET', 'POST'])\n@cross_origin()\ndef showWords():\n    print(\"showWords\")\n    data = request.get_json()\n    print(data)\n    if data == {}:\n        return jsonify({'flask': 'nothing received'})\n    website = data['website']\n    print(website)\n    print(d)\n\n    #store stats of website into server\n    if website not in websites.keys():\n        websites[website] = webscrape(website)\n\n    temp2 = websites[website]\n    temp2[0] = temp2[0][:10]\n    return jsonify({\"webstats\": temp2})\n\n\nif __name__ == '__main__':\n    #open file\n    if os.stat('data.json').st_size != 0:#check if file exists and is not empty\n        try:\n            with open('data.json') as f:\n                d = json.load(f)\n        except FileNotFoundError:\n            pass\n\n    print(d)\n    app.run(debug = True)", "178": "from django.shortcuts import render\n\n# Create your views here.\nfrom webscrape_app.webscrape_util import instascrape\n\n\ndef search_by_tag_view(request):\n    if request.POST:\n        data = request.POST.get('search_hashtag')\n        instascrape.instascrape.delay(\n            \"/home/emil/Desktop/instagram_influencers_application/webscrape_app/data/influenc_d.json\", 10, data)\n    return render(request, \"input_page.html\")\n", "179": "from contextlib import suppress\nfrom json import JSONDecodeError\n\nfrom requests import HTTPError, RequestException\nfrom tqdm import tqdm\n\nfrom scrape.company_result import CompanyResult\nfrom scrape.configs import JobScrapeConfig\nfrom scrape.web_scraper import webscrape_results\n\n\ndef parse_results(\n    base_url: str, querystring: dict, page: int, config: JobScrapeConfig\n) -> list[CompanyResult]:\n    \"\"\"Takes the params provided in main.py and generates dataclasses for\n    each job listing in BuiltInNYC, the job name, company info, and so forth.\n    \"\"\"\n    company_results = []\n    docs = webscrape_results(base_url, querystring=querystring)\n    jobs = [item.get(\"title\") for item in docs[\"jobs\"]]\n    job_desc = [item.get(\"body\") for item in docs[\"jobs\"]]\n    company_names = [item.get(\"title\") for item in docs[\"companies\"]]\n    alii = [item.get(\"alias\") for item in docs[\"companies\"]]\n    for idx, company in enumerate(\n        tqdm(\n            company_names,\n            desc=f\"Evaluating Companies | Bundle {page} of {config.total_pages}\",\n        )\n    ):\n        alias = alii[idx]\n        alias = alias[9:]\n        company_dict = company_lookup(alias)\n        results = CompanyResult(\n            inner_id=idx,\n            alias=alias,\n            company_name=company,\n            company_desc=company_dict.get(\"mission\"),\n            job_name=jobs[idx],\n            job_description=job_desc[idx],\n            industries=company_dict.get(\"industries\"),\n            street_address=company_dict.get(\"street_address\"),\n            suite=company_dict.get(\"suite\"),\n            city=company_dict.get(\"city\"),\n            state=company_dict.get(\"state\"),\n            zip=company_dict.get(\"zip\"),\n            adjectives=company_dict.get(\"adjectives\"),\n            url=company_dict.get(\"url\"),\n            twitter=company_dict.get(\"twitter\"),\n            email=company_dict.get(\"email\"),\n        )\n        company_results.append(results)\n    return company_results\n\n\ndef company_lookup(company_alias: str) -> dict:\n    \"\"\"Looks up the company JSON in BuiltInNYC. It passes this along to the superceding parse_results method,\n    which places it within the CompanyResult dataclass.\n    \"\"\"\n    with suppress(\n        JSONDecodeError, RequestException, HTTPError, TypeError, AttributeError\n    ):\n        company_page_url = f\"https://api.builtin.com/companies/alias/{company_alias}\"\n        comp_docs = webscrape_results(company_page_url, querystring={\"region_id\": \"5\"})\n        industries = [item.get(\"name\") for item in comp_docs[\"industries\"]]\n        data = {\n            \"street_address\": comp_docs.get(\"street_address_1\"),\n            \"suite\": comp_docs.get(\"street_address_2\"),\n            \"city\": comp_docs.get(\"city\"),\n            \"state\": comp_docs.get(\"state\"),\n            \"zip\": comp_docs.get(\"zip\"),\n            \"mission\": comp_docs.get(\"mission\"),\n            \"url\": comp_docs.get(\"url\"),\n            \"adjectives\": comp_docs.get(\"adjectives\"),\n            \"industries\": industries,\n            \"twitter\": comp_docs.get(\"twitter\"),\n            \"email\": comp_docs.get(\"email\"),\n        }\n        return data\n", "180": "#!/usr/bin/env python\n\nimport urllib2\nimport re\n# not all systems have readline...if not, just pass and continue.\ntry:\n    import readline  # nice when you need to use arrow keys and backspace\nexcept:\n    pass\nimport sys\n\n\ndef scrape():\n    site = raw_input(\"Enter page: \")\n\n    #open site. read so we can read in a string context\n    #test for valid and complete URL\n    try:\n        data = urllib2.urlopen(site).read()\n    except ValueError:\n        print \"INVALID URL: Be sure to include protocol (e.g. HTTP)\"\n        return\n    \n    #print data\n\n    #try an open the pattern file.\n    try:\n        patternFile = open('config/webscrape.dat', 'r').read().splitlines()\n    except:\n        print \"There was an error opening the webscrape.dat file\"\n        raise\n    #create counter for counting regex expressions from webscrape.dat\n    counter = 0\n    #for each loop so we can process each specified regex\n    for pattern in patternFile:\n        m = re.findall(pattern, data)\n        #m will return as true/false. Just need an if m:\n        if m:\n            for i in m:\n                #open output/results file...append because we are cool\n                outfile = open('scrape-RESULTS.txt', 'a')\n            #print m\n                outfile.write(str(i))\n                outfile.write(\"\\n\")  # may be needed. can always be removed.\n\n            #close the file..or else\n                outfile.close()\n                counter+=1\n                print \"Scrape item \" + str(counter) + \" successsful. Data output to scrape-RESULTS.txt.\"\n        else:  # only need an else because m is boolean\n            counter+=1\n            print \"No match for item \" + str(counter) + \". Continuing.\"\n            # Continue the loop if not a match so it can go on to the next\n            # sequence\n            # NOTE: you don't *really* need an else here...\n            continue\n", "181": "#!/usr/bin/env python\n\nimport urllib2\nimport re\n# not all systems have readline...if not, just pass and continue.\ntry:\n    import readline  # nice when you need to use arrow keys and backspace\nexcept:\n    pass\nimport sys\n\n\ndef scrape():\n    site = raw_input(\"Enter page: \")\n\n    #open site. read so we can read in a string context\n    #test for valid and complete URL\n    try:\n        data = urllib2.urlopen(site).read()\n    except ValueError:\n        print \"INVALID URL: Be sure to include protocol (e.g. HTTP)\"\n        return\n    \n    #print data\n\n    #try an open the pattern file.\n    try:\n        patternFile = open('config/webscrape.dat', 'r').read().splitlines()\n    except:\n        print \"There was an error opening the webscrape.dat file\"\n        raise\n    #create counter for counting regex expressions from webscrape.dat\n    counter = 0\n    #for each loop so we can process each specified regex\n    for pattern in patternFile:\n        m = re.findall(pattern, data)\n        #m will return as true/false. Just need an if m:\n        if m:\n            for i in m:\n                #open output/results file...append because we are cool\n                outfile = open('scrape-RESULTS.txt', 'a')\n            #print m\n                outfile.write(str(i))\n                outfile.write(\"\\n\")  # may be needed. can always be removed.\n\n            #close the file..or else\n                outfile.close()\n                counter+=1\n                print \"Scrape item \" + str(counter) + \" successsful. Data output to scrape-RESULTS.txt.\"\n        else:  # only need an else because m is boolean\n            counter+=1\n            print \"No match for item \" + str(counter) + \". Continuing.\"\n            # Continue the loop if not a match so it can go on to the next\n            # sequence\n            # NOTE: you don't *really* need an else here...\n            continue\n", "182": "from utils.const import __CUR_DIR__\nfrom utils.Pipeline import *\n\n\ndef require_input(recommend, question, requirement):\n    while True:\n        print(recommend)\n        inp = input(question)\n\n        if requirement(inp) is True:\n            break\n\n    return inp\n\ndef yes_or_no(inp):\n    if inp == \"y\":\n        return True\n    elif inp == \"n\":\n        return True\n    else:\n        print(\"Input must be 'y' or 'n'\")\n        return False\n\n\ndef file_exists(inp):\n    if not os.path.isfile(os.path.join(__CUR_DIR__, inp+\".xlsx\")):\n        print(\"File not found\")\n        return False\n    return True\n\ndef is_integer(inp):\n    try:\n        int(inp)\n        return True\n    except:\n        print(\"Must be an integer\")\n        return False\n\n# DEFAULTS\nbus_dir_file = \"data\\\\london_bus_dir\"\narea_table_file = \"areas\\\\london\"\nkey = \"Eyelashes\"\nsession_limits = 4000\n\n\nprint(\"Use Defaults\")\nuse_def = require_input(\"Recommend -> Defaults? [y/n]:y\", \"Defaults? [y/n]:\", yes_or_no)\n\nif use_def == \"n\":\n\n    print(\"1. Choose business directory name\")\n    bus_dir_file = require_input(\"Do not put file extension, Recommend -> bus_dir:data\\\\london_bus_dir\",\n                                 \"bus_dir:\", lambda inp: True)\n\n    print(\"\\n2. Set GoogleAPI request limit\")\n    session_limits = require_input(\"Recommend -> session_requests:4000\", \"session_requests:\", is_integer)\n\n\n    print(\"\\n3. Choose location input file\")\n    area_table_file = require_input(\n        \"Location list, ###.xlsx must contain column called 'Location String' which it will use for the search \\nRecommend -> area_file:areas\\\\london\",\n        \"area_file:\", file_exists)\n\n    print(\"\\n4. Choose places search keyword \\nUse different search keyword than 'Eyelashes'?\")\n    change_key = require_input(\"Recommend -> [y/n]:n\", \"Change keyword from 'Eyelashes'? [y/n]:\", yes_or_no)\n\n    if change_key == \"n\":\n        key = \"Eyelashes\"\n    else:\n        key = input(\"Enter keyword:\")\n\n\nprint(\"\\n1. GoogleAPI Places Search\")\nskip_places = require_input(\"Recommend -> Skip? [y/n]:n\", \"Skip? [y/n]:\", yes_or_no)\n\nprint(\"\\n2. GoogleAPI Detailed Search\")\nskip_detailed = require_input(\"Recommend -> Skip? [y/n]:n\", \"Skip? [y/n]:\", yes_or_no)\n\nprint(\"\\n3. Webscrape\")\nskip_scrape = require_input(\"Recommend -> Skip? [y/n]:n\", \"Skip? [y/n]:\", yes_or_no)\n\n\nGoogleAPI.session_request_limit = int(session_limits)\nbus_dir = TableManger(os.path.join(__CUR_DIR__, bus_dir_file))\narea_tab = TableManger(os.path.join(__CUR_DIR__, area_table_file))\n\nif skip_places == \"n\":\n    places_search(area_tab, key, bus_dir, True)\nelse:\n    places_search(area_tab, key, bus_dir, False)\n\nif skip_detailed == \"n\":\n    detailed_search(bus_dir, True)\nelse:\n    detailed_search(bus_dir, False)\n\nif skip_scrape == \"n\":\n    webscrape(bus_dir, True)\nelse:\n    webscrape(bus_dir, False)\n\nexport(bus_dir)\n\n", "183": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun May  9 00:12:23 2021\n\n@author: Administrator\n\"\"\"\n\nfrom bs4 import BeautifulSoup as bs\nimport requests\nimport pandas as pd\nimport pathlib\n\ndef webscrape(URL):\n    page = requests.get(URL)\n    soup = bs(page.content,'html.parser')\n    \n    table = soup.find_all('table')[0] \n    \n    dict = {\"Date\":[],\"Rate\":[]};\n    \n    for row in table.find_all('tr'):\n        dict[\"Date\"].append(row.text[0:11].strip())\n        dict[\"Rate\"].append(row.text[25:35].strip())\n        \n    forex_df = pd.DataFrame(dict)\n    \n    forex_df['Date']= pd.to_datetime(forex_df['Date'])\n    \n    forex_df = forex_df[forex_df.Rate != 'ND']\n    \n    forex_df[\"Rate\"] = forex_df[\"Rate\"].astype(float).round(5)\n    print(forex_df.dtypes)\n    \n    forex_df.to_csv(r'forexscrape.csv')\n\ndef checkforwebscrape():\n    file = pathlib.Path('forexscrape.csv')\n    if file.exists ():\n        \"File already exists\"\n    else:\n        print(\"Webscraping\")\n        webscrape('https://www.federalreserve.gov/releases/h10/hist/dat00_eu.htm')\n\nif __name__ == \"__main__\":\n    checkforwebscrape()", "184": "# from \"../runner.py\" import mapper \nfrom webscrape import webscrape\n\nglobal txtfile, headers = webscrape(\"http://www.eecs70.org/\")\n\nclass Node():\n    def __init__(self, topic, dist, children):\n        self.topic = topic\n        self.dist = dist\n        self.children = children\n\n\n\ndef BFS(Node n):\n    \"\"\" Traverse n by 2 levels and add every node's topic traversed to a set\n    1. access new subnodes through Wikimap\n    2. traverse every node in the first level and add to set\n    3. recurse on each node in the first level. Stop after.\n\n    Input -> Node\n    Output -> Set of subnodes connected to Node w/in 2 lvls\n    \"\"\"\n    set = set([])\n\n\n\ndef populate_map():\n    \"\"\" creates hashmap between each topic node and the set of subnodes it is connected to within 2 levels\n    key -> topic node\n    value -> set of subnodes\n\n    Input -> none  (global var headers)\n    Output -> set of nodes and its corresponding subnodes\n    \"\"\"\n    map = {}\n    for header in headers:\n        header_node = Node(header, 0, None)\n        subnodes_set = BFS(header_node)\n        map.add(header_node, subnodes_set)\n\n\ndef calc_relations(map):\n    \"\"\" Given the hashmap, we look for duplicates in the values of every 2-combination of topics\n    and create a mapping from every 2 combinations to a true or false, indicating whether there is\n    a relation between the 2 topics or not.\n    \"\"\"\n", "185": "import requests, bs4\nfrom app.settings import Settings\n\n# URL needed to check if chinese character exists in verify_html \n# (I also use it for the web scrape but the webscrape function can adapt to a different site)\nerror_validation_url = 'https://chinese.yabla.com/chinese-english-pinyin-dictionary.php?define='\n\n\nforbidden_characters = [']', '[', '\u3001', '!', '\uff1f', '>', '<', '|', '?', ' ', ':', '@', '#', '$', '%', '^', '&', '*',\n                            '+', '_', '-', '{', '}',  '(', ')','=', \"'\", '\u3002', '\uff0c', ',', '.', 'A', 'B', 'C', 'D', 'E',\n                            'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n                            'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o',\n                            'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '1', '1', '2', '3', '4', '5', '6',\n                            '7', '8', '9', '0']\n\n\n# translate online continues if pages exists and the character is chinese (not forbidden)\ndef error_free(character):\n    forbidden_characters_list = forbidden_characters\n    for forbidden_character in forbidden_characters_list:\n        if forbidden_character in str(character):\n            return False\n    if not verify_html(error_validation_url, character):\n        return False\n    return True\n\n\n# makes request to site with specific character, if page doesn't exist, translation will not continue\ndef verify_html(url, character):\n    verified = True\n    html = requests.get(f'{url}{character}')\n    try:\n        html.raise_for_status()\n    except Exception as exc:\n        verified = False\n    return verified\n\n\n# returns html page or error message (if new forbidden character encountered) for url page of specific character\ndef request_html(url, character):\n    html = requests.get(f'{url}{character}')\n    try:\n        html.raise_for_status()\n    except Exception as exc:\n        print(f'Add {character} to list of forbidden characters')\n        pass\n    return html\n\n\n# beautiful soup parses html\ndef parse_html(html):\n    parsed_html = bs4.BeautifulSoup(html.text, 'lxml')\n    return parsed_html\n\n\n# takes in info of specific tag that wants to be scraped and returns those tags\ndef produce_tags(parsed_html, tag_type, tag_attr, tag_name):\n    tag_list = []\n    needed_tags = parsed_html.findAll(lambda tag: tag.name == f'{tag_type}' and tag.get(f'{tag_attr}') == [f'{tag_name}'])\n    for needed_tag in needed_tags:\n        tag_list.append(needed_tag.text)\n    return tag_list\n\n\n# General formatting function for strings to clean up how they are presented in the html,\n# returns one string with words separated only by commas and a space. This function does an\n# okay job for general cleaning but it's best to use a custom function made in settings.py\n# to make the formatting specific to how the tags come out for the specific webscrape\ndef format_translations(tag_list):\n    formatted_tag_list = []\n    for tag in tag_list:\n        tag_string = tag.replace('\\n', ' ')\n        tag_string = tag_string.replace(',', ' ')\n        tag_string = tag_string.replace('  ', ' ')\n        tag_string = tag_string.replace(' ', ', ')\n        formatted_tag_list.append(tag_string)\n    return ', '.join(formatted_tag_list)\n\n# cuts string down to the number of definitions desired, must be passed a string of words \n# separated by commas to work correctly, so the formatting function is important\ndef limit_string(string, string_number):\n    commas_index = []\n    for pos, char in enumerate(string):\n        if char == ',':\n            commas_index.append(pos)\n\n    string_produced = False\n    i = string_number\n\n    while string_produced is False and i > 0:\n        try:\n            string = string[0: commas_index[i - 1]]\n            string_produced = True\n        except Exception as exc:\n            i = i - 1\n\n    return string\n\n\n# actually does webscraping work, calls all related webscraping functions\ndef webscrape_mule(url, character, tag_type, tag_attr, tag_name, format_translations, string_number):\n    html = request_html(url, character)\n    parsed_html = parse_html(html)\n    tags = produce_tags(parsed_html, f'{tag_type}', f'{tag_attr}', f'{tag_name}')\n    formatted_translations = format_translations(tags)\n    translation = limit_string(formatted_translations, string_number)\n    return translation\n\n\n# I wanted only the character and desired translation numbers to be passed in the translator\n# page, but in case I (or someone else) would like to scrape a different site for definitions\n# this is the function where all the necessary values can be changed; a unique formatting\n# function will have to be created because not all tag text from beautiful soup html\n# will be in the desired format for displaying translations. I put a general format translations\n# function at the bottom that works pretty well for most cases, but it can't be perfect, depending\n# on the site layout\ndef webscrape(character, string_number):\n    translation = webscrape_mule(\n          url=Settings.url,\n          character=f'{character}',\n          tag_type=Settings.tag_type,\n          tag_attr=Settings.tag_attr,\n          tag_name=Settings.tag_name,\n          format_translations=Settings.format,\n          string_number=string_number,\n          )\n    return translation\n", "186": "from bs4 import BeautifulSoup\nfrom time import sleep\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nimport const\n\n\nclass WebScraper:\n    def __init__(self):\n        self.s = Service(const.CHROME_DRIVER)\n        self.options = Options()\n        self.options.add_argument(const.CHROME_DETAILS)\n        self.driver = webdriver.Chrome(service=self.s, options=self.options)\n        self.project_names = []\n        self.project_deadlines = []\n        self.project_tam_sum = []\n        self.project_tam_intenz = []\n        self.project_tam_form = []\n        self.project_bead_kezdet = []\n        self.project_tam_min_sum = []\n\n    def webscrape_checker(self):\n        print(self.project_names)\n        print(self.project_deadlines)\n        print(self.project_tam_sum)\n        print(self.project_tam_intenz)\n        print(self.project_tam_form)\n        print(self.project_bead_kezdet)\n        print(self.project_tam_min_sum)\n\n    def webscrape(self):\n        self.driver.get(const.TENDER_LINK)\n        sleep(3)\n        tender_on = True\n        i = 1\n        while tender_on:\n            html_source = self.driver.page_source\n            with open(f'html_files/file{i}.html', mode=\"w\", encoding=\"utf-8\") as fp:\n                fp.write(html_source)\n\n            with open(f'html_files/file{i}.html', mode=\"r\", encoding=\"utf-8\") as fp:\n                content = fp.read()\n\n            soup = BeautifulSoup(content, 'html.parser')\n\n            project_titles = soup.find_all(name='p', class_=\"project-title\")\n            project_titles_ = [project_title.get_text() for project_title in project_titles]\n            for item in project_titles_:\n                self.project_names.append(item)\n\n            project_descriptions = soup.find_all(name='div', class_=\"project-description\")\n            project_descriptions_ = [project_description.get_text() for project_description in project_descriptions]\n            project_descriptions_string = ''.join(project_descriptions_).replace(chr(160), ' ').replace('T\u00e1mogat\u00e1s maximum \u00f6sszege ', '').replace('T\u00e1mogat\u00e1si intenzit\u00e1s ', '').replace('T\u00e1mogat\u00e1s form\u00e1ja ','').replace('Bead\u00e1s kezdete ', '').replace('T\u00e1mogat\u00e1s minimum \u00f6sszege ', '').replace('P\u00e1ly\u00e1zati dokument\u00e1ci\u00f3Ter\u00fcleti szerepl\u0151k', '').replace('P\u00e1ly\u00e1zati dokument\u00e1ci\u00f3', '').replace('P\u00e1ly\u00e1zati kit\u00f6lt\u0151', '')\n            project_descriptions_fixed = project_descriptions_string.split('Bead\u00e1si hat\u00e1rid\u0151 :')\n            project_descriptions_fixed.remove('')\n\n            for item in project_descriptions_fixed:\n                list = item.split(':')\n                self.project_deadlines.append(list[0])\n                self.project_tam_sum.append(list[1])\n                self.project_tam_intenz.append(list[2])\n                self.project_tam_form.append(list[3])\n                self.project_bead_kezdet.append(list[4])\n                self.project_tam_min_sum.append(list[5])\n\n            i = i + 1\n            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n            sleep(2)\n            page = soup.find(name=\"span\", class_=\"react-bootstrap-table-pagination-total\").get_text().replace('- ','').replace('/ ', '').split(' ')\n            print(page)\n            if page[1] != page[2]:\n                self.driver.find_element(By.XPATH,'//*[@id=\"root\"]/div/div[5]/div/div/div[3]/div[2]/div[2]/ul/li[13]/span/button').click()\n                sleep(5)\n            else:\n                tender_on = False\n\n        self.webscrape_checker()", "187": "# Getting the data and manipulating it\nfrom Data_processing_and_webscrape.Web_scrape import all_fighter_df, dimension_conversion, most_recent_event\nfrom Utils import *\n\n# All the functions & classes used in this program\nclass stance_conversion:\n    def Orthadox_stance_conversion(df):\n        df['Stance_Orth'] = df.apply(lambda x: x['Stance'] == 'Orthodox', axis = 1)\n        df['Stance_Orth'] = df['Stance_Orth'].replace([True, False], [1, 0])\n        return df\n    def Southpaw_stance_conversion(df):\n        df['Stance_South'] = df.apply(lambda x: x['Stance'] == 'Southpaw', axis=1)\n        df['Stance_South'] = df['Stance_South'].replace([True, False], [1, 0])\n        return df\n    def Switch_stance_conversion(df):\n        df['Stance_Switch'] = df.apply(lambda x: x['Stance'] == 'Switch', axis=1)\n        df['Stance_Switch'] = df['Stance_Switch'].replace([True, False], [1, 0])\n        df = df.drop(columns='Stance')\n        return df\nclass Normalization:\n    def normalization_minmax(df):\n        df = (df - min(df)) / (max(df) - min(df))\n        return df\n    def normalization_of_df(df):\n        df['Reach'] = Normalization.normalization_minmax(df['Reach'])\n        df['Ht.'] = Normalization.normalization_minmax(df['Ht.'])\n        df['L'] = Normalization.normalization_minmax(df['L'])\n        return df\nclass ht_reach_manipulation:\n    def removing_str_from_int(df):\n        df['Ht.'] = df['Ht.'].replace(\"\\'\", '', regex=True).replace('\"', '', regex=True).replace(\n            ' ', '', regex=True).replace('--', '', regex=True)\n        df = df[df['Ht.'] != '']\n        df = df[df['Reach'] != '--']\n        return df\n    def ht_manipulation(df):\n        df['Ht.inch_0'] = df['Ht.'].str[0]\n        df['Ht.inch_1'] = df['Ht.'].str[1]\n        df['Ht.inch_2'] = df['Ht.'].str[2]\n        df = df.fillna('')\n        df['Ht.inch_0'] = df['Ht.inch_0']\n        df['Ht.inch_1'] = df['Ht.inch_1']\n        df['Ht.inch_2'] = df['Ht.inch_2']\n        df['Ht.inch_3'] = df['Ht.inch_1'] + df['Ht.inch_2']\n        df['Ht.inch_1'] = df['Ht.inch_3']\n        df['Ht.inch_0'] = dimension_conversion.ft_to_cm(dimension_conversion.str_to_int_convers(df['Ht.inch_0']))\n        df['Ht.inch_1'] = dimension_conversion.inch_to_cm(dimension_conversion.str_to_int_convers(df['Ht.inch_1']))\n        df['Ht.'] = df['Ht.inch_1'] + df['Ht.inch_0']\n        df = df.drop(columns=['Ht.inch_3', 'Ht.inch_2', 'Ht.inch_1', 'Ht.inch_0'])\n        return df\n    def reach_manipulation(df):\n        df['Reach'] = df['Reach'].replace('\"', '', regex=True)\n        df['Reach'] = dimension_conversion.inch_to_cm(dimension_conversion.str_to_int_convers(all_fighter_df['Reach']))\n        return df\nclass event_test_set:\n    def fighter_name_latest_event(df):\n        df = pd.DataFrame(df['Fighter'].str.split('  ', 1, expand=True).stack(). \\\n                          reset_index(level=1, drop=True))\n        df[['First', 'Last']] = df[0].str.split(' ', 1, expand=True)\n        df = df[['First', 'Last']]\n        return df\n    def real_event_test_set(webscrape_df):\n        webscrape_df = webscrape_df.fillna(null_number)\n        webscrape_df = webscrape_df[webscrape_df.First != null_number].drop(columns=['Nickname', 'Wt.', 'Belt',\n                                                                                     'W', 'D'])\n        return webscrape_df\n    def removing_null_fighter(df):\n        Null_fighter_information = df[df.isna().any(axis=1)]\n        Null_fighter_information = Null_fighter_information.drop(columns=['Ht.', 'Reach', 'L', 'Stance_Orth',\n                                                                          'Stance_South', 'Stance_Switch'])\n        return Null_fighter_information\n    @staticmethod\n    def real_test_set_manipulation():\n        real_test_set = even_or_odd_index(event_test_set.removing_null_fighter(merged_test_set), merged_test_set)\n        real_test_set = real_test_set.drop(columns=['First', 'Last'])\n        real_test_set = real_test_set[['L', 'Stance_Orth', 'Stance_South', 'Stance_Switch', 'Ht.', 'Reach']]\n        real_test_set['Reach'] = Normalization.normalization_minmax(real_test_set['Reach'])\n        real_test_set['Ht.'] = Normalization.normalization_minmax(real_test_set['Ht.'])\n        real_test_set['L'] = Normalization.normalization_minmax(real_test_set['L'])\n        return real_test_set\ndef even_or_odd_index(null_df_fighter, df):\n    for i in null_df_fighter.index:\n        df = df.drop(index=i)\n        if i % 2 != 0:\n            df = df.drop(index = i - 1)\n        elif i % 2 == 0:\n            df = df.drop(index = i + 1)\n    return df\ndef exporting_names_to_excel(df):\n    export_dataframe = df[['First', 'Last']]\n    export_dataframe.columns = ['First', 'Last']\n    return export_dataframe\n\n# The array used to represent the column names to drop\ncolumns_to_drop_from_raw_df = ['Winner','Stance','Fighter', 'W',\n                               'Wt.', 'Date of Fight', 'Birth Date',\n                               'Age at Fight']\n\n# Random value to signify null. column != np.nan isn't working. Converted the null to this value and dropped it from df\nnull_number = 100000000\n\n# Importing the raw data used in the KNN (Note: this is a CSV file located on my computer)\nraw_data = pd.read_csv('Input/Data_for_model_no_organization.csv').dropna()\\\n    .drop(columns=columns_to_drop_from_raw_df)\n\n# Creating the X and Y values\ny = pd.DataFrame(raw_data['Winner_binary'])\nX = raw_data.drop(columns='Winner_binary')\n\n# Normalizing the X Values\nX = Normalization.normalization_of_df(X)\n\n# Creating the training & test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state = 0, shuffle = False)\n\n# Manipulating the webscraping dataframe.\nall_fighter_df = event_test_set.real_event_test_set(all_fighter_df)\n\n# Converting the stances into binary columns\nall_fighter_df = stance_conversion.Orthadox_stance_conversion(all_fighter_df)\nall_fighter_df = stance_conversion.Southpaw_stance_conversion(all_fighter_df)\nall_fighter_df = stance_conversion.Switch_stance_conversion(all_fighter_df)\n\n# Removing the following: ' ', ' -- ', or '\\' from the column to get only integers\nht_reach_manipulation.removing_str_from_int(all_fighter_df)\n\n# Manipulating the height column to get the height in cm\nall_fighter_df = ht_reach_manipulation.ht_manipulation(all_fighter_df)\n\n# Manipulating the reach column to get the reach in cm\nall_fighter_df = ht_reach_manipulation.reach_manipulation(all_fighter_df)\n\n# Merging the people from the input event I'm looking for and removing anyone who isn't on the card.\nmerged_test_set = pd.merge(all_fighter_df, event_test_set.fighter_name_latest_event(most_recent_event[0]),\n                         how='right', on=['First', 'Last'])\n\n# Creating the real test set to see who will win the chosen card\nreal_test_set = event_test_set.real_test_set_manipulation()\n\n# Creating a dataframe that has the name of the fighters. This is used to export to Excel on my desktop\nexport_dataframe = exporting_names_to_excel(merged_test_set)\n", "188": "# Modules Imported\nfrom webanalysis import WebAnalysis\n\nfrom webscrape import *\n\n\n\n# This class inherits the class and methods of the webanalysis and webscrape module  \nclass GraphValues(WebAnalysis) :\n    def x_axis(self):\n        x_axis_values = [item[0] for item in self.word_count()]\n        return x_axis_values\n    \n    def y_axis(self):\n        y_axis_values = [item[1] for item in self.word_count()]\n        return y_axis_values\n", "189": "import os, sys\nimport json\nimport datetime\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef get_td_name(td):\n    a_tag = td.findAll('a')\n    for a in a_tag:\n        if 'href' in a.attrs:\n            name = a.attrs['href'].split('/')[-1]\n            # td_name = name.replace('_', '/')\n            return name\n        else:\n            print(a)   \n    \n    return None\n\n    \n# def get_ImageCollection(name):\n    \n#     all_name_options = [name.replace('_', '/')]\n#     options_count = len(name.split('_'))\n    \n#     if options_count-1 > 1:\n#         for i in range(options_count):\n#             # first option all replaced already in list\n#             split_name = name.split('_')\n\n#     else:\n#         # add in the original text, with underscore\n#         all_name_options.append(name)\n\n        \n#     # find the right name that will satisfy the ImageCollection argument\n#     for id_option in all_name_options:\n    \n#         try:\n#             IC = ee.ImageCollection(id_option)\n#             if IC:\n#                 return id_option, IC\n\n#         except Exception as e:\n#             print(e)\n#             print(\"NEED TO ADJUST NAME: \", name, all_name_options)\n            \n    \n\ndef get_ImageCollection_tags(td):\n    tag_list = []\n    a_tag = td.findAll('a')\n    for a in a_tag:\n        if 'href' in a.attrs:\n            tag = a.attrs['href'].split('/')[-1]\n            if tag not in tag_list:\n                tag_list.append(tag)\n            \n    return tag_list\n\n\ndef get_tbody_info(tbody):\n    tbody_info = {}\n    td_list = tbody.findAll('td')\n\n    for td in td_list:\n        if td.attrs['class'] == ['ee-dataset']:\n            td_name = get_td_name(td)\n            \n        elif td.attrs['class'] == ['ee-dataset-description-snippet']:\n            quick_description = td.text\n            \n        elif td.attrs['class'] == ['ee-tag-buttons', 'ee-small']:\n            tag_list = get_ImageCollection_tags(td)\n            \n        else:\n            print(\"Undefined attributes in td object: \", td.attrs)\n        \n\n    # IC_id, image_collection = get_ImageCollection(td_name)\n        \n    tbody_info = {'dataset': td_name,\n                   'tags': tag_list,\n                   'description': quick_description\n                  }\n\n    return tbody_info\n\n\ndef parse_code_block(code_block):\n    \"\"\" Break out the Code Embedded on Earth-Engine Dataset URL\n\n    Example output of code_block:\n\n        ee.ImageCollection(\"\")\n\n    dataset_type follows the ee.-> before first '('\n    dataset_id inside \" \"\n\n    \"\"\"\n    # Extract Datatepy in code block\n    dataset_type = code_block.text.split('.')[1]\n    dataset_type = dataset_type.split('(')[0]\n\n    # Extract ID from the code block 'ee.Image...(\"\")'\n    dataset_id = code_block.text.split('(')[1]\n    dataset_id = dataset_id.split(')')[0]\n    dataset_id = dataset_id.replace('\"','')\n\n    return dataset_type, dataset_id\n\n\n\ndef validate_availability(text):\n    \"\"\" we will use this a test function to extract a start/end date\"\"\"\n\n    # Test 0: ensure the text is actually a string before splitting\n    try:\n        assert isinstance(text, str), \"not a string\"\n    except Exception as e:\n        # raise Exception('Date Validate Failed: {}'.format(e))\n        print(e)\n        return None\n\n    # Test 1: The text must be split evenly by ' - '\n    split_text = text.split(' - ')\n    try:\n        assert len(split_text) == 2, \"len()==2 Test\"\n    except Exception as e:\n        # raise Exception('Date Validate Failed: {}'.format(e))\n        print(e)\n        return None\n\n    # Test 2: DateTime Formatting\n    # Test 2a: First split has DateTime format (Most Common ISO format)\n    try:\n        # date_start = datetime.strptime(split_text[0], '%Y-%m-%')\n        date_start = datetime.datetime.fromisoformat(split_text[0])\n\n    # Test 2b: Use custom strptime, non-ISO\n    except Exception as e:\n        date_start = datetime.datetime.strptime(split_text[0], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n        assert isinstance(date_start, datetime.datetime), \"Not a Datetime object\"\n\n    except:\n        # raise Exception('Date Validate Failed: {}'.format(e))\n        print(\"Not a datetime object: {}\".format(split_text[0]))\n        return None\n\n    finally:\n        # Since we are packing this into a json file, the best way is to keep string\n        # the date end will be determined later, but if we succeeded in date start\n        # We have found our Date Availability Match!\n        date_range = {'dataset_start': split_text[0], 'dataset_end': split_text[1]}\n        return date_range\n\n\ndef seek_date_availability(soup):\n    \"\"\" This is difficult to be exact, the html is not clearly/uniquely marked\"\"\"\n    date_range = None\n    # Method 1: the first dd on the page:\n    try:\n        dd = soup.find(\"dd\")\n        date_range = validate_availability(dd.text)\n        assert date_range is not None, \"Method 1\"\n        return date_range\n    except Exception as e:\n        # raise Exception('Failed on {}, {}'.format(dd, e))\n        print(e)\n\n    # Method 2: all dd's on page, find the one that satisfies a time format validation\n    for dd in soup.findAll(\"dd\"):\n        date_range = validate_availability(dd.text)\n        if date_range is not None:\n            return date_range\n\n    print(\"METHOD 2 FAILED TO FIND DATE AVAILABILITY in each Method, SET DEFAULT\")\n    # determine Earliest and Latest Date and set those as defaults\n    default_start = None\n    default_end = None\n    date_range = {'dataset_start': default_start, 'dataset_end': default_end}\n    return date_range\n\n\n\n\ndef follow_dataset_link(tag_name, URL):\n    dataset_url = URL + '/' + tag_name\n    response = requests.get(dataset_url)\n    htmlCode = response.text\n    soup = BeautifulSoup(response.text, 'html.parser')\n    code = soup.find(\"code\")\n\n    dataset_type, dataset_id = parse_code_block(code)\n    result = {'dataset_id': dataset_id, 'dataset_type': dataset_type}\n    result.update(seek_date_availability(soup))\n\n    return result\n\n\ndef save_catalog(results, partial=False):\n    \"\"\"Save this simple metadata to json \"\"\"\n    PLUGIN_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    METADATA_DIR = os.path.join(PLUGIN_DIR, \"metadata\") \n\n    # Base Filename\n    fn = 'gee_catalog'\n    ftype = '.json'\n    \n    # if we have partial results, track the last result index\n    if partial:\n        fn = fn + '_(' + str(len(results)-1) + ')'\n\n    # Add all\n    fn = fn + ftype\n    # now get full file path + name\n    filename = os.path.join(METADATA_DIR, fn)\n\n    with open(filename, 'w') as outfile:\n        json.dump(results, outfile)\n\n\n\ndef get_catalog():\n    #=====================================\n    # 0. Run the requests on the Google Earth Engine Datasets Catalog\n    #=====================================\n\n    # From the \"View all datasets\" Tab on developers.google.com/earth-engine\n    url = \"https://developers.google.com/earth-engine/datasets/catalog\"\n    response = requests.get(url) \n    htmlCode = response.text \n    soup = BeautifulSoup(response.text, 'html.parser') \n\n    # the 'tbody' element seems to be the best way to extract the useful metadata for each dataset\n    all_tbodies = soup.findAll(\"tbody\")\n\n    # total GEE datasets: 409 as of 2020-07-21\n    # print(len(all_tbodies))\n\n    #=====================================\n    # 1. Iterate through each tbody to extract metadata\n    #=====================================\n\n    # this will result in a list of each dataset saved as a json metadata file\n    # for allowing user to filter on tags/geography/time before \n    # making requests to the gee server\n    webscrape_results = []\n\n    for tbody in all_tbodies:\n        # 1a. tbody will have 3 basic infos: ImageCollection Name, Tags, Description\n        \n        try:\n            this_result = get_tbody_info(tbody)\n            more_result = follow_dataset_link(this_result['dataset'], url)\n            this_result.update(more_result)\n            webscrape_results.append(this_result)\n\n            update_msg = \"Webscrape Status: {} out of {}\".format(len(webscrape_results), len(all_tbodies))\n            print(update_msg, end=\"\\r\", flush=True)\n\n        except Exception as e:\n            print(e)\n            print(\"failed data extract on: \", tbody)\n            save_catalog(webscrape_results, partial=True)\n            return\n\n\n    #=====================================\n    # 2. Save All metadata to json, all tests passed\n    #=====================================\n    save_catalog(webscrape_results, partial=False)\n\n\n\ndef read_catalog():\n    catalog_fn = os.path.join('metadata', 'gee_catalog.json')\n    fpath = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    fn = os.path.join(fpath, catalog_fn)\n\n    if os.path.exists(fn):\n        with open(fn, 'r') as in_file:\n            data = json.load(in_file)\n            return data\n\n    else:\n        print(\"No Catalog Found...\\nnow running ee_catalog.py\")\n        get_catalog()\n        return read_catalog()\n\n\n\ndef test():\n    save_catalog({})\n\nif __name__ == '__main__':\n    read_catalog()\n", "190": "from django.contrib import admin\n\n# Register your models here.\n\nfrom scrapes_data_app.models import * \n\nclass WebscrapeAdmin(admin.ModelAdmin):\n\n    list_display = ('name','price', 'hour', 'twenty_hours','seven_days','market_cap','volume','circulating_supply')\n\n\nadmin.site.register(Webscrap,WebscrapeAdmin)\n", "191": "from web_scraper import webscrape\ndata = webscrape(\"https://stats.foldingathome.org/donor/1437\")\nprint(data)\n\n", "192": "# -*- coding: utf-8 -*-\r\n\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport pyautogui\r\nfrom selenium import webdriver\r\n\r\nclass WebScrape: \r\n\r\n    def enter_proxy_auth(self, proxy_username, proxy_password):\r\n        pyautogui.typewrite(proxy_username)\r\n        pyautogui.press('tab')\r\n        pyautogui.typewrite(proxy_password)\r\n        pyautogui.press('enter')\r\n    \r\n    def open_a_page(self, driver, url):\r\n        driver.get(url)\r\n        \r\n    def get_table(self, rows): \r\n        self.rows = 1+rows\r\n        self.cols = len(driver.find_elements_by_xpath(\r\n            \"/html/body/table/tbody/tr/th\"))-1\r\n        table = []\r\n        for r in range(2, self.rows +1): # rows+1\r\n            for p in range(1, self.cols+1):\r\n                # obtaining the text from each column of the table\r\n                t = driver.find_element_by_xpath(\r\n                        \"/html/body/table/tbody/tr[\"+str(r)+\"]/td[\"+str(p)+\"]\").text\r\n                table.append(t)\r\n        table = np.array( table)\r\n        table = np.reshape( table, (self.rows -1, self.cols))\r\n        table[0,0] = table[0,0].split()[-1]\r\n        df= pd.DataFrame(data=table, index=table[:,0], columns = ['Date', '1 col',\t\r\n                         '2 col',\t'3 col',\t'4 col',\t'5 col',\t'6 col',\t\r\n                         '7 col',\t'8 col',\t'9 col',\t'10 col'])\r\n        return df.iloc[: , 1:]\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    proxy_username = 'USERNAME'\r\n    proxy_password = 'PASSWORD'\r\n    path = 'C:/.../chromedriver.exe'\r\n    url = 'https://...'\r\n    \r\n    driver = webdriver.Chrome(path)\r\n    \r\n    webscrape = WebScrape()\r\n    webscrape.open_a_page(driver, url)\r\n    webscrape.enter_proxy_auth(proxy_username, proxy_password)\r\n    df = webscrape.get_table(30)\r\n    \r\n\r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n    ", "193": "import ast\r\nimport requests\r\nimport urllib.request\r\nimport bs4 as bs\r\nimport pandas as pd\r\nimport datetime\r\nimport os\r\nfrom datetime import timedelta\r\n\r\npath = \"C:/Users/Jordan/Desktop/MSA/2020/Phase 2/\"\r\n\r\n# Method of finding current day\r\ndef addDays(num):\r\n    return ((num / 6) * 7)\r\n\r\nbase_d = datetime.datetime(2020, 9, 10) # 15996\r\nbase_d - timedelta(days = 7) # 15990\r\n\r\ninitial_d = base_d - timedelta(days = addDays(15996)) # 0 - Inital day\r\n\r\ninitial_d + timedelta(days = addDays(16002))\r\n\r\nresponse = requests.get('https://forum.netmarble.com/api/game/nanagb/official/forum/7ds_en/article/list?rows=15&start=0&menuSeq=1&_=1600263540539')\r\nr = response.json()\r\n\r\nnewList = []\r\n\r\n# Extracting date and reaction information\r\nfor page in r[\"articleList\"]:\r\n    newDict = {}\r\n    newDict[\"regDate\"] = int(str(page[\"regDate\"])[:5])\r\n    newDict[\"reactionInfo\"] = page[\"reactionInfo\"]\r\n    newList.append(newDict)\r\n\r\ndateList = []\r\nposList = []\r\nneuList = []\r\nnegList = []\r\n# 1-Love, 2-Haha, 3-Like, 4-Wow, 5-Sad, 6-Angry\r\n# 1,2,3 - Positive, 4 - Neutral, 5,6 - Negative\r\n\r\n# Creating a total count of each category of sentiment\r\nfor ele in newList:\r\n    negCount = 0\r\n    neuCount = 0\r\n    posCount = 0\r\n    count = 1\r\n    for reaction in ele[\"reactionInfo\"]:\r\n        reactionCount = reaction[\"cnt\"]\r\n        if count <= 3:\r\n            posCount += reactionCount\r\n        elif count == 4:\r\n            neuCount += reactionCount\r\n        else:\r\n            negCount += reactionCount\r\n        count += 1\r\n\r\n    if ele[\"regDate\"] in dateList:\r\n        index = dateList.index(ele[\"regDate\"])\r\n        posList[index] += posCount\r\n        neuList[index] += neuCount\r\n        negList[index] += negCount\r\n    else:\r\n        dateList.append(ele[\"regDate\"])\r\n        posList.append(posCount)\r\n        neuList.append(neuCount)\r\n        negList.append(negCount)\r\n\r\n# Creating a percentage positive list\r\nsentimentList = []\r\nfor i in range(len(dateList)):\r\n    sentimentList.append(posList[i]/(posList[i] + neuList[i] + negList[i]))\r\n    dateList[i] = initial_d + timedelta(days = addDays(dateList[i]))\r\n    dateList[i] = dateList[i].strftime(\"%Y-%m-%d\")\r\n\r\n# New DataFrame\r\nwebScrape = pd.DataFrame({\r\n    \"Days\":dateList,\r\n    \"PercentPos\":sentimentList\r\n})\r\n\r\n# Saving as csv\r\ndataDir = os.getcwd() + \"\\\\data\\\\\"\r\ndataScrape = dataDir + \"webScrape.csv\"\r\n\r\nwebScrape.to_csv(dataScrape, index=False)", "194": "\"\"\"\nASGI config for webscrape project.\n\nIt exposes the ASGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.0/howto/deployment/asgi/\n\"\"\"\n\nimport os\n\nfrom django.core.asgi import get_asgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webscrape.settings')\n\napplication = get_asgi_application()\n", "195": "\"\"\"\nWSGI config for webscrape project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.0/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webscrape.settings')\n\napplication = get_wsgi_application()\n", "196": "from flask import Flask, render_template, redirect\nfrom flask_pymongo import PyMongo\nimport mission_to_mars\n\napp = Flask(__name__)\n\n#flask_pymongo to set up mongo connection\nmongo = PyMongo(app, uri=\"mongodb://localhost:27017/mars_app\")\n\n@app.route(\"/\")\ndef index():\n    mars_data = mongo.db.mars_data.find()\n    return render_template(\"index.html\", mars_data=mars_data)\n\n@app.route(\"/scrape\")\ndef scraper():\n    mars_data = mongo.db.mars_data\n    mars_webscrape_re = mission_to_mars.scrape()\n    mars_data.update({}, mars_webscrape_re, upsert=True)\n    return redirect(\"/\", code=302)\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n", "197": "import pandas as pd\nimport switcher\nimport amazon_india as amazon\nimport flipkart_india as flipkart\nimport models.product as productsmodel\nimport sys,os\nimport getopt\n\nrecords_to_consider = 5\nglobal products\n\n\n\ndef main(argv):\n    pname = \"\"\n    websiteToSearch = \"all\"\n    try:\n        opts,args = getopt.getopt(argv,\"n:w:\",[\"name=\",\"website=\"])\n    except getopt.GetoptError:\n        exc_type, exc_obj, exc_tb = sys.exc_info()\n        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n        print(exc_type, fname, exc_tb.tb_lineno)\n        print('start.py -n  [-w \"amazon\"|\"flipkart\"|\"all\"]')\n        print('start.py --name  [--website \"amazon\"|\"flipkart\"|\"all\"]')\n        sys.exit(2)\n    #opts = x for x in opts if x)\n    print(opts)    \n    for opt in opts:\n        if opt:\n            if opt[0] in ('-n','--name'):\n                pname = opt[1]\n                print(pname)\n            if opt[0] in ('-w','--website'):\n                websiteToSearch = opt[1]     \n        #else:\n        #    print('start.py -n  [-w \"amazon\"|\"flipkart\"|\"all\"]')\n        #    sys.exit()\n    if(pname != \"\"):            \n        print(\"Searching for product name:\" + pname)\n        search(websiteToSearch,pname)\n    else:\n        print('Product name to search cannot be empty')\n        print('start.py -n  [-a,-f]')\n        print('start.py --name  [-a,-f]')\n        print('-a = Search in amazon')\n        print('-f = Search in flipkart')\n        sys.exit()\n \nclass WebScrape(object):\n    def start(self, website, productname):\n            method_name = website+'WebScrape'\n            method = getattr(self, method_name, lambda: 'Invalid')\n            return method(productname)\n\n    def amazonWebScrape(self, productname):\n        print(\"amazon:\"+productname)\n        amazonproducts = amazon.search(productname)\n        return amazonproducts[:records_to_consider]\n\n    def flipkartWebScrape(self, productname):\n        flipkartproducts = flipkart.search(productname)\n        return flipkartproducts[:records_to_consider]\n\n    def allSitesWebScrape(self, productname):\n        print(\"all:\"+productname)\n        amazonproducts = amazon.search(productname)\n        flipkartproducts = flipkart.search(productname)\n        return amazonproducts[:records_to_consider] + flipkartproducts[:records_to_consider]\n       \ndef search(websiteToSearch,name):\n    search = WebScrape()\n    result = search.start(websiteToSearch,name)\n\n    print(\"------ completed web scrapping ------\")\n    for product in result:\n        print(\"Name:\" + product.name + \" Current Price:\" + product.price + \" Original Price:\" + product.orginal_price + \" No of user rated:\" + product.no_of_users_rated + \" Rating:\" + product.rating + \" Website:\"+ product.website)      \n                \n\n\nif __name__ == \"__main__\":\n   main(sys.argv[1:])\n", "198": "#Imports\nimport os\nimport sys\nimport pygame\n\n#google text to speech\nfrom gtts import gTTS\n\n#requests and BeautifulSoup\nimport requests\nimport bs4\n\n#pygame audio player\nfrom pygame import mixer\n\n#tkinter ui\nfrom tkinter import *\nfrom tkinter import ttk\nfrom tkinter import filedialog\nfrom tkinter import messagebox\n\n#mp3 -> wav\nfrom os import path\nfrom pydub import AudioSegment\n\n\n#variable declarations\nx = str()\nvol = float(1.0)\n\n#Play button\ndef Play():\n    mixer.init()\n    mixer.music.load(\"sound.ogg\")\n    mixer.music.play()\n\n\n#play and resume\ndef pause():\n    mixer.music.pause()\n\ndef unpause():\n    mixer.music.unpause()\n\ndef quitpy():\n    sys.exit()\n\n\ndef volup():\n    global vol\n    mixer.music.set_volume(vol + 0.1)\n\ndef voldown():\n    global vol\n    mixer.music.set_volume(vol - 0.1)\n\n\n\n#UI build and declarations\nroot = Tk()\nroot.title(\"AP Text to Speech\")\n\nmainframe = ttk.Frame(root, padding=\"3 3 12 12\")\nmainframe.grid(column=0, row=0, sticky=(N, W, E, S))\nroot.columnconfigure(0, weight=2)\nroot.rowconfigure(0, weight=2)\n\n#variable declarations 2\nurl = StringVar()\ncountryvar = StringVar()\nm = countryvar.get()\nm = str(m)\nprint(m)\n\ndef check():\n    m = countryvar.get()\n    m = str(m)\n    print(m)\n    return str(m)\n\n#webscrape function\ndef webscrape():\n    global x\n    global b\n    global thing\n    global countryvar\n    src = \"sound.mp3\"\n    dst = \"sound.ogg\"\n    murl = str(url.get())\n    response = requests.get(murl)\n    response.raise_for_status()\n    m = countryvar.get()\n    m = str(m)\n    print(m)\n    parse = bs4.BeautifulSoup(response.text, 'html.parser')\n    x = str(parse.get_text())\n    print(x)\n    text = gTTS(x, lang=m)\n    text.save(\"sound.mp3\")\n    AudioSegment.from_mp3(src).export(dst, format='ogg')\n    b.state(['!disabled'])\n\nurl_entry = ttk.Entry(mainframe, width=7, textvariable=url)\nurl_entry.grid(column=3, row=2, sticky=(W, E))\n\nttk.Label(mainframe, text=\"Enter URL:\").grid(column=3, row=1, sticky=S)\n\nttk.Button(mainframe, text=\"Scrape\", command=webscrape).grid(column=3, row=3, sticky=S)\n\nttk.Button(mainframe, text=\"Pause\", command=pause).grid(column=2, row=5, sticky=S)\nb = ttk.Button(mainframe, text=\"Play\", command=Play)\nb.grid(column=3, row=4, sticky=S)\nb.state(['disabled'])\nttk.Button(mainframe, text=\"Resume\", command=unpause).grid(column=4, row=5, sticky=S)\nttk.Button(mainframe, text=\"Quit\", command=quitpy).grid(column=3, row=5, sticky=S)\n\nttk.Label(mainframe, text=\"Set Volume:\").grid(column=3, row=6, sticky=S)\nttk.Button(mainframe, text=\"+\", command=volup).grid(column=2, row=7, sticky=S)\nttk.Button(mainframe, text=\"-\", command=voldown).grid(column=4, row=7, sticky=S)\n\nthing = ttk.Combobox(mainframe, textvariable=countryvar, values=[\"en\", \"fr\", \"es\", \"pt\"], state='readonly', width=5).grid(column=3, row=8)\nthing.current(0)\nttk.Button(mainframe, text=\"check\", command=check).grid(column=3, row=9)\n\nfor child in mainframe.winfo_children(): child.grid_configure(padx=5, pady=5)\n\nroot.bind('', webscrape)\n\nroot.mainloop()\n", "199": "# Scrapy settings for webscrape project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'webscrape'\n\nSPIDER_MODULES = ['webscrape.spiders']\nNEWSPIDER_MODULE = 'webscrape.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'webscrape (+http://www.yourdomain.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'webscrape.middlewares.WebscrapeDownloaderMiddleware': 543,\n#}\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n   'webscrape.pipelines.WebscrapePipeline': 300,\n}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n", "200": "from pathlib import Path\n\nfrom tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage\nimport tkinter as tk\nimport threading\nimport pyautogui\nimport time\nimport cv2\nfrom pytesseract import *\nfrom PIL import Image\nfrom selenium import webdriver\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.chrome.service import Service\n\nser = Service(executable_path=ChromeDriverManager().install())\ndriver = webdriver.Chrome(service=ser)\nfrom selenium.webdriver.common.by import By\nimport mss\nimport mss.tools\n\npytesseract.tesseract_cmd = r'...tesseract.exe'\n\nOUTPUT_PATH = Path(__file__).parent\nASSETS_PATH = OUTPUT_PATH / Path(\"./assets\")\n\n\ndef relative_to_assets(path: str) -> Path:\n    return ASSETS_PATH / Path(path)\n\n\nif __name__ == \"__main__\":\n\n    window = tk.Tk()\n    window.geometry(\"431x550\")\n    window.configure(bg=\"#ECECEC\")\n    window.title(\"BOT\")\n    window.iconbitmap(\"icon.ico\")\n\n\n    def webscrape(username, original):\n        t = time.time()\n\n        options = webdriver.ChromeOptions()\n        options.headless = True\n        driver = webdriver.Chrome(options=options)\n        driver.get(f'https://cod.tracker.gg/warzone/profile/atvi/{username}/overview')\n        page_title = driver.find_elements(By.CLASS_NAME, 'lead')\n\n        if not page_title or page_title[0] == \"WARZONE STATS NOT FOUND\":\n            print(\"WARZONE STATS NOT FOUND - Private profile\")\n            usernameBox.delete(0, tk.END)\n            usernameBox.insert(0, \"WARZONE STATS NOT FOUND - Private profile\")\n\n        else:\n            usernameBox.delete(0, tk.END)\n            usernameBox.insert(0, original)\n            search = driver.find_elements(By.CLASS_NAME, 'value')\n\n            if len(search) > 4:\n                print(\"Wins:\", search[0].text)\n                winsBox.delete(0, tk.END)\n                winsBox.insert(0, search[0].text)\n\n                print(\"Win %:\", search[1].text)\n                winPercentageBox.delete(0, tk.END)\n                winPercentageBox.insert(0, search[1].text)\n\n                print(\"Kills:\", search[2].text)\n                killsBox.delete(0, tk.END)\n                killsBox.insert(0, search[2].text)\n\n                print(\"K/D:\", search[3].text)\n                KDBox.delete(0, tk.END)\n                KDBox.insert(0, search[3].text)\n\n                print(\"Score/min:\", search[4].text)\n                scoreMinBox.delete(0, tk.END)\n                scoreMinBox.insert(0, search[4].text)\n\n            else:\n                print(\"Incorrect name or private profile\")\n\n                usernameBox.delete(0, tk.END)\n                usernameBox.insert(0, original)\n\n                winsBox.delete(0, tk.END)\n                winsBox.insert(0, \"-----\")\n\n                winPercentageBox.delete(0, tk.END)\n                winPercentageBox.insert(0, \"-----\")\n\n                killsBox.delete(0, tk.END)\n                killsBox.insert(0, \"-----\")\n\n                KDBox.delete(0, tk.END)\n                KDBox.insert(0, \"-----\")\n\n                scoreMinBox.delete(0, tk.END)\n                scoreMinBox.insert(0, \"-----\")\n\n        elapsed = time.time() - t\n        print(elapsed, \"Time to webscrape\")\n        webscrapeBox.delete(0, tk.END)\n        webscrapeBox.insert(0, str(round(elapsed, 2)) + \" seconds\")\n\n        driver.close()", "201": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        # <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n            column_names=, # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n            destination_table= # <- !!! ENTER HERE THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "202": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        props[\"last_date_amazon\"] = '2020-01-01'\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=df.values,\n            column_names=df.columns.to_list(),\n            destination_table=\"yankee_candle_reviews\"\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in table yankee_candle_reviews.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "203": "from bs4 import BeautifulSoup\n\nimport requests\n\nvisited = [\"\"]\ninitialurl = str(\"\")\ndef webscrape(url):\n\tr  = requests.get(url)\n\tdata = r.text\n\tsoup = BeautifulSoup(data,'html.parser')\n\tfor link in soup.find_all('a'):\n\t\tmylink = str(link.get('href'))\n\t\t#print(mylink)\t\t\n\t\tif (':' not in mylink and \"//\" not in mylink and mylink != \"\"):\n\t\t\tmylink = str(initialurl + \"/\" + mylink)\n\t\t\t#print (\"inside if: \" , mylink)\n\t\tif initialurl in mylink and mylink not in visited and mylink [0:4] == \"http\" and \"#\" not in mylink and \"None\" not in mylink:\n\t\t\tvisited.append(mylink)\n\t\t\tprint(mylink)\n\t\t\twith open(str(len(visited))+\".html\", \"w\", encoding=\"utf-8\") as filehandle:\n\t\t\t\tfilehandle.write(str(soup))\n\t\t\t\tfilehandle.close()\n\t\t\twebscrape(mylink)\n\n\nurl = input(\"Enter a website to extract the URL's from: \")\n#url = \"www.syedfaaizhussain.com\"\nurl = \"https://\" + url\n#url = \"http://www.learnyouahaskell.com\"\ninitialurl=url\nwebscrape(url)", "204": "#Imports\nimport os\nimport sys\nimport pygame\n\n#google text to speech\nfrom gtts import gTTS\n\n#requests and BeautifulSoup\nimport requests\nimport bs4\n\n#pygame audio player\nfrom pygame import mixer\n\n#tkinter ui\nfrom tkinter import *\nfrom tkinter import ttk\nfrom tkinter import filedialog\nfrom tkinter import messagebox\n\n#mp3 -> wav\nfrom os import path\nfrom pydub import AudioSegment\n\nimport speech_recognition as spr\nimport pyttsx3\n\nr = spr.Recognizer()\n\n#variable declarations\nx = str()\nvol = float(1.0)\n\n#Play button\ndef Play():\n    mixer.init()\n    mixer.music.load(\"sound.ogg\")\n    mixer.music.play()\n\n\n#play and resume\ndef pause():\n    mixer.music.pause()\n\ndef unpause():\n    mixer.music.unpause()\n\ndef quitpy():\n    sys.exit()\n\n\ndef volup():\n    global vol\n    mixer.music.set_volume(vol + 0.1)\n\ndef voldown():\n    global vol\n    mixer.music.set_volume(vol - 0.1)\n\n\n\n#UI build and declarations\nroot = Tk()\nroot.title(\"AP Text to Speech\")\n\nmainframe = ttk.Frame(root, padding=\"3 3 12 12\")\nmainframe.grid(column=0, row=0, sticky=(N, W, E, S))\nroot.columnconfigure(0, weight=2)\nroot.rowconfigure(0, weight=2)\n\n#variable declarations 2\nurl = StringVar()\ncountryvar = StringVar()\nm = countryvar.get()\nm = str(m)\nprint(m)\n\ndef check():\n    m = countryvar.get()\n    m = str(m)\n    print(m)\n    return str(m)\n\n#webscrape function\ndef webscrape():\n    global x\n    global b\n    global thing\n    global countryvar\n    src = \"sound.mp3\"\n    dst = \"sound.ogg\"\n    murl = str(url.get())\n    response = requests.get(murl)\n    response.raise_for_status()\n    m = countryvar.get()\n    m = str(m)\n    print(m)\n    parse = bs4.BeautifulSoup(response.text, 'html.parser')\n    x = str(parse.get_text())\n    print(x)\n    text = gTTS(x, lang=m)\n    text.save(\"sound.mp3\")\n    AudioSegment.from_mp3(src).export(dst, format='ogg')\n    b.state(['!disabled'])\n\nurl_entry = ttk.Entry(mainframe, width=7, textvariable=url)\nurl_entry.grid(column=3, row=2, sticky=(W, E))\n\nttk.Label(mainframe, text=\"Enter URL:\").grid(column=3, row=1, sticky=S)\n\nttk.Button(mainframe, text=\"Scrape\", command=webscrape).grid(column=3, row=3, sticky=S)\n\nttk.Button(mainframe, text=\"Pause\", command=pause).grid(column=2, row=5, sticky=S)\nb = ttk.Button(mainframe, text=\"Play\", command=Play)\nb.grid(column=3, row=4, sticky=S)\nb.state(['disabled'])\nttk.Button(mainframe, text=\"Resume\", command=unpause).grid(column=4, row=5, sticky=S)\nttk.Button(mainframe, text=\"Quit\", command=quitpy).grid(column=3, row=5, sticky=S)\n\nttk.Label(mainframe, text=\"Set Volume:\").grid(column=3, row=6, sticky=S)\nttk.Button(mainframe, text=\"+\", command=volup).grid(column=2, row=7, sticky=S)\nttk.Button(mainframe, text=\"-\", command=voldown).grid(column=4, row=7, sticky=S)\n\nthing = ttk.Combobox(mainframe, textvariable=countryvar, values=[\"en\", \"fr\", \"es\", \"pt\"], state='readonly', width=5).grid(column=3, row=8)\ncountryvar.set('en')\n\ndef stt():\n    x = 1\n    while(1):\n        with spr.Microphone() as source2:\n\n            r.adjust_for_ambient_noise(source2, duration=0)\n\n            audio2 = r.listen(source2)\n\n            myText = r.recognize_google(audio2)\n            myText = myText.lower()\n\n            print(myText)\n            if (myText == 'quit'):\n                break\n\nttk.Button(mainframe, text=\"stt\", command=stt).grid(column=3, row=9)\n\nfor child in mainframe.winfo_children(): child.grid_configure(padx=5, pady=5)\n\nroot.bind('', webscrape)\n\nroot.mainloop()\n", "205": "#Imports\nimport os\nimport sys\nimport pygame\n\n#google text to speech\nfrom gtts import gTTS\n\n#requests and BeautifulSoup\nimport requests\nimport bs4\n\n#pygame audio player\nfrom pygame import mixer\n\n#tkinter ui\nfrom tkinter import *\nfrom tkinter import ttk\nfrom tkinter import filedialog\nfrom tkinter import messagebox\n\n#mp3 -> wav\nfrom os import path\nfrom pydub import AudioSegment\n\nimport speech_recognition as spr\nimport pyttsx3\n\nr = spr.Recognizer()\n\n#variable declarations\nx = str()\nvol = float(1.0)\n\n#Play button\ndef Play():\n    mixer.init()\n    mixer.music.load(\"sound.ogg\")\n    mixer.music.play()\n\n\n#play and resume\ndef pause():\n    mixer.music.pause()\n\ndef unpause():\n    mixer.music.unpause()\n\ndef quitpy():\n    sys.exit()\n\n\ndef volup():\n    global vol\n    mixer.music.set_volume(vol + 0.1)\n\ndef voldown():\n    global vol\n    mixer.music.set_volume(vol - 0.1)\n\n\n\n#UI build and declarations\nroot = Tk()\nroot.title(\"AP Text to Speech\")\n\nmainframe = ttk.Frame(root, padding=\"3 3 12 12\")\nmainframe.grid(column=0, row=0, sticky=(N, W, E, S))\nroot.columnconfigure(0, weight=2)\nroot.rowconfigure(0, weight=2)\n\n#variable declarations 2\nurl = StringVar()\ncountryvar = StringVar()\nm = countryvar.get()\nm = str(m)\nprint(m)\n\ndef check():\n    m = countryvar.get()\n    m = str(m)\n    print(m)\n    return str(m)\n\n#webscrape function\ndef webscrape():\n    global x\n    global b\n    global thing\n    global countryvar\n    src = \"sound.mp3\"\n    dst = \"sound.ogg\"\n    murl = str(url.get())\n    response = requests.get(murl)\n    response.raise_for_status()\n    m = countryvar.get()\n    m = str(m)\n    print(m)\n    parse = bs4.BeautifulSoup(response.text, 'html.parser')\n    x = str(parse.get_text())\n    print(x)\n    text = gTTS(x, lang=m)\n    text.save(\"sound.mp3\")\n    AudioSegment.from_mp3(src).export(dst, format='ogg')\n    b.state(['!disabled'])\n\nurl_entry = ttk.Entry(mainframe, width=7, textvariable=url)\nurl_entry.grid(column=3, row=2, sticky=(W, E))\n\nttk.Label(mainframe, text=\"Enter URL:\").grid(column=3, row=1, sticky=S)\n\nttk.Button(mainframe, text=\"Scrape\", command=webscrape).grid(column=3, row=3, sticky=S)\n\nttk.Button(mainframe, text=\"Pause\", command=pause).grid(column=2, row=5, sticky=S)\nb = ttk.Button(mainframe, text=\"Play\", command=Play)\nb.grid(column=3, row=4, sticky=S)\nb.state(['disabled'])\nttk.Button(mainframe, text=\"Resume\", command=unpause).grid(column=4, row=5, sticky=S)\nttk.Button(mainframe, text=\"Quit\", command=quitpy).grid(column=3, row=5, sticky=S)\n\nttk.Label(mainframe, text=\"Set Volume:\").grid(column=3, row=6, sticky=S)\nttk.Button(mainframe, text=\"+\", command=volup).grid(column=2, row=7, sticky=S)\nttk.Button(mainframe, text=\"-\", command=voldown).grid(column=4, row=7, sticky=S)\n\nthing = ttk.Combobox(mainframe, textvariable=countryvar, values=[\"en\", \"fr\", \"es\", \"pt\"], state='readonly', width=5).grid(column=3, row=8)\ncountryvar.set('en')\n\ndef stt():\n    x = 1\n    while(1):\n        with spr.Microphone() as source2:\n\n            r.adjust_for_ambient_noise(source2, duration=0)\n\n            audio2 = r.listen(source2)\n\n            myText = r.recognize_google(audio2)\n            myText = myText.lower()\n\n            print(myText)\n            if (myText == 'quit'):\n                break\n\nttk.Button(mainframe, text=\"stt\", command=stt).grid(column=3, row=9)\n\nfor child in mainframe.winfo_children(): child.grid_configure(padx=5, pady=5)\n\nroot.bind('', webscrape)\n\nroot.mainloop()\n", "206": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Aug 17 10:41:29 2021\n\n@author: Timur Guler\n\nDraft Script for Craigslist Seagull/Eastman Search\n\"\"\"\n\n# import packages\nimport pandas as pd\nimport numpy as np\nimport os\nimport requests\nimport json\nfrom bs4 import BeautifulSoup\nfrom datetime import date, timedelta\n\n# function to extract html as  beatiful soup object given url and headers\ndef extract_html(url, headers=None):\n    r = requests.get(url, headers = headers)\n    result = BeautifulSoup(r.text, 'html')\n    return result\n\n# function to get results based on key\ndef conditional_bs4_results_key(bs, tag, result_key, condition_key, condition_value):\n    '''\n    Function to extract tag value from webscrape result if another key condition is met. Useful when a tag\n    is used in multiple situations but is only relevant when other tag conditions are met\n    \n    Inputs:\n    - bs: BeatifulSoup object containing the webscrape result\n    - tag - tag used in main search\n    - result_key - key whose result is to be extracted\n    - condition_key - key which is sometimes present, and identifies relevant results when certain condition is met\n    - condition_result - value of key when condition is met\n    \n    Outputs:\n    - list of results\n    '''\n    output = []   \n    for res in bs.find_all(tag):\n        try:\n            cond_actual = res[condition_key]\n            if condition_value in list(cond_actual):\n                output.append(res[result_key])\n        except:\n            pass\n        \n    return list(set(output))\n\n\n# function to get results based on key\ndef conditional_bs4_results_text(bs, tag, condition_key, condition_value):\n    '''\n    Function to extract text value from webscrape result if another key condition is met. Useful when a tag\n    is used in multiple situations but is only relevant when other tag conditions are met\n    \n    Inputs:\n    - bs: BeatifulSoup object containing the webscrape result\n    - tag - tag used in main search\n    - condition_key - key which is sometimes present, and identifies relevant results when certain condition is met\n    - condition_result - value of key when condition is met\n    \n    Outputs:\n    - list of results\n    '''\n    output = []   \n    for res in bs.find_all(tag):\n        try:\n            cond_actual = [res[condition_key]]\n            if condition_value in cond_actual:\n                output.append(res.text)\n        except:\n            pass\n        \n    return list(set(output))\n\n\ndef get_table(urls, headers=None):\n    '''\n    \n\n    Parameters\n    ----------\n    urls : list of individual posting urls\n    headers : dictionary of headers - just user-agent string\n\n    Returns\n    -------\n    output : pandas dataframe\n        dataframe including price, title, posted and updated time, and text description of each guitar.\n\n    '''\n    \n    # set up lists to hold column values\n    titles = []\n    prices = []\n    posteds = []\n    updateds = []\n    descriptions = []\n    \n    # loop through each potential guitar listing page\n    for url in urls:\n        \n        # get searchable bs4 object\n        result = extract_html(url, headers=headers)\n        \n        # get title\n        title = conditional_bs4_results_text(result, 'span', 'id', 'titletextonly')[0]\n        titles.append(title)\n        \n        # get price\n        price = result.find_all('span', 'price')[0].text\n        prices.append(price)\n        \n        # get all dates - min will be posting date, max will be updating date\n        times = [res.text.replace('\\n', '').strip() for res in result.find_all('time')]\n        dates = pd.to_datetime(times).date\n        \n        posted = dates.min()\n        updated = dates.max()\n        \n        posteds.append(posted)\n        updateds.append(updated)\n        \n        # get body text description\n        body = conditional_bs4_results_text(result, 'section', 'id', 'postingbody')[0]\n        body = body.replace('QR Code Link to This Post', '').strip()\n        \n        descriptions.append(body)\n    \n    # create table\n    table_dict = {'title' : titles,\n                  'price' : prices, \n                  'posted' : posteds,\n                  'updated' : updateds,\n                  'body' : descriptions,\n                  'url' : urls}\n    \n    # convert to pd\n    output = pd.DataFrame(table_dict)\n    return output\n\n###########\n# Main Section\n###########\n\n# user agent and urls for seagull and eastman search\nheaders = {'user-agent' : 'Timur Guler search for seagull guitars tguler8@gmail.com'}\nseagull_url = 'https://charlottesville.craigslist.org/d/musical-instruments/search/msa?query=seagull'\neastman_url = 'https://charlottesville.craigslist.org/d/musical-instruments/search/msa?query=eastman'\n\n# get webscraping results as bs4 objects\nall_seagulls = extract_html(seagull_url, headers=headers)\nall_eastmans = extract_html(eastman_url, headers=headers)\n\n# get urls of individual posting pages\nseagull_urls = conditional_bs4_results_key(all_seagulls, 'a', 'href', 'class', 'result-image')\neastman_urls = conditional_bs4_results_key(all_eastmans, 'a', 'href', 'class', 'result-image')\nguitar_urls = seagull_urls + eastman_urls\n\n# get title, price, dates, and descriptions from post\nguitars = get_table(guitar_urls, headers= headers)\n\n# convert price to numeric\nguitars.price = guitars.price.str.replace('$', '').str.replace(',', '').astype(int)\n\ncutoff_date = date.today()-timedelta(days=21)\nguitars = guitars[(guitars.price < 1000) & (guitars.updated >= cutoff_date)]\n\n\n# save as csv\nguitars.to_csv('..\\guitars.csv', index=False)\n\n\n\n", "207": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Sat Sep 11 16:40:57 2021\r\n\r\n@author: Kiran B\r\n\"\"\"\r\n\r\nimport streamlit as st\r\nimport spacy_streamlit as sts\r\n \r\nfrom PIL import Image\r\n\r\nfrom bs4 import BeautifulSoup #converts the contents of a page into a proper format\r\nimport requests #used to get the content from a web page\r\nimport spacy\r\n\r\nimport pandas as pd\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nimport seaborn as sns\r\n\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\nst.set_option('deprecation.showPyplotGlobalUse', False)\r\n\r\nst.title('EMAIL TEMPLATE GENERATION')\r\n\r\nst.write(\"Using Webscraping and NLP Techniques to Generate an E-mail Template\")\r\n\r\nimage = Image.open('WebScraping_EmailTemplate.png')\r\nst.image(image, caption='Email Template Generation by Web-Scraping')\r\n\r\ndef Webscrape_divID(URL, div_id):\r\n    '''This function scrapes the website from the URL given to it.\\\r\n    It collects the entire website data and stores the data in the html format\\\r\n        Also it extracts the data segment based on the div_id'''\r\n    \r\n    HEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\r\n    \r\n    # Making the HTTP Request\r\n    webpage = requests.get(URL, headers=HEADERS)\r\n  \r\n    # Creating the Soup Object containing all data\r\n    soup = BeautifulSoup(webpage.content, \"html.parser\")\r\n\r\n    results = soup.find(id=div_id)\r\n    \r\n    st.write(results.get_text())\r\n    \r\n    # model = [\"en_core_web_trf\"]\r\n    # visualizers = [\"parser\"]\r\n    # sts.visualize(model, results.get_text(), visualizers)\r\n    \r\n    return results.get_text()\r\n\r\n\r\ndef Webscrape_Classname(URL, classname):\r\n    \r\n    '''This function scrapes the website from the URL given to it.\\\r\n    It collects the entire website data and stores the data in the html format \\\r\n    Also it extracts the data segment based on the classname'''\r\n    \r\n    HEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\r\n    \r\n    # Making the HTTP Request\r\n    webpage = requests.get(URL, headers=HEADERS)\r\n  \r\n    # Creating the Soup Object containing all data\r\n    soup = BeautifulSoup(webpage.content, \"html.parser\")\r\n   \r\n    results = soup.find(\"div\", class_= classname)\r\n    \r\n    st.write(results.get_text())\r\n    \r\n    return results.get_text()\r\n\r\n\r\ndef Word_Frequency(spacy_text):\r\n    '''Visualize the Noun and Verb frequencies in the extracted text'''\r\n    \r\n    #Filtering for nouns and verbs only\r\n    nouns_verbs = [token.text for token in spacy_text if token.pos_ in ('NOUN', 'VERB')]\r\n    \r\n    cv = CountVectorizer()\r\n    X = cv.fit_transform(nouns_verbs)\r\n    sum_words = X.sum(axis=0)\r\n    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\r\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\r\n    wf_df = pd.DataFrame(words_freq)\r\n    wf_df.columns = ['word', 'count']\r\n    \r\n    # sns.set(rc={'figure.figsize':(12,8)})\r\n    sns.barplot(x = 'count', y = 'word', data = wf_df, palette=\"GnBu_r\")\r\n    st.pyplot()\r\n    \r\n    st.write(\"**Word Count(Noun & Verb) of the Extracted Text:\\n**\")\r\n    st.write(wf_df)\r\n    \r\n\r\ndef POS_Tag(data):\r\n    '''Tag Parts of Speech to the Extracted data and visualize'''\r\n    \r\n    sts.visualize_parser(data)\r\n    sts.visualize_ner(data, labels=nlp.get_pipe(\"ner\").labels)\r\n    \r\n\r\n # Use case 1\r\ndef Replace_Content1(token):\r\n    '''Find and replace selected tokens for Usecase 1'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and (token.text == 'John' or token.text == 'John Dooley'):\r\n        return '[Your Name]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jennifer':\r\n        return \"[Your Manager's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return '[Date]'\r\n    return token.text\r\n\r\ndef FindnReplace1(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content1, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n# Use Case 2\r\ndef Replace_Content2(token):\r\n    '''Find and replace selected tokens for Usecase 2'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and (token.text == 'Joe' or token.text == 'Joe Brown'):\r\n        return '[Your Name]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Steve':\r\n        return \"[Your Manager's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return '[Sickness Date]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'ORG':\r\n        return '[Hospital/Clinic Name]'\r\n    if token.text == 'Joejoe.brown765@email.com555':\r\n        return '[Your Name]\\n [Your Email ID]'\r\n    if token.text == '555':\r\n        return '\\n [Your Contact'\r\n    if token.text == '5555':\r\n        return 'Number]'\r\n    return token.text\r\n\r\ndef FindnReplace2(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content2, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n# Use Case 3\r\ndef Replace_Content3(token):\r\n    '''Find and replace selected tokens for Usecase 3'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Smith':\r\n        return \"[Your Colleague's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonny\\n':\r\n        return \"[Your Name]\\n [Your Designation]\"\r\n    if token.text == \"Formal\":\r\n        return 'Formal Birthday Wishes'\r\n    if token.text == \"Mr.\":\r\n        return ''\r\n    return token.text\r\n\r\ndef FindnReplace3(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content3, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n# Use Case 4\r\ndef Replace_Content4(token):\r\n    '''Find and replace selected tokens for Usecase 4'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Paul JonesPhoneEmail':\r\n        return \"Regards,\\n [Your Name]\\n [Your Contact No.]\\n [Your Email ID]\\n\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return \"[Years of Experience]\"\r\n    if token.text == \"Address\":\r\n        return \"\"\r\n    if token.text == \"store\":\r\n        return \"\\b\"\r\n    if token.text == \"retail\":\r\n        return \"\\b\"\r\n    \r\n    return token.text\r\n\r\ndef FindnReplace4(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content4, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n# Use Case 5\r\ndef Replace_Content5(token):\r\n    '''Find and replace selected tokens for Usecase 5'''\r\n    \r\n    if token.text == \",\":\r\n        return \"\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Amy':\r\n        return \"[Your Colleague's Name],\\n\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonathan':\r\n        return \"\\n [Your Name]\\n [Your Contact number]\"\r\n    if token.text == \"Sincerely\":\r\n        return \"\\n Sincerely,\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return \"[Timeline] and [Reason for Appreciation]\"\r\n    return token.text\r\n\r\ndef FindnReplace5(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content5, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n# Use Case 6\r\ndef Replace_Content6(token):\r\n    '''Find and replace selected tokens for Usecase 6'''\r\n    \r\n    if token.text == \",\":\r\n        return \"\"\r\n    if token.text == \"Hello\":\r\n        return \"Dear [Sender's Name],\\n\"\r\n    if token.text == \"COLLEAGUE\":\r\n        return \"[Your Colleague's Name]\"\r\n    if token.text == \"Regards\":\r\n        return \"\\n Regards,\"\r\n    if token.text == \"NAME\":\r\n        return \"\\n [Your Name]\"\r\n    if token.text == \"do\":\r\n        return \"don't\"\r\n    if token.text == \"n\u00e2\u20ac\u2122t\":\r\n        return \"\\b\"\r\n    return token.text\r\n\r\ndef FindnReplace6(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content6, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n# Use Case 7\r\ndef Replace_Content7(token):\r\n    '''Find and replace selected tokens for Usecase 7'''\r\n    \r\n    if token.text == \",\":\r\n        return \"\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Sam':\r\n        return \"[Your Partner's Name],\\n\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonathan':\r\n        return \"\\n[Your Name]\\n[Your Contact Number]\"\r\n    if token.text == \"Please\":\r\n        return \"\\nPlease\"\r\n    if token.text == \"Thank\":\r\n        return \"\\nThank\"\r\n    if token.text == \"again!Sincerely\":\r\n        return \"\\b\\b, again!\\nSincerely,\"\r\n    return token.text\r\n\r\ndef FindnReplace7(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content7, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n# Collect User Input    \r\nst.write(\"**Select your desrired category for the Email Template:**\")\r\noption = st.selectbox(\"Drop down options\",\\\r\n                      ('Vacation Leave Email Template', 'Sick Leave Email Template',\\\r\n                       'Birthday Wishes Email Template', 'Cover Letter Email Template',\\\r\n                           'Employee work appreciation Email Template', 'Out of Office Email Template',\\\r\n                               'Business Deal Closure Email Template'))\r\nst.write('\\n**You have selected**:', option)\r\nst.text(\"Scraping Web......Done!\\n\")\r\nst.write('\\n\\n**Exracted text to be analyzed:**')\r\n\r\nnlp = spacy.load('en_core_web_trf')\r\n\r\nif option == 'Vacation Leave Email Template':\r\n    URL = \"https://www.thebalancecareers.com/formal-leave-of-absence-letter-request-example-2060597\"\r\n    \r\n    div_id = \"mntl-sc-block-callout-body_1-0-3\"\r\n    extract = Webscrape_divID(URL, div_id)\r\n        \r\n    # Parse the text with spaCy\r\n    spacy_text1 = nlp(extract)\r\n    \r\n    Word_Frequency(spacy_text1)\r\n    \r\n    POS_Tag(spacy_text1)\r\n    sts.visualize_tokens(spacy_text1, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n     \r\n    # Generate Template\r\n    template1 = FindnReplace1(spacy_text1)\r\n    st.write(\"**Your Template for Vacation Leave Email**\\n\")\r\n    st.text(template1)\r\n    \r\n    \r\nelif option == 'Sick Leave Email Template':\r\n    URL = \"https://www.thebalancecareers.com/sample-sickness-absence-excuse-letter-2060603\"\r\n    \r\n    div_id = \"mntl-sc-block-callout-body_1-0-3\"\r\n    extract = Webscrape_divID(URL, div_id)\r\n    \r\n    # Parse the text with spaCy\r\n    spacy_text2 = nlp(extract)\r\n    \r\n    Word_Frequency(spacy_text2)\r\n    \r\n    POS_Tag(spacy_text2)\r\n    sts.visualize_tokens(spacy_text2, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n        \r\n    # Generate Template\r\n    template2 = FindnReplace2(spacy_text2)\r\n    st.write(\"**Your Template for Sick Leave Email**\\n\")\r\n    st.text(template2)\r\n    \r\n    \r\nelif option == 'Birthday Wishes Email Template':\r\n    URL = \"https://www.targettraining.eu/happy-birthday-emails/\"\r\n    \r\n    classname = \"avia-promocontent\"\r\n    extract1 = Webscrape_Classname(URL, classname)\r\n        \r\n    # Parse the text with spaCy\r\n    spacy_text3 = nlp(extract1)\r\n    \r\n    Word_Frequency(spacy_text3)\r\n    \r\n    POS_Tag(spacy_text3)\r\n    sts.visualize_tokens(spacy_text3, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n    \r\n    # Generate Template\r\n    template3 = FindnReplace3(spacy_text3)\r\n         \r\n    st.write(\"**Your Template for Birthday Wishes Email**\\n\")\r\n    st.text(template3)\r\n    \r\n    \r\nelif option == 'Cover Letter Email Template':\r\n    URL = \"https://www.thebalancecareers.com/email-cover-letter-samples-2060246\"\r\n    \r\n    div_id = \"mntl-sc-block-callout-body_1-0-1\"\r\n    extract = Webscrape_divID(URL, div_id)\r\n    \r\n    phr1 = \"Store Manager Position\"\r\n    temp1 = extract.replace(str(phr1),\"[Role you are applying for]\")\r\n    \r\n    phr2 = \"Your Name\"\r\n    temp2 = temp1.replace(str(phr2), \"[Your Name]\")\r\n    \r\n    phr3 = \"Store Manager position\"\r\n    temp3 = temp2.replace(str(phr3),\"[Role you are applying for]\")\r\n    \r\n    phr4 = \"Payroll management, scheduling, reports, and inventory control expertise\"\r\n    temp4 = temp3.replace(str(phr4),\"\")\r\n    \r\n    phr5 = \"Extensive work with visual standards and merchandising high-ticket items\"\r\n    temp5 = temp4.replace(str(phr5),\"\")\r\n    \r\n    phr6 = \"retail management\"\r\n    temp6 = temp5.replace(str(phr6),\"[Your previous role]\")\r\n    \r\n    phr7 = \"XYZ Company:\"\r\n    temp7 = temp6.replace(str(phr7),\"[Company name you are applying for]:\\n[Your Skill Set]...for example\")\r\n    \r\n    # Parse the text with spaCy\r\n    spacy_text4 = nlp(temp7)\r\n    \r\n    Word_Frequency(spacy_text4)\r\n    \r\n    POS_Tag(spacy_text4)\r\n    sts.visualize_tokens(spacy_text4, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n    \r\n    # Generate Template\r\n    template4 = FindnReplace4(spacy_text4)\r\n         \r\n    st.write(\"**Your Template for Cover Letter Email**\\n\")\r\n    st.text(template4)\r\n\r\n\r\nelif option == 'Employee work appreciation Email Template':\r\n    URL = \"https://talkroute.com/7-sample-thank-you-notes-for-business/\"\r\n    \r\n    div_id = \"x-content-band-5\"\r\n    extract = Webscrape_divID(URL, div_id)\r\n    \r\n    phr1 = \"You showed\"\r\n    temp1 = extract.replace(str(phr1),\"\\nYou showed\")\r\n    \r\n    phr2 = \"I am\"\r\n    temp2 = temp1.replace(str(phr2),\"\\nI am\")\r\n    \r\n    # Parse the text with spaCy\r\n    spacy_text5 = nlp(temp2)\r\n    \r\n    Word_Frequency(spacy_text5)\r\n    \r\n    POS_Tag(spacy_text5)\r\n    sts.visualize_tokens(spacy_text5, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n    \r\n    # Generate Template    \r\n    template5 = FindnReplace5(spacy_text5)\r\n    \r\n    st.write(\"**Your Template for Employee Work Appreciation Email**\\n\")\r\n    st.text(template5)\r\n    \r\n\r\nelif option == 'Out of Office Email Template':\r\n    URL = \"https://www.ionos.com/digitalguide/e-mail/technical-matters/perfect-out-of-office-message-examples-and-templates/\"\r\n    \r\n    div_id = \"c118391\"\r\n    extract = Webscrape_divID(URL, div_id)\r\n    \r\n    phrase = \"Formal out of office reply with referral for customers\"\r\n    temp = extract.replace(str(phrase),\"\")\r\n    \r\n    phr1 = \"Feel free\"\r\n    temp1 = temp.replace(str(phr1),\"\\nFeel free\")\r\n    \r\n    phr2 = \"You can\"\r\n    temp2 = temp1.replace(str(phr2),\"\\nYou can\")\r\n    \r\n    phr3 = \"Thank you\"\r\n    temp3 = temp2.replace(str(phr3),\"\\nThank you\")\r\n    \r\n    # Parse the text with spaCy\r\n    spacy_text6 = nlp(temp3)\r\n    \r\n    Word_Frequency(spacy_text6)\r\n    \r\n    POS_Tag(spacy_text6)\r\n    sts.visualize_tokens(spacy_text6, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n    \r\n    # Generate Template\r\n    temp1 = FindnReplace6(spacy_text6)\r\n    \r\n    phr = \"MM / DD / YY\"\r\n    temp2 = temp1.replace(str(phr), \"[Your Date of Return]\")\r\n    \r\n    phr1 = \"( colleague@example.com )\"\r\n    temp3 = temp2.replace(str(phr1), \"[Your Colleague's Email ID]\")\r\n    \r\n    phr2 = \"( XXX - XXXX )\"\r\n    template6 = temp3.replace(str(phr2), \"[Your Colleague's Contact No.]\")\r\n         \r\n    st.write(\"**Your Template for Out of Office Email**\\n\")\r\n    st.text(template6)\r\n    \r\n\r\nelif option == 'Business Deal Closure Email Template':\r\n    URL = \"https://talkroute.com/7-sample-thank-you-notes-for-business/\"\r\n    \r\n    div_id = \"x-content-band-6\"\r\n    extract = Webscrape_divID(URL, div_id)\r\n    \r\n    # Parse the text with spaCy\r\n    spacy_text7 = nlp(extract)\r\n    \r\n    Word_Frequency(spacy_text7)\r\n    \r\n    POS_Tag(spacy_text7)\r\n    sts.visualize_tokens(spacy_text7, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n    \r\n    # Generate Template\r\n    template7 = FindnReplace7(spacy_text7)\r\n         \r\n    st.write(\"**Your Template for Business Deal Closure Email**\\n\")\r\n    st.text(template7)\r\n\r\n\r\n", "208": "# apps modules\nfrom chatbot_app.modules.dialogflow_msg import Server\nfrom chatbot_app.modules.status_news import StatusNews\nfrom chatbot_app.modules.dist2hospital import Dist2Hospital\nfrom chatbot_app.modules.diagnosis import Diagnosis\nfrom chatbot_app.modules.feedback import Feedback\nfrom chatbot_app.modules.cbot_response import CbotResponse\nfrom chatbot_app.modules.users_database import UserDB\nfrom chatbot_app.modules.webscrape import Webscrape\nfrom chatbot_app.modules.generate_graph import Gen_graph\nfrom chatbot_app.modules.notification import Notification\n\n#### FOR GLOBAL STATUS - FOR INFECTION STATUS INTENT ####\n\nclass Feature(Server):\n    \n    def __init__(self, request):\n        super().__init__(request)\n        self.sn = StatusNews(request)\n        self.d2h = Dist2Hospital(request)\n        self.dgs = Diagnosis(request)\n        self.fb = Feedback(request)\n        self.cr = CbotResponse(request)\n        self.udb = UserDB(request)\n        self.wbs = Webscrape()\n        self.gg = Gen_graph()\n        self.ntf = Notification()\n\n        self.intent = super().rcvIntent()\n        self.udb.storeId()\n\n    def main(self):\n        # --------------------------#\n        # INFECTION STATUS INTENT   #\n        # --------------------------#\n        if self.intent == \"infection-status-covid\":\n            return self.sn.infectionStatus()\n\n        # --------------------------#\n        # HEADLINE NEWS INTENT      #\n        # --------------------------#\n        elif self.intent == \"latest-news-covid\":\n            return self.sn.headlineNews()\n\n        # --------------------------#\n        # Distance to Hospital      #\n        # --------------------------#\n        elif self.intent == \"nearest-hospital-covid\" or self.intent == \"treatment-covid.yes.address\":\n            return self.d2h.dist2hospital()\n\n        # --------------------------#\n        # DIAGNOSIS INTENT          #\n        # --------------------------#\n        elif self.intent == \"diagnosis-covid\":\n            return self.dgs.diagnosis()\n\n        # --------------------------#\n        # SYNC  INTENT              #\n        # --------------------------#\n        elif self.intent == \"sync\":\n            # try:\n            self.wbs.statusScrapper()\n            self.wbs.newsScrapper()\n            self.gg.plot_it()\n            self.dgs.updateResponses()\n            self.main_text = \"Sync/update completed.\"\n            # except:\n            #     self.main_text=\"Error occurred. Contact admin to debug.\"\n            #     print(\"There is an error!\")\n            # finally:\n            return super().sendMsg(single=True)\n\n        # --------------------------#\n        # FEEDBACK GATHER           #\n        # --------------------------#\n        elif self.intent == \"feedback-bad\" or self.intent == \"feedback-good\":\n            return self.fb.store_fb()\n\n        # --------------------------#\n        # FEEDBACK COMMENT          #\n        # --------------------------#\n        elif self.intent == \"feedback\":\n            return self.fb.store_text_fb()\n        \n        # --------------------------#\n        # GOODBYE                   #\n        # --------------------------#\n        elif self.intent == \"goodbye\":\n            return self.cr.goodbye()\n        \n        # --------------------------#\n        # SUBSCRIPTION              #\n        # --------------------------#\n        elif self.intent == \"subscribe\":\n            return self.udb.subscribe()\n        elif self.intent == \"unsubscribe\":\n            return self.udb.unsubscribe()\n\n        # --------------------------#\n        # CHECKIN AFTER ASSESSMENT  #\n        # --------------------------#\n        elif self.intent == \"checkin-yes\" or self.intent == \"checkin-no\":\n            return self.udb.checkin()\n\n        # --------------------------#\n        # CHECKIN NOTIFICATION      #\n        # --------------------------#\n        elif self.intent == \"checkin-notification\":\n            self.ntf.send_checkin_days()\n            self.main_text = \"Notification sent!\"\n            return super().sendMsg(single=True)\n\n", "209": "\"\"\"\nWSGI config for mw_webscrape project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.1/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'mw_webscrape.settings')\n\napplication = get_wsgi_application()\n", "210": "from django.urls import reverse\nfrom django.shortcuts import render, redirect\nfrom django.views.generic import View, TemplateView\nfrom django.utils.decorators import method_decorator\nfrom django.http import HttpResponse, JsonResponse, Http404, HttpResponseNotFound\n\n\nfrom rest_framework import generics\nfrom rest_framework import viewsets\nfrom rest_framework.views import APIView\nfrom rest_framework.response import Response\nfrom rest_framework.renderers import TemplateHTMLRenderer\n\n\nfrom webscrape.application.models import *\nfrom webscrape.application.responses import *\n\nfrom webscrape.application.services import *\n\n\n\nclass NiftyGainers(viewsets.ViewSet):\n\n\trenderer_classes = [TemplateHTMLRenderer]\n\ttemplate_name = 'application/listings.html'\n\n\tdef list(self, request, *args, **kwrgs):\n\n\t\ttry:\n\t\t\tresult = NiftyGainersService.retrieve()\n\n\t\t\tif result.get('success'):\n\t\t\t\tdata = result.get('data')\n\t\t\telse:\n\t\t\t\tdata = list()\n\n\t\t\treturn Response({'data': data})\n\n\t\texcept Exception as e:\n\t\t\traise e\n\n\tdef retrieve(self, request, *args, **kwrgs):\n\n\t\ttry:\n\t\t\tresult = NiftyGainersService.filter()\n\n\t\t\tif result.get('success'):\n\t\t\t\tdata = result.get('data')\n\t\t\telse:\n\t\t\t\tdata = list()\n\n\t\t\treturn Response({'data': data}, template_name='application/lists.html')\n\n\t\texcept Exception as e:\n\t\t\traise e\n", "211": "#importing required libraries\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n\nclass Webscrape :\n# get URL to perform scraping using request\n    wiki = requests.get('https://en.wikipedia.org/wiki/Machine_learning')\n    soup = BeautifulSoup(wiki.text, \"html.parser\")\n    # displaying the title\n    def display_title(self):\n        for title in self.soup.find_all('title'):\n            print('\\033[1m' + \"Title of the website is : \",title.get_text() + '\\033[0m') # prints title in bold letters\n\n# Find all the links in the page (\u2018a\u2019 tag)\n    def display_all_links(self):\n        f = open(\"allLinks.txt\", \"w\")              # opening a file in write mode\n        # traverse paragraphs from soup\n        for link in self.soup.find_all(\"a\"):\n                data = str(link.get('href'))\n                f.write(data)\n                f.write(\"\\n\")\n        f.close()\n        print('\\033[1m' +\"All links output is stored in webscrape1.txt file \"+ '\\033[0m')\n\n# Iterate over each tag(above) then return the link using attribute \"href\" using get\n    def display_href_links(self):\n        f = open(\"hreflinks.txt\", \"w\")\n        # traverse paragraphs from soup and has attribute href to find all links\n        for link in self.soup.find_all(\"a\",attrs={'href': re.compile(\"^http\")}):\n            data = str(link.get('href'))\n            f.write(data)\n            f.write(\"\\n\")\n        f.flush()\n        f.close()\n        print('\\033[1m' + \"All href output links are stored in webscrape2.txt file  \" + '\\033[0m')\n\nif __name__ == '__main__':\n    # create class Webscrape object\n    web = Webscrape()\n    # call functions to dispaly title,all links and href links\n    web.display_title()\n    web.display_all_links()\n    web.display_href_links()\n\n\n\n\n\n\n\n\n", "212": "#Import dependencies\nimport os\nimport pandas as pd\nimport numpy as np\nimport sqlalchemy\nfrom sqlalchemy.ext.automap import automap_base\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import create_engine\nfrom flask import Flask, jsonify, render_template\nimport sqlalchemy\nfrom flask import Flask, render_template, redirect\nfrom flask_pymongo import PyMongo\nimport renewable_scrape\nimport json\n\n\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\nengine = create_engine(\"sqlite:///project.sqlite\")\n\n\n# reflect an existing database into a new model\nBase = automap_base()\n# reflect the tables\nBase.prepare(engine, reflect=True)\n\n# Save reference to the table\nDataset = Base.classes.dataset\n\n#Create Flask App\napp = Flask(__name__)\n\nSITE_ROOT = os.path.realpath(os.path.dirname(__file__))\njson_url = os.path.join(SITE_ROOT, \"templates\", \"USA.geojson\")\nheatmapdata = json.load(open(json_url))\n\n#Connect to Mongo DB\napp.config[\"MONGO_URI\"] = \"mongodb://localhost:27017/renewables\"\nmongo = PyMongo(app)\n\n@app.route(\"/\") \ndef welcome():\n\n    return render_template(\"index.html\")\n\n#Create route for scrape\n@app.route(\"/scrape\")\ndef scrape():\n    # Run the scrape function\n    renewable_data = renewable_scrape.renewable_scrape()\n\n    # Update the Mongo database using update and upsert=True\n    mongo.db.renewables.replace_one({}, renewable_data, upsert=True)\n    return redirect(\"/webscrape_sunburst\")\n\n#Create route for hydro-electric energy production\n@app.route(\"/hydro\")\ndef hydro():\n\n    return render_template(\"hydro.html\")\n\n#Create route for wind energy production\n@app.route(\"/wind\")\ndef wind():\n\n    return render_template(\"wind.html\")\n\n#Create route for heatmap\n@app.route(\"/heatmap\")\ndef heatmap():\n\n    return render_template(\"heatmap.html\")\n\n#Create route for solar energy production\n@app.route(\"/solar\")\ndef solar():\n\n    return render_template(\"solar.html\")\n\n#Create route for location\n@app.route(\"/location\")\ndef location():\n\n    return render_template(\"location.html\")\n\n#Create route for webscrape and sunburst\n@app.route(\"/webscrape_sunburst\")\ndef webscrape_sunburst():\n\n    #Take one instance from the Mongo DB\n    data = mongo.db.renewables.find_one()\n\n    return render_template(\"Webscrape_sunburst.html\",r_last_refresh=data[\"renewable_refresh\"],renewable_title_0=data[\"renewable_titles\"][0],renewable_link_0=data[\"renewable_links\"][0],renewable_title_1=data[\"renewable_titles\"][1],renewable_link_1=data[\"renewable_links\"][2], renewable_title_2 = data[\"renewable_titles\"][2],renewable_link_2=data[\"renewable_links\"][4],renewable_title_3=data[\"renewable_titles\"][3],renewable_link_3=data[\"renewable_links\"][6])\n\n#Create route for heatmap\n@app.route(\"/api/heatmap\")\ndef heatmapgeojson():\n    return jsonify(data = heatmapdata)\n\n\n@app.route(\"/data\")\ndef data():\n    \"\"\"Return dashboard.html.\"\"\"\n    return render_template(\"data.html\")\n\n\nif __name__ == '__main__':\n    app.run(debug=True)\n", "213": "from webscraper import WIKI_PREFIX\nimport time\nimport json\nimport os\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\n\n\nFILENAME = 'wikidata.json'\n\nclass WikiGraph(object):\n    def __init__(self):\n        self.graph = dict() #graph of keys:values where values are list types\n        self.visited = dict()\n\n    def wiki_bfs(self, s, destination): #do BFS given source node as link and destination link\n        def savedata(s, links, filename = FILENAME):\n            try:\n                with open(filename) as savefile1:\n                    data = json.load(savefile1)\n                    data[s] = links\n                with open(filename, 'w') as savefile2:\n                    json.dump(data, savefile2, indent = 2)\n\n            except FileNotFoundError:\n                edata = {s : links}\n                with open(filename, 'w') as newsavefile:\n                        json.dump(edata, newsavefile, indent = 2)\n\n        queue = []\n        self.visited = {s:True} #visited dictionary to track which nodes are visited\n        queue.append(s)\n        self.addEdges(s)  #neighbor links of source_node\n        start_time = time.time()\n        layer = 0\n\n        try:\n            with open(FILENAME) as jsonfile:\n                jsondata = json.load(jsonfile)\n        except FileNotFoundError:\n            print('Data file not found. Creating one...')\n\n        while queue:\n            print('#' * 100)\n            print(s)\n            print(f\"Moving from layer {layer} to {layer + 1}...\")\n            print('#' * 100)\n            layer += 1\n\n            for link in self.graph[s]: #each link in webscrape(s)   ##possible to refactor this into a map function?\n\n                if link not in self.visited: #hasn't been visited and not recorded\n                    print(link)\n                    self.visited[link] = True\n                    queue.append(link)\n                    try:\n                        if link in jsondata:\n                            self.graph[link] = jsondata[link]\n                            print(f'Pulled from {FILENAME}')\n                        else:\n                            self.addEdges(link)\n                    except:\n                        self.addEdges(link)\n\n                    savedata(link, self.graph[link]) #save to json\n\n                    # check if destination is reached\n\n                    if destination in self.graph[link]:\n                        print(\n                            f'Process finished with exit code 1: Breadth First Search Completed. Layers: {layer} Time: {round(time.time() - start_time, 3)}')\n                        return\n                    elif destination == link:\n                        print(f'Process finished with exit code 2: Breadth First Search Completed. Layers: {layer} Time: {round(time.time() - start_time, 3)}')\n                        return\n            s = queue.pop(0)\n\n    def addEdges(self,p): #adds a neighborlink set from webscrape function v to page p\n        self.graph[p] = self.webscrape(p)\n\n    def webscrape(self, url):  # takes wiki url and returns set of all the links\n        source = requests.get(url).text\n        soup = BeautifulSoup(source, 'html.parser')\n        # print(soup.prettify())\n\n        links = soup.find_all('a', href=True)  # filter html for href links\n        # print(links)\n        final_links = []\n        for link in links:  # extract links with regex\n            linkregex = re.compile(r'href=\"(\\w|/)+\"')\n            final_link = linkregex.search(str(link))\n            if final_link is not None and final_link not in self.visited:\n                # print(final_link.group())\n                final_links.append(f'{WIKI_PREFIX}{final_link.group()[6:-1]}')\n        final_links = list(set(final_links))  # rid duplicates\n        final_links.remove(f'{WIKI_PREFIX}/wiki/Main_Page')  # rid unnecessary link\n        # print(final_links,len(final_links))\n        return final_links\n", "214": "from selenium import webdriver\r\nfrom random import random\r\nfrom urllib.request import urlopen\r\nfrom bs4 import BeautifulSoup\r\nimport pandas as pd\r\nimport time\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport mplcursors\r\n\r\n\r\n\r\nclass webscrape():\r\n        \r\n        \r\n\r\n\r\n\r\n\r\n    def performCalculation(int):\r\n        url = \"https://www.basketball-reference.com/leagues/NBA_{}_totals.html\".format(year);\r\n        html = urlopen(url);\r\n\r\n        soup = BeautifulSoup(html);\r\n        soup.findAll('tr', limit =2);\r\n        headers = [th.getText() for th in soup.findAll('tr', limit=2)[0].findAll('th')]\r\n        print(\"1\");\r\n\r\n        headers = headers[1:];\r\n        rows = soup.findAll('tr')[1:]\r\n        player_stats = [[td.getText() for td in rows[i].findAll('td')] for i in range(len(rows))]\r\n\r\n        print(\"2\");\r\n\r\n        stats2 = pd.DataFrame(player_stats, columns = headers);\r\n        stats2.head(10);\r\n\r\n        stats = stats\r\n\r\n\r\n        stats.to_csv(r\"C:\\Users\\Liam\\Documents\\NBA_{}.csv\".format(year), index = False)\r\n        print(\"3\");\r\n\r\n    \r\ndriver = webdriver.Chrome(executable_path=r'C:\\Users\\Liam\\Documents\\chromedriver.exe');\r\n\r\n\r\nyear = int(input(\"Enter a year: \"))\r\nwebscrape.performCalculation(year);\r\nstats = pd.read_csv(r\"C:\\Users\\Liam\\Documents\\NBA_{}.csv\".format(year))\r\n'''t = stats.PTS/stats.G\r\nr = stats.STL/stats.G\r\n#Shows points per game, need to make it where it shows the names when hovered over\r\n\r\nfig, ax = plt.subplots()\r\nlines = ax.scatter(t, r)\r\nax.set_title(\"PPG for Players\")\r\ncursor = mplcursors.cursor(lines, hover=True)\r\ncursor.connect(\"add\", lambda sel: sel.annotation.set_text(stats.loc[sel.target.index].Player))\r\n\r\nplt.show()\r\n'''\r\n\r\n    \r\n", "215": "import logging\nfrom scrape import WebScrape\nimport write_to_csv\nimport marge_sort\n\n\ndef mainfunction():\n    ws = WebScrape()\n    covid_india = ws.covidIndia()\n    write_to_csv.write_csv(covid_india, \"india_report.csv\")\n    state_wise = ws.tableScrape()\n    header = ws.header\n    for n, row in enumerate(state_wise[1:]):\n        row.insert(0, n + 1)\n    logging.info(\"Sorting the States in alphabetical order\")\n    sorted_list = marge_sort.mergesort(state_wise[1:])\n    logging.info(\"Finished sorting\")\n    sorted_list.insert(0, header)\n    write_to_csv.write_csv(sorted_list, \"state_wise_table.csv\")\n    ws.close_webpage()\n\n\nif __name__ == \"__main__\":\n    mainfunction()\n", "216": "import urllib2\nfrom bs4 import BeautifulSoup\nimport sys\n\n#------------------------For webscrape 1\npage_count_acm=1\n#------------------------For webscrape 1\n\ndictionary = {}\n\n#------------------------For webscrape 2\npage_count_indeed = 0\nc_indeed = 1\ntest_indeed = 0\ncount_total_indeed = 0\ncounter_indeed = 1\ncal_page_indeed = 0\nval_indeed = 0\n#------------------------For webscrape 2\n\n#------------------------For webscrape 3\npage_count_ieee = 1\n#------------------------For webscrape 3\n\n#------------------------Dictionary count\ndic_count = 1\n\n\ndef webscrape(stream,area):\n\tglobal page_count_acm\n\tglobal dictionary\n\tif area == \"\":\n\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'?page='+str(page_count_acm)\n\telif stream == \"\": \n\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+area+'?page='+str(page_count_acm)\n\telse: quote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'/'+area+'?page='+str(page_count_acm)\n\tprint quote_page_acm,'\\n'\n\tpage_acm = urllib2.urlopen(quote_page_acm)\n\tsoup_acm = BeautifulSoup(page_acm, 'html.parser')\n\tbox_acm = soup_acm.find_all('div',attrs={'class':'aiResultsMainDiv'})\n\ttemp_acm = soup_acm.find('span',attrs={'class':'aiPageTotalTop'}) \n\tif  temp_acm == None:\n\t\tprint \"No results found\"\n\t\tsys.exit(1)\n\tcount_total_acm = int(temp_acm.get_text())\n\tfor bx_acm in box_acm:\n\t\ttitle_acm = bx_acm.find('div',attrs={'class':'aiResultTitle'}).get_text().strip()\n\t\t#print 'Title:',bx.find('div',attrs={'class':'aiResultTitle'}).get_text().strip()\n\t\turl_acm = bx_acm.find('div',attrs={'class':'aiResultTitle'}).find('h3').find('a').get('href')\n\t\t#print 'URL:',bx.find('div',attrs={'class':'aiResultTitle'}).find('h3').find('a').get('href')\n\t\tdetails_acm = bx_acm.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find_all('li')\n\t\tcompany_acm = details_acm[0].get_text().strip()\n\t\t#print 'Company:',details[0].get_text().strip()\n\t\tlocation_acm = details_acm[1].get_text().strip()\n\t\t#print 'Location:',details[1].get_text().strip()\n\t\tdate_acm = details_acm[2].get_text().strip()\n\t\t#print 'Date:',details[2].get_text().strip()+\"\\n\"\n\t\tif bx_acm.find('li',attrs={'id':'searchResultsCategoryDisplay'}) != None:\n\t\t\tcategory_acm = details_acm[3].get_text().strip()\n\t\telse :\n\t\t\tcategory_acm = 'None'\n\t\tdescription_acm = bx_acm.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip()\n\t\t#print 'Description:', bx.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip()\n\t\t\n\t\t\n\n\t\tif (title_acm,location_acm) in dictionary:\n\t\t\tdictionary[title_acm,location_acm] = (dictionary[title_acm,location_acm],[company_acm,date_acm,category_acm,description_acm,'acm'])\n\t\n\t\telse:\n\t\t\tdictionary[title_acm,location_acm] = ([company_acm,date_acm,category_acm,description_acm,'acm'])\n  \n\n\n\n    \t#dont use as of now\n    \t#print 'Company:',bx.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find('li',attrs={'class':'aiResultsCompanyName'}).get_text().strip()\n\t\t#print 'Location:',bx.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find_all('li')[1].get_text() + \"\\n\"\n\t\n\t\t#print bx.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip()\n\t\t#print box.find('div',attrs={'class':'aiResultTitle'})\n\t\t#find('h3').get_text()\n\n\tif (page_count_acm < count_total_acm):\n\t\tpage_count_acm = page_count_acm + 1\n\t\twebscrape(stream,area)\n\t#for (x,y) in dictionary:\n\t#\tprint x\n\t#\tprint y\n\t#\tprint dictionary[x,y][0]\n\t#\tprint dictionary[x,y][1]\n\t#\tprint dictionary[x,y][2]\n\t#\tprint dictionary[x,y][3]\n\t#\tprint \"\\n\"\n\t#print dictionary  \n\n\n\ndef webscrape1(stream,area):\n\tglobal page_count_indeed\n\tglobal c_indeed\n\tglobal test_indeed\n\tglobal count_total_indeed \n\tglobal counter_indeed\n\tglobal cal_page_indeed\n\tglobal val_indeed\n\tglobal dictionary\n\t\n\tquote_page_indeed = 'https://www.indeed.com/jobs?q='+stream+'&l='+area+'&start='+str(page_count_indeed)\n\tprint quote_page_indeed,'\\n'\n\tpage_indeed = urllib2.urlopen(quote_page_indeed)\n\tsoup_indeed = BeautifulSoup(page_indeed, 'html.parser')\n\t\n\t#a = str(soup_indeed)\n\t#f = open('1.html','w')\n\t#f.write(a)\n\t#f.close()\n\t\n\t#print soup_indeed\n\n\t\n\tbox_indeed = soup_indeed.find_all('div',attrs={'class':'row'})\n\ttemmp_indeed = soup_indeed.find('div',attrs={'id':'searchCount'})\n\tif temmp_indeed == None:\n\t\tprint 'No results found'\n\t\tsys.exit(1)\n\tif test_indeed == 0:\n\t\tcount_total_indeed = int(temmp_indeed.get_text().split()[5].replace(',',''))\n\t\twhile val_indeed <= count_total_indeed:\n\t\t\tcal_page_indeed = cal_page_indeed + 1\n\t\t\tval_indeed = 25 * cal_page_indeed\n\t\ttest_indeed = 1\n\t\tcal_page_indeed = cal_page_indeed + 1\n\t\tif cal_page_indeed > 50:\n\t\t\tcal_page_indeed = 50\n\t\t#print page_indeed\n\t\t#print val_indeed\n\t#print count_total_indeed\n\t\n\tfor bx_indeed in box_indeed:\n\t\t#print c_indeed\n\t\t#print 'Title:',bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get_text().strip()\n\t\ttitle_indeed = bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get_text().strip()\n\t\t#print 'Company:', bx_indeed.find('span',attrs={'class':'company'}).get_text().strip()\n\t\tcompany_indeed = bx_indeed.find('span',attrs={'class':'company'}).get_text().strip()\n\t\t#print 'Location:', bx_indeed.find('span',attrs={'class':'location'}).get_text().strip()\n\t\tlocation_indeed = bx_indeed.find('span',attrs={'class':'location'}).get_text().strip()\n\t\t#print 'Description:', bx_indeed.find('span',attrs={'class':'summary'}).get_text().strip(),'\\n'\n\t\tdescription_indeed = bx_indeed.find('span',attrs={'class':'summary'}).get_text().strip()\n\t\t#c_indeed = c_indeed+1\n\t\tdate_indeed = 'None'\n\t\tcategory_indeed = 'None'\n\t\n\tif (title_indeed,location_indeed) in dictionary:\n\t\tdictionary[title_indeed,location_indeed] = (dictionary[title_indeed,location_indeed],[company_indeed,date_indeed,category_indeed,description_indeed,'indeed'])\n\telse:\n\t\tdictionary[title_indeed,location_indeed] = ([company_indeed,date_indeed,category_indeed,description_indeed,'indeed'])\t\n\n\t\n\tif (counter_indeed", "217": "import webscrapeData\nimport dataCleaning\nimport uploadMySQL\nimport executeMySQLScripts\nimport createLukesPicksTxt\nimport sendEmail\n\n\nwebscrapeData.get_data()\ndataCleaning.clean_data()\nuploadMySQL.upload_MySQL()\nexecuteMySQLScripts.executeScripts()\ncreateLukesPicksTxt.create_Lukes_Picks()\nsendEmail.send_email()", "218": "# set up working directory\nimport  os\nos.chdir('/Users/michaelboles/Michael/Coding/2019/Realestate') # Mac\n#os.chdir('C:\\\\Users\\\\bolesmi\\\\Lam\\\\Coding\\\\Python\\\\2019\\\\Realestate') # PC\n\n# import zipcodes\nfrom csvreader import csvread\nfilename = 'zipcodes.csv'\nzipcodes_all = csvread(filename)\n\n# select subset of zip codes\nzipcodes = ['94025', '94026', '94027', '94028']\n#zipcode = zipcodes[0]\n\n# scrape MLS listings\nfrom scrapeweb_realtor import webscrape\ndata_raw = webscrape(zipcodes)\n\n# write .csv file with data\ndata_raw.to_csv('./data/data_menlo.csv')\n", "219": "import re\nfrom contextlib import suppress\nfrom json import JSONDecodeError\n\nfrom requests import HTTPError, RequestException\nfrom tqdm import tqdm\n\nfrom scrape.configs import CompanyResult, JobScrapeConfig\nfrom scrape.web_funcs import webscrape_results\n\n\ndef parse_results(\n    base_url: str, querystring, page: int, config: JobScrapeConfig\n) -> list[CompanyResult]:\n    \"\"\"Takes the params provided in main.py and generates dataclasses for\n    each job listing in BuiltInNYC, the job name, company info, and so forth.\n    \"\"\"\n    company_results = []\n    docs = webscrape_results(base_url, querystring=querystring)\n    jobs = [item.get(\"title\") for item in docs[\"jobs\"]]\n    job_desc = [item.get(\"body\") for item in docs[\"jobs\"]]\n    company_names = [item.get(\"title\") for item in docs[\"companies\"]]\n    alii = [item.get(\"alias\") for item in docs[\"companies\"]]\n    for idx, company in enumerate(\n        tqdm(\n            company_names,\n            desc=f\"Evaluating Companies | Bundle {page} of {config.total_pages}\",\n            unit=\"company\",\n        )\n    ):\n        job_title = re.sub(r\"\\(.*?\\)\", \"\", jobs[idx])\n        alias = alii[idx][9:]\n        company_dict = company_lookup(alias)\n        results = CompanyResult(\n            company_name=company,\n            job_name=job_title,\n            job_description=job_desc[idx],\n            inner_id=idx,\n            **company_dict,  # type: ignore\n        )  # type: ignore\n        company_results.append(results)\n    return company_results\n\n\ndef company_lookup(company_alias: str):\n    \"\"\"Looks up the company JSON in BuiltInNYC.\n    It passes this along to the superceding parse_results method,\n    which places it within the CompanyResult dataclass.\n    \"\"\"\n    with suppress(\n        JSONDecodeError, RequestException, HTTPError, TypeError, AttributeError\n    ):\n        company_page_url = f\"https://api.builtin.com/companies/alias/{company_alias}\"\n        comp_docs = webscrape_results(company_page_url, querystring={\"region_id\": \"5\"})  # type: ignore\n        data = {\n            \"street_address\": comp_docs.get(\"street_address_1\"),\n            \"suite\": comp_docs.get(\"street_address_2\"),\n            \"city\": comp_docs.get(\"city\"),\n            \"state\": comp_docs.get(\"state\"),\n            \"zip_code\": comp_docs.get(\"zip\"),\n            \"url\": comp_docs.get(\"url\"),\n        }\n        return data\n", "220": "\"\"\"\n    Implements functionality to webscrape job information from LinkedIn pages.\n\n    Author  :   Nishanth Jayram (https://github.com/njayram44)\n                Sean Touchstone (https://github.com/seant001)\n                Jay Wei         (https://github.com/Jay1020431)\n    Date    :   January 16, 2021\n\"\"\"\n\n# Import packages to do webscraping\nfrom bs4 import BeautifulSoup\nimport time\nimport requests\n\n# Relevant, easy-to-scrape information contained in a list\nDATA_LOC = [\n    ('span', {'class' : 'topcard__flavor'}), ('span', {'class' : 'topcard__flavor topcard__flavor--bullet'}),\n    ('h1', {'class' : 'topcard__title'})\n]\n\n# Used to handle more difficult scraping of job criteria further down the page\nJOB_CRIT = ('span', {'class' : 'job-criteria__text job-criteria__text--criteria'})\n\n\"\"\"Given a valid LinkedIn URL, scrapes pertinent information and returns a list of data values.\"\"\"\ndef scrape_page(URL):\n    r = requests.get(URL)\n    html_content = r.text\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    try:\n        info = [soup.find(*loc).text for loc in DATA_LOC]\n        s_level = soup.find(*JOB_CRIT)\n        e_type = s_level.find_next('span')\n        info.extend([s_level.text, e_type.text])\n        return info\n    except AttributeError:\n        return False\n\n\"\"\"Given a file of several (valid) LinkedIn URLs, scrapes all and combines information into a\nlist of lists.\"\"\"\ndef scraper(filename='jobslist.txt'):\n    output = []\n    job_dir = open('jobslist.txt', 'r')\n    for job in job_dir:\n        s = scrape_page(job)\n        if not s:\n            continue\n        output.append(s)\n    return output\n\n# Debugging procedures\ndef main():\n    print(\"Running webscrape on a single LinkedIn page: https://www.linkedin.com/jobs/view/2348402842/?refId=f1abe8b6-755d-4c06-a2db-f3bf1c70a591\")\n    print(\"Output:\", scrape_page(\"https://www.linkedin.com/jobs/view/2348402842/?refId=f1abe8b6-755d-4c06-a2db-f3bf1c70a591\"))\n\n    print(\"Running webscrape on a text file of LinkedIn pages: jobslist.txt\")\n    print(\"Output:\", scraper())\n\nif __name__ == '__main__':\n    main()", "221": "import os\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport logging\n\noptions = Options()\noptions.add_argument('--headless')\noptions.add_argument('--disable-gpu')\nPATH = os.getcwd() + \"/chromedriver.exe\"\n\n\nclass webScrape:\n\n    def __init__(self):\n        # driver = webdriver.Chrome(PATH, options=options)\n        self.driver = webdriver.Chrome(PATH)\n        self.driver.get(\"https://catalog.registrar.uiowa.edu/your-program/\")\n\n    def search_major(self, program):\n        # search_bar = self.driver.find_element_by_id(\"quicksearch\")\n        # search_bar.clear()\n        # search_bar.send_keys(program)\n        # search_bar.send_keys(Keys.RETURN)\n        # program_name = self.driver.find_element_by_class_name(\"filterimage\")\n        # span_name = program_name.find_element_by_tag_name(\"span\")\n        # for x in span_name:\n            # print(\"Hi\")\n        # except:\n            # print(\"This program does not exist, please enter another\")\n        buttons = WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable(\n            (By.XPATH, \"//*[contains(text(), '{},')]\".format(program)))).click()\n        # for btn in buttons:\n            # btn.click()\n            # break\n        WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable(\n            (By.XPATH, '//a[text()=\"Requirements\"]'))).click()\n        requirements_button = self.driver.find_element_by_id(\"requirementstexttab\")\n        requirements_button.click()\n        course_list = self.driver.find_element_by_class_name(\"sc_courselist\")\n        course_odd = course_list.find_elements_by_class_name('odd')\n        for course in course_odd:\n            try:\n                course_info = course.find_element_by_class_name(\"codecol\")\n                course_name = course_info.find_element_by_tag_name(\"td\")\n            except:\n                pass\n        course_even = course_list.find_elements_by_class_name(\"even\")\n        print(self.driver.title)\n        check_answer = False\n        while not check_answer:\n            user_input = input(\"Enter 0 to quit: \")\n            if user_input == \"0\":\n                check_answer = True\n                self.driver.close()\n                print(\"Hello\")\n\n\nstart_program = webScrape()\n# user_input1 = input(\"Enter Major: \")\nuser_input1 = \"Computer Science\"\nstart_program.search_major(user_input1)\n", "222": "# Webscrape and data from the website\nfrom Utils import *\n\nclass dimension_conversion:\n    def ft_to_cm(df_column):\n        df_column = df_column * 30.48\n        return df_column\n    def inch_to_cm(df_column):\n        df_column = df_column * 2.54\n        return df_column\n    def str_to_int_convers(df_column):\n        df_column = pd.to_numeric(df_column, errors = 'coerce')\n        return df_column\ndef webscrape_function(array):\n    df = pd.DataFrame()\n    for i in array:\n        df = pd.concat([df, pd.read_html('http://ufcstats.com/statistics/fighters?char=' + i + '&page=all')[0]])\n    return df\n\n# Array to obtain all fighters\nalphabet_array = ['a', 'b', 'c', 'd',\n                  'e', 'f', 'g', 'h',\n                  'i', 'j', 'k', 'l',\n                  'm', 'n', 'o', 'p',\n                  'q', 'r', 's', 't',\n                  'u', 'v', 'w', 'x',\n                  'y', 'z']\n\n# Choosing the event based on the link provided in the input function\nevent_name = input('Please enter link to event -> from ( www.ufcstats.com/event ): ')\n# Taking the event and putting it into a dataframe\nmost_recent_event = pd.read_html(event_name)\n\n# Concatenating the data to one dataframe\nall_fighter_df = webscrape_function(alphabet_array)\n", "223": "#!/usr/bin/env python\n\nimport os\nimport sys\nimport json\n\nsys.path.insert(0, 'src')\nfrom etl import convert_txt\nfrom model import autophrase\nfrom weight_phrases import change_weight\nfrom webscrape import webscrape\nfrom website import activate_website\nfrom utils import convert_report\n\n#def main(targets):\ndef main():\n    data_config = json.load(open('config/data-params.json'))\n    model_config = json.load(open('config/model-params.json'))\n    weight_config = json.load(open('config/weight-params.json'))\n    webscrape_config = json.load(open('config/webscrape-params.json'))\n    website_config = json.load(open('config/website-params.json'))\n    report_config = json.load(open('config/report-params.json'))\n    test_config = json.load(open('config/test-params.json'))\n\n    os.system('git submodule update --init')\n    \n    # Getting the target\n    # If no target is given, then run 'website'\n    if len(sys.argv) == 1:\n        targets = 'website'\n    else:\n        targets = sys.argv[1]\n        \n    if 'data' in targets:\n        convert_txt(**data_config)\n    if 'autophrase' in targets:\n        autophrase(data_config['outdir'], data_config['pdfname'], model_config['outdir'], model_config['filename'])\n    if 'weight' in targets:\n        try:\n            unique_key = '_' + sys.argv[2]\n            change_weight(unique_key, **weight_config)\n        except:\n            change_weight(unique_key='', **weight_config)\n    if 'webscrape' in targets:\n        try:\n            unique_key = '_' +  sys.argv[2]\n            webscrape(unique_key, **webscrape_config)\n        except:\n            webscrape(unique_key='', **webscrape_config)\n    if 'report' in targets:\n        convert_report(report_config['experiment_in_path'], report_config['experiment_out_path'])\n        convert_report(report_config['analysis_in_path'], report_config['analysis_out_path'])\n    if 'website' in targets:\n        activate_website(**website_config)\n    if 'test' in targets:\n        convert_txt(test_config['indir'], data_config['outdir'], test_config['pdfname'],)\n        autophrase(data_config['outdir'], test_config['pdfname'], model_config['outdir'], model_config['filename'])\n        change_weight(unique_key='', **weight_config)\n        webscrape(unique_key='', **webscrape_config)\n        convert_report(report_config['experiment_in_path'], report_config['experiment_out_path'])\n        convert_report(report_config['analysis_in_path'], report_config['analysis_out_path'])\n    return\n\n#if __name__ == '__main__':\n#    # run via:\n#    # python main.py data features model\n#    targets = sys.argv\n#    main(targets)\nmain()\n", "224": "from .WebScrape import WebScrape", "225": "# -*- coding: utf-8 -*-\n\"\"\"\nSpyder Editor\n\nThis is a temporary script file.\n\"\"\"\n\nfrom urllib import request\nfrom bs4 import BeautifulSoup\nimport re\nfrom math import ceil\nimport csv\n \n# Determine the number of pages to webscrape\nscac = \"http://securities.stanford.edu/filings.html\"\npage = request.urlopen(scac)\nsoup = BeautifulSoup(page, 'html.parser')\nheading = soup.find_all('h4')[-1].get_text()\ntotal_record_num = re.findall(r'\\d+', heading)[0]\ntotal_page_num = ceil(int(total_record_num) / 20)\n \n# Webscrape all pages\ncontainer = [(\"filing_name\", \"filing_date\", \"district_court\", \"exchange\", \"ticker\")]\ni = 1\nwhile i <= total_page_num:\n    url = scac + \"?page=\" + repr(i)\n    print(url)\n    page = request.urlopen(url)\n    soup = BeautifulSoup(page, 'html.parser')\n    table = soup.find('table', class_ = 'table table-bordered table-striped table-hover')\n    tbody = table.find('tbody')\n    for row in tbody.find_all('tr'):\n        columns = row.find_all('td')\n        c1 = re.sub(r'[\\t\\n]', '', columns[0].get_text()).strip()\n        c2 = re.sub(r'[\\t\\n]', '', columns[1].get_text()).strip()\n        c3 = re.sub(r'[\\t\\n]', '', columns[2].get_text()).strip()\n        c4 = re.sub(r'[\\t\\n]', '', columns[3].get_text()).strip()\n        c5 = re.sub(r'[\\t\\n]', '', columns[4].get_text()).strip()\n        container.append((c1, c2, c3, c4, c5))\n    i = i + 1\n \n    # Write to a CSV file\n    with open('scac.csv', 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerows(container)", "226": "from unittest import TestCase\nfrom data_collector import webscrape\nimport codecs\n\n\nclass TestDataCollector(TestCase):\n\n\n\n    def test_webscrape_user(self):\n        petition_html = codecs.open(\"test_petition.html\", 'r').read()\n        user_html = codecs.open(\"test_user.html\", 'r').read()\n\n        new_fields = webscrape(petition_html, user_html, \"user\")\n\n        self.assertEqual(new_fields[\"creator_type\"], \"user\")\n        self.assertFalse(new_fields[\"creator_has_website\"])\n        self.assertEqual(new_fields[\"creator_city\"], \"dallas\")\n        self.assertEqual(new_fields[\"creator_first_name\"], \"Clenesha\")\n        self.assertEqual(new_fields[\"creator_last_name\"], \"Garland\")\n        self.assertEqual(new_fields[\"creator_country\"], \"US\")\n        self.assertEqual(new_fields[\"creator_description\"], None)\n        self.assertEqual(new_fields[\"creator_display_name\"], \"Clenesha Garland\")\n        self.assertEqual(new_fields[\"creator_locale\"], \"en-US\")\n        self.assertEqual(new_fields[\"creator_photo\"], \"photos/2/im/tf/JDiMTfDinqfEyda-fullsize.jpg\")\n        self.assertEqual(new_fields[\"creator_state\"], \"TX\")\n        self.assertEqual(new_fields[\"creator_fb_permissions\"], 0)\n        self.assertTrue(new_fields[\"creator_has_slug\"])\n\n        self.assertEqual(new_fields[\"num_past_petitions\"], 1)\n        self.assertEqual(new_fields[\"num_past_victories\"], 1)\n        self.assertEqual(new_fields[\"num_past_verified_victories\"], 1)\n        self.assertEqual(new_fields[\"last_past_victory_date\"], '2015-12-18')\n        self.assertEqual(new_fields[\"last_past_verified_victory_date\"], '2015-12-18')\n\n\n        self.assertEqual(new_fields[\"ask\"], 'President Barack Obama: Sharanda Jones does not deserve to die in prison')\n        self.assertEqual(new_fields[\"calculated_goal\"], 300000)\n        self.assertGreater(len(new_fields[\"description\"]), 10)\n        self.assertTrue(new_fields[\"discoverable\"])\n        self.assertEqual(new_fields[\"display_title\"], 'President Barack Obama: Sharanda Jones does not deserve to die in prison')\n        self.assertEqual(new_fields[\"displayed_signature_count\"], 279890)\n        self.assertFalse(new_fields[\"is_pledge\"], False)\n        self.assertEqual(new_fields[\"is_victory\"], True)\n        self.assertEqual(new_fields[\"is_verified_victory\"], True)\n        self.assertEqual(new_fields[\"languages\"], ['en'])\n        self.assertEqual(new_fields[\"original_locale\"], 'en-US')\n        self.assertAlmostEqual(new_fields[\"progress\"], 93.2966666667)\n        self.assertEqual(len(new_fields[\"tags\"]), 9)\n        self.assertEqual(new_fields[\"victory_date\"], '2015-12-18')\n        self.assertTrue(new_fields[\"has_video\"])\n        self.assertEqual(new_fields[\"photo\"], \"photos/8/ut/wj/vnuTWJCdPnZLdes-fullsize.jpg\")\n        self.assertEqual(len(new_fields[\"targets_detailed\"]), 1)\n\n\n        self.assertEqual(len(new_fields), 35)\n\n\n\n    def test_webscrape_org(self):\n        petition_html = codecs.open(\"test_petition.html\", 'r').read()\n        user_html = codecs.open(\"test_org.html\", 'r').read()\n\n        new_fields = webscrape(petition_html, user_html, \"org\")\n\n        self.assertEqual(new_fields[\"creator_type\"], \"org\")\n        self.assertTrue(new_fields[\"creator_has_website\"])\n        self.assertEqual(new_fields[\"creator_city\"], \"Washington\")\n        self.assertEqual(new_fields[\"creator_photo\"], \"photos/5/rv/ss/EBrVSSjjVJpDkvK-fullsize.jpg\")\n        self.assertEqual(new_fields[\"creator_country\"], \"US\")\n        self.assertEqual(new_fields[\"creator_state\"], \"DC\")\n        self.assertTrue(new_fields[\"creator_has_slug\"])\n\n\n        self.assertTrue(new_fields[\"creator_has_address\"])\n        self.assertFalse(new_fields[\"creator_has_contact_email\"])\n        self.assertFalse(new_fields[\"creator_has_fb_page\"], None)\n        self.assertTrue(new_fields[\"creator_mission\"])\n        self.assertEqual(new_fields[\"creator_org_name\"], \"International Labor Rights Forum\")\n        self.assertEqual(new_fields[\"creator_tax_country_code\"], None)\n        self.assertEqual(new_fields[\"creator_tax_state_code\"], None)\n        self.assertEqual(new_fields[\"creator_zipcode\"], \"20006\")\n        self.assertEqual(new_fields[\"creator_postal_code\"], \"20006\")\n\n        self.assertFalse(new_fields[\"creator_has_twitter\"])\n        self.assertFalse(new_fields[\"creator_has_verified_req\"])\n        self.assertFalse(new_fields[\"creator_has_verified_by\"])\n        self.assertFalse(new_fields[\"creator_has_verified_at\"])\n        self.assertFalse(new_fields[\"creator_has_video\"])\n\n\n        self.assertEqual(new_fields[\"num_past_petitions\"], 5)\n        self.assertEqual(new_fields[\"num_past_victories\"], 5)\n        self.assertEqual(new_fields[\"num_past_verified_victories\"], 5)\n        self.assertEqual(new_fields[\"last_past_victory_date\"], '2011-10-18')\n        self.assertEqual(new_fields[\"last_past_verified_victory_date\"], '2011-10-18')\n\n        self.assertEqual(len(new_fields), 43)\n\n\n\n\n\n\n", "227": "from selenium import webdriver \nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport csv\nimport urllib.request\nfrom lxml import html\nimport time\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.chrome.options import Options\nfrom pprint import pprint as pp\nfrom tabulate import tabulate\n\n\nscrape_file = r'Classic-Imports-and-Design\\python webscrape\\url-lists\\eichholtz-sku.csv'\nlogin_url = 'https://eichholtzusa.com/customer/account/login/'\n\nwith open(scrape_file, encoding='UTF-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    scrape_file = list(reader)\n\nwritefile = open('Classic-Imports-and-Design\\python webscrape\\output.csv', 'w+', encoding='UTF8', newline='')\noutput_file = csv.writer(writefile, delimiter=\",\")\n\n\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option('excludeSwitches', ['enable-logging'])\ndriver = webdriver.Chrome('Classic-Imports-and-Design\\python webscrape\\chromedriver.exe', options=options)\n\n#login\nprint(\"Please log into the wholesale account.\")\nprint('regencyantiqmd@aol.com')\nprint('108Alaska!07')\ndriver.get(login_url)\ninput()\n\nfile_header = ['sku', 'Name', 'Brand', 'Categories', 'Wholesale', 'Retail', 'Description', 'Images']\noutput_file.writerow(file_header)\nfails = []\n\nfor product in scrape_file:\n    try:\n        product[0] = product[0].strip()\n        product[1] = product[1].strip()\n        data = []\n        #Search for product\n        try:\n            driver.get('https://eichholtzusa.com/catalogsearch/result/?q=' + product[0])\n        except: \n            fails.append(product[0] + ' Search did not succeed') \n        #Click search result\n        try:\n            element_present = EC.presence_of_element_located((By.CLASS_NAME, 'product-item-link'))\n            WebDriverWait(driver, 6).until(element_present)\n            driver.find_elements(By.CLASS_NAME, 'product-item-link')[0].click()\n        except TimeoutException:\n            fails.append(product[0] + ' took too long to load')\n        #Wait for product page to load\n        time.sleep(3)\n        #sku\n        try:\n            info_wrap = driver.find_elements(By.CLASS_NAME, 'product-info-price')[0]\n            sku = info_wrap.find_element(By.CLASS_NAME, 'value').text\n            data.append(sku)\n        #name\n            name = driver.find_element(By.CLASS_NAME, 'page-title').text.title()\n            data.append(name)\n        except:\n            fails.append(product[0] + 'has no sku / name')\n            continue\n        #brand\n        data.append('Eichholtz')\n        #Catagories\n        data.append(product[1])\n        #wholesale\n        try:\n            wholesale = float( info_wrap.find_elements(By.TAG_NAME, 'meta')[0].get_attribute('content') )\n            data.append(wholesale)\n            #retail\n            data.append(wholesale*2.5)\n        except:\n            fails.append(product[0] + ' has no price')\n            data.append(0)\n            data.append(0)\n        #description & DIMENSIONS\n        dimensions = ''\n        product_finish = ''\n        description = ''\n        try:\n            dimensions = info_wrap.find_element(By.CLASS_NAME, 'measurement-inch').get_attribute('innerHTML') + '\\n'\n        except:\n            fails.append(product[0] + ' has no dimensions')\n        try:\n            product_finish = info_wrap.find_elements(By.TAG_NAME, 'p')[0].text + '\\n'\n        except:\n            fails.append(product[0] + ' has no finish')\n        try:\n            desc_wrap = driver.find_element(By.CLASS_NAME, 'product-description')\n            description = desc_wrap.find_elements(By.CLASS_NAME, 'additional-info')[0].text\n        except:\n            fails.append(product[0] + 'has no description')\n        data.append( dimensions + product_finish + description )\n        #images\n        try:\n            time.sleep(2)\n            image_list = driver.find_elements(By.CLASS_NAME, 'fotorama__nav__frame--thumb')\n            image_urls = []\n            image_filenames = []\n            c = 0\n            if not image_list:\n                image_urls.append( driver.find_element(By.CLASS_NAME, 'fotorama__loaded--img').get_attribute('href') )\n            else:\n                for pic in image_list:\n                    if c != 0:\n                        pic.click()\n                        time.sleep(2)\n                    image_urls.append( driver.find_element(By.CLASS_NAME, 'fotorama__active').get_attribute('href') )\n                    c = c+1\n            c = 1\n            for pic in image_urls:\n                file_name = sku + '-' + str(c)\n                print(pic)\n                urllib.request.urlretrieve(pic, 'Classic-Imports-and-Design/python webscrape/product-images/' + file_name + '.webp')\n                image_filenames.append(file_name)\n                c = c + 1\n            try: \n                pic = driver.find_element(By.CLASS_NAME, 'dimensions-image').get_attribute('src')\n                file_name = sku + '-' + str(c)\n                urllib.request.urlretrieve(pic, 'Classic-Imports-and-Design/python webscrape/product-images/' + file_name + '.png')\n                image_filenames.append(file_name)\n            except:\n                print('Standard Dimensions')\n        except:\n            fails.append(product[0] + ' FAILED TO GRAB IMAGES')\n        \n        pp(fails)\n        data.append( ','.join(image_filenames) )\n        print_table = data.copy()\n        print_table[6] = 'desc...'\n        print_table[7] = 'images...'\n        print(tabulate([file_header] + [print_table]))\n        print(data[6])\n        print(data[7] + '\\n')\n        output_file.writerow(data)\n    except:\n        fails.append(product[0] + ' FAILED DUE TO UNKNOWN ERROR')\npp(fails)\nwritefile.close()\n\n#driver.get(url)\n#time.sleep()\n#content = driver.page_source\n#soup = BeautifulSoup(content, \"html.parser\")\n#tree = html.fromstring(driver.page_source) \n\n#driver.find_elements(By.XPATH, XPATH)\n#driver.find_elements(By.CLASS_NAME, CLASSNAME)\n#soup.find(\"TAG-TYPE\", class_=\"CLASS-NAME\").text\n\n#urllib.request.urlretrieve(IMG-URL, FILENAME) \n\n#output_file.writerow(data)", "228": "# -*- coding: utf-8 -*-\nimport os, re, json, traceback\nimport requests\nfrom bs4 import BeautifulSoup\nfrom pyltp import SentenceSplitter\nhrefs = []\nmeaning = []\n\nclass WebScrape(object):\n    def __init__(self, word, url):\n        self.url = url\n        self.word = word\n\n    # \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n    def web_parse(self):\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n        req = requests.get(url=self.url, headers=headers)\n\n        # \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n        if req.status_code == 200:\n            soup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n            return soup\n        return None\n\n    # \u722c\u53d6url\n    def get_url(self):\n        soup = self.web_parse()\n        if soup:\n            lis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n            if lis:\n                for li in lis('li'):\n                    if '", "229": "# -*- coding: utf-8 -*-\n# author: songwei\n# place: Shenzhen Guangdong\n# time: 2020/4/28 13:57\nimport os, re, json, traceback\nimport requests\nfrom bs4 import BeautifulSoup\nfrom pyltp import SentenceSplitter\n\nclass WebScrape(object):\n    def __init__(self, word, url):\n        self.url = url\n        self.word = word\n\n    # \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n    def web_parse(self):\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n        req = requests.get(url=self.url, headers=headers)\n\n        # \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n        if req.status_code == 200:\n            soup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n            return soup\n        return None\n\n    # \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n    def get_gloss(self):\n        soup = self.web_parse()\n        if soup:\n            lis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n            if lis:\n                for li in lis('li'):\n                    if '", "230": "import os, discord, webscrape\nfrom dotenv import load_dotenv\nfrom discord.ext import commands , tasks\n\n\n# Load secrets from .env file.\nload_dotenv()\nTOKEN = os.getenv('DISCORD_TOKEN') \nCHANNELID = int(os.getenv('CHANNELID'))\n\n# Setup the Client connection\nintents = discord.Intents.default()\nintents.message_content = True\nbot = commands.Bot(command_prefix='$', intents=intents)\n\n@bot.event\nasync def on_ready():\n    print(f'We have logged in as {bot.user}')\n\n    # Start task\n    on_change.start()\n \n\n# Bot functions \n@bot.command()\nasync def test(ctx, arg):\n    await ctx.send(arg)\n\n\n#Get last post\nasync def get_last_msg_async():\n    channel = bot.get_channel(int(CHANNELID))\n    if channel is None:\n        return\n  \n    \n    message = await channel.fetch_message(channel.last_message_id)\n    if message.embeds:   \n        return message.embeds[0]\n\n    return\n\n# Posting news\n@tasks.loop(minutes=2)   \nasync def on_change():\n    print('Getting news')\n    last_news = await get_last_msg_async()\n    post = webscrape.get_last_news()\n    embed = discord.Embed(title=post[0], description=post[1], url=post[2])\n\n    if last_news == embed:\n        return\n    else:  \n        channel = bot.get_channel(CHANNELID)\n        await channel.send(embed=embed)\n\nbot.run(TOKEN)\n\n", "231": "from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nimport datetime as dt\nfrom airflow.utils.dates import days_ago\n\ndefault_args = {\n    'owner': 'Jason',\n    'start_date': days_ago(0),\n    'email': ['123@gmail.com'],\n    'email_on_failure':False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': dt.timedelta(minutes = 5),\n}\n\ndag = DAG('housing_data_dag',\n            default_args= default_args,\n            description = 'Housing data extraction pipeline',\n            schedule_interval = dt.timedelta(days = 14),)\n\nt1 = BashOperator(\n    task_id = 'initial webscrape of housing data',\n    bash_command = 'python3 /Users/jasontruong/Downloads/Learn/Rental/Housing/webscrape/seleniumWebscrape.py',\n    dag = dag\n)\n\nt2 = BashOperator(\n    task_id = 't2',\n    bash_command = 'echo \"yes\"',\n    dag = dag\n)\n# t3 = BashOperator(\n#     task_id = 't2',\n#     bash_command = 'echo \"yes\"',\n#     dag = dag\n# )\n# t4 = BashOperator(\n#     task_id = 't2',\n#     bash_command = 'echo \"yes\"',\n#     dag = dag\n# )\n\n\nt1 >> t2\n\n", "232": "#!/usr/bin/env python\n\nimport urllib2\nimport re\n# not all systems have readline...if not, just pass and continue.\ntry:\n    import readline  # nice when you need to use arrow keys and backspace\nexcept:\n    pass\nimport sys\n\n\ndef scrape():\n    site = raw_input(\"Enter page: \")\n\n    #open site. read so we can read in a string context\n    #test for valid and complete URL\n    try:\n        data = urllib2.urlopen(site).read()\n    except ValueError:\n        print \"INVALID URL: Be sure to include protocol (e.g. HTTP)\"\n        return\n    \n    #print data\n\n    #try an open the pattern file.\n    try:\n        patternFile = open('config/webscrape.dat', 'r').read().splitlines()\n    except:\n        print \"There was an error opening the webscrape.dat file\"\n        raise\n    #create counter for counting regex expressions from webscrape.dat\n    counter = 0\n    #for each loop so we can process each specified regex\n    for pattern in patternFile:\n        m = re.findall(pattern, data)\n        #m will return as true/false. Just need an if m:\n        if m:\n            for i in m:\n                #open output/results file...append because we are cool\n                outfile = open('scrape-RESULTS.txt', 'a')\n            #print m\n                outfile.write(str(i))\n                outfile.write(\"\\n\")  # may be needed. can always be removed.\n\n            #close the file..or else\n                outfile.close()\n                counter+=1\n                print \"Scrape item \" + str(counter) + \" successsful. Data output to scrape-RESULTS.txt.\"\n        else:  # only need an else because m is boolean\n            counter+=1\n            print \"No match for item \" + str(counter) + \". Continuing.\"\n            # Continue the loop if not a match so it can go on to the next\n            # sequence\n            # NOTE: you don't *really* need an else here...\n            continue\n", "233": "from product_info_get import WebScrape\n\nclass UserData:\n\n    def __init__(self, name):\n        self.name = name\n        self.freq_data = {}\n        self.freq_data[1] = set()\n        self.list_current = []\n        self.price_getter = WebScrape()\n\n    def list_reset(self):\n        self.list_current = []\n\n    def find_freqkey(self, product):\n        key = None\n        for k in self.freq_data.keys():\n            if product in self.freq_data[k]:\n                key = k\n                break\n\n        return key\n\n    def is_recommended_product(self, product):\n        return self.find_freqkey() > 0\n\n    def freq_upgrade(self, product, freq_prev):\n        self.freq_data[freq_prev].remove(product)\n\n        if freq_prev + 1 in self.freq_data.keys():\n            self.freq_data[freq_prev + 1].add(product)\n        else:\n            self.freq_data[freq_prev + 1] = set()\n            self.freq_data[freq_prev + 1].add(product)\n\n    def add_product(self, product):\n        self.list_current.append(product)\n        product_freqkey = self.find_freqkey(product)\n        if not product_freqkey:\n            self.freq_data[1].add(product)\n        else:\n            self.freq_upgrade(product, product_freqkey)\n\n    def get_ordered_recs_prices(self):\n        recs = []\n        for freq in sorted(list(self.freq_data.keys()), reverse = True):\n            recs.extend(self.freq_data[freq])\n\n        price = self.price_getter.selenium_script(recs)\n        return recs, price\n\n    def debug_print(self):\n        print(self.freq_data, self.list_current)\n\n\n\n", "234": "from EU_Legislation_NLP.web_scraping.bs4_EUR_Lex import scrape_contents\nfrom EU_Legislation_NLP.api_calls.stage_1 import getEntities_1\nfrom EU_Legislation_NLP.api_calls.stage_2 import getEntities_2\nfrom EU_Legislation_NLP.data.file_operations import write_json, read_json\nimport json\nimport sys\n\ndef main():\n\n    #Prompt for URL input\n    url = input(\"Enter link to legislation on EUR-Lex: \")\n\n    #TESTING DATA (use splice [500:800])\n    #url = 'https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1536601415699&uri=CELEX:32018R0643'\n\n\n    #--------------PHASE 1: Legislation Text Extraction---------------------------------------------------\n    #Obtain legislation title and text by calling scrape_contents()\n    try:\n        legislation = scrape_contents(url)\n\n    except ValueError:\n        #ValueError is thrown when input is not a supported EUR-Lex URL\n        print(\"Invalid URL\")\n        sys.exit(-1)\n\n    #Export and reload JSON data to 'data/legislation_webscrape.txt' for offline processing\n    write_json(legislation, 'data/legislation_webscrape.txt')\n    legislation = read_json('data/legislation_webscrape.txt')\n\n    #text = legislation[\"text\"]\n\n    text = legislation[\"text\"][500:800] #Limit range for testing purposes\n    # print(text) #Debugging purposes\n\n    #TESTING DATA\n    # text = 'Directive (EU) 2015/849 to facilitate supervision by competent authorities. As defined in Regulation (EC) No 1049/2001 of the European Parliament and of the Council'\n    # text = '\u2018household customer\u2019 means household customer as defined in point 25 of Article 2 of Directive 2009/73/EC; 2.   Products listed in Annex I which are admitted free of import duties pursuant to Council Regulation (EC) No 1186/2009 (9) shall not be subject to the additional import duty. Article 8. Regulation (EC) No 673/2005 is repealed.'\n\n\n\n    #--------------PHASE 2: Stage 1 Model---------------------------------------------------\n    #Call getEntities_1 with text to obtain sections containing the Directives/Regulations referenced in it\n    response_1_raw = getEntities_1(text)\n    # print(json.dumps(response_1_raw, indent=3)) #Debugging purposes\n\n    #Export and reload JSON data to 'data/stage1_data.txt' for offline processing\n    write_json(response_1_raw, 'data/stage1_data.txt')\n    response_1_raw = read_json('data/stage1_data.txt')\n\n\n\n    #--------------Stage 2 Preprocessing---------------------------------------------------\n    #Extract text sections containing references to be analysed in Stage 2\n    response_1_extract = [] #List to store dicts containing each reference text and type\n\n    for item in response_1_raw['entities']: #Iterate through detected entities\n        text = item['text'].replace('/', \" / \") #Extract text and add spaces between the forward slash\n        text = text.replace('(', \" (\")\n        type = item['type'] #Extract reference type\n        response_1_extract.append({'text': text, 'type': type})\n    response_1_extract = {'reference': response_1_extract}\n\n    #Export and reload dictionary with references to 'data/stage1_extract.txt' for offline processing\n    write_json(response_1_extract, 'data/stage1_extract.txt')\n    response_1_extract = read_json('data/stage1_extract.txt')\n    # print(json.dumps(response_1_extract, indent=3)) #Debugging purposes\n\n\n\n    #--------------PHASE 3: Stage 2 Model---------------------------------------------------\n    #Call getEntities_2 with each reference to obtain breakdown of its components\n    response_2_raw = []\n    for counter, item in enumerate(response_1_extract['reference']):\n        response_2_raw.append(getEntities_2(item['text']))\n\n    #Export and reload dictionary to 'data/stage2_data.txt'\n    write_json(response_2_raw, 'data/stage2_data.txt')\n    response_2_raw = read_json('data/stage2_data.txt')\n    #print(json.dumps(response_2_raw[1]['entities'], indent=3)) #Debugging purposes\n\n\n    #--------------Results Processing---------------------------------------------------\n    response_2_extract = [] #List to store dicts containing each component of reference\n    for item in response_2_raw:\n        response_2_component_dict = {}\n        for component in item['entities']:\n            type = component['type']\n            text = component['text']\n            response_2_component_dict[type] = text\n        response_2_extract.append(response_2_component_dict)\n    response_2_extract = {'result': response_2_extract}\n\n    # Export and reload dictionary with results to 'data/stage2_extract.txt' for offline processing\n    write_json(response_2_extract, 'data/stage2_extract')\n    response_2_extract = read_json('data/stage2_extract')\n    print(json.dumps(response_2_extract, indent=3))\n\n\n\n\n#-----------------Execute program-------------------------------------------\nif __name__ == '__main__':\n    main()", "235": "import argparse\n\nfrom instruments.extract import webscrape\nfrom instruments.transform import document\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--scrape', action='store_true', help='scrape data from Gibson website')\nparser.add_argument('--mongo', action='store_true', help='insert data into MongoDB')\n\nargs = parser.parse_args()\n\nif args.scrape:\n    webscrape.main()\n\nif args.mongo:\n    document.main()\n", "236": "\"\"\"\nWSGI config for webscrape_the_one project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.2/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webscrape_the_one.settings')\n\napplication = get_wsgi_application()\n", "237": "\"\"\"\nWSGI config for webscrape_project project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.2/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webscrape_project.settings')\n\napplication = get_wsgi_application()\n", "238": "from django.apps import AppConfig\n\n\nclass WebscrapeConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'webScrape'\n", "239": "from telegram.ext import CommandHandler\nfrom telegram.ext import CallbackContext\nfrom telegram.ext import  Updater\nimport constants\nimport telegram\nimport webscrape\nimport datetime\nimport pytz\n\ndef callback_alarm(context: CallbackContext):\n    soup = webscrape.webscrape()\n    for index,i in enumerate(soup.select('tr[align=\"left\"]')):\n        if index != 0:\n            mylist=[td.text for td in i.find_all('td')]\n            jobtitle = mylist[0]\n            jobtype = mylist[1]\n            joblocation = mylist[2]\n            jobarea = mylist[3]\n            joblink='https://careers.a-star.edu.sg/'+i.find('a')['href']\n            finaltext = f\"\"\"({index}).\\nJob title: {jobtitle}\\nJob type: {jobtype}\\nJob Location: {joblocation}\\nJob Field: {jobarea}\\nJob Link: {joblink}\\n\"\"\"\n            context.bot.send_message(chat_id=context.job.context, text=finaltext)\n    context.bot.send_message(chat_id=context.job.context, text=\"DONE POSTING.\")\ndef start(update: telegram.Update, context: CallbackContext):\n    context.bot.send_message(chat_id=update.message.chat_id,\n                             text='Welcome. I will pull data at 9:00 AM , 12:00 PM and 5:30 PM.')\n\n    context.job_queue.run_daily(callback_alarm,datetime.time(hour=9, minute=00, tzinfo=pytz.timezone('Asia/Singapore')),days=(0, 1, 2, 3, 4, 5, 6) , context=update.message.chat_id)\n    context.job_queue.run_daily(callback_alarm,datetime.time(hour=12, minute=00, tzinfo=pytz.timezone('Asia/Singapore')),days=(0, 1, 2, 3, 4, 5, 6) , context=update.message.chat_id)\n    context.job_queue.run_daily(callback_alarm,datetime.time(hour=17, minute=30, tzinfo=pytz.timezone('Asia/Singapore')),days=(0, 1, 2, 3, 4, 5, 6) , context=update.message.chat_id)\n\ndef main():\n    u = Updater(constants.TOKEN, use_context=True)\n    timer_handler = CommandHandler('start', start)\n    u.dispatcher.add_handler(timer_handler)\n    u.start_polling()\n\nif __name__ == '__main__':\n    main()\n", "240": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Tue Nov  9 03:57:47 2021\r\n\r\n@author: Marcos Maciel\r\n\r\nDescription: \r\nThis is basically a bot created for Warzone that reads the screen and detects the people you kill/get killed by \r\nand it webscrapes their stats from cod.tracker.gg. This can be used to help you identify a cheater.\r\n\"\"\"\r\n\r\nimport pyautogui\r\nimport time\r\nimport cv2\r\nfrom pytesseract import *\r\nfrom PIL import Image\r\n#import cloudscraper\r\nfrom selenium import webdriver\r\n#from webdriver_manager.chrome import ChromeDriverManager\r\ndriver = webdriver.Chrome(executable_path=r\"C:/Users/.../chromedriver.exe\")\r\nfrom selenium.webdriver.common.by import By\r\nimport mss\r\nimport mss.tools\r\n\r\npytesseract.tesseract_cmd = r'C:\\Users\\...\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\r\n\r\ndef webscrape(username):\r\n    t = time.time()\r\n    pcent = username.index('%')\r\n    username = username[:pcent+1] + '23' + username[pcent+1:]\r\n    print(username)\r\n    \r\n    options = webdriver.ChromeOptions()\r\n    options.headless = True\r\n    driver = webdriver.Chrome(options=options)\r\n    #driver = webdriver.Chrome()\r\n    driver.get(f'https://cod.tracker.gg/warzone/profile/atvi/{username}/overview')\r\n    #print(driver.content)\r\n    page_title = driver.find_elements(By.CLASS_NAME, 'lead')\r\n    \r\n    if not page_title or page_title[0] == \"WARZONE STATS NOT FOUND\":\r\n        print(\"WARZONE STATS NOT FOUND - Private profile\")\r\n    \r\n    else:\r\n        search = driver.find_elements(By.CLASS_NAME, 'value')\r\n        if len(search) > 4:\r\n            print(\"Wins:\", search[0].text)\r\n            print(\"Win %:\", search[1].text)\r\n            print(\"Kills:\", search[2].text)\r\n            print(\"K/D:\", search[3].text)\r\n            print(\"Score/min:\", search[4].text)\r\n        else:\r\n            print(\"Incorrect name or private profile\")\r\n    elapsed = time.time() - t\r\n    print(elapsed, \"Time to webscrape\")\r\n    \r\n    #time.sleep(1)\r\n    driver.close() \r\n    runCrappyCode()\r\n\r\ndef runCrappyCode():\r\n        \r\n    found = False\r\n    time.sleep(1)\r\n    while not found:\r\n        #Marcos, you might have to compare to different examples to get more accuracy and make this headless\r\n        t = time.time()\r\n        coords = pyautogui.locateOnScreen('1ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n        coords1 = pyautogui.locateOnScreen('2ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n        coords2 = pyautogui.locateOnScreen('3ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n        coords3 = pyautogui.locateOnScreen('6ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n\r\n        \r\n        elapsed = time.time() - t\r\n        #print(elapsed)\r\n        if coords or coords1 or coords2 or coords3:\r\n            t = time.time()\r\n            with mss.mss() as sct:\r\n                # The screen part to capture\r\n                region = {'top': 938, 'left': 696, 'width': 339, 'height': 35}\r\n            \r\n                # Grab the data\r\n                img = sct.grab(region)\r\n            \r\n                # Save to the picture file\r\n                mss.tools.to_png(img.rgb, img.size, output='screenshot2.png')\r\n            \r\n            #im2 = pyautogui.screenshot(region = (700, 937, 263, 33))\r\n            elapsed = time.time() - t\r\n            #print(elapsed)\r\n            print('found something')\r\n            #im2.save(r\"screenshot2.png\")\r\n            \r\n            img = cv2.imread('screenshot2.png')\r\n            img = cv2.resize(img, dsize=(526, 66), interpolation=cv2.INTER_CUBIC)\r\n            '''\r\n            dsize = (526, 66)\r\n\r\n            # resize image\r\n            output = cv2.resize('screenshot2.png', dsize)\r\n            cv2.imwrite('gray1.png',output)\r\n            \r\n            '''\r\n            cv2.imwrite('screenshot2.png', img)\r\n            #img = Image.open('screenshot2.png')\r\n            result = pytesseract.image_to_string(img)\r\n            result = result.rstrip()\r\n            \r\n            if result == '' or '#' not in result:\r\n                print(\"No string found\")\r\n                runCrappyCode()\r\n                \r\n            if ']' in result:\r\n                slicing = result.find(']')\r\n                newResult = result[slicing+1:]\r\n                newResult = newResult.replace('#', '%')\r\n                print(newResult)\r\n                \r\n                webscrape(newResult)\r\n                \r\n            elif ']' not in result: \r\n                newResult = result.replace('#', '%')\r\n                print(newResult)\r\n                webscrape(newResult)\r\n                \r\n        else:\r\n            #print('ImageNotFound exception')\r\n            runCrappyCode()\r\n        found = True\r\n\r\nrunCrappyCode()", "241": "from __future__ import absolute_import, unicode_literals\nimport os\nfrom celery import Celery\nfrom celery.schedules import crontab\nfrom datetime import timedelta\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webscrape_the_one.settings')\n\napp = Celery('webscrape_the_one')\napp.config_from_object('django.conf:settings', namespace='CELERY')\n\napp.autodiscover_tasks()\n\n# app.conf.beat_schedule = {\n#     'scrape-every-minute': {\n#         'task': 'the_one.tasks.scrape_periodically',\n#         'schedule': crontab(),\n#     }\n# }\n", "242": "from bs4 import BeautifulSoup\nimport requests\nimport databaseRoutines\n\n\n\ndef webscrape():\n    # grab the information from the website we'll be scrapping\n    request = requests.get('https://mtgadecks.net/')\n    soup = BeautifulSoup(request.text, 'html.parser')\n\n    # get the links for all tier 1 decks\n    links = soup.find(class_='col-md-3 float-left')\n\n    # grab the first four decks, because that is all that will fit in tier 1\n    webscrapedDecks = []\n    tierDecks = soup.find_all(class_='tier')\n    for index, deck in enumerate(tierDecks):\n        if (index < 4):\n            newDeck = deck.get_text().replace('\\n', '').replace(' ', '')\n            webscrapedDecks.append(newDeck)\n\n    linkNames = []\n    for link in links.find_all('a', href=True):\n        linkNames.append(\"https://mtgadecks.net\" + link['href'])\n\n    # connect to the database. If it doesn't exist, create it\n    databaseRoutines.createDBIfNonexistent()\n\n    # add to db any new decks, and send an email with the change(s)\n    databaseRoutines.addDecksToDB(webscrapedDecks, linkNames)\n\n    # update the database of any removals\n    databaseRoutines.updateDB(webscrapedDecks)\n\nwebscrape() #actually run the program", "243": "import kijiji_webscrape as scraper\nimport centris_webscrape as scraper2\nimport csv\n\nif __name__ == \"__main__\":\n    url = \"https://www.kijiji.ca/b-ordinateurs/ville-de-montreal/tablette/k0c16l1700281?rb=true&dc=true\"\n    \n    # url2 = \"https://www.centris.ca/fr/condo-appartement~a-louer?uc=2\"\n\n    scrapin = scraper.KijijiScraper(url,\"allo\")\n    appartz = scrapin.findApparts()\n\n    # scrapin2 = scraper2.CentrisScraper(url2,\"allo\")\n    # appartz2 = scrapin2.findApparts()\n\n    # with open(\"EzAppart.csv\", mode=\"w\", encoding='utf-8') as f:\n    #     appartz_writer = csv.writer(f, delimiter=\",\")\n    #     appartz_writer.writerow([\"Title\", \"Price\", \"Description\"])\n\n    #     for i in range(len(appartz[0])):\n    #         appartz_writer.writerow([appartz[0][i], appartz[1][i], appartz[2][i]])\n\n\n\n", "244": "import os, sys\nimport json\nimport datetime\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef get_td_name(td):\n    a_tag = td.findAll('a')\n    for a in a_tag:\n        if 'href' in a.attrs:\n            name = a.attrs['href'].split('/')[-1]\n            # td_name = name.replace('_', '/')\n            return name\n        else:\n            print(a)   \n    \n    return None\n\n    \n# def get_ImageCollection(name):\n    \n#     all_name_options = [name.replace('_', '/')]\n#     options_count = len(name.split('_'))\n    \n#     if options_count-1 > 1:\n#         for i in range(options_count):\n#             # first option all replaced already in list\n#             split_name = name.split('_')\n\n#     else:\n#         # add in the original text, with underscore\n#         all_name_options.append(name)\n\n        \n#     # find the right name that will satisfy the ImageCollection argument\n#     for id_option in all_name_options:\n    \n#         try:\n#             IC = ee.ImageCollection(id_option)\n#             if IC:\n#                 return id_option, IC\n\n#         except Exception as e:\n#             print(e)\n#             print(\"NEED TO ADJUST NAME: \", name, all_name_options)\n            \n    \n\ndef get_ImageCollection_tags(td):\n    tag_list = []\n    a_tag = td.findAll('a')\n    for a in a_tag:\n        if 'href' in a.attrs:\n            tag = a.attrs['href'].split('/')[-1]\n            if tag not in tag_list:\n                tag_list.append(tag)\n            \n    return tag_list\n\n\ndef get_tbody_info(tbody):\n    tbody_info = {}\n    td_list = tbody.findAll('td')\n\n    for td in td_list:\n        if td.attrs['class'] == ['ee-dataset']:\n            td_name = get_td_name(td)\n            \n        elif td.attrs['class'] == ['ee-dataset-description-snippet']:\n            quick_description = td.text\n            \n        elif td.attrs['class'] == ['ee-tag-buttons', 'ee-small']:\n            tag_list = get_ImageCollection_tags(td)\n            \n        else:\n            print(\"Undefined attributes in td object: \", td.attrs)\n        \n\n    # IC_id, image_collection = get_ImageCollection(td_name)\n        \n    tbody_info = {'dataset': td_name,\n                   'tags': tag_list,\n                   'description': quick_description\n                  }\n\n    return tbody_info\n\n\ndef parse_code_block(code_block):\n    \"\"\" Break out the Code Embedded on Earth-Engine Dataset URL\n\n    Example output of code_block:\n\n        ee.ImageCollection(\"\")\n\n    dataset_type follows the ee.-> before first '('\n    dataset_id inside \" \"\n\n    \"\"\"\n    # Extract Datatepy in code block\n    dataset_type = code_block.text.split('.')[1]\n    dataset_type = dataset_type.split('(')[0]\n\n    # Extract ID from the code block 'ee.Image...(\"\")'\n    dataset_id = code_block.text.split('(')[1]\n    dataset_id = dataset_id.split(')')[0]\n    dataset_id = dataset_id.replace('\"','')\n\n    return dataset_type, dataset_id\n\n\n\ndef validate_availability(text):\n    \"\"\" we will use this a test function to extract a start/end date\"\"\"\n\n    # Test 0: ensure the text is actually a string before splitting\n    try:\n        assert isinstance(text, str), \"not a string\"\n    except Exception as e:\n        # raise Exception('Date Validate Failed: {}'.format(e))\n        print(e)\n        return None\n\n    # Test 1: The text must be split evenly by ' - '\n    split_text = text.split(' - ')\n    try:\n        assert len(split_text) == 2, \"len()==2 Test\"\n    except Exception as e:\n        # raise Exception('Date Validate Failed: {}'.format(e))\n        print(e)\n        return None\n\n    # Test 2: DateTime Formatting\n    # Test 2a: First split has DateTime format (Most Common ISO format)\n    try:\n        # date_start = datetime.strptime(split_text[0], '%Y-%m-%')\n        date_start = datetime.datetime.fromisoformat(split_text[0])\n\n    # Test 2b: Use custom strptime, non-ISO\n    except Exception as e:\n        date_start = datetime.datetime.strptime(split_text[0], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n        assert isinstance(date_start, datetime.datetime), \"Not a Datetime object\"\n\n    except:\n        # raise Exception('Date Validate Failed: {}'.format(e))\n        print(\"Not a datetime object: {}\".format(split_text[0]))\n        return None\n\n    finally:\n        # Since we are packing this into a json file, the best way is to keep string\n        # the date end will be determined later, but if we succeeded in date start\n        # We have found our Date Availability Match!\n        date_range = {'dataset_start': split_text[0], 'dataset_end': split_text[1]}\n        return date_range\n\n\ndef seek_date_availability(soup):\n    \"\"\" This is difficult to be exact, the html is not clearly/uniquely marked\"\"\"\n    date_range = None\n    # Method 1: the first dd on the page:\n    try:\n        dd = soup.find(\"dd\")\n        date_range = validate_availability(dd.text)\n        assert date_range is not None, \"Method 1\"\n        return date_range\n    except Exception as e:\n        # raise Exception('Failed on {}, {}'.format(dd, e))\n        print(e)\n\n    # Method 2: all dd's on page, find the one that satisfies a time format validation\n    for dd in soup.findAll(\"dd\"):\n        date_range = validate_availability(dd.text)\n        if date_range is not None:\n            return date_range\n\n    print(\"METHOD 2 FAILED TO FIND DATE AVAILABILITY in each Method, SET DEFAULT\")\n    # determine Earliest and Latest Date and set those as defaults\n    default_start = None\n    default_end = None\n    date_range = {'dataset_start': default_start, 'dataset_end': default_end}\n    return date_range\n\n\n\n\ndef follow_dataset_link(tag_name, URL):\n    dataset_url = URL + '/' + tag_name\n    response = requests.get(dataset_url)\n    htmlCode = response.text\n    soup = BeautifulSoup(response.text, 'html.parser')\n    code = soup.find(\"code\")\n\n    dataset_type, dataset_id = parse_code_block(code)\n    result = {'dataset_id': dataset_id, 'dataset_type': dataset_type}\n    result.update(seek_date_availability(soup))\n\n    return result\n\n\ndef save_catalog(results, partial=False):\n    \"\"\"Save this simple metadata to json \"\"\"\n    PLUGIN_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    METADATA_DIR = os.path.join(PLUGIN_DIR, \"metadata\") \n\n    # Base Filename\n    fn = 'gee_catalog'\n    ftype = '.json'\n    \n    # if we have partial results, track the last result index\n    if partial:\n        fn = fn + '_(' + str(len(results)-1) + ')'\n\n    # Add all\n    fn = fn + ftype\n    # now get full file path + name\n    filename = os.path.join(METADATA_DIR, fn)\n\n    with open(filename, 'w') as outfile:\n        json.dump(results, outfile, indent=4)\n\n\n\ndef get_catalog():\n    #=====================================\n    # 0. Run the requests on the Google Earth Engine Datasets Catalog\n    #=====================================\n\n    # From the \"View all datasets\" Tab on developers.google.com/earth-engine\n    url = \"https://developers.google.com/earth-engine/datasets/catalog\"\n    response = requests.get(url) \n    htmlCode = response.text \n    soup = BeautifulSoup(response.text, 'html.parser') \n\n    # the 'tbody' element seems to be the best way to extract the useful metadata for each dataset\n    all_tbodies = soup.findAll(\"tbody\")\n\n    # total GEE datasets: 409 as of 2020-07-21\n    # print(len(all_tbodies))\n\n    #=====================================\n    # 1. Iterate through each tbody to extract metadata\n    #=====================================\n\n    # this will result in a list of each dataset saved as a json metadata file\n    # for allowing user to filter on tags/geography/time before \n    # making requests to the gee server\n    webscrape_results = []\n\n    for tbody in all_tbodies:\n        # 1a. tbody will have 3 basic infos: ImageCollection Name, Tags, Description\n        \n        try:\n            this_result = get_tbody_info(tbody)\n            more_result = follow_dataset_link(this_result['dataset'], url)\n            this_result.update(more_result)\n            webscrape_results.append(this_result)\n\n            update_msg = \"Webscrape Status: {} out of {}\".format(len(webscrape_results), len(all_tbodies))\n            print(update_msg, end=\"\\r\", flush=True)\n\n        except Exception as e:\n            print(e)\n            print(\"failed data extract on: \", tbody)\n            save_catalog(webscrape_results, partial=True)\n            return\n\n\n    #=====================================\n    # 2. Save All metadata to json, all tests passed\n    #=====================================\n    save_catalog(webscrape_results, partial=False)\n\n\n\ndef working_catalog():\n    \"\"\" Manual filter for current working version\"\"\"\n    all_catalog = read_catalog()\n    working_IC_set = [\n            'WorldPop/GP', \n            'S5P', \n            'NCEP_RE', \n            'NOAA'\n            # 'NOAA/GOES', \n            # 'NOAA/NWS',\n            # 'NOAA/VIIRS'\n            ]\n    non_working_IC_set = [\n            'NOAA/CFSR'\n            'NOAA/DMSP-OLS/NIGHTTIME_LIGHTS'\n            ]\n\n    non_working_FC_set = [\n            'BLM/AIM/v1/TerrADat/TerrestrialAIM',\n            'FAO/GAUL/2015/level0',\n            'NOAA/NHC/HURDAT2/atlantic',\n            'NOAA/NHC/HURDAT2/pacific'\n            ]\n\n    use_catalog = []\n\n    for data in all_catalog:\n\n        if data['dataset_type'] == 'FeatureCollection':\n            if data['dataset_id'] not in non_working_FC_set:\n                use_catalog.append(data)\n\n        if data['dataset_type'] == 'ImageCollection':\n            if data['dataset_id'] in non_working_IC_set:\n                pass\n            else:\n                for working in working_IC_set:\n                    if working in data['dataset_id']:\n                        use_catalog.append(data)\n        \n\n\n    save_catalog(use_catalog)\n    return\n\n\ndef read_catalog():\n    catalog_fn = os.path.join('metadata', 'gee_catalog.json')\n    fpath = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    fn = os.path.join(fpath, catalog_fn)\n\n    if os.path.exists(fn):\n        with open(fn, 'r') as in_file:\n            data = json.load(in_file)\n            return data\n\n    else:\n        print(\"No Catalog Found...\\nnow running ee_catalog.py\")\n        get_catalog()\n        return read_catalog()\n\n\n\ndef test():\n    working_catalog()\n\nif __name__ == '__main__':\n    read_catalog()\n    # working_catalog()\n", "245": "from selenium import webdriver \nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport csv\nimport urllib.request\nfrom lxml import html\nimport time\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom pprint import pprint as pp\n\nscrape_file = 'Classic-Imports-and-Design/python webscrape/url-lists/interlude-sku.csv'\nlogin_url = 'https://www.interludehome.com/'\n\nwith open(scrape_file) as f:\n    reader = csv.reader(f, delimiter=',')\n    scrape_file = list(reader)\n\nwritefile = open('Classic-Imports-and-Design\\python webscrape\\output.csv', 'w+', encoding='UTF8')\noutput_file = csv.writer(writefile, delimiter=\",\")\n\ndriver = webdriver.Chrome('Classic-Imports-and-Design\\python webscrape\\chromedriver.exe')\n#login\ndriver.get(login_url)\nprint('regencyantiqmd@aol.com')\nprint('108Rudin!07')\nprint(\"Please log into the wholesale account.\")\ninput()\n\ndata = ['sku', 'Name', 'Brand', 'Categories', 'Wholesale', 'Retail', 'Description', 'Images', 'Length', 'Width', 'Height']\noutput_file.writerow(data)\nfails = []\n\nfor product in scrape_file:\n    product[0] = product[0].strip()\n    product[1] = product[1].strip()\n    data = []\n    driver.get('https://www.interludehome.com/default/catalogsearch/result/?q=' + product[0])\n    time.sleep(2.5)\n    try:\n        item_search_result = driver.find_elements(By.XPATH, '/html/body/div[1]/main/div[2]/div/div[6]/div/div[2]/ol/li/div/div/strong/a')[0]\n        item_search_result.click()\n    except:\n        fails.append(product[0] + ' - could not load product page.')\n        print(product[0] + ' does not exist')\n        continue\n    #Product Page\n    time.sleep(3)\n    #SKU\n    data.append(product[0])\n    #Name\n    try:\n        product_name = driver.find_elements(By.CLASS_NAME, 'pro-name')[0].text\n        data.append(product_name)\n    except:\n        fails.append([product[0] +  ' has no name.'])\n        continue\n    #Brand\n    data.append('Interlude Home')\n    #Category\n    data.append(product[1])\n    #Wholesale\n    try:\n        wholesale = driver.find_elements(By.CLASS_NAME, 'price')[0].text\n        wholesale = wholesale.lstrip('$').replace(',','')\n        data.append(wholesale)\n    #Retail\n        retail = round(float(wholesale)*2)\n        data.append(retail)\n    except:\n        data.append('0')\n        data.append('0')\n        fails.append(product[0] + ' has no wholesale price : ' + product[1])\n\n    #Description\n    try:\n        description = driver.find_elements(By.XPATH, '//*[@id=\"description\"]/div[1]/div[1]')[0].text\n        data.append(description)\n    except:\n        data.append(product_name)\n        fails.append(product[0] + ' has no description!')\n    #Images\n    try:\n        image_caro = driver.find_elements(By.CLASS_NAME, 'fotorama__nav__frame--thumb') \n        c = 1\n        img_filenames = []\n        for img in image_caro:\n            img.click()\n            time.sleep(1.5)\n            try:\n                image = driver.find_elements(By.CLASS_NAME, 'fotorama__active')[0].get_attribute('href')\n            except:\n                image = driver.frind_elements(By.XPATH, '//*[@id=\"maincontent\"]/div[2]/div/div[3]/div[2]/div[2]/div[2]/div[1]/div[3]/div/img')[0].get_attribute('src')\n            urllib.request.urlretrieve(image, \"Classic-Imports-and-Design/python webscrape/product-images/\" + product[0] + \"-\" + str(c) +\".jpg\")\n            img_filenames.append(product[0] + '-' + str(c))\n            c = c+1\n        data.append(','.join(img_filenames))\n    except:\n        fails.append(product[0] + ' - cannot get images')\n        continue\n    #Dimensions\n    try:\n        dimensions = driver.find_elements(By.CLASS_NAME, 'prdimens')[0].text\n        dimensions = dimensions.split('X')\n        data.extend((' ',' ',' ')) #this way if there are only 2 dims listed, it doesnt fail the try \n        data.append(dimensions[2].strip())\n        data.append(dimensions[1].strip())\n        data.append(dimensions[0].strip())\n    except:\n        fails.append(product[0] + ' has no dimensions.')\n\n    pp(data)\n    output_file.writerow(data)\n\npp(fails)\nwritefile.close()\n#driver.get(url)\n#time.sleep()\n#content = driver.page_source\n#soup = BeautifulSoup(content, \"html.parser\")\n#tree = html.fromstring(driver.page_source) \n\n#tree.xpath()[0].text (.text needed if end of xpath NOT /text)\n#soup.find(\"TAG-TYPE\", class_=\"CLASS-NAME\").text\n\n#urllib.request.urlretrieve(IMG-URL, FILENAME) \n\n#output_file.writerow(data)", "246": "from urllib.request import Request, urlopen\nfrom bs4 import BeautifulSoup as soup\nimport sys\ndef webscrape():\n\turl = \"https://ensaf.ac.ma/?controller=pages&action=info\"\n\treq = Request(url , headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'})\n\tuh=urlopen(req)\n\trawhtml=uh.read()\n\tscrape=soup(rawhtml,'lxml')\n\tinfo=scrape.find('div',class_='table-responsive')\n\ttest=info.tbody\n\tresults=test.find_all('tr')\n\tmodules=[]\n\tfor result in results:\n\t\t\tmodule=result.select_one(\"tr td:nth-of-type(2)\")\n\t\t\tif (module is not None):\n\t\t\t\tsousmodule=module.text\n\t\t\t\tsousmodule=sousmodule.replace('\\n', '')\n\t\t\t\tsousmodule=sousmodule.replace('\\t', '')\n\t\t\t\tsousmodule=sousmodule.replace('1.','')\n\t\t\t\tsousmodule=sousmodule.replace('\u00e9','e')\n\t\t\t\tsousmodule=sousmodule.replace('\u00e8','e')\n\t\t\t\tsousmodule=sousmodule.replace(\"\u2019\",\" \")\n\t\t\t\tsousmodule=sousmodule.replace(\"\u00f4\",'o')\n\t\t\t\tsousmodule=sousmodule.replace(\"'\",'')\n\t\t\t\tsousmodule=sousmodule.replace('\u00fb','u')\n\t\t\t\tsousmodule=sousmodule.replace('\u00e0','a')\n\t\t\t\tsousmodule=sousmodule.replace('\u0153','oe')\n\t\t\t\tmodules.append(sousmodule.strip())\n\tmodules2=[y for x in modules for y in x.split('2.')]\n\tmodules3=[y for x in modules2 for y in x.split('3.')]\n\twhile (\"\" in modules3):\n\t\tmodules3.remove(\"\")\n\tmodules3 = [y.strip() for y in modules3]\n\tmodules3=[x for x in modules3 if not x.startswith(\"Projet\") and not x.startswith(\"PFA\") and not x.startswith(\"TEC\") and not x.startswith(\"Fran\") and not x.startswith(\"Allemand\") and not x.startswith(\"Anglais\") and not x.startswith(\"Langue\") and not x.startswith(\"Espagnol\") and not x.startswith(\"Stage\")]\n\treturn modules3\nprint(webscrape())", "247": "import scrapy\nfrom ..items import WebscrapeItem\n\nclass AmazonSpiderSpider(scrapy.Spider):\n    name = 'amazon'\n    start_urls = ['https://www.amazon.com/s?k=masks+50pcs&ref=nb_sb_noss_1']\n    pageNumber = 2\n\n    def parse(self, response):\n        items = WebscrapeItem()\n\n        product_name = response.css('.a-color-base.a-text-normal').css('::text').extract()\n        product_price = response.css('.a-price-whole::text').extract()\n\n        items['product_name'] = product_name\n        items['product_price'] = product_price\n\n        yield items\n\n        nextPage = 'https://www.amazon.com/s?k=masks+50pcs&page=' + str(AmazonSpiderSpider.pageNumber)\n\n        if AmazonSpiderSpider.pageNumber <= 100:\n            AmazonSpiderSpider.pageNumber += 1\n            yield response.follow(nextPage, callback = self.parse)", "248": "# Import libraries\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nimport time\n\n\ndef webscrape(url_page, path_download_dir, path_geckodriver, path_xpath1, path_xpath2, sleep_time=5):\n    \"\"\"\n    Function to download files via webscrapping, using selenium and fireforx's\n    geckodriver, which can be downloaded here: https://github.com/mozilla/geckodriver/releases\n\n    How to webscrape:\n    0. Download all required packages (incl. selenium and geckodriver)\n    1. Open the url with the download links in Firefox\n    2. right cklick on the first download link and select 'inspect'\n    3. right click again on the higlighted entry in firefox developer mode and choose 'copy path' -> 'xpath'\n    4. do the same for the last download link\n\n    Args:\n    - url_page (str): address of webpage where files should be downloaded\n    - path_download_dir (str): path to dir in which downloads should be saved\n    - path_geckodriver (str): path to folder in which geckodriver is stored\n    - path_xpath1(str): xpath of file where download should start with\n    - path_xpath2 (str): xpath of file where download should stop (all download links in between will be downloaded)\n    - sleep_time (int): on some pages it takes some time to load all download links; here, a timer can be set to wait\n\n    Returns:\n    -\n\n    Last update: 21/06/21. By Felix.\n\n    \"\"\"\n    # intitialise download\n    print('Starting to download from ' + url_page)\n\n    # Set options for webdriver\n    options = FirefoxOptions()\n    # options.add_argument(\"--headless\")\n    #options.headless = True\n\n    # in this case we set the options to avoid pop up windows when downloading\n    fp = webdriver.FirefoxProfile()\n\n    # Set 0 = desktop, 1 = default download folder, 2 = specified donwload folder\n    fp.set_preference(\"browser.download.folderList\", 2)\n    fp.set_preference(\"browser.download.manager.showWhenStarting\", False)\n\n    # Setting for specific download folder (downloadDir)\n    fp.set_preference(\"browser.download.dir\", path_download_dir)\n\n    # Setting to disable download pop up window and directly download file\n    fp.set_preference(\n        \"browser.helperApps.neverAsk.saveToDisk\",\n        \"application/zip, application/octet-stream, multipart/x-zip, application/zip-compressed, application/x-zip-compressed\")\n\n    # Update preferences\n    fp.update_preferences()\n\n    # options.profile(fp)\n\n    # Run firefox webdriver from executable path of your choice\n    driver = webdriver.Firefox(firefox_profile=fp, executable_path=path_geckodriver, options=options)\n    #driver = webdriver.Firefox(executable_path = path_geckodriver, options=options)\n\n    # -----------------------------------------------------------------------------\n\n    # Get web page\n    driver.get(url_page)\n\n    # Sleep for 15s to load feed\n    print('Waiting {} secs to let all download links load...'.format(sleep_time))\n    time.sleep(sleep_time)\n    print('-------------')\n\n    # Find elements by xpath\n    #download_elems = driver.find_elements_by_xpath((path_xpath1))\n    #print('Number of downloadable files: ', len(download_elems))\n    # print('-------------')\n\n    # Find where two xpath strings have different entries\n    list_pos_diff = [i for i in range(len(path_xpath1)) if path_xpath1[i] != path_xpath2[i]]\n    # Get first position\n    pos_diff = list_pos_diff[0]\n\n    # Get pos of '['\n    str_front = path_xpath1[0:pos_diff]\n    # here we use rindex to get last '['\n    pos_start = str_front.rindex('[')\n\n    # Get pos of ']'\n    str_end = path_xpath1[pos_start:]\n    pos_end = str_end.index(']')\n    # Build part 1 and part 2 of xpath\n    xpath_pt1 = path_xpath1[0:pos_start + 1]\n    xpath_pt2 = path_xpath1[pos_start + pos_end:]\n\n    # Get start for range (starting value between [])\n    a = int(path_xpath1[pos_start + 1:pos_start + pos_end])\n    # Get end for range (end value between [])\n    b = 1 + int(path_xpath2[pos_start + 1:(pos_start + pos_end + abs(len(path_xpath2) - len(path_xpath1)))])\n\n    # the range is definded from the xpath path_xpath1 to xpath path_xpath2\n    for i in range(a, b):\n        # build xpath for loop\n        path = xpath_pt1 + str(i) + xpath_pt2\n        print(path)\n        # use selenium webdriver to click and download\n        results = driver.find_elements_by_xpath((path))[0]\n        results.click()\n        time.sleep(1)\n\n        if i % 1000 == 0:\n            print('Number of Downloaded files: ', i)\n\n    # -----------------------------------------------------------------------------\n    # Sleep for 5s to ensure that everythings loaded properly\n    time.sleep(5)\n    driver.quit()\n\n    # -----------------------------------------------------------------------------\n    print('-------------')\n    print('Download sucessfull!')\n    print('{} Files downloaded to '.format(b - a) + path_download_dir)\n\n\ndef main():\n\n    # Paths for webscrapper\n    urlpage = 'https://services.cuzk.cz/gml/inspire/bu/epsg-4258/'\n    xpath1 = '/html/body/div[2]/div/div[7]/div[2]/ul/li[1]/div[6]/div[2]/ul/li[1]/a'\n    xpath2 = '/html/body/div[2]/div/div[7]/div[2]/ul/li[1]/div[6]/div[2]/ul/li[3]/a'\n\n    geckodriver_dir = '/Users/Felix/Documents/Studium/PhD/05_Projects/02_Estimate_Building_Heights/preprocessing/Webscraper_Thueringen/geckodriver'\n\n    # Path of output dir\n    download_dir = \"/Users/Felix/Desktop/test\"\n\n    # Start to webscrape\n    webscrape(urlpage, download_dir, geckodriver_dir, xpath1, xpath2, 15)\n\n\nif __name__ == \"__main__\":\n    main()\n", "249": "# Import libraries\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nimport time\n\n\ndef webscrape(url_page, path_download_dir, path_geckodriver, path_xpath1, path_xpath2, sleep_time=5):\n    \"\"\"\n    Function to download files via webscrapping, using selenium and fireforx's\n    geckodriver, which can be downloaded here: https://github.com/mozilla/geckodriver/releases\n\n    How to webscrape:\n    0. Download all required packages (incl. selenium and geckodriver)\n    1. Open the url with the download links in Firefox\n    2. right cklick on the first download link and select 'inspect'\n    3. right click again on the higlighted entry in firefox developer mode and choose 'copy path' -> 'xpath'\n    4. do the same for the last download link\n\n    Args:\n    - url_page (str): address of webpage where files should be downloaded\n    - path_download_dir (str): path to dir in which downloads should be saved\n    - path_geckodriver (str): path to folder in which geckodriver is stored\n    - path_xpath1(str): xpath of file where download should start with\n    - path_xpath2 (str): xpath of file where download should stop (all download links in between will be downloaded)\n    - sleep_time (int): on some pages it takes some time to load all download links; here, a timer can be set to wait\n\n    Returns:\n    -\n\n    Last update: 21/06/21. By Felix.\n\n    \"\"\"\n    # intitialise download\n    print('Starting to download from ' + url_page)\n\n    # Set options for webdriver\n    options = FirefoxOptions()\n    # options.add_argument(\"--headless\")\n    #options.headless = True\n\n    # in this case we set the options to avoid pop up windows when downloading\n    fp = webdriver.FirefoxProfile()\n\n    # Set 0 = desktop, 1 = default download folder, 2 = specified donwload folder\n    fp.set_preference(\"browser.download.folderList\", 2)\n    fp.set_preference(\"browser.download.manager.showWhenStarting\", False)\n\n    # Setting for specific download folder (downloadDir)\n    fp.set_preference(\"browser.download.dir\", path_download_dir)\n\n    # Setting to disable download pop up window and directly download file\n    fp.set_preference(\n        \"browser.helperApps.neverAsk.saveToDisk\",\n        \"application/zip, application/octet-stream, multipart/x-zip, application/zip-compressed, application/x-zip-compressed\")\n\n    # Update preferences\n    fp.update_preferences()\n\n    # options.profile(fp)\n\n    # Run firefox webdriver from executable path of your choice\n    driver = webdriver.Firefox(firefox_profile=fp, executable_path=path_geckodriver, options=options)\n    #driver = webdriver.Firefox(executable_path = path_geckodriver, options=options)\n\n    # -----------------------------------------------------------------------------\n\n    # Get web page\n    driver.get(url_page)\n\n    # Sleep for 15s to load feed\n    print('Waiting {} secs to let all download links load...'.format(sleep_time))\n    time.sleep(sleep_time)\n    print('-------------')\n\n    # Find elements by xpath\n    #download_elems = driver.find_elements_by_xpath((path_xpath1))\n    #print('Number of downloadable files: ', len(download_elems))\n    # print('-------------')\n\n    # Find where two xpath strings have different entries\n    list_pos_diff = [i for i in range(len(path_xpath1)) if path_xpath1[i] != path_xpath2[i]]\n    # Get first position\n    pos_diff = list_pos_diff[0]\n\n    # Get pos of '['\n    str_front = path_xpath1[0:pos_diff]\n    # here we use rindex to get last '['\n    pos_start = str_front.rindex('[')\n\n    # Get pos of ']'\n    str_end = path_xpath1[pos_start:]\n    pos_end = str_end.index(']')\n    # Build part 1 and part 2 of xpath\n    xpath_pt1 = path_xpath1[0:pos_start + 1]\n    xpath_pt2 = path_xpath1[pos_start + pos_end:]\n\n    # Get start for range (starting value between [])\n    a = int(path_xpath1[pos_start + 1:pos_start + pos_end])\n    # Get end for range (end value between [])\n    b = 1 + int(path_xpath2[pos_start + 1:(pos_start + pos_end + abs(len(path_xpath2) - len(path_xpath1)))])\n\n    # the range is definded from the xpath path_xpath1 to xpath path_xpath2\n    for i in range(a, b):\n        # build xpath for loop\n        path = xpath_pt1 + str(i) + xpath_pt2\n        print(path)\n        # use selenium webdriver to click and download\n        results = driver.find_elements_by_xpath((path))[0]\n        results.click()\n        time.sleep(1)\n\n        if i % 1000 == 0:\n            print('Number of Downloaded files: ', i)\n\n    # -----------------------------------------------------------------------------\n    # Sleep for 5s to ensure that everythings loaded properly\n    time.sleep(5)\n    driver.quit()\n\n    # -----------------------------------------------------------------------------\n    print('-------------')\n    print('Download sucessfull!')\n    print('{} Files downloaded to '.format(b - a) + path_download_dir)\n\n\ndef main():\n\n    # Paths for webscrapper\n    urlpage = 'https://geoportal.geoportal-th.de/gaialight-th/_apps/atomfeedexplorer/?#feed=http%3A//geoportal.geoportal-th.de/dienste/atom_th_gebaeude%3Ftype%3Ddataset%26id%3D97d152b8-9e00-49f3-9ae4-8bbb30873562'\n    xpath1 = '/html/body/div[2]/div/div[7]/div[2]/ul/li[1]/div[6]/div[2]/ul/li[1]/a'\n    xpath2 = '/html/body/div[2]/div/div[7]/div[2]/ul/li[1]/div[6]/div[2]/ul/li[3]/a'\n\n    geckodriver_dir = '/Users/Felix/Documents/Studium/PhD/05_Projects/02_Estimate_Building_Heights/preprocessing/Webscraper_Thueringen/geckodriver'\n\n    # Path of output dir\n    download_dir = \"/Users/Felix/Desktop/test\"\n\n    # Start to webscrape\n    webscrape(urlpage, download_dir, geckodriver_dir, xpath1, xpath2, 15)\n\n\nif __name__ == \"__main__\":\n    main()\n", "250": "#!/usr/bin/python3\n\n\nimport argparse\nimport requests\nimport json\nimport csv\n\n\nclass Webscrape():\n    '''classes are cool, no other real reason to use this - probably going to only have one function'''\n    def __init__(self):\n        self.webpath = \"https://api.tracker.gg/api/v2/rocket-league/standard/profile/\"\n        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n\n    def retrieveDataRLTrackerFromURL(self,url):\n        headers = self.headers\n        playerdata = {} # define the playerdata dict\n        page = requests.get(url, headers=headers)\n        if page.status_code == 200:\n            data = json.loads(page.text)\n            segs = data[\"data\"][\"segments\"]\n            for segment in segs:\n                        if \"playlist\" in segment['type']:\n                            playerdata[segment['metadata']['name']] = {'rank': segment['stats']['tier']['metadata']['name'], 'iconUrl': segment['stats']['tier']['metadata']['iconUrl'], 'division': segment['stats']['division']['metadata']['name'], 'mmr': segment['stats']['rating']['value']}\n\n        return playerdata\n\n    def retrieveDataRLTracker(self,gamertag=\"reasel\",platform=\"steam\"):\n        webpath = self.webpath\n        headers = self.headers\n        playerdata = {} # define the playerdata dict\n        playerdata[gamertag] = {} # define the gamertag dict\n        page = requests.get(\"%(webpath)s%(platform)s/%(gamertag)s\" % locals(), headers=headers)\n        if page.status_code == 200:\n            data = json.loads(page.text)\n            segs = data[\"data\"][\"segments\"]\n            for segment in segs:\n                        if \"playlist\" in segment['type']:\n                            playerdata[segment['metadata']['name']] = {'rank': segment['stats']['tier']['metadata']['name'], 'iconUrl': segment['stats']['tier']['metadata']['iconUrl'], 'division': segment['stats']['division']['metadata']['name'], 'mmr': segment['stats']['rating']['value']}\n\n        return playerdata\n\n\ndef singleRun(gamertag,platform):\n    '''Single run of Webscrape.retrieveDataRLTracker'''\n    scrape = Webscrape()\n    data = scrape.retrieveDataRLTracker(gamertag=gamertag,platform=platform)\n    if data is not None:\n        pprint(data)\n\ndef manyRun(playerCsv):\n    scrape = Webscrape()\n    with open(playerCsv, newline='', mode='r+') as csvfile:\n        reader = csv.DictReader(csvfile)\n        with open('output.csv',  mode='r+', newline='') as outputfile:\n            writer = csv.DictWriter(outputfile, fieldnames=reader.fieldnames)\n            writer.writeheader()\n            for row in reader:\n                data = scrape.retrieveDataRLTrackerFromURL(url=row['Link to Stats'])\n                if 'Ranked Duel 1v1' in data:\n                    row['1\\'s Icon']           = data['Ranked Duel 1v1']['iconUrl']\n                    row['1\\'s Division']       = data['Ranked Duel 1v1']['division']\n                    row['1\\'s MMR']            = data['Ranked Duel 1v1']['mmr']\n                    row['1\\'s Rank']           = data['Ranked Duel 1v1']['rank']\n\n                if 'Ranked Doubles 2v2' in data:\n                    row['2\\'s Icon']           = data['Ranked Doubles 2v2']['iconUrl']\n                    row['2\\'s Division']       = data['Ranked Doubles 2v2']['division']\n                    row['2\\'s MMR']            = data['Ranked Doubles 2v2']['mmr']\n                    row['2\\'s Rank']           = data['Ranked Doubles 2v2']['rank']\n\n                if 'Ranked Standard 3v3' in data:\n                    row['3\\'s Icon']           = data['Ranked Standard 3v3']['iconUrl']\n                    row['3\\'s Division']       = data['Ranked Standard 3v3']['division']\n                    row['3\\'s MMR']            = data['Ranked Standard 3v3']['mmr']\n                    row['3\\'s Rank']           = data['Ranked Standard 3v3']['rank']\n\n                if 'Un-Ranked' in data:\n                    row['Casual Icon']         = data['Un-Ranked']['iconUrl']\n                    row['Casual Division']     = data['Un-Ranked']['division']\n                    row['Casual MMR']          = data['Un-Ranked']['mmr']\n                    row['Casual Rank']         = data['Un-Ranked']['rank']\n\n                if 'Tournament Matches' in data:\n                    row['Tournament Icon']     = data['Tournament Matches']['iconUrl']\n                    row['Tournament Division'] = data['Tournament Matches']['division']\n                    row['Tournament MMR']      = data['Tournament Matches']['mmr']\n                    row['Tournament Rank']     = data['Tournament Matches']['rank']\n\n                if 'Hoops' in data:\n                    row['Hoops Icon']          = data['Hoops']['iconUrl']\n                    row['Hoops Division']      = data['Hoops']['division']\n                    row['Hoops MMR']           = data['Hoops']['mmr']\n                    row['Hoops Rank']          = data['Hoops']['rank']\n\n                if 'Snowday' in data:\n                    row['Snowday Icon']        = data['Snowday']['iconUrl']\n                    row['Snowday Division']    = data['Snowday']['division']\n                    row['Snowday MMR']         = data['Snowday']['mmr']\n                    row['Snowday Rank']        = data['Snowday']['rank']\n\n                if 'Dropshot' in data:\n                    row['Dropshot Icon']       = data['Dropshot']['iconUrl']\n                    row['Dropshot Division']   = data['Dropshot']['division']\n                    row['Dropshot MMR']        = data['Dropshot']['mmr']\n                    row['Dropshot Rank']       = data['Dropshot']['rank']\n\n                if 'Rumble' in data:\n                    row['Rumble Icon']         = data['Rumble']['iconUrl']\n                    row['Rumble Division']     = data['Rumble']['division']\n                    row['Rumble MMR']          = data['Rumble']['mmr']\n                    row['Rumble Rank']         = data['Rumble']['rank']\n                writer.writerow(row)\n        \n        \n        \n\n    \nif __name__ == \"__main__\":\n    '''Run locally to this script'''\n\n    from pprint import pprint # pprint is cool\n    #Pass arguments for name and platform\n    parser = argparse.ArgumentParser(description='Scrape Commandline Options', add_help=True)\n    parser.add_argument('-p', action='store', dest='platform', help='platform options. Example: steam', choices=('steam','psn','xbl'), default='steam')\n    parser.add_argument('-g', action='store', dest='gamertag', help='your gamertag', default='Reasel')\n    parser.add_argument('-l', action='store', dest='playerCsv', help='path to csv', default='False')\n\n    results = parser.parse_args()\n    platform = results.platform\n    gamertag = results.gamertag\n    playerCsv = results.playerCsv\n    \n    if(playerCsv != \"False\"):\n        manyRun(playerCsv)\n    else:\n        singleRun(gamertag,platform)", "251": "# importing the required modules\r\nfrom bs4 import BeautifulSoup\r\nimport webbrowser\r\nimport requests\r\nimport re\r\n\r\n# accessing the website\r\nweb_read = requests.get(\"https://fmovies.hn/home\").text\r\nbsoup = BeautifulSoup(web_read, 'lxml')\r\n\r\n# reading the webpage.\r\ntrend_movies = bsoup.find('div', class_=\"tab-pane active\")\r\n\r\n# asking the user for the required movie.\r\nmovie_name = input(\"Enter the movie name: \")\r\n\r\n\r\nmovie_fin = trend_movies.find('div', class_=\"film_list-wrap\")\r\nmovie_fins = movie_fin.find_all('h3', class_=\"film-name\")\r\n\r\n# initialising a dictionary to access the weblink of the required movie in the further process.\r\nmovie_dict = {}\r\n\r\n# opening a file to write the extracted details into it.\r\nfile = open(\"D:\\WEBSCRAPE\\end.txt\", 'w')\r\nfor movie in movie_fins:\r\n    x = movie.text.replace('\\n', '')\r\n    file.write(x)\r\n\r\n    file.write(\"\\n\")\r\n\r\n    s = \"fmovies.hn\" + movie.a['href']\r\n    x = x.lower()\r\n    movie_dict[x] = s\r\n    file.write(\"\\n\")\r\n    file.write(s)\r\n    file.write(\"\\n\")\r\n    file.write(\"-----------------------------------\")\r\n    file.write(\"\\n\")\r\n\r\n# Function that checks if the movie is in the extracted data or not\r\n\r\n\r\ndef get_movie(name, movie_dict):\r\n    for i in movie_dict:\r\n        if(re.search(name, i) != None):\r\n            return movie_dict[i]\r\n    return 0\r\n\r\n\r\n# If the movie is present then the hyperlink to the movie is being stored\r\nlink = get_movie(movie_name, movie_dict)\r\nprint(link)\r\n# Opening the hyperlink.\r\nwebbrowser.open(link)\r\n\r\nlatest_movies = bsoup.find(\r\n    'section', class_='block_area block_area_home section-id-02')\r\n\r\n\r\nmovie_fin = latest_movies.find('div', class_=\"film_list-wrap\")\r\nmovie_fins = movie_fin.find_all('h3', class_=\"film-name\")\r\nfile1 = open(\"D:\\WEBSCRAPE\\end1.txt\", 'w')\r\nfor movie in movie_fins:\r\n    file1.write(movie.text)\r\n    file1.write(\"\\n\")\r\n    s = \"fmovies.hn\" + movie.a['href']\r\n    file1.write(s)\r\n    file1.write(\"\\n\")\r\n    file1.write(\"-----------------------------------\")\r\n    file1.write(\"\\n\")\r\n", "252": "\"\"\"\nWSGI config for webscrape project.\n\nThis module contains the WSGI application used by Django's development server\nand any production WSGI deployments. It should expose a module-level variable\nnamed ``application``. Django's ``runserver`` and ``runfcgi`` commands discover\nthis application via the ``WSGI_APPLICATION`` setting.\n\nUsually you will have the standard Django WSGI application here, but it also\nmight make sense to replace the whole Django WSGI application with a custom one\nthat later delegates to the Django one. For example, you could introduce WSGI\nmiddleware here, or combine a Django application with an application of another\nframework.\n\n\"\"\"\nimport os\nimport sys\n\nfrom django.core.wsgi import get_wsgi_application\n\n# This allows easy placement of apps within the interior\n# webscrape directory.\napp_path = os.path.abspath(\n    os.path.join(os.path.dirname(os.path.abspath(__file__)), os.pardir)\n)\nsys.path.append(os.path.join(app_path, \"webscrape\"))\n# We defer to a DJANGO_SETTINGS_MODULE already in the environment. This breaks\n# if running multiple sites in the same mod_wsgi process. To fix this, use\n# mod_wsgi daemon mode with each site in its own daemon process, or use\n# os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"config.settings.production\"\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"config.settings.production\")\n\n# This application object is used by any WSGI server configured to use this\n# file. This includes Django's development server, if the WSGI_APPLICATION\n# setting points here.\napplication = get_wsgi_application()\n# Apply WSGI middleware here.\n# from helloworld.wsgi import HelloWorldApplication\n# application = HelloWorldApplication(application)\n", "253": "from flask import Flask\nfrom flask import render_template, send_from_directory\nimport webscrape\napp = Flask(__name__)\n\n\nclass Config():\n    APP_NAME = 'FarmBuild'\n\n    # Enable debug mode\n    DEBUG = True\n    ALLOWED_HEADERS = ['Origin', 'Accept', 'Content-Type', 'X-Requested-With', 'X-CSRF-Token']\n    ALLOWED_ORIGINS = '*'\n    ALLOWED_METHODS = ['GET', 'HEAD', 'POST', 'OPTIONS', 'PUT', 'PATCH', 'DELETE']\n\n    # TODO\n    # This is where frontend should go, create a route for all UI files\n    # Setup template folder for webpages\n    TEMPLATE_FOLDER = \"templates\"\n\n\napp = Flask(\n    Config.APP_NAME, \n    template_folder=Config.TEMPLATE_FOLDER, \n    static_folder=Config.TEMPLATE_FOLDER\n)\n\n@app.route('/')\ndef index_page():\n    return render_template('index.html')\n\n@app.route('/earthquake.html')\ndef scrape_page():\n    return render_template('earthquake.html')\n\n@app.route('/')\ndef run_script():\n    file = open(r'real_time/webscrape.py', 'r').read()\n    return exec(file)\n\n@app.route('/js/')\ndef send_js(path):\n    print(path)\n    return send_from_directory('templates/js', path)\n\n@app.route('/css/')\ndef send_css(path):\n    return send_from_directory('templates/css', path)\n\n@app.route('/img/')\ndef send_img(path):\n    return send_from_directory('templates/img', path)\n\n@app.route('/json/')\ndef send_json(path):\n    return send_from_directory('templates/json', path)\n\n@app.route('/assets/')\ndef assets(path):\n    return send_from_directory('templates/assets', path)\n\n    \nif __name__ == \"__main__\":\n\tapp.run(debug= True)\n", "254": "# coding=utf-8\n\nimport logging\nlogging.basicConfig( format='%(levelname)-8s %(asctime)s %(filename)s %(lineno)d %(message)s', level=logging.DEBUG )\n\nclass WebScrape :\n\n    def __init__(self) :\n        pass\n    pass\n\n    def doScrape(self) : \n        import xlsxwriter\n        workbook = xlsxwriter.Workbook( \"\uc624\ub298\uc758 \uc99d\uad8c\uc2dc\uc138.xlsx\" )\n        worksheet = workbook.add_worksheet()\n        row = 0\n        col = 0 \n\n        line = \"\\n\" + \"#\"*80 + \"\\n\"\n\n        # import libraries\n        from bs4 import BeautifulSoup\n\n        # specify the url\n        html_url = \"http://vip.mk.co.kr/newSt/rate/item_all.php\"\n\n        # query the website and return the html to the variable \u2018page\u2019\n\n        logging.info( \"Getting a html data from %s\" % html_url )\n\n        from urllib.request import urlopen\n        html_page = urlopen( html_url )\n        html_src = html_page.read()\n\n        logging.info( \"Done. Getting a html data from %s\" % html_url )\n\n        # parse the html using beautiful soup and store in variable `soup`\n        soup = BeautifulSoup( html_src, \"html.parser\" , from_encoding=\"euc-kr\" )\n\n        print( \"title = %s\" % soup.title ) \n        print( \"title.string = %s\" % soup.title.string )\n        table = soup.table        \n        table_subs = table.find_all( \"table\" )\n        print( line )         \n        jisu = table_subs[ 1 ]\n        trs = jisu.find_all( \"tr\" )\n        for tr in trs :\n            tds = tr.find_all( \"td\" )\n            col = 0 \n            for td in tds : \n                print( \"td = %s\" % td.string )\n                worksheet.write(row, col, td.string )\n                col += 1\n            pass\n            row += 1\n        pass \n\n        jisu = table_subs[ 4 ]\n        trs = jisu.find_all( \"tr\" )\n        for tr in trs :\n            tds = tr.find_all( \"td\" )\n            col = 0 \n            for td in tds : \n                print( \"td = %s\" % td.string )\n                worksheet.write(row, col, td.string )\n                col += 1\n            pass\n            row += 1\n        pass \n        print( line )\n        workbook.close() \n    pass\n\npass\n\nif __name__ == '__main__' :   \n    web_scrape = WebScrape()\n    web_scrape.doScrape()\npass", "255": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Feb  9 20:57:29 2020\n\n@author: Mishaun_Bhakta\n\nThe purpose of this program is to automate the process of preparing and closing \nBureau of Land Management (BLM) federal oil and gas lease sales\n\nThe first phase of the program will visit the website, scrape the lease information\nfor each tract, and parse the information into a template sale note spreadsheet\nIt will also download the shapefile of the sale, which is needed for evaluating tracts on\nDrillingInfo.com\n\nOnce the spreadsheet is prepared and the sale takes place, the program will \nscrape the webpage again and gather information regarding won parcels.\nThe scrape will get the bonus bid for each parcel we won by evaluating our bidder number\n\nFinally, a dataframe will be created based on won lots and the information will be \npassed into a pdf fill form function to create paperwork needed to send to \nBureau of Land Management\n\n\"\"\"\nimport os, re, shutil\n\n#sale parameters\nstate = \"Wyoming\"\nstinitials = \"WY\"\ndate = \"Mar 24, 2020\"\nbidder = '20'\nurl = 'https://www.energynet.com/govt_listing.pl?sg=5214'\n\n#Navigate to energynet/govt sale and get sale page\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\n\n#getting filepath of this file\nfilepath = os.path.dirname(__file__)\n\n#driver will be used based on operating system - windows or mac\ntry:\n    driver = webdriver.Chrome(filepath + \"/chromedriver.exe\")\nexcept:\n    driver = webdriver.Chrome(filepath + \"/chromedriver\")\n\n#selenium driver object will go to url of sale\ndriver.implicitly_wait(30)\ndriver.get(url)\n\n#storing html content in variable after reaching target sale page\nsalehtml = BeautifulSoup(driver.page_source, \"html.parser\")\n\ndef webscrape_presale(parsepage):\n    '''This function will take a page and scrape its data for sale lot information\n    It will also download sale shapefile and move it to directory of this script file\n    '''\n\n    #webscrape sale page - gathering lot serial numbers from html \n    serialnums = parsepage.find_all(\"span\", \"lot-name\")\n    #ist comprehension - appending text into serialnums\n    serialnums = [i.text for i in serialnums]\n    \n    #storing all data from tag 'td's with clas name \"lot-legal\n    #this html container/tag has 3 pieces of information\n    legalinfo = parsepage.find_all(\"td\", \"lot-legal\")\n    \n    #initializing empty arrays\n    acres = []\n    desc = []\n    county = []\n    \n    for item in legalinfo:\n        county.append(item.contents[0].text)\n        desc.append(item.contents[1].text)\n        #getting acres by splitting at : and blankspace to get string of numerical value - taking out a comma if above 1000 in order to convert to float\n        acres.append(float(re.split(\":\\W\",item.contents[2])[1].replace(',','')))\n\n    ##getting shapefile from webpage  \n    #clicking link of where shapefile is stored on sale page    \n    driver.find_element_by_link_text(\"GIS Data WGS84\").click()\n    driver.find_element_by_link_text(\"Notice of Competitive Oil and Gas Internet-Based Lease Sale\").click()\n    \n    #getting list of filenames in downloads\n    try:\n        downloaddir = \"/Users/Mishaun_Bhakta/Downloads/\"\n        downloads = os.listdir(downloaddir)\n    except:\n        downloaddir = \"C:/Users/mishaun/Downloads/\"\n        downloads = os.listdir(downloaddir) \n        \n    #pattern will find downloaded file name of shapefile\n    pattern = \"BLM\"+ stinitials + \"\\S*.zip\"\n    \n    #searching through filenames in downlaods folder\n    finds = []\n    for file in downloads:\n        if re.findall(pattern, file):\n            finds.append(file)\n            break\n        \n    #moving file from downloads folder to directory of this script file - then renaming it to a cleaner name\n    shutil.copy(downloaddir + finds[0], filepath)\n    \n    try:\n        os.rename(finds[0], \"BLM \" + stinitials + \" \" + date + \" Shapefile.\" + finds[0].split(\".\")[1])\n    except:\n       pass\n   \n    return acres, desc, county, serialnums\n\n#storing global variables of web scrape information\nacres, descriptions, counties, serials = webscrape_presale(salehtml)\n\n#Open sale template and update insert information from webscrape\nimport openpyxl\n\ndef fillexcel():\n    '''\n    This function will take scraped (global) values for lots and insert into sale spreadsheet \n    '''    \n    \n    #opening template sale notebook for modifications\n    #preserving vba to keep formatting of workbook preserved - also keeping formulas \n    wb = openpyxl.load_workbook(\"BLM Sale Notes Template.xlsm\", keep_vba = True)\n    sheet = wb.active\n    \n    #updating sheet title to sale title\n    sheet[\"B6\"] = \"BLM {} {} Sale Notes\".format(stinitials, date)\n    \n    #inserting values from webscrape into spreadsheet -8th row is where data rows begin\n    for i in range(0,len(serials)):\n        sheet.cell(row = 8+i, column = 2, value = serials[i])\n        sheet.cell(row = 8+i, column = 6, value = acres[i])\n        sheet.cell(row = 8+i, column = 7, value = counties[i])\n        sheet.cell(row = 8+i, column = 8, value = descriptions[i])\n    \n    #checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\n    if os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n        print(\"File already exists - Preventing overwrite of changes in excel file\")\n    else:\n        wb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n        wb.close()\n\n#checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\nif os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n    print(\"File already exists - Preventing overwrite of changes in excel file\")\nelse:\n    fillexcel()\n\nbidtags = salehtml.find_all(\"td\", \"lot-bid\")\nourwinnings = {}\n\nfor i in range (0,len(bidtags)):\n    textCont = bidtags[i].text\n    \n    #extracting bidder number from lot bid tags - try statement prevents break if no bids were received\n    try:\n        winBidder = re.findall('#\\d+', textCont)[0].replace(\"#\",'')\n    except:\n        print(\"no bids received for parcel: \" + serials[i])\n        winBidder = type(None)\n        pass\n    \n    #if we won the bid, then capture the winning bid amount\n    if bidder == winBidder:\n        winAmount = re.findall('\\$\\d+', textCont)[0]\n        winAmount = winAmount.replace('$','')\n    \n        ourwinnings[i] = winAmount\n\ndef fillwinnings():\n    '''This function will take ourwinnings dictionary and add values to created spreadsheet\n    '''\n    \n    #### insert our winnings into sale spreadsheet\n    wb = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), keep_vba = True)\n    sheet = wb.active\n    \n    for i in range(0,len(ourwinnings)):\n        #row 8 is the starting row for parcels in teh spreadsheet, inserting data relative to 8th row by adding parcel number of sale\n        sheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 17, value = ourwinnings[list(ourwinnings.keys())[i]])\n        sheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 16, value = 'Y')\n    \n    wb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n    wb.close()\n\n#create dataframe for completed sale sheet\nimport pandas as pd\n\n#use pdf reader to fill in form\n# conda install -c conda-forge pdfrw\nimport pdfrw\n\n#copied code and function from article: https://bostata.com/how-to-populate-fillable-pdfs-with-python/\n##############################################################################\nANNOT_KEY = '/Annots'\nANNOT_FIELD_KEY = '/T'\nANNOT_VAL_KEY = '/V'\nANNOT_RECT_KEY = '/Rect'\nSUBTYPE_KEY = '/Subtype'\nWIDGET_SUBTYPE_KEY = '/Widget'\n\ndef write_fillable_pdf(input_pdf_path, output_pdf_path, data_dict):\n    '''\n    This function will fill in pdf's forms based on a form pdf\n    '''\n    \n    template_pdf = pdfrw.PdfReader(input_pdf_path)\n    annotations = template_pdf.pages[0][ANNOT_KEY]\n    for annotation in annotations:\n        if annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n            if annotation[ANNOT_FIELD_KEY]:\n                key = annotation[ANNOT_FIELD_KEY][1:-1]\n                if key in data_dict.keys():\n                    annotation.update(\n                        pdfrw.PdfDict(V='{}'.format(data_dict[key]))\n                    )\n    pdfrw.PdfWriter().write(output_pdf_path, template_pdf)\n##############################################################\n\n\ndef wonlotsDF():\n    '''\n    This function will create a dataframe of the won lots by reading information\n    from completed sale note spreadsheet\n    The dataframe will then be used to parse pdf's \n    '''\n    #using openpyxl in order to read formulated values from spreadsheet\n    # NOTE: have to manually open excel and save sheet for formulated cells to read after filling in values\n    data_onlyWB = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), data_only = True, keep_vba = True)\n    dataSheet = data_onlyWB.active\n    \n    #covnerting spreadsheet into dataframe\n    df = pd.DataFrame(dataSheet.values)\n    \n    #slicing the dataframe to get only relevant data\n    df = df.iloc[6:,1:25]\n    #setting columns to first row of dataframe\n    df.columns = df.iloc[0]\n    #dropping the repeated row with column names\n    df = df.drop(index =[6])\n    \n    #filtering data frame with values only won by magnum\n    wonlotsdf = df[df[\"Magnum Won (Y/N)\"] == 'Y']\n    return wonlotsdf\n\n\ndef createBidSheets():\n    '''\n    This function will take a template pdf and generate pdf's based on wonlots dataframe\n    '''\n    #calling funciton wonlotsDF in ordre for bid sheets to be created \n    wonlotsdf = wonlotsDF()\n\n    templatePDF = 'bidsheet template.pdf'\n    \n    for i in range(0,len(wonlotsdf.index)):\n    \n        OutputPath = filepath +\"/Bid Sheets/\" + wonlotsdf.iloc[i][\"Serial numbers\"] + \" Bid Sheet.pdf\"\n        \n        fields = {\n                \"State\": stinitials,\n                \"Date of Sale\": date,\n                'Check Box for Oil and Gas' : \"x\",\n                \"Oil and Gas/Parcel No\" : wonlotsdf.iloc[i][\"Serial numbers\"],\n                \"TOTAL BID FOR Oil and Gas Lease\" : wonlotsdf.iloc[i][\"Total Bid (Number on BLM Bid Sheet)\"],\n                \"PAYMENT SUBMITTED WITH BID for Oil and Gas\" : wonlotsdf.iloc[i][\"Min Due\"],\n                \"Print or Type Name of Lessee\" : \"R&R Royalty, LTD\",\n                \"Address of Lessee\": \"500 N Shoreline Blvd, Ste 322\",\n                \"City\" : \"Corpus Christi\",\n                \"State_2\": \"TX\",\n                \"Zip Code\" : \"78401\"\n                }\n        \n        write_fillable_pdf(templatePDF, OutputPath, fields)\n\ndef openDI():\n    '''This function will open up DrillingInfo and log user in\n    '''\n    \n    driver = webdriver.Chrome(filepath + \"/chromedriver.exe\")\n    wait = WebDriverWait(driver, 20)\n        \n    driver.get(\"https://app.drillinginfo.com/gallery/\")\n    userfield = driver.find_element_by_name(\"username\")\n    passfield = driver.find_element_by_name(\"password\")\n    \n    userfield.click()\n    userfield.send_keys(\"mbhaktamgm\")\n    passfield.click()\n    passfield.send_keys(\"itheCwe\")\n    passfield.send_keys(Keys.RETURN)\n    \n    myworkspaces = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\\\"workspaces-section\\\"]/div[1]/div[1]')))\n    myworkspaces.click()\n    \n    default_workspace = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"workspaces-section\\\"]/div[3]/di-carousel/section/div[2]/table/tbody/tr[2]/a/span[2]/span\")))\n    default_workspace.click()\n    \n#splitting counties in counties variable at the comma+space to formulate string \n#for drilling info paste and filter\n    \nsplitCounties = [item.split(\", \") for item in counties]\n\nDIcounties =  []\nfor item in splitCounties:\n    #taking \"County\" out of word\n    temp = item[0].upper().replace(\"COUNTY\", \"\")\n    formattedCounty = temp + \"(\" + item[1] + \")\"\n    DIcounties.append(formattedCounty)\n\n\n\n        \n    \n", "256": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Feb  9 20:57:29 2020\n\n@author: Mishaun_Bhakta\n\nThe purpose of this program is to automate the process of preparing and closing \nBureau of Land Management (BLM) federal oil and gas lease sales\n\nThe first phase of the program will visit the website, scrape the lease information\nfor each tract, and parse the information into a template sale note spreadsheet\nIt will also download the shapefile of the sale, which is needed for evaluating tracts on\nDrillingInfo.com\n\nOnce the spreadsheet is prepared and the sale takes place, the program will \nscrape the webpage again and gather information regarding won parcels.\nThe scrape will get the bonus bid for each parcel we won by evaluating our bidder number\n\nFinally, a dataframe will be created based on won lots and the information will be \npassed into a pdf fill form function to create paperwork needed to send to \nBureau of Land Management\n\n\"\"\"\nimport os, re, shutil\n\n#sale parameters\nstate = \"Wyoming\"\nstinitials = \"WY\"\ndate = \"Jun 23, 2020\"\nbidder = '20'\nurl = 'https://www.energynet.com/govt_listing.pl?sg=5262'\n\n#Navigate to energynet/govt sale and get sale page\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\n\n#getting filepath of this file\nfilepath = os.path.dirname(__file__)\n\n#driver will be used based on operating system - windows or mac\ntry:\n    driver = webdriver.Chrome(filepath + \"/chromedriver.exe\")\nexcept:\n    driver = webdriver.Chrome(filepath + \"/chromedriver\")\n\n#selenium driver object will go to url of sale\ndriver.implicitly_wait(30)\ndriver.get(url)\n\n#storing html content in variable after reaching target sale page\nsalehtml = BeautifulSoup(driver.page_source, \"html.parser\")\n\ndef webscrape_presale(parsepage):\n    '''This function will take a page and scrape its data for sale lot information\n    It will also download sale shapefile and move it to directory of this script file\n    '''\n\n    #webscrape sale page - gathering lot serial numbers from html \n    serialnums = parsepage.find_all(\"span\", \"lot-name\")\n    #ist comprehension - appending text into serialnums\n    serialnums = [i.text for i in serialnums]\n    \n    #storing all data from tag 'td's with clas name \"lot-legal\n    #this html container/tag has 3 pieces of information\n    legalinfo = parsepage.find_all(\"td\", \"lot-legal\")\n    \n    #initializing empty arrays\n    acres = []\n    desc = []\n    county = []\n    \n    for item in legalinfo:\n        county.append(item.contents[0].text)\n        desc.append(item.contents[1].text)\n        #getting acres by splitting at : and blankspace to get string of numerical value - taking out a comma if above 1000 in order to convert to float\n        acres.append(float(re.split(\":\\W\",item.contents[2].text)[1].replace(',','')))\n\n    ##getting shapefile from webpage  \n    #clicking link of where shapefile is stored on sale page    \n    driver.find_element_by_link_text(\"GIS Data WGS84\").click()\n    driver.find_element_by_link_text(\"Notice of Competitive Oil and Gas Internet-Based Lease Sale\").click()\n    \n    #getting list of filenames in downloads\n    try:\n        downloaddir = \"/Users/Mishaun_Bhakta/Downloads/\"\n        downloads = os.listdir(downloaddir)\n    except:\n        downloaddir = \"C:/Users/mishaun/Downloads/\"\n        downloads = os.listdir(downloaddir) \n        \n    #pattern will find downloaded file name of shapefile\n    pattern = \"BLM\"+ stinitials + \"\\S*.zip\"\n    \n    #searching through filenames in downlaods folder\n    finds = []\n    for file in downloads:\n        if re.findall(pattern, file):\n            finds.append(file)\n            break\n        \n    #moving file from downloads folder to directory of this script file - then renaming it to a cleaner name\n    shutil.copy(downloaddir + finds[0], filepath)\n    \n    try:\n        os.rename(finds[0], \"BLM \" + stinitials + \" \" + date + \" Shapefile.\" + finds[0].split(\".\")[1])\n    except:\n       pass\n   \n    return acres, desc, county, serialnums\n\n#storing global variables of web scrape information\nacres, descriptions, counties, serials = webscrape_presale(salehtml)\n\n#Open sale template and update insert information from webscrape\nimport openpyxl\n\ndef fillexcel():\n    '''\n    This function will take scraped (global) values for lots and insert into sale spreadsheet \n    '''    \n    \n    #opening template sale notebook for modifications\n    #preserving vba to keep formatting of workbook preserved - also keeping formulas \n    wb = openpyxl.load_workbook(\"BLM Sale Notes Template.xlsm\", keep_vba = True)\n    sheet = wb.active\n    \n    #updating sheet title to sale title\n    sheet[\"B6\"] = \"BLM {} {} Sale Notes\".format(stinitials, date)\n    \n    #inserting values from webscrape into spreadsheet -8th row is where data rows begin\n    for i in range(0,len(serials)):\n        sheet.cell(row = 8+i, column = 2, value = serials[i])\n        sheet.cell(row = 8+i, column = 6, value = acres[i])\n        sheet.cell(row = 8+i, column = 7, value = counties[i])\n        sheet.cell(row = 8+i, column = 8, value = descriptions[i])\n    \n    #checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\n    if os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n        print(\"File already exists - Preventing overwrite of changes in excel file\")\n    else:\n        wb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n        wb.close()\n\n#checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\nif os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n    print(\"File already exists - Preventing overwrite of changes in excel file\")\nelse:\n    fillexcel()\n\nbidtags = salehtml.find_all(\"td\", \"lot-bid\")\nourwinnings = {}\n\nfor i in range (0,len(bidtags)):\n    textCont = bidtags[i].text\n    \n    #extracting bidder number from lot bid tags - try statement prevents break if no bids were received\n    try:\n        winBidder = re.findall('#\\d+', textCont)[0].replace(\"#\",'')\n    except:\n        print(\"no bids received for parcel: \" + serials[i])\n        winBidder = type(None)\n        pass\n    \n    #if we won the bid, then capture the winning bid amount\n    if bidder == winBidder:\n        winAmount = re.findall('\\$\\d+', textCont)[0]\n        winAmount = winAmount.replace('$','')\n    \n        ourwinnings[i] = winAmount\n\ndef fillwinnings():\n    '''This function will take ourwinnings dictionary and add values to created spreadsheet\n    '''\n    \n    #### insert our winnings into sale spreadsheet\n    wb = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), keep_vba = True)\n    sheet = wb.active\n    \n    for i in range(0,len(ourwinnings)):\n        #row 8 is the starting row for parcels in teh spreadsheet, inserting data relative to 8th row by adding parcel number of sale\n        sheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 17, value = ourwinnings[list(ourwinnings.keys())[i]])\n        sheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 16, value = 'Y')\n    \n    wb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n    wb.close()\n\n#create dataframe for completed sale sheet\nimport pandas as pd\n\n#use pdf reader to fill in form\n# conda install -c conda-forge pdfrw\nimport pdfrw\n\n#copied code and function from article: https://bostata.com/how-to-populate-fillable-pdfs-with-python/\n##############################################################################\nANNOT_KEY = '/Annots'\nANNOT_FIELD_KEY = '/T'\nANNOT_VAL_KEY = '/V'\nANNOT_RECT_KEY = '/Rect'\nSUBTYPE_KEY = '/Subtype'\nWIDGET_SUBTYPE_KEY = '/Widget'\n\ndef write_fillable_pdf(input_pdf_path, output_pdf_path, data_dict):\n    '''\n    This function will fill in pdf's forms based on a form pdf\n    '''\n    \n    template_pdf = pdfrw.PdfReader(input_pdf_path)\n    annotations = template_pdf.pages[0][ANNOT_KEY]\n    for annotation in annotations:\n        if annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n            if annotation[ANNOT_FIELD_KEY]:\n                key = annotation[ANNOT_FIELD_KEY][1:-1]\n                if key in data_dict.keys():\n                    annotation.update(\n                        pdfrw.PdfDict(V='{}'.format(data_dict[key]))\n                    )\n    pdfrw.PdfWriter().write(output_pdf_path, template_pdf)\n##############################################################\n\n\ndef wonlotsDF():\n    '''\n    This function will create a dataframe of the won lots by reading information\n    from completed sale note spreadsheet\n    The dataframe will then be used to parse pdf's \n    '''\n    #using openpyxl in order to read formulated values from spreadsheet\n    # NOTE: have to manually open excel and save sheet for formulated cells to read after filling in values\n    data_onlyWB = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), data_only = True, keep_vba = True)\n    dataSheet = data_onlyWB.active\n    \n    #covnerting spreadsheet into dataframe\n    df = pd.DataFrame(dataSheet.values)\n    \n    #slicing the dataframe to get only relevant data\n    df = df.iloc[6:,1:25]\n    #setting columns to first row of dataframe\n    df.columns = df.iloc[0]\n    #dropping the repeated row with column names\n    df = df.drop(index =[6])\n    \n    #filtering data frame with values only won by magnum\n    wonlotsdf = df[df[\"Magnum Won (Y/N)\"] == 'Y']\n    return wonlotsdf\n\n\ndef createBidSheets():\n    '''\n    This function will take a template pdf and generate pdf's based on wonlots dataframe\n    '''\n    #calling funciton wonlotsDF in ordre for bid sheets to be created \n    wonlotsdf = wonlotsDF()\n\n    templatePDF = 'bidsheet template.pdf'\n    \n    for i in range(0,len(wonlotsdf.index)):\n    \n        OutputPath = filepath +\"/Bid Sheets/\" + wonlotsdf.iloc[i][\"Serial numbers\"] + \" Bid Sheet.pdf\"\n        \n        fields = {\n                \"State\": stinitials,\n                \"Date of Sale\": date,\n                'Check Box for Oil and Gas' : \"x\",\n                \"Oil and Gas/Parcel No\" : wonlotsdf.iloc[i][\"Serial numbers\"],\n                \"TOTAL BID FOR Oil and Gas Lease\" : wonlotsdf.iloc[i][\"Total Bid (Number on BLM Bid Sheet)\"],\n                \"PAYMENT SUBMITTED WITH BID for Oil and Gas\" : wonlotsdf.iloc[i][\"Min Due\"],\n                \"Print or Type Name of Lessee\" : \"R&R Royalty, LTD\",\n                \"Address of Lessee\": \"500 N Shoreline Blvd, Ste 322\",\n                \"City\" : \"Corpus Christi\",\n                \"State_2\": \"TX\",\n                \"Zip Code\" : \"78401\"\n                }\n        \n        write_fillable_pdf(templatePDF, OutputPath, fields)\n\ndef openDI():\n    '''This function will open up DrillingInfo and log user in\n    '''\n    \n    driver = webdriver.Chrome(filepath + \"/chromedriver.exe\")\n    wait = WebDriverWait(driver, 20)\n        \n    driver.get(\"https://app.drillinginfo.com/gallery/\")\n    userfield = driver.find_element_by_name(\"username\")\n    passfield = driver.find_element_by_name(\"password\")\n    \n    userfield.click()\n    userfield.send_keys(\"mbhaktamgm\")\n    passfield.click()\n    passfield.send_keys(\"itheCwe\")\n    passfield.send_keys(Keys.RETURN)\n    \n    myworkspaces = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\\\"workspaces-section\\\"]/div[1]/div[1]')))\n    myworkspaces.click()\n    \n    default_workspace = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"workspaces-section\\\"]/div[3]/di-carousel/section/div[2]/table/tbody/tr[2]/a/span[2]/span\")))\n    default_workspace.click()\n    \n#splitting counties in counties variable at the comma+space to formulate string \n#for drilling info paste and filter\n\nimport pandas as pd\n    \nsplitCounties = [item.split(\", \") for item in counties]\n\nDIcounties =  []\nfor item in splitCounties:\n    #taking \"County\" out of word\n    temp = item[0].upper().replace(\"COUNTY\", \"\")\n    formattedCounty = temp + \"(\" + item[1] + \")\"\n    DIcounties.append(formattedCounty)\n    \n#overwriting counties list and putting into a ddatframe to remove duplicates\ndfCounties = pd.DataFrame()\ndfCounties[\"Counties\"] = DIcounties\ndfCounties.drop_duplicates(inplace = True)\n        \n    \n", "257": "import yaml\nimport os\nimport sys\n\n\ntry:\n    from config import BASE_DIR\nexcept:\n    from flockfysh.config import BASE_DIR\n\nif not os.path.abspath(BASE_DIR) in sys.path:\n    sys.path.append(os.path.abspath(BASE_DIR))\n\nfor pth in [ f.name for f in os.scandir(os.path.join(BASE_DIR, 'scraper')) if f.is_dir() ]:\n    if not os.path.abspath(os.path.join(BASE_DIR, 'scraper', pth)) in sys.path:\n        sys.path.append(os.path.abspath(os.path.join(BASE_DIR, 'scraper', pth)))\nfor pth in [ f.name for f in os.scandir(os.path.join(BASE_DIR, 'utilities')) if f.is_dir() ]:\n    if not os.path.abspath(os.path.join(BASE_DIR, 'utilities', pth)) in sys.path:\n        sys.path.append(os.path.abspath(os.path.join(BASE_DIR, 'utilities', pth)))\n\n\nfrom utilities.pipelines.training_webscrape_loop import run_training_object_detection_webscrape_loop\nfrom utilities.pipelines.annotation_loop import run_object_detection_annotation_loop\nfrom utilities.parse_config.input_validator import get_jobs_from_yaml_params, get_input_yaml \nfrom utilities.parse_config.job_config import EXPORT_JOB, TRAIN_SCRAPE_JOB, ANNOTATE_JOB, DOWNLOAD_JOB, SUPPORTED_DOWNLOAD_APIS\nfrom utilities.output_generation.misc_output import generate_json_file\nfrom utilities.dataset_setup.download_dataset import download_dataset\nfrom utilities.dataset_setup.download_dataset import export_dataset\n\nif sys.version_info[0] < 3:\n    raise Exception(\"Must be using Python 3\")\n\ndef run():\n    #Load the YAML params first\n\n    yaml_params = get_input_yaml(sys.argv[1])\n\n\n    jobs = get_jobs_from_yaml_params(yaml_params)\n    output_dir, _ = os.path.split(os.path.abspath(sys.argv[1]))\n\n    for job in jobs:\n        if job['job_type'] == TRAIN_SCRAPE_JOB:\n            print(f'Running train scrape job with name {job[\"job_name\"]}')      \n            job['output-working-dir'] = output_dir\n            job['input-dir'] = os.path.abspath(os.path.join(output_dir, job['input-dir']))\n            run_training_object_detection_webscrape_loop(**job)\n            generate_json_file(job)\n        \n        elif job['job_type'] == ANNOTATE_JOB:\n            job['output-working-dir'] = output_dir\n            job['input-dir'] = os.path.abspath(os.path.join(output_dir, job['input-dir']))\n            print(f'Running annotate job with name {job[\"job_name\"]}')\n            run_object_detection_annotation_loop(**job)\n\n        elif job['job_type'] == DOWNLOAD_JOB:\n            if job['api-name'] in SUPPORTED_DOWNLOAD_APIS:\n                job['output-working-dir'] = output_dir\n\n            print(f'Running download job with name {job[\"job_name\"]} and api: {job[\"api-name\"]}')      \n            download_dataset(job['api-name'], job)\n\n        elif job['job_type'] == EXPORT_JOB:\n            job['output-working-dir'] = output_dir\n            \n            print(f'Exporting dataset now to ...')\n            export_dataset(job)\n        else:\n            print(f'Unrecognized job type - {job[\"job-type\"]} - for job {job[\"job_name\"]}')\n        \n\n\nif __name__ == '__main__':\n    run()\n", "258": "import spacy\nimport json\nimport argparse\nfrom ws_nbc import WebScrape\n\nclass Model:\n    '''\n    This class initializes the Spacy pipline for NER.  You can initialize it by calling\n    model = model() then to pass a text you call model.ner(text).  To get a dataframe of\n    N number of articles then you call model.get_ner_for_all(articles).\n    '''\n    def __init__(self):\n        self.model = spacy.load(\"en_core_web_sm\")\n\n    def ner(self, content):\n        return self.model(content)\n\n    def get_ner_for_all(self, article):\n        ''''\n        This function is used to obtain NER results for each content in the article\n        and is place in a new dataframe.\n        '''\n        final_out = article.copy()\n        for index, row in final_out.iterrows():\n            spacy_results = self.model(row['article content'])\n            article_ner = get_unique_results(spacy_results)\n            final_out.iloc[[index], [1]] = [article_ner]\n        return final_out\n\ndef get_unique_results(model_output):\n    '''\n    This function prepares a dictionary:\n\n    article = {\n        'NAME' : [...],\n        'ORGANIZATION' : [...],\n        'LOCATION' : [...]\n    }\n\n    It then appends the desired entities from the NER model output into the dictionary.  The entities are,\n    ORG (company, organizations, etc.), PERSON (Name or person), and GPE (Location).\n    '''\n\n    article = {'NAME':[], 'ORGANIZATION':[], 'LOCATION':[]}\n\n    for word in model_output.ents:\n        if word.label_ == 'PERSON' and (word.text not in article[\"NAME\"]):\n            article[\"NAME\"].append(word.text)\n        elif word.label_ == 'ORG' and (word.text not in article[\"ORGANIZATION\"]):\n            article[\"ORGANIZATION\"].append(word.text)\n        elif word.label_ == 'GPE' and (word.text not in article[\"LOCATION\"]):\n            article[\"LOCATION\"].append(word.text)\n    return article\n\ndef save_to_json(results, path):\n    outputDict = results.set_index('article link').to_dict()['article content']\n\n    with open(path+'output.json', 'w') as fp:\n        json.dump(outputDict, fp,  indent=4)\n\ndef save_to_csv(results, path):\n    results.set_index('article link').to_csv(path+'output.csv')\n\nif __name__ == '__main__':\n\n    parser= argparse.ArgumentParser()\n    parser.add_argument('--nbc_url', type=str, default='https://www.nbcnews.com/')\n    parser.add_argument('--nbc_article_url', type=str, default='https://www.nbcnews.com/politics/biden-says-considering-gas-tax-holiday-rcna34419')\n    parser.add_argument('--save_path', type=str, default='../data/model_output/')\n    parser.add_argument('--num_articles', type=int, default=5)\n    args = parser.parse_args()\n\n    spacy_ner = Model()\n    nbc_news = WebScrape(args.nbc_url)\n    nbc_article = WebScrape(args.nbc_article_url)\n\n    # For multiple Articles\n    multi_article = nbc_news.scrape_n_articles(num_articles=args.num_articles)\n    output = spacy_ner.get_ner_for_all(multi_article)\n\n    # For a single article\n    article = nbc_article.scrape_news_article()\n    model_out = spacy_ner.ner(article.get('article content'))\n    unique_results = get_unique_results(model_out)\n\n    save_to_json(output, args.save_path)\n    save_to_csv(output, args.save_path)", "259": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Thu Aug  4 21:13:47 2022\r\n\r\n@author: Mrunmay Junagade\r\n\"\"\"\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n\r\ninfile=r'G:\\My Drive\\Research\\Cables\\cbra\\input.csv'\r\noutfile=r'G:\\My Drive\\Research\\Cables\\cbra\\out2.xlsx'\r\ninp=pd.read_csv(infile)\r\nout=pd.read_excel(outfile)\r\ninp=inp[1:1000]\r\ninp=inp.fillna(0)\r\ninp=inp.drop(columns=['BaseDateTime','LAT','LON','Heading','COG','Status','CallSign'])\r\n\r\nfor i in range(0,len(inp)):\r\n    \r\n    import requests\r\n    from bs4 import BeautifulSoup as bs\r\n    inpt=inp.iloc[i]\r\n    MMSI=str(int(inpt.MMSI))\r\n    my_url = 'https://www.myshiptracking.com/vessels/'+MMSI  #webscrape myshiptracking.com\r\n            \r\n    page=requests.get(my_url)\r\n    soup = bs(page.content)\r\n    a=soup.find_all('td',{})\r\n    if len(a)>=7: \r\n        b=str(a[7])\r\n        if b[5]!='-':\r\n            b=b.replace('','')\r\n            b=b.replace(' Tons','')\r\n            b=int(b.replace(',',''))\r\n            source='myshiptracking.com'\r\n        elif inpt.VesselName==0 or inpt.IMO==0:\r\n            b=0\r\n            source='failed'\r\n        elif b[5]=='-':\r\n            name=str(inpt.VesselName)\r\n            headers={'User-Agent': 'LMAO TRICKED YOU'}\r\n            name=name.replace(' ','-')\r\n            IMO=str(inpt.IMO)\r\n            IMO=IMO.replace('IMO','')\r\n            loc='https://www.vesselfinder.com/vessels/'+name+'-IMO-'+IMO+'-MMSI-'+MMSI #webscrape vesselfinder.com\r\n            r = requests.get(loc,headers=headers)\r\n            soup = bs(r.content, 'lxml')\r\n            tags=soup.find_all('td',{'class':'v3'})\r\n            if  len(tags)>=16:\r\n                b=str(tags[16])\r\n                b=b.replace('','')\r\n                b=b.replace('','')\r\n                if b=='-':\r\n                    b=0\r\n                    source='failed'\r\n                else:\r\n                    b=int(b)\r\n                    source='vesselfinder.com'\r\n            else:\r\n                b=0\r\n                source='failed'\r\n                    \r\n        else: \r\n            b=0\r\n            source='failed'\r\n        \r\n    else:\r\n        b=0\r\n        source='failed'\r\n    \r\n    l=inpt.IMO\r\n            \r\n    print(i,':',b,':',l, source)\r\n        \r\n            \r\n", "260": "import curses\nimport game\nimport webscrape\nfrom webscrape import Scraper\n\ndef update_anim_window(win, anim_index, teams, qstring):\n  win.clear()\n  win.addstr(0, 0, \"-\"*80)\n  for i in range(24):\n    win.addstr(i, 0, \"|\")\n    #win.addstr(i, 79, \"|\")\n  win.addstr(0, 6, \"| Net Bet! v0.1.0 | \" + teams + \" | \" + qstring + \" |\")\n  match anim_index:\n    case 0:\n      for (i, line) in enumerate(game.dunk.splitlines(), 1):\n        win.addstr(i, 1, line)\n    case 1:\n      for (i, line) in enumerate(game.jumpshot.splitlines(), 1):\n        win.addstr(i, 1, line)\n    case 2:\n      for (i, line) in enumerate(game.layup.splitlines(), 1):\n        win.addstr(i, 1, line)\n    case 3:\n      for (i, line) in enumerate(game.freethrow.splitlines(), 1):\n        win.addstr(i, 1, line)\n  win.refresh()\n\ndef update_play_window(win, score, text, playtext):\n  win.clear()\n  win.addstr(0, 0, \"-\"*80)\n  #win.addstr(4, 0, \"-\"*80)\n  \n  for i in range(5):\n    win.addstr(i, 0, \"|\")\n    #win.addstr(i, 79, \"|\")\n  win.addstr(0, 6, \"| play information | press space to advance |\")\n  win.addstr(1, 3, score)\n  #adds the string to the w indo w\n  win.addstr(2, 3, text)\n  win.addstr(3, 3, playtext)\n  win.refresh()", "261": "# set up working directory\nimport sys, os\nos.chdir('/Users/michaelboles/Michael/Coding/2019/Realestate') # Mac\n#os.chdir('C:\\\\Users\\\\bolesmi\\\\Lam\\\\Coding\\\\Python\\\\2019\\\\Realestate') # PC\n\n# import zipcodes\nfrom csvreader import csvread\nfilename = './Data/zipcodes.csv'\nzipcodes_all = csvread(filename)\n\n# select subset of zip codes\nzipcodes = zipcodes_all[100:110]\n\n# scrape MLS listings\nfrom scrapeweb import webscrape\ndata_all = webscrape(zipcodes)\n\n## find zip codes with no listings found\n#import numpy as np\n#missing_zips = np.setdiff1d(zipcodes,data_all.loc[:,'Zip'])\n\n# write .csv file with data\ndata_all.to_csv('data_all.csv')\n\n# Calls the above functions\ndef main():\n    if len(sys.argv) != 2:\n        print('usage: python realestate.py zipcodes')\n        sys.exit(1)\n    zips = csvread(sys.argv[1])\n    print(zips)\n\n# Calls the main function\nif __name__ == '__main__':\n  main()\n", "262": "# set up working directory\nimport sys, os\nos.chdir('/Users/michaelboles/Michael/Coding/2019/Realestate') # Mac\n#os.chdir('C:\\\\Users\\\\bolesmi\\\\Lam\\\\Coding\\\\Python\\\\2019\\\\Realestate') # PC\n\n# import zipcodes\nfrom csvreader import csvread\nfilename = 'zipcodes.csv'\nzipcodes_all = csvread(filename)\n\n# select subset of zip codes\nzipcodes = zipcodes_all[315:]\n\n# scrape MLS listings\nfrom webscraper2 import webscrape\ndata_all = webscrape(zipcodes)\n\n## find zip codes with no listings found\n#import numpy as np\n#missing_zips = np.setdiff1d(zipcodes,data_all.loc[:,'Zip'])\n\n# write .csv file with data\ndata_all.to_csv('data_from_315.csv')\n    \n\n# Calls the above functions\ndef main():\n    if len(sys.argv) != 2:\n        print('usage: python realestate.py zipcodes')\n        sys.exit(1)\n    zips = csvread(sys.argv[1])\n    print(zips)\n\n# Calls the main function\nif __name__ == '__main__':\n  main()\n", "263": "from flask import Flask, render_template, redirect\r\nimport scrape_mars\r\nfrom flask_pymongo import PyMongo\r\n\r\napp = Flask(__name__)\r\n\r\napp.config['MONGO_URI'] = 'mongodb://localhost:27017/mars_db'\r\nmongo = PyMongo(app)\r\n\r\n\r\n@app.route(\"/\")\r\ndef index():\r\n    data = mongo.db.webscrapes.find_one()\r\n    # print(data)\r\n    return render_template('index.html',data = data)\r\n\r\n\r\n@app.route(\"/scrape\")\r\ndef scrape():\r\n    webscrapes_handle = mongo.db.webscrapes\r\n    webscrape = scrape_mars.scrape()\r\n    webscrapes_handle.update({}, webscrape, upsert=True)\r\n    return redirect(\"/\",code=302)\r\n\r\nif __name__ == '__main__':\r\n    app.run(debug=True)", "264": "'''\nCreated on 2021-07-31\n\n@author: wf\n'''\nimport unittest\nfrom corpus.datasources.webscrape import WebScrape\nfrom corpus.datasources.wikicfpscrape import CrawlType\nfrom tests.datasourcetoolbox import DataSourceTest\n\n\nclass TestWebScrape(DataSourceTest):\n    '''\n    test getting rdfA based triples from Webpages\n    '''\n\n    def testCrawlType(self):\n        '''\n        test CrawlType isValid\n        '''\n        self.assertTrue(CrawlType.isValid(\"Event\"))\n        self.assertFalse(CrawlType.isValid(\"Homepage\"))\n\n    def testWebScrape(self):\n        '''\n        test getting rdfA encoded info from a webpage\n        '''\n        debug=self.debug\n        url=\"http://ceur-ws.org/Vol-2635/\"\n        scrape=WebScrape(timeout=20 if self.inCI() else 3)\n        scrapeDescr=[\n            {'key':'acronym', 'tag':'span','attribute':'class', 'value':'CEURVOLACRONYM'},\n            {'key':'title',   'tag':'span','attribute':'class', 'value':'CEURFULLTITLE'},\n            {'key':'loctime', 'tag':'span','attribute':'class', 'value':'CEURLOCTIME'}\n        ]\n        scrapedDict=scrape.parseWithScrapeDescription(url,scrapeDescr)\n        if scrape.err:\n            print(scrape.err)\n            print(\"We might not be able to do anything about it\")\n            return\n        if debug:\n            print(scrapedDict)\n        self.assertEqual('DL4KG2020',scrapedDict[\"acronym\"])\n        self.assertEqual('Heraklion, Greece, June 02, 2020',scrapedDict[\"loctime\"])\n        self.assertEqual('Proceedings of the Workshop on Deep Learning for Knowledge Graphs (DL4KG2020)',scrapedDict[\"title\"])\n        pass\n\n\nif __name__ == \"__main__\":\n    #import sys;sys.argv = ['', 'Test.testName']\n    unittest.main()", "265": "'''\nPython script used to scrape dictionary.com for slang words, acrynonyms, \ndefinitions, and examples. The words, acrynonyms, and definitions were\nlater saved into a json format and added into our Firestore database. \nThe examples were transformed into the dataturks formatting and used\nto add more data to our model. \n'''\n\nimport requests\nimport pickle\nimport json\nfrom bs4 import BeautifulSoup\nimport re\n\n#url for slang and first slang word\ndict_url = 'https://www.dictionary.com/e/slang/'\nword = 'ad hoc'\n\n#url for acrynonyms and first acronyms\nac_url = 'https://www.dictionary.com/e/acronyms/'\nac = 'aafes'\n\n\ndef write_content(examples):\n    '''\n    Method: given a list of examples, annotates and writes examples into\n        a file in dataturks formatting\n    Input: list of examples - list of strings\n    '''\n\n    search = word\n    #search = ac\n\n    filename = \"../twitter_data/acronyms.json\"\n\n    for e in examples:\n        #Clean example \n        e = e.lower()\n        starts = [m.start() for m in re.finditer(search, e)]\n\n        #Check to see starting points of word exists\n        if not starts:\n            continue\n\n        #Set list of ends\n        ends = [i+len(search)-1 for i in starts]\n        \n\n        #Annotate examples into dictionaries\n        labels = [\"Slang\" for _ in starts]\n        points = [{\"start\":s, \"end\":e, \"text\":ac} for s,e in zip(starts,ends)]\n        \n        annotation = []\n        for i in range(len(starts)):\n            annotation.append({\n                \"label\": [labels[i]],\n                \"points\": [points[i]], \n            })\n\n        d = {\n            \"content\": e,\n            \"annotation\":annotation,\n        }\n\n        #Output dictionaries as a json\n        with open(filename, 'a') as outfile:\n            json_data = json.dumps(d)\n            outfile.write(json_data)\n            outfile.write('\\n')\n\n\ndef webscrape():\n    '''\n    Method: webscrape all words, and examples from dictionary.com/e/slang\n    '''\n    global word\n    new_url = dict_url + \"ad-hoc\"\n\n    while (True):\n        print(word)\n        try:\n            #request html from specified url \n            html = requests.get(new_url).text\n            parsed_html = BeautifulSoup(html, 'html.parser')\n\n            #parse out list of examples and clean punctuation\n            examples = parsed_html.findAll('div', class_=\"examples__item__content\")\n            examples = list(filter(None, [BeautifulSoup(str(e), 'html.parser').text.strip() for e in examples]))\n            examples = [e.replace(\"\u201c\", \"\\\"\").replace(\"\u2019\", \"\\'\").replace(\"\u201d\", \"\\\"\").replace('\u2026', \"\") for e in examples]\n\n            #write examples into a json file \n            write_content(examples)\n\n            #find next link and set next url \n            links = parsed_html.findAll('a', class_=\"next\")\n            if len(links) == 0:\n                break\n            link = str(links[0])\n            s = link.find('href=\"') + 6\n            link = link[s:]\n            e = link.find('\">')\n            link = link[:e]\n            new_url = link\n\n            #find next word and set word \n            word = new_url.split('/')[-1].replace('-', ' ')\n        except:\n            continue\n\ndef get_defs():\n    '''\n    Method: webscrape all words and definitions from dictionary.com/e/slang\n    '''\n\n    global word\n    data = {\n        'definition': [],\n    }\n    new_url = dict_url + \"ad-hoc\"\n\n    while (True):\n        try:\n            #request html from specified url \n            html = requests.get(new_url).text\n            parsed_html = BeautifulSoup(html, 'html.parser')\n            definition = parsed_html.findAll('div', class_=\"article-word__header__content__holder\")\n\n            #find next link and set next url \n            links = parsed_html.findAll('a', class_=\"next\")\n            if len(links) == 0:\n                break\n            link = str(links[0])\n            s = link.find('href=\"') + 6\n            link = link[s:]\n            e = link.find('\">')\n            link = link[:e]\n            new_url = link\n\n            #parse out definition of slang\n            if len(definition) < 1:\n                break\n\n            #clean definition from html and save definition to dict\n            result = re.search('(.*)', str(definition[0]))\n            final_def = result.group(1)\n            final_def = re.sub(\"<[^>]*>\", \"\", final_def);\n            d = {\n                'word': word,\n                'def': final_def,\n            }\n            data['definition'].append(d)\n\n            #set next word\n            word = new_url.split('/')[-1].replace('-', ' ')\n            \n        except:\n            continue\n\n    #write definitions of slang to file \n    with open(\"../twitter_data/definitions3.json\", 'a') as outfile:\n            json_data = json.dumps(data)\n            outfile.write(json_data)\n\ndef webscrape2():\n    '''\n    Method: webscrape all acronyms, definitions, and examples from dictionary.com/e/acronyms\n    '''\n\n    global ac\n    data = {\n        'definition': [],\n    }\n    new_url = ac_url + ac\n\n    while (True):\n        print(ac)\n        try:\n            #request html from specified url \n            html = requests.get(new_url).text\n            parsed_html = BeautifulSoup(html, 'html.parser')\n\n            #parse out list of examples and clean punctuation\n            examples = parsed_html.findAll('div', class_=\"examples__item__content text\")\n            examples = list(filter(None, [BeautifulSoup(str(e), 'html.parser').text.strip() for e in examples]))\n            examples = [e.replace(\"\u201c\", \"\\\"\").replace(\"\u2019\", \"\\'\").replace(\"\u201d\", \"\\\"\").replace('\u2026', \"\") for e in examples]\n            \n            #write examples into a json file \n            write_content(examples)\n\n            #parse out definition of acronym\n            definition = parsed_html.findAll('div', class_=\"article-word__header__content__holder\")\n            \n            #find next link and set next url \n            links = parsed_html.findAll('a', class_=\"next\")\n            if len(links) == 0:\n                break\n            link = str(links[0])\n            s = link.find('href=\"') + 6\n            link = link[s:]\n            e = link.find('\">')\n            link = link[:e]\n            new_url = link\n\n            #clean definition from html and save definition to dict\n            if len(definition) < 1:\n                break\n            result = re.search('(.*)', str(definition[0]))\n            final_def = result.group(1)\n            final_def = re.sub(\"<[^>]*>\", \"\", final_def);\n            d = {\n                'word': ac,\n                'def': final_def,\n            }\n            data['definition'].append(d)\n\n            #set next word\n            ac = new_url.split('/')[-1].replace('-', ' ')\n\n        except:\n            #continue\n            pass\n\n    #write definitions of acronyms to file \n    with open(\"../twitter_data/definitions3.json\", 'a') as outfile:\n            json_data = json.dumps(data)\n            outfile.write(json_data)\n\n#Method calls\n#webscrape2()", "266": "# After each one is assigned, it does a page refresh so bye bye code, it needs to be injected over and over again\n# It will throw an error if Refresh 3,5.. min is active. Needs to be on 0\n\n# if cookies not empty ( cookies.txt will be deleted automatically from drive every 12 hours)\n# run the short, load way . Delete cookies.txt for fresh cookies if it fails to work\n\n# RUN THE GUI and press the buttons!\n\n# for LOGS.txt, replace pickle with plain read and write ;)\n\nimport pickle\nimport pprint\nfrom selenium import webdriver\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom pathlib import Path\nimport traceback\nimport logging\nfrom selenium.webdriver.chrome.options import Options # headless\nimport datetime\n\n\n\nclass AutoSD:\n\n    def __init__(self):\n        print('initiating the bot...')\n        global USER, PASSWORD, COOKIES_PATH, COOKIES, chrome, session_ID, URL, LOGS, LOGS_PATH\n        USER = 'antonoium'\n        PASSWORD = 'madutzu93classiC'\n        COOKIES = \"C:\\\\Users\\\\antonoium\\\\Desktop\\\\venv\\\\webscrape\\\\webscrape\\\\SeleniumSD\\\\cookies.txt\"\n        CHROMEDRIVER_PATH = \"C:\\\\Users\\\\antonoium\\\\Desktop\\\\venv\\\\webscrape\\\\webscrape\\\\SeleniumSD\\\\chromedriver.exe\"\n        COOKIES_PATH = Path(COOKIES)\n        LOGS = \"C:\\\\Users\\\\antonoium\\\\Desktop\\\\venv\\\\webscrape\\\\webscrape\\\\SeleniumSD\\\\logs.txt\"\n        LOGS_PATH = Path(LOGS)\n\n        # Headless ( no GUI)\n        options = Options()\n        options.add_argument('--headless')\n        options.add_argument('--disable-gpu')\n        #driver = webdriver.Chrome(options=options)\n        chrome = webdriver.Chrome(executable_path=CHROMEDRIVER_PATH, options=options)  # This will open the Chrome window\n        chrome.minimize_window()  # for the browser to start minimized\n\n\n        session_ID = chrome.session_id\n        URL = chrome.command_executor._url\n        #print(session_ID, URL)\n\n    @staticmethod\n    def save_cookies(driver, location):\n        pickle.dump(driver.get_cookies(), open(location, \"wb\"))\n\n    @staticmethod\n    def load_cookies(driver, location, url='https://servicedesk.csiltd.co.uk/'):\n        cookies = pickle.load(open(location, \"rb\"))\n        driver.delete_all_cookies()\n        url = \"https://google.com\" if url is None else url\n        driver.get(url)\n        for cookie in cookies:\n            driver.add_cookie(cookie)\n\n    def obtain_sd_cookies(self):\n\n        chrome.get('https://servicedesk.csiltd.co.uk/')  # Group Unassigned\n\n        user_field = \"//input[@id='username']\"\n        password_field = \"//input[@id='password']\"\n        login_button = \"//button[@id='loginSDPage']\"\n\n        user_element = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(user_field))\n        user_element.send_keys(USER)\n\n        password_element = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(password_field))\n        password_element.send_keys(PASSWORD)\n\n        login_element = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(login_button))\n        login_element.click()\n\n        pprint.pprint(chrome.get_cookies())  # to visualise them in console\n        self.save_cookies(chrome, COOKIES)\n        chrome.quit()\n\n    @staticmethod\n    def inject_javascript():\n        # get script content from separate file\n        my_file = open(\"C:\\\\Users\\\\antonoium\\\\Desktop\\\\venv\\\\webscrape\\\\webscrape\\\\SeleniumSD\\\\highlight_script.js\", \"rt\")\n        contents = my_file.read()\n        my_file.close()\n        chrome.execute_script(contents) # replace with selenium doing things\n\n\n\n    # Only for logs.txt\n    @staticmethod\n    def get_logs(ticket_id, agent, current_technician, subject, priority):\n        x = datetime.datetime.now()\n        formatted = \"%a, %d %b %Y, at %H:%M\"\n        date_time = x.strftime(formatted)\n        print(date_time)\n\n        text = date_time + \" - \" + \" [\" + priority + \"] \" + ticket_id + \" : \" + agent + \" , \" + current_technician + \" , \" + \"[\" + subject + \"]\"\n\n        if LOGS_PATH.is_file():\n\n            # Fix for : EOFError: Ran out of input\n            try:\n                logs = pickle.load(open(LOGS, \"rb\"))  # read from logs already\n                saved_text = logs + \"\\n\" + text\n                return saved_text\n            except EOFError:\n                saved_text = text\n                return saved_text\n\n        else:\n            # In case logs.txt doesn't exist in the location\n            saved_text = text\n            return saved_text\n\n    @staticmethod\n    def save_logs(text, location):\n        pickle.dump(text, open(location, \"wb\"))\n\n\n    # Main functions\n    @staticmethod\n    def open_assigned_to(nr): # This is recreation of my highlight_script.js as Selenium\n        row = nr # starts with 0 being first, 4 means 5th row\n        popup_path = \"//table[@class='DialogBox']\"\n        ticket_id_path = \"//td[@id='RequestsView_r_\"+row+\"_5']//div\"\n        subject_path = \"//td[@id='RequestsView_r_\"+row+\"_6']//div/a\"\n        assig_cell = \"//td[@id='RequestsView_r_\"+row+\"_7']//div/div\"\n        priority_path = \"// td[ @ id = 'RequestsView_r_\"+row+\"_8']//div/table/tbody/tr/td[last()]\"\n        all_rows = \"//*[@id='RequestsView_TABLE']/tbody/tr[position() > 2]\"\n\n\n        print(assig_cell)\n\n        # Cookies might have expired if you get an error here\n        assigned_cell = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(assig_cell))\n        agent = assigned_cell.text.strip()\n\n\n        # Counting total tickets number\n        all_rows_len = WebDriverWait(chrome, 10).until(lambda chrome:chrome.find_elements_by_xpath(all_rows))\n        count = len(all_rows_len)\n        print('---- Group Unassigned : ' + str(count) + ' tickets -----')\n\n        #repeating_tickets and their count\n\n\n        ticket_id = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(ticket_id_path)).text.strip()\n        subject = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(subject_path)).text.strip()\n        priority = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(priority_path)).text.strip()\n\n        if agent == 'Unassigned':\n            assigned_cell.click()\n            popup = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(popup_path))\n            return popup, agent, ticket_id, subject, priority, row, 'Step1: Popup clicked:'\n\n        raise Exception(\"Fail - Not Unassigned\")\n\n    @staticmethod\n    def get_technician(msg2):\n        technician = \"//div[@id='s2id_selectTechnician']\"\n        tech_name_xpath = \"//div[@id='s2id_selectTechnician']//a[@class='select2-choice']//span\"\n\n        if msg2 is not None:\n            current_tech = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(tech_name_xpath)).text.strip() # these 2 lines represent the first span text of technician\n            # technician_elem.click()\n\n            return current_tech, 'Step2: Current technician: '\n\n        raise Exception(\"Technician is NULL 394934834\")\n\n    @staticmethod\n    def open_group(current_technician):\n        if current_technician == \"NONE\":\n            group_xpath = \"//div[@id='s2id_assignGroup']//a[@class='select2-choice']\"\n            open_group = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_element_by_xpath(group_xpath)).click()\n            return open_group, 'Step3: Group list opened'\n\n        raise Exception(\"Fail - Current technician is not NONE\")\n\n    @staticmethod\n    def select_service_desk(msg):\n        if msg == \"Step3: Group list opened\":\n            groups_ul_xpath = \"//div[@id='select2-drop']//ul//li\"\n            groups = WebDriverWait(chrome, 10).until(lambda chrome: chrome.find_elements_by_xpath(groups_ul_xpath))\n            #return groups[5].text 'Csi Facilities'\n\n            for group in groups:\n                if group.text == \"Service Desk\":\n                    group.click()\n                    return 'Step4: Chosen Service Desk'\n\n        raise Exception(\"Fail - groups list was not opened\")\n\n\n    #def GROUP_UNASSIGNED(self):\n\n    def main(self, nr):\n        try:\n            if not COOKIES_PATH.is_file():\n                print('Cookies.txt not found, obtaining cookies.')\n                self.obtain_sd_cookies()  # If cookies.txt not to be found, log in and create em\n\n                # Otherwise skip logging and use cookies\n            self.load_cookies(chrome, COOKIES)\n            chrome.get(\n                'https://servicedesk.csiltd.co.uk/WOListView.do?requestViewChanged=true&viewName=38020_MyView&globalViewName=All_Requests')\n\n            # Main execution chain\n            popup, agent, ticket_id, subject, priority, row, msg1 = self.open_assigned_to(nr)\n            print(msg1 + \" \" + priority + \" \" + ticket_id + \" - \" + agent + \" , \" + \"[\" + subject + \"]\")  # \"Opened AssignedTo popup for:\"\n\n            current_technician, msg2 = self.get_technician(msg1)\n            print(msg2 + \" \" + current_technician)  # 'Current technician: '\n\n            group, msg3 = self.open_group(current_technician)\n            print(msg3)\n\n            msg4 = self.select_service_desk(msg3)\n            print(msg4)\n\n            txt = self.get_logs(ticket_id, agent, current_technician, subject, priority)\n            self.save_logs(txt, LOGS)\n\n            #self.inject_javascript()\n\n            return \"[\" + row + \"]\" + priority + \" \" + ticket_id + \" - \" + agent + \" , \" + \"[\" + subject + \"]\"\n\n        except Exception as e:\n            logging.error(traceback.format_exc())\n            # Logs the error appropriately.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        # Get Js Code from other file as string for selenium browser execute\n        # myfile = open(\"lorem.txt\", \"rt\") # open lorem.txt for reading text\n        # contents = myfile.read()         # read the entire file into a string\n        # myfile.close()                   # close the file\n        # print(contents)                  # print contents\n\n    #def repeated(self):\n", "267": "# -*- coding: utf-8 -*-\n# ryder franklin\n# FYP - AI news extraction\n\nimport seleniumDataExtraction as dataScraper\nimport hotTopicExtraction as hte\nimport wordCloudGeneration as wc\n\nif __name__ == \"__main__\":\n    # create the driver object.\n    takingQueries = True\n    while takingQueries:\n        print(\"Do you wish to: \"\n              \"[1] Run a complete webscrape and analysis on a topic \"\n              \"[2] Run a preset analysis on available data \"\n              \"[3] Manually analyse data \"\n              \"[4] Close program \")\n        userManual = input(\"> \")\n        if userManual == \"1\":\n            print(\"Please ensure you have a valid version of chrome \"\n                  \"and its corresponding chromedriver installed before scraping.\")\n            print(\"Input topic to extract articles on: \")\n            userQuery = input(\"> \")\n            dataScraper.webScrape(userQuery)\n            csvName = 'CSV-Articles/articlesData-' + userQuery + '.csv'\n            hte.runExtraction(csvName)\n            wc.wordCloud(csvName)\n        elif userManual == \"2\":\n            print(\"Input the filepath to the CSV file you wish to extract topics from and create a word cloud: \")\n            csvChoice = input(\"> \")\n            hte.runExtraction(csvChoice)\n            print(\"Use image mask? [1] Yes [2] No\")\n            imgMskChoice = input(\"> \")\n            if imgMskChoice == \"1\":\n                print(\"Input filepath to mask: \")\n                maskName = input(\"> \")\n                wc.wordCloudWithMask(csvChoice, maskName)\n            else:\n                wc.wordCloud(csvChoice)\n        elif userManual == \"3\":\n            print(\"Do you wish to: \"\n                  \"[1] Scrape for articles on a specified topic \"\n                  \"[2] Generate a wordcloud on available data \"\n                  \"[3] Run hot topic extraction on available data \")\n            userChoice = input(\"> \")\n            if userChoice == \"1\":\n                print(\"Please ensure you have a valid version of chrome \"\n                      \"and its corresponding chromedriver installed before scraping.\")\n                print(\"Input topic to extract articles on: \")\n                userQuery = input(\"> \")\n                dataScraper.webScrape(userQuery)\n            elif userChoice == \"2\":\n                # EDA with word cloud\n                print(\"Input the filepath of the csv file you wish to generate a word cloud on: \")\n                fileName = input(\"> \")\n                print(\"Does this file/topic have a corresponding mask? [1] Yes [2] No\")\n                maskYes = input(\"> \")\n                if maskYes == \"2\":\n                    wc.wordCloud(fileName)\n                elif maskYes == \"1\":\n                    print(\"Input the filepath of the jpg or png file you wish to use for mask: \")\n                    maskName = input(\"> \")\n                    wc.wordCloudWithMask(fileName, maskName)\n            elif userChoice == \"3\":\n                print(\"Input the filepath of the csv file you wish to commit hot topic extraction on: \")\n                fileName = input(\"> \")\n                hte.runExtraction(fileName)\n        elif userManual == \"4\":\n            print(\"Exiting program now.\")\n            takingQueries = False\n        else:\n            print(\"Unclear input, retry.\")\n", "268": "# Import Libraries (heroku requires a . before local imports)\nfrom flask import Flask, jsonify, render_template_string\nfrom .scrape import webscrape_single_section, webscrape_all_sections\nfrom .render import docs_html\nfrom .db import *\n\n# Configure as a flask server\napp = Flask(__name__)\n\n# Root endpoints only return README from github repo\n@app.route('/')\n@app.route('/v1/')\ndef render_docs():\n    return render_template_string(docs_html())\n\n\n# GET /v1///\n\n\n@app.route('/v1///', methods=['GET'])\n# Returns only the course requested\ndef single_course(term, course, section):\n    # Formatting\n    term = term.lower()\n    course = course.lower()\n    url = f\"{course}.{section}.{term}\"\n\n    # Scrape coursebook\n    class_info = webscrape_single_section(url)\n\n    # Send response\n    return jsonify({'data': class_info})\n\n# GET /v1/\n@app.route('/v1//', methods=['GET'])\n# Returns class data for all the sections in the current semester\ndef all_courses(course):\n    course = course.lower()\n    course_list = webscrape_all_sections(course)\n    return jsonify({\"data\": course_list})\n\n\n@app.route('/v1/grades///', methods=['GET'])\ndef single_course_grade(term, course, section):\n    grade_data = get_single_course_grade(term, course, section)\n    return jsonify({\"data\": grade_data})\n\n\n@app.route('/v1/grades//', methods=['GET'])\ndef all_course_grades(term, course):\n    grade_data = get_all_course_grades(term, course)\n    return jsonify({\"data\": grade_data})\n\n\n@app.route('/v1/prof/', methods=['GET'])\ndef get_prof_data(name):\n    prof_data = fetch_prof(name)\n    return jsonify({\"data\": prof_data})\n\n\n# Serve the server\nif __name__ == '__main__':\n    app.run(threaded=True)\n", "269": "import os\nfrom filtering import Cleaningkata, Cleaningquery, convert, Countwords\nfrom webscrape import webscrape\n\n\n#Fungsi ini berfungsi untuk menghasilkan vektor terms dari files\ndef generateTermsFromFiles(basedir):\n    basedir = basedir + \"/static\"\n    multFiles = []\n    uniqueTerms = dict()  # Declare Empty Dict untuk nyimpen semua unique terms\n    fullMatrix = []  # array of dict\n    fileNames = [\"skip this\"]\n    for filenames in os.listdir(basedir):\n        if filenames.endswith(\".html\"):\n            fileNames.append(filenames)\n            joinpath = os.path.join(basedir, filenames)\n            file = open(joinpath).read()\n            docs = convert(Cleaningkata(file))\n            docs = docs.split()\n            docs = Countwords(docs)\n            termDocs = docs.copy()\n            termDocs = {x: 0 for x in termDocs}  # Dictionary comprehension\n            uniqueTerms.update(termDocs)\n            multFiles.append(docs)\n        else:\n            continue\n    return generateMatrixFromTerms(fullMatrix, uniqueTerms, multFiles, fileNames)\n\n\n# Fungsi ini berfungsi untuk membangun matrix dari terms yang dihasilkan fungsi sebelumnya\ndef generateMatrixFromTerms(fullMatrix, uniqueTerms, multFiles, fileNames):\n    elem = dict()\n    for keys in uniqueTerms:\n        elem[keys] = 0\n    fullMatrix.append(elem)\n    for docs in multFiles:\n        elem = dict()\n        for keys in uniqueTerms:\n            if keys in docs:\n                elem[keys] = docs[keys]\n            else:  # keys not in docs\n                elem[keys] = 0\n        fullMatrix.append(elem)\n\n    return [uniqueTerms, fullMatrix, fileNames]\n\n\n#Fungsi ini berfungsi untuk membangun matrix dari vektor terms untuk sumber dokumen dari webscraping\ndef generateTermsFromWebscrap():\n    fullMatrix = []\n    semiFullMatrix = []\n    uniqueTerms = dict()\n    webs = ['https://www.worldoftales.com/fairy_tales/Hans_Christian_Andersen/Andersen_fairy_tale_47.html#gsc.tab=0',\n            'https://www.worldoftales.com/fairy_tales/Brothers_Grimm/THE%20GOOSE-GIRL.html#gsc.tab=0', 'https://www.worldoftales.com/European_folktales/English_folktale_116.html#gsc.tab=0']\n    for web in webs:\n        docs = webscrape(web)\n        termDocs = docs.copy()\n        termDocs = {x: 0 for x in termDocs}\n        uniqueTerms.update(termDocs)\n        semiFullMatrix.append(docs)\n    return generateMatrixFromWebTerms(uniqueTerms, fullMatrix, semiFullMatrix, webs)\n\n#Fungsi ini berfungsi untuk membangun matrix dari vektor terms\ndef generateMatrixFromWebTerms(uniqueTerms, fullMatrix, semiFullMatrix, webs):\n    elem = dict()\n    for keys in uniqueTerms:\n        elem[keys] = 0\n    fullMatrix.append(elem)\n    for docs in semiFullMatrix:\n        elem = dict()\n        for keys in uniqueTerms:\n            if keys in docs:\n                elem[keys] = docs[keys]\n            else:\n                elem[keys] = 0\n        fullMatrix.append(elem)\n    return [uniqueTerms, fullMatrix, webs]\n\n\n#Fungsi ini berfungsi untuk membangun vektor terms dari query yang diberikan dari searching frontend\ndef generateQueryVector(query):\n    docs = convert(Cleaningquery(query))\n    docs = docs.split()\n    return Countwords(docs)\n\n#Fungsi ini berfungsi untuk memperbaharui terms yang ada di salah satu vektor terms\ndef updateTerms(fullMatrix, queryVector):\n    newFullMatrix = []\n    for docsDict in fullMatrix:\n        tempDocsDict = docsDict.copy()\n        docsDict = {x: 0 for x in queryVector}\n        docsDict.update(tempDocsDict)\n        newFullMatrix.append(docsDict)\n    return newFullMatrix  # terms newFullMatrix sudah lengkap bersama query\n", "270": "from django.shortcuts import render, redirect\nfrom rest_framework import viewsets\nfrom .models import Cadastro\nfrom .serializer import CadastroSerializer\nfrom bs4 import BeautifulSoup\nimport requests\nfrom .webscrapping import webscrape\n#Home- Pagina inicial exibindo a listagem dos itens.\ndef home(request):\n    cadastros = Cadastro.objects.all()\n    return render(request, 'home.html', {'cadastros': cadastros})\n#busca dos itens do cadastro no banco de dado\nclass CadastroViewSet(viewsets.ModelViewSet):\n    queryset = Cadastro.objects.all()\n    serializer_class = CadastroSerializer\n#class lista_itens():\n    #cadastro = Cadastro.objects,all()\n    #render(, 'home.html', {'cadastro': cadastro})\ndef add(request):\n    page = requests.get('https://nerdstore.com.br/categoria/especiais/game-of-thrones/')\n    soup = BeautifulSoup(page.content, 'html.parser')\n    # Se odownloadfor realizado \u00e9 retornado com status 200\n    # page.status_code\n    # exibir conteudo da pagina\n    # page.content\n    # definicao da div para busca de itens, div que contem os itens desejados.\n    itens = soup.find_all('li', class_='product')\n    # teste para verificar se est\u00e1 trazendo os itens\n    # print(len(itens))\n    # for para percorrer a div que cont\u00e9m os itens e capturar os itensdesejados\n    for i in itens:\n        # buscar pela class\n        nome = i.find('h2', class_='woocommerce-loop-product__title').text\n        link = i.find('a', class_='woocommerce-LoopProduct-link')['href']\n        preco = i.find('span', class_='woocommerce-Price-amount amount').text\n        cad = Cadastro()\n        cad.nome = nome\n        cad.link = link\n        cad.preco = preco\n        cad.save()\n    #adiciona = CadastroSerializer(request.webscrape() or None)\n    #adiciona.save()\n    cadastros = Cadastro.objects.all()\n    return render(request, 'home.html', {'cadastros': cadastros})", "271": "#SQL_DATABASE for windows. Uses mysql.connector.\n\nimport mysql.connector\nfrom data_indsamler import indsaml, webScrape\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nsql_host = os.getenv(\"sql_host\")\nsql_user = os.getenv(\"sql_user\")\nsql_pass = os.getenv(\"sql_pass\")\ndb_name = os.getenv(\"db_name\")\ndb_table = os.getenv(\"db_table\")\n\nlectio_user = os.getenv(\"lectio_user\")\nlectio_pass = os.getenv(\"lectio_pass\")\nlectio_opgaver = os.getenv(\"lectio_opgaver\")\nlectio_base = os.getenv(\"lectio_base\")\n\n\n\ndef connect():\n    mindb = mysql.connector.connect(\n        host=sql_host,\n        user=sql_user,\n        password=sql_pass\n    )\n\n    mincursor = mindb.cursor()\n\n    sql = f\"USE {db_name}\"\n\n    mincursor.execute(sql)\n    mindb.commit()\n    mincursor.close()\n    #print(\"Connected til database...\")\n    return mindb\n\ndef makeCursor(mindb):\n    mincursor = mindb.cursor()\n    return mincursor\n\n# Reset af tabel\ndef resetTable():\n    mindb = connect()\n    mincursor = makeCursor(mindb)\n    sql = f\"DROP TABLE {db_table}\"\n    mincursor.execute(sql)\n\n    sql = f\"\"\"CREATE TABLE `{db_name}`.`{db_table}` (\n      `idlektier` INT NOT NULL AUTO_INCREMENT,\n      `titel` VARCHAR(45) NOT NULL,\n      `frist` DATETIME NOT NULL,\n      `elevtid` VARCHAR(45),\n      `slettet` BOOL,\n      PRIMARY KEY (`idlektier`));\"\"\"\n    mincursor.execute(sql)\n\n    print(\"Tabel resat!\")\n    mincursor.close()\n    mindb.close()\n\n# Upload\ndef uploadToTable():\n    names, frister, elevtid = indsaml(webScrape(lectio_base + \"login.aspx\", lectio_base + lectio_opgaver, lectio_user, lectio_pass))\n    if len(os.listdir(\"html/\")) >= 4:\n        os.remove(\"html/\" + sorted(os.listdir(\"html/\"))[0])\n\n    mindb = connect()\n    mincursor = makeCursor(mindb)\n    for i in range(len(names)):\n        sql = f\"INSERT INTO {db_table} (titel, frist, elevtid, slettet) VALUES (%s, %s, %s, False)\"\n        val = (names[i], frister[i], elevtid[i])\n    \n        #print(val)\n        mincursor.execute(sql, val)\n        \n        mindb.commit()\n    print(\"Data uploadet!\")\n    mincursor.close()\n    mindb.close()\n\n# Vis data i tabeller\ndef showNext(limitval):\n    mindb = connect()\n    mincursor = makeCursor(mindb)\n    sql = f\"SELECT titel, frist, elevtid FROM {db_table} WHERE frist > curtime() AND slettet = 0 ORDER BY frist LIMIT {limitval}\"\n    mincursor.execute(sql)\n    resultat = mincursor.fetchall()\n\n\n    resultater = []\n\n    for x in resultat:\n        resultater.append(x)\n    \n    #print(\"Lektier optalt...\")\n    mincursor.close()\n    mindb.close()\n    #print(list(resultater[0]))\n    #print(resultater)\n    return resultater\n\n# Slet gamle opgaver\ndef deleteOld():\n    mindb = connect()\n    mincursor = makeCursor(mindb)\n    \n    # Fjerner gamle lektier\n    sql = f\"UPDATE {db_table} SET slettet = True WHERE frist < curtime() AND slettet = False\"\n    #sql = f\"DELETE FROM {db_table} WHERE frist < curtime()\"\n    \n    mincursor.execute(sql)\n    mindb.commit()\n\n    # Fjerner duplicates -- Beholder den lektier med lavest ID\n    sql = f\"DELETE t1 FROM {db_table} t1 INNER JOIN {db_table} t2 WHERE t1.idlektier > t2.idlektier AND t1.titel = t2.titel;\"\n    mincursor.execute(sql)\n    mindb.commit()\n\n    print(\"Gamle lektier fjernet!\")\n    mincursor.close()\n    mindb.close()\n\n\n\n#webScrape(lectio_base + \"login.aspx\", lectio_base + lectio_opgaver, lectio_user, lectio_pass)\n#mindb = connect()\n#resetTable()\n#uploadToTable()\n#print(showNext(3))\n#print(showNext(3)[0][1])\n\n# Updates database\n\nuploadToTable()\ndeleteOld()", "272": "#SQL_DATABASE for linux/raspberry pi. Uses mariadb.\n\nimport mariadb\nfrom data_indsamler import indsaml, webScrape\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nsql_host = os.getenv(\"sql_host\")\nsql_user = os.getenv(\"sql_user\")\nsql_pass = os.getenv(\"sql_pass\")\ndb_name = os.getenv(\"db_name\")\ndb_table = os.getenv(\"db_table\")\n\nlectio_user = os.getenv(\"lectio_user\")\nlectio_pass = os.getenv(\"lectio_pass\")\nlectio_opgaver = os.getenv(\"lectio_opgaver\")\nlectio_base = os.getenv(\"lectio_base\")\n\n\n\ndef connect():\n    mindb = mariadb.connect(\n        host=sql_host,\n        user=sql_user,\n        password=sql_pass,\n        database=db_name\n    )\n\n    mincursor = mindb.cursor()\n    mincursor.close()\n    #print(\"Connected til database...\")\n    return mindb\n\ndef makeCursor(mindb):\n    mincursor = mindb.cursor()\n    return mincursor\n\n# Reset af tabel\ndef resetTable():\n    mindb = connect()\n    mincursor = makeCursor(mindb)\n    sql = f\"DROP TABLE {db_table}\"\n    mincursor.execute(sql)\n\n    sql = f\"\"\"CREATE TABLE `{db_name}`.`{db_table}` (\n      `idlektier` INT NOT NULL AUTO_INCREMENT,\n      `titel` VARCHAR(45) NOT NULL,\n      `frist` DATETIME NOT NULL,\n      `elevtid` VARCHAR(45),\n      `slettet` BOOL,\n      PRIMARY KEY (`idlektier`));\"\"\"\n    mincursor.execute(sql)\n\n    print(\"Tabel resat!\")\n    mincursor.close()\n    mindb.close()\n\n# Upload\ndef uploadToTable():\n    names, frister, elevtid = indsaml(webScrape(lectio_base + \"login.aspx\", lectio_base + lectio_opgaver, lectio_user, lectio_pass))\n    if len(os.listdir(\"html/\")) >= 4:\n        os.remove(\"html/\" + sorted(os.listdir(\"html/\"))[0])\n\n    mindb = connect()\n    mincursor = makeCursor(mindb)\n    for i in range(len(names)):\n        sql = f\"INSERT INTO {db_table} (titel, frist, elevtid, slettet) VALUES (%s, %s, %s, False)\"\n        val = (names[i], frister[i], elevtid[i])\n    \n        #print(val)\n        mincursor.execute(sql, val)\n        \n        mindb.commit()\n    print(\"Data uploadet!\")\n    mincursor.close()\n    mindb.close()\n\n# Vis data i tabeller\ndef showNext(limitval):\n    mindb = connect()\n    mincursor = makeCursor(mindb)\n    sql = f\"SELECT titel, frist, elevtid FROM {db_table} WHERE frist > curtime() AND slettet = 0 ORDER BY frist LIMIT {limitval}\"\n    mincursor.execute(sql)\n    resultat = mincursor.fetchall()\n\n\n    resultater = []\n\n    for x in resultat:\n        resultater.append(x)\n    \n    #print(\"Lektier optalt...\")\n    mincursor.close()\n    mindb.close()\n    #print(list(resultater[0]))\n    #print(resultater)\n    return resultater\n\n# Slet gamle opgaver\ndef deleteOld():\n    mindb = connect()\n    mincursor = makeCursor(mindb)\n    \n    # Fjerner gamle lektier\n    sql = f\"UPDATE {db_table} SET slettet = True WHERE frist < curtime() AND slettet = False\"\n    #sql = f\"DELETE FROM {db_table} WHERE frist < curtime()\"\n    \n    mincursor.execute(sql)\n    mindb.commit()\n\n    # Fjerner duplicates -- Beholder den lektier med lavest ID\n    sql = f\"DELETE t1 FROM {db_table} t1 INNER JOIN {db_table} t2 WHERE t1.idlektier > t2.idlektier AND t1.titel = t2.titel;\"\n    mincursor.execute(sql)\n    mindb.commit()\n\n    print(\"Gamle lektier fjernet!\")\n    mincursor.close()\n    mindb.close()\n\n\n\n#webScrape(lectio_base + \"login.aspx\", lectio_base + lectio_opgaver, lectio_user, lectio_pass)\n#mindb = connect()\n#resetTable()\n#uploadToTable()\n#print(showNext(3))\n#print(showNext(3)[0][1])\n\n# Updates database\n\nuploadToTable()\ndeleteOld()", "273": "from flask_login import login_required, current_user\nfrom flask import Flask, render_template,request, jsonify\nfrom amazonreviews import main_func as m\n\nfrom datetime import datetime\nimport time\nimport os\n\nimport json\nimport threading\n\nfrom extensions import db\nfrom extensions import migrate\nfrom extensions import login_manager\nfrom google.cloud import bigquery\n\nfrom auth import auth as auth_blueprint\nfrom model import model as model_blueprint\n\nfrom celery import Celery\nfrom celery_once import QueueOnce\n\n\ndef make_celery(app):\n    celery = Celery(\n        app.import_name,\n        backend=app.config['CELERY_RESULT_BACKEND'],\n        broker=app.config['CELERY_BROKER_URL']\n    )\n    celery.conf.update(app.config)\n\n    class ContextTask(celery.Task):\n        def __call__(self, *args, **kwargs):\n            with app.app_context():\n                return self.run(*args, **kwargs)\n\n    celery.Task = ContextTask\n\n    return celery\n\ndef create_app():\n    app = Flask(__name__)\n    app.secret_key = 'need to set os env variable for value'\n    with app.app_context():\n        app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite://///Users/jiaweitchea/desktop/fyp/webscrap/loreal_db.sqlite3'\n        #app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite://///mnt/c/users/ryan/work_ryan/y4s1/fyp/webscrap-website/loreal_db.sqlite3'\n        app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n        app.secret_key = os.urandom(24)\n\n        #Celery configuration\n        app.config['CELERY_RESULT_BACKEND'] = 'redis://localhost:6379/0'\n        app.config['CELERY_BROKER_URL'] = 'redis://localhost:6379/0'\n        app.config['CELERY_CREATE_MISSING_QUEUES'] = True\n\n        db.init_app(app)\n        migrate.init_app(app,db)\n        login_manager.init_app(app)\n\n        app.register_blueprint(auth_blueprint)\n        app.register_blueprint(model_blueprint)\n    return app\n\ndef __init__(self,name,price,last_scraped):\n    self.name = name\n    self.price = price\n    self.last_scraped = last_scraped\n\n\ndef configure_setting(app):\n    with app.app_context():\n        from model import Setting\n        from model import Directory\n\n        set_obj = db.session.query(Setting).order_by(Setting.id.desc()).first()\n        dir_obj = db.session.query(Directory).order_by(Directory.id.desc()).first()\n\n        rotate_proxy = set_obj.rotate_proxy\n        fetch_proxies = set_obj.fetch_proxies\n        rotating_proxy_page_retry = set_obj.rotating_proxy_page_retry\n        no_of_concurrent_request = set_obj.no_of_concurrent_request\n        download_delay = set_obj.download_delay\n        download_timeout = set_obj.download_timeout\n        no_of_retry = set_obj.no_of_retry\n        tracker_output = dir_obj.tracker_filepath\n\n        configure_setting = {\n        'RETRY_TIMES': no_of_retry,\n        'CONCURRENT_REQUESTS': no_of_concurrent_request,\n        'DOWNLOAD_DELAY': download_delay,\n        'DOWNLOAD_TIMEOUT': download_timeout,\n        'NUMBER_OF_PROXIES_TO_FETCH': fetch_proxies ,\n        'ROTATING_PROXY_PAGE_RETRY_TIMES': rotating_proxy_page_retry,\n        'ROTATED_PROXY_ENABLED': rotate_proxy,\n        'tracker_output' : tracker_output\n        }\n\n    return configure_setting\n\napp = create_app()\ncelery = make_celery(app)\n\n#A user loader tells Flask-Login how to find a specific user from the ID that is stored in their session cookie\nlogin_manager.login_view = 'auth.login'\n@login_manager.user_loader\ndef load_user(user_id):\n    from model import User\n    # since the user_id is just the primary key of our user table, use it in the query for the user\n    return User.query.get(int(user_id))\n\n\ndef check_non_empty_space_in_val(input):\n    if input and not input.isspace():\n        return True\n    return False\n\n@celery.task()\ndef query_reviews():\n    from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n    from PIL import Image\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    project = 'crafty-chiller-276910'\n    cwd = os.getcwd()\n    #change secret key path based on where you store it\n    secret_key_path = os.path.join(cwd,'credential_file.json')\n\n    #initialize bq client\n    client = bigquery.Client.from_service_account_json(secret_key_path)\n\n    query = '''\n            SELECT cleaned_text\n            FROM `crafty-chiller-276910.cleaned_items.reviews`\n            '''\n\n    start = time.time()\n    query_job = client.query(query)\n    print(\"Time taken to query:\",time.time() - start)\n\n    #Convert query to list type\n    result_query = list(query_job.result())\n\n    reviews = \" \"\n\n    start = time.time()\n\n    #Concatenate all reviews to string format\n    for i in range(len(result_query)):\n        review = result_query[i][0]\n        if review is not None:\n            reviews += review\n\n    #Remove stop words to refine review text\n    stopwords = ['en' ,'tous' ,'ca' ,'le' ,'also', 'im', 'like', 'wa', 'ha', 'tey','est','go','doe','give']\n    preprocessed_text = reviews.split()\n    resultwords  = [word for word in preprocessed_text if word.lower() not  in stopwords and word.isnumeric() == False]\n    result = ' '.join(resultwords)\n\n    #Generate Review wordcloud\n    alice_mask = np.array(Image.open(cwd +'/static/img/alice3.jpg'))\n\n    wordcloud = WordCloud(background_color='white',\n                      mask=alice_mask,contour_width=1.5, contour_color='steelblue').generate(result)\n\n    image_output_path = cwd +'/static/img/alice.jpg'\n    wordcloud.to_file(image_output_path)\n    print(\"Time taken to generate wordcloud:\",time.time() - start)\n\n@celery.task()\ndef query_reviews_contributors():\n    cwd = os.getcwd()\n    #change secret key path based on where you store it\n    secret_key_path = os.path.join(cwd,'credential_file.json')\n\n    #initialize bq client\n    client = bigquery.Client.from_service_account_json(secret_key_path)\n\n    query = '''\n        SELECT\u00a0profile_name,count(*)\u00a0as\u00a0number_of_reviews\u00a0FROM\u00a0`crafty-chiller-276910.cleaned_items.reviews`\n        where\u00a0profile_name\u00a0!= \"Amazon Customer\" AND profile_name\u00a0!= \"Kindle Customer\"\n        group\u00a0by\u00a0profile_name\n        order\u00a0by\u00a0number_of_reviews\u00a0desc\n        Limit 10\n            '''\n    # get df with query result\n    df = client.query(query).to_dataframe()\n\n    #Write dataframe into JSON file\n    df.to_json(\"dashboard_data/top_review_contributors.json\", orient='records')\n    print(\"Successfully written 'top_review_contributors.json' file\")\n\n@celery.task()\ndef query_review_numbers():\n    cwd = os.getcwd()\n    #change secret key path based on where you store it\n    secret_key_path = os.path.join(cwd,'credential_file.json')\n\n    #initialize bq client\n    client = bigquery.Client.from_service_account_json(secret_key_path)\n    #open product mapping file\n    file_path = os.path.join(cwd,'dashboard_data/product_mapping.json')\n\n    query = '''\n        SELECT\u00a0ASIN,count(*)\u00a0as\u00a0number_of_reviews\u00a0FROM\u00a0`crafty-chiller-276910.cleaned_items.reviews`\n        group\u00a0by\u00a0ASIN\n        order\u00a0by\u00a0number_of_reviews\u00a0desc\n        LIMIT 10\n            '''\n\n    # get df with query result\n    df = client.query(query).to_dataframe()\n\n    '''\n    # replace ASIN with product names (for top few items)\n    with open(file_path, \"r\") as file:\n        mapping = json.load(file)\n        for value in df['ASIN']:\n            if value in mapping:\n                df['ASIN'] = df['ASIN'].replace([value],mapping[value])\n    '''\n\n    #Write dataframe into JSON file\n    df.to_json(\"dashboard_data/products_with_most_reviews.json\", orient='records')\n    print(\"Successfully written 'products_with_most_reviews.json' file\")\n\ndef retrieve_data_from_json(filepath):\n    f = open(filepath, 'r+')\n    data = json.load(f)\n    data = json.dumps(data) #stringify json value\n    return data\n\n@app.route('/dashboard')\n@login_required\ndef index():\n\n    webscrape_data = retrieve_data_from_json('dashboard_data/webscrape_counter.json')\n    review_contributors_data = retrieve_data_from_json('dashboard_data/top_review_contributors.json')\n    product_reviews_data = retrieve_data_from_json('dashboard_data/products_with_most_reviews.json')\n    outstanding_data = retrieve_data_from_json('amazonreviews/output/logs/outstanding_items.json')\n\n    #pass product_mapping json value without stringify\n    f = open('dashboard_data/product_mapping.json', 'r+')\n    product_mapping = json.load(f)\n\n    #remove product descriptiona and store product names\n    i = 0\n    new_product_mapping = {}\n    for key,value in product_mapping.items():\n        new_product_mapping[key] = value.split(\",\")[0]\n        if i == 4:\n            break\n        i+=1\n\n    return render_template(\"dashboard.html\",name=current_user.name,url ='/static/img/alice.jpg',\n                           webscrape_data=webscrape_data,\n                           review_contributors_data=review_contributors_data,\n                           product_reviews_data=product_reviews_data,\n                           outstanding_data=outstanding_data,\n                           product_mapping = new_product_mapping)\n\n@app.route('/dashboard_update',methods=['POST'])\ndef dashboard_update():\n    update_status = request.form['update_status']\n\n    #Run celery task when update btn is triggered\n    if request.method == \"POST\":\n        query_reviews.apply_async(queue='queue3')\n        query_reviews_contributors.apply_async(queue='queue4')\n        query_review_numbers.apply_async(queue='queue5')\n\n    webscrape_data = retrieve_data_from_json('dashboard_data/webscrape_counter.json')\n    review_contributors_data = retrieve_data_from_json('dashboard_data/top_review_contributors.json')\n    product_reviews_data =  retrieve_data_from_json('dashboard_data/products_with_most_reviews.json')\n    outstanding_data = retrieve_data_from_json('amazonreviews/output/logs/outstanding_items.json')\n\n    return render_template(\"dashboard.html\",name=current_user.name,url ='/static/img/alice.jpg',\n                           webscrape_data=webscrape_data,\n                           review_contributors_data=review_contributors_data,\n                           product_reviews_data=product_reviews_data,\n                           outstanding_data=outstanding_data)\n\n@app.route('/webscrape')\n@login_required\ndef webscrape():\n    from model import Product\n    products = Product.query.all()\n\n    return render_template(\"webscrape.html\",name=current_user.name,products=products)\n\n#Clear content of counter file at the beginning of running webscrape tool\ndef clear_file(filepath,filename):\n    open(filepath+filename,'w').close()\n\n# Update counter of web scrape tool each time it is activated (queue 1 with get_review_profile)\ndef update_counter_file():\n    # get current date\n    current_date = datetime.today().strftime('%Y-%m-%d')\n\n    # load json counter file to read/write to\n    with open('dashboard_data/webscrape_counter.json', 'r+') as infile:\n        # load current file unless it is empty which causes the jsondecode error\n        try:\n            current_counts = json.load(infile)\n            print(\"JSON WEBSCRAPE COUNTER FILE LOADED SUCCESSFULLY\", current_counts)\n            if current_date in current_counts:\n                current_counts[f'{current_date}'] += 1\n            else:\n                current_counts[f'{current_date}'] = 1\n        except ValueError: #catches json decode error\n            # initialize the first time when the json counter file is empty\n            print(\"JSON WEBSCRAPE COUNTER FILE NOT LOADED DUE TO VALUE ERROR\")\n            current_counts = {}\n            current_counts[f'{current_date}'] = 1\n        with open('dashboard_data/webscrape_counter.json', 'w') as outfile:\n            # write updated count to counter file\n            json.dump(current_counts, outfile)\n            infile.close()\n            outfile.close()\n\ndef update_product_scrapetime():\n    from datetime import datetime\n    from model import Product\n\n    #Retrieve crawled asin from review file in crawl_progress folder\n    asins = set()\n    with open('crawl_progress/review.txt','r') as f:\n        for url in f:\n            asin = url.split(\"/\")[-2]\n            asins.add(asin)\n\n    if 'product-reviews' in asins:\n        asins.remove('product-reviews')\n\n    #Iterate asins to update last_scraped of products\n    for asin in asins:\n        last_scraped = datetime.now()\n        product = Product.query.filter_by(asin= asin).first()\n        product.last_scraped = last_scraped\n        db.session.commit()\n        print(\"product:\",product,\"time:\",product.last_scraped )\n\n\n#update scrape task to be true when celery task has been completed\ndef update_status(task):\n    with open('./crawl_progress/status.txt','a') as f:\n        f.write(task + \",\")\n\n@celery.task()\ndef get_review_profile(config,com_review_output_path,com_review_con_path,com_profile_output_path, com_profile_con_path):\n    clear_file('./crawl_progress/','review.txt')\n    clear_file('./crawl_progress/','profile.txt')\n    update_counter_file()\n\n    m.get_reviews(config)\n    m.get_outstanding_reviews(config)\n    m.update_outstanding_reviews(config)\n    m.combine_reviews(com_review_output_path, com_review_con_path)\n\n    #Update review status when crawling has been completed\n    update_status('review')\n\n    #update datetime of products that have been scraped\n    update_product_scrapetime()\n\n    # Obtain profile urls from scraped reviews in raw\n    m.get_profile_urls(config)\n\n    # Scrape profiles\n    m.get_profiles(config)\n    m.get_outstanding_profiles(config)\n    m.update_outstanding_profiles(config)\n    m.combine_profiles(com_profile_output_path, com_profile_con_path)\n\n    #Update profile status when crawling has been completed\n    update_status('profile')\n\n@celery.task()\ndef get_product(config,com_product_output_path, com_product_con_path):\n    clear_file('./crawl_progress/','product.txt')\n    m.get_products(config)\n    m.get_outstanding_products(config)\n    m.update_outstanding_products(config)\n    m.combine_products(com_product_output_path, com_product_con_path)\n\n    #Update product status when crawling has been completed\n    update_status('product')\n\nreview_url_count = 0\nprofile_url_count = 0\nproduct_url_count = 0\ndata_not_uploaded = True\n###################################################\n@app.route('/scrapeproduct',methods=['POST'])\n@login_required\ndef scrape_product():\n\n    from model import Setting\n    from model import Directory\n\n    #Retrieve last record in setting model\n    set_obj = db.session.query(Setting).order_by(Setting.id.desc()).first()\n    dir_obj = db.session.query(Directory).order_by(Directory.id.desc()).first()\n\n    input_path = dir_obj.input_filepath\n    output_path = dir_obj.output_filepath\n    con_path = dir_obj.consolidated_filepath\n    log_path = dir_obj.log_filepath\n    no_of_pg_crawl = set_obj.no_of_pg_crawl\n    no_of_retry = set_obj.no_of_retry\n\n\n    config = {\n        'input_path': input_path,\n        'output_path': output_path,\n        'no_of_pg_crawl': no_of_pg_crawl,\n        'no_of_retry': no_of_retry,\n        'con_path': con_path,\n        'log_path': log_path\n    }\n\n    #Retrieve ASIN from selected products and stored it in a list\n    asin_list = []\n    if request.method == \"POST\":\n        data = request.form['myJSONArrs']\n        data = json.loads(data)\n        print(data,type(data))\n\n        for record in data:\n            asin = record[1]\n            asin_list.append(asin)\n\n    from model import Product\n\n    # create urls to scrape reviews and products from a csv containing product ASINs\n    m.create_urls(asin_list)\n\n    #Find the basepath to this project folder\n    basepath = os.path.dirname(__file__)\n\n    #basepath for all combined funcs (review,product,profile)\n    combined_output_basepath = os.path.join(basepath,'amazonreviews',config['output_path'])\n    combined_con_basepath = os.path.join(basepath,'amazonreviews',config['con_path'])\n\n    #paths for combine_review parameters\n    com_review_output_path = os.path.join(combined_output_basepath, 'reviews')\n    com_review_con_path = os.path.join(combined_con_basepath,'reviews')\n\n    #paths for combine_profile parameters\n    com_profile_output_path = os.path.join(combined_output_basepath, 'profiles')\n    com_profile_con_path = os.path.join(combined_con_basepath,'profiles')\n\n    #clear status in status file which checks for celery task completion\n    clear_file('./crawl_progress/','status.txt')\n\n    #invoke review and profile celery task (reviews ->profile)\n    get_review_profile.apply_async(queue='queue1',args=(config,com_review_output_path,com_review_con_path,com_profile_output_path, com_profile_con_path))\n\n    #paths for combine_products parameters\n    com_product_output_path = os.path.join(combined_output_basepath, 'products')\n    com_product_con_path = os.path.join(combined_con_basepath,'products')\n\n    #invoke product celery task\n    get_product.apply_async(queue='queue2',args=(config,com_product_output_path,com_product_con_path))\n\n    msg = \"Webscrape tool has been successfully activated. It might take a while before web crawling is completed. Please check back again later.\"\n\n    return render_template(\"webscrape_progress.html\",name=current_user.name,msg=msg,review_url_count = review_url_count,profile_url_count=profile_url_count,product_url_count=product_url_count)\n\ndef status_result():\n    f = open('./crawl_progress/status.txt','r')\n    result = f.read()\n    result = result[:-1].split(\",\")\n    return result\n\n@app.route('/webscrapestatus',methods=['GET','POST'])\n@login_required\ndef webscrapestatus():\n    #Product: no. of product scraped count by ASIN\n    #Profile: each url corresponds to one profile\n    #Review: no. of url scraped approx 10 reviews per page\n    def countScrapedProductReview(filepath):\n        scrapedList = []\n        a_file = open(filepath,'r')\n        for line in a_file:\n            scrapedList.append(line.strip())\n\n        #Get unique items\n        scrapedList = set(scrapedList)\n\n        scrapedCount = len(scrapedList)\n\n        return scrapedCount\n\n    def countScrapedProfile(filepath):\n            scrapedList = []\n            a_file = open(filepath,'r')\n            myList = []\n            for line in a_file:\n                url_asin = line.strip().split(\"/\")[-1].split(\"?\")[0]\n                if \"ref=cm_cr_dp_d_show_all_btm\" not in url_asin:\n                    myList.append(url_asin)\n\n            #Get unique items\n            scrapedList = set(scrapedList)\n\n            scrapedCount = len(scrapedList)\n\n            return scrapedCount\n\n    global review_url_count\n    global profile_url_count\n    global product_url_count\n    global data_not_uploaded\n\n    review_url_count = countScrapedProductReview('./crawl_progress/review.txt') * 10\n    profile_url_count = countScrapedProfile('./crawl_progress/profile.txt')\n    product_url_count = countScrapedProductReview('./crawl_progress/product.txt')\n\n    msg = \"Webscrape tool has been successfully activated. It might take a while before web crawling is completed. Please check back again later.\"\n    complete_msg = \"\"\n    config = {\n    \"con_path\": \"output/consolidated\",\n    \"output_path\": \"output/raw\",\n    \"log_path\": \"output/logs/\"\n    }\n\n    result = status_result()\n    print(\"result:\",result)\n\n    #if 'product' and 'review' and 'profile' in result :\n    if 'product' and 'review' in result:\n        complete_msg = \"Web scraping has been completed.\"\n            \n        if data_not_uploaded:\n            #invoke upload consolidated csvs and recreate output folder task after queue 1 and queue2\n            cwd = os.getcwd()\n            #change secret key path based on where you store it\n            secret_key_path = os.path.join(cwd,'credential_file.json')\n            m.upload_consolidated_csvs(secret_key_path, 'crafty-chiller-276910', 'scraped_items_test', config)\n            m.clear_output_folders(config)\n            data_not_uploaded = False\n\n    print(\"check status:\",result)\n\n    return render_template(\"webscrape_progress.html\",name=current_user.name,\n                           review_url_count = review_url_count,\n                           profile_url_count=profile_url_count,\n                           product_url_count=product_url_count,\n                           msg=msg,complete_msg=complete_msg)\n\n\n@app.route('/directory')\n@login_required\ndef directory():\n    #Retrieve last setting record\n    from model import Directory\n    obj = db.session.query(Directory).order_by(Directory.id.desc()).first()\n\n    input_path = obj.input_filepath\n    output_path = obj.output_filepath\n    tracker_path = obj.tracker_filepath\n    con_path = obj.consolidated_filepath\n    log_path = obj.log_filepath\n\n    return render_template(\n        \"directory.html\",name=current_user.name,\n        input_path = input_path,\n        output_path = output_path,\n        con_path = con_path,\n        log_path = log_path,\n        tracker_path = tracker_path)\n\n\n@app.route('/setting')\n@login_required\ndef setting():\n    #Retrieve last setting record\n    from model import Setting\n    obj = db.session.query(Setting).order_by(Setting.id.desc()).first()\n\n    no_of_pg_crawl = obj.no_of_pg_crawl\n    no_of_retry = obj.no_of_retry\n\n    rotate_proxy = obj.rotate_proxy\n    fetch_proxies = obj.fetch_proxies\n    rotating_proxy_page_retry = obj.rotating_proxy_page_retry\n    no_of_concurrent_request = obj.no_of_concurrent_request\n    download_delay = obj.download_delay\n    download_timeout = obj.download_timeout\n\n    return render_template(\n            \"setting.html\",name=current_user.name,\n            no_of_pg_crawl = no_of_pg_crawl,\n            no_of_retry = no_of_retry,\n            rotate_proxy = rotate_proxy,\n            fetch_proxies = fetch_proxies,\n            rotating_proxy_page_retry = rotating_proxy_page_retry,\n            no_of_concurrent_request = no_of_concurrent_request,\n            download_delay = download_delay,\n            download_timeout = download_timeout\n            )\n@app.route('/filescrapeconfig',methods=['POST'])\n@login_required\ndef insert_directory_record():\n    from model import Directory\n\n    input_path = request.form.get('input_path')\n    output_path = request.form.get('output_path')\n    con_path = request.form.get('con_path')\n    log_path = request.form.get('log_path')\n    tracker_path = request.form.get('tracker_path')\n\n    #display result status of inserting new product into db\n    msg = \"\"\n\n    if (check_non_empty_space_in_val(input_path) and check_non_empty_space_in_val(output_path) and\n        check_non_empty_space_in_val(con_path) and check_non_empty_space_in_val(log_path) and\n        check_non_empty_space_in_val(tracker_path)):\n\n        new_directory = Directory(input_filepath = input_path,output_filepath = output_path,\n                                  consolidated_filepath = con_path,\n                                  log_filepath = log_path,tracker_filepath = tracker_path)\n\n        #add new record setting to database\n        db.session.add(new_directory)\n        db.session.commit()\n\n        msg = \"File directories have been successfully modified in the database.\"\n\n    else:\n        msg = \"Failed to change file directories into database.\"\n\n    return render_template('directory_status.html',msg=msg,name=current_user.name)\n\n@app.route('/webscrapeconfig',methods=['POST'])\n@login_required\ndef insert_setting_record():\n    from model import Setting\n\n    no_of_pg_crawl = int(request.form.get('no_of_pg_crawl'))\n    no_of_retry = int(request.form.get('no_of_retry'))\n    rotate_proxy = bool(request.form.get('rotate_proxy'))\n    fetch_proxies = int(request.form.get('fetch_proxies'))\n    rotating_proxy_page_retry = int(request.form.get('rotating_proxy_page_retry'))\n    no_of_concurrent_request = int(request.form.get('no_of_concurrent_request'))\n    download_delay = int(request.form.get('download_delay'))\n    download_timeout = int(request.form.get('download_timeout'))\n\n    #display result status of inserting new product into db\n    msg = \"\"\n\n    #Validate inputs are string and integers before inserting records\n    if (isinstance(no_of_pg_crawl,int) and isinstance(no_of_retry,int) and\n    isinstance(fetch_proxies,int) and isinstance(rotating_proxy_page_retry,int) and\n    isinstance(no_of_concurrent_request,int) and isinstance(download_delay,int) and\n    isinstance(download_timeout,int)):\n\n        new_setting = Setting(no_of_pg_crawl = no_of_pg_crawl,no_of_retry = no_of_retry,\n                              rotate_proxy = rotate_proxy, fetch_proxies = fetch_proxies,\n                              rotating_proxy_page_retry = rotating_proxy_page_retry,no_of_concurrent_request = no_of_concurrent_request,\n                              download_delay = download_delay, download_timeout = download_timeout)\n\n\n        #add new record setting to database\n        db.session.add(new_setting)\n        db.session.commit()\n\n        msg = \"All webscrap variables have been successfully added to the database.\"\n\n    else:\n        msg = \"Failed to insert variable configs into database.\"\n\n\n    return render_template('setting_status.html',msg=msg,name=current_user.name)\n\n@app.route('/report')\n@login_required\ndef report():\n    return render_template(\"report.html\",name=current_user.name)\n\n@app.route('/newproduct')\n@login_required\ndef create_product():\n    return render_template('new_products.html',name=current_user.name)\n\n#from model import db\n@app.route('/newproducts',methods=['POST'])\ndef new_product():\n    from model import Product\n\n    asin =request.form.get('asin')\n    name = request.form.get('name')\n    category = request.form.get('category')\n    price = float(request.form.get('price'))\n\n    print(\"asin:\",asin,\"name:\",name,\"cat:\",category,\"price:\",price)\n\n    #display result status of inserting new product into db\n    msg = \"\"\n\n    #check valid value type and string is non-empty/space\n    if isinstance(price,float) and isinstance(name,str) and check_non_empty_space_in_val(name) and check_non_empty_space_in_val(asin):\n        new_product = Product(asin=asin,name=name,price=price,category=category)\n\n        #add new product to database\n        db.session.add(new_product)\n        db.session.commit()\n\n        insert_status = \"successful\"\n        msg = name + \" of price $\"+ str(price) + \" has been successfully added to the database.\"\n    else:\n        msg = \"Failed to insert \" + name + \" product.\"\n\n    return render_template('product_status.html',msg=msg,name=name)\n", "274": "\"\"\"\nDjango settings for webscrape_project project.\n\nGenerated by 'django-admin startproject' using Django 2.2.5.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.2/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/2.2/ref/settings/\n\"\"\"\n\nimport os\n\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, \"polls/static\")\n]\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/2.2/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = '=+#e$$3=@3liza^6r5&%(d0q@@+o(l^0+$vsn$1r0v^nv@%h(2'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'polls.apps.PollsConfig',  # The polls app\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'webscrape_project.urls'\n\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [os.path.join(BASE_DIR, 'templates')]\n        ,\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'webscrape_project.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/2.2/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/2.2/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/2.2/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'GMT'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/2.2/howto/static-files/\n\nSTATIC_URL = '/static/'\n\nimport django\ndjango.setup()\n", "275": "\"\"\"Application Flow Architecture\"\"\"\n# from auth import validate_ssn_number, validate_account_number, verify_otp\n# from check import iscustomer\n# from information_helper import extract_creditscore, extract_downpayment, \\\n# extract_loan_amount, extract_loan_duration, extract_location\n# from start import start\n# from final import customername, phone\n# from repeat import repeat_last_statement,repeat_question\n# from extra_functionalities import account_balance,account_balance_helper, \\\n# stockprice, stockprice_helper, news, connect_to_hmc\n# from Models import Bank_Customer_Data\n# from Chatbot.prediction import predict_response, predict_intent\nfrom Chatbot.newfeatures import stockprice, stockprice_helper, news\nfrom Chatbot.webscrape.getNews import get_news\n\n# from Mail_to_HMC import mail\n# from webscrape.getNews import get_news\n# from webscrape.getCity import get_city\n# from VideoCall.zoomlink import create_meeting\n\n\ndef get_prediction_first(text):\n    if stockprice(text):\n        answer = stockprice_helper(text)\n        return [answer]\n\n    if news(text):\n        answer = get_news()\n        return [answer]\n    \n    # getresponse = predict_response(text)\n    # return [getresponse]\n    return [text]\n\n", "276": "\"\"\"\nCreated 3/6/2022\n@author: Ralph Miller\n         Nils Brown\n\n         Driver program for DiningHall 411 Demo\n         \n\"\"\"\n\n# Proof of concept for the dining hall 411 text program\nfrom dbsetup import createMenuDB, addMenuItem, findMenuItem\nfrom menuScraper import webScrape\nfrom setup import startupScript\nimport os\n\n\n# On startup run webscraper and install any libraries needed\n\nif os.path.exists(\"DHmenus.db\"):\n    os.remove(\"DHmenus.db\")\n\nstartupScript()\ncollege_names = [\"College910\", \"CowellStevenson\", \"CrownMerrill\", \"PorterKresge\"]\nfor items in college_names:\n    webScrape(items)\n\nwhile True:\n    print()\n    print(\"               |-----------------------------|\")\n    print(\"               | Welcome to Dining Hall 411! |\")\n    print(\"               |-----------------------------|\")\n    print()\n    response = input(\"\"\"Respond with one or both of the following options or [Q] to quit:\n    -Location \n    -Food \\n\"\"\")\n\n    print() # add newline for nice-ness\n\n    if response.upper() == \"Q\":\n        print(\"Bye <3\")\n        break\n\n    #storing response in lowercase, splitting at options\n    response = response.lower()\n    splitone = response.split(\"-\")\n\n    #storing location, date, food in temp variables\n    location = None\n    food = None\n\n    #remove argument from input\n    for i in splitone:\n        if \"location\" in i:\n            temp = i.split(\" \", 1)\n            location = temp[-1]\n            #if there is a space in location, remove it\n            if \" \" in location:\n                location = location.replace(\" \", \"\")\n        if \"food\" in i:\n            temp = i.split(\" \", 1)\n            food = temp[-1]\n\n    #Query Database\n    query = findMenuItem(location, food)\n\n    #clean query and respond\n    #if a specific food is specified print a single message with where it is\n    #otherwise, loop through all the menu items for the requested date, and\n    #print the meal time (lunch, dinner, etc) followed by the food name.\n    if query == []:\n        print(\"Nothing found for your request\")\n    if food:\n        for i in query:\n            print(\"{} is at {} for {}\".format(i[1],i[0], i[2]))\n    else:\n        temp = query[0]\n        temp = temp[1]\n        print(\"The menu for the {} location is:\".format(temp))\n        for i in query:\n            print(\"{}: {}\".format(i[2], i[1]))\n\n\n", "277": "from json import loads\nfrom json.decoder import JSONDecodeError\nfrom time import sleep\nfrom typing import Any, Optional\n\nfrom bs4 import BeautifulSoup\nfrom requests import get\nfrom requests.exceptions import HTTPError, RequestException\n\nfrom scrape.log import logger\n\n\ndef webscrape_results(\n    target_url: str, run_beautiful_soup: bool = False, querystring: Optional[str] = None\n) -> Any:\n    \"\"\"webscrape_results takes a target_url, id and querystring to extract results for further parsing purposes.\n\n    Args:\n        - target_url (str): a website to be scraped\n        - id (int, optional): an ID that determines the output.\n            1 is for JSON scraping.\n            2 is for HTML scraping with BeautifulSoup.\n        Defaults to 1.\n        - querystring (Optional[str], optional): A querystring to govern the requested results. Defaults to None.\n\n    Returns:\n        Any: Is either text from JSON, text from BeautifulSoup, or None if no results were found.\n    \"\"\"\n    sleep(2)\n    try:\n        r = get(target_url, params=querystring)\n        if r.ok:\n            response_text = r.text\n            try:\n                if not run_beautiful_soup:\n                    return loads(response_text)\n                elif run_beautiful_soup:\n                    return BeautifulSoup(response_text, \"html.parser\")\n            except (\n                JSONDecodeError,\n                RequestException,\n                HTTPError,\n                AttributeError,\n            ) as exception:\n                logger.warning(\n                    f\"[jobscraper] An error has occurred, moving to next item in sequence.\\\n                    Cause of error: {exception}\"\n                )\n        else:\n            logger.debug(r.status_code)\n    except (JSONDecodeError, RequestException, HTTPError, AttributeError) as exception:\n        logger.warning(\n            f\"[jobscraper] An error has occurred, moving to next item in sequence.\\\n            Cause of error: {exception}\"\n        )\n", "278": "\"\"\"\nWSGI config for WebScrape project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.0/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"WebScrape.settings\")\n\napplication = get_wsgi_application()\n", "279": "from pynput.keyboard import Key\n\n\nclass Settings:\n    desired_translations = 1\n    event_key = Key.alt_l      # key to copy highlight text\n    exit_key = Key.esc\n\n    def format_translations_yabla(tag_list): # used by webscraper to format, if webscrape url changed,\n        formatted_tag_list = []              # formatting will need editing\n        for tag in tag_list:\n            tag_string = tag.replace('\\n', ', ')\n            formatted_tag_list.append(tag_string)\n        return ', '.join(formatted_tag_list)\n\n    # webscrape details\n    url = 'https://chinese.yabla.com/chinese-english-pinyin-dictionary.php?define='\n    tag_type = 'div'\n    tag_attr = 'class'\n    tag_name = 'meaning'\n    format = format_translations_yabla\n\n\n", "280": "import requests\nfrom bs4 import BeautifulSoup\nimport time\n\n\nindex = 0\npage=1\ndurl = \"https://www.foxla.com/\"\nurl = 'https://www.foxla.com/tag/crime-publicsafety'\n\n\nimport json\ndef WebScrape():\n    #Index is used to indicate which story of the page is being used\n    #Ex. Story 3 of page 1 would be index 4 page 1\n    index = 0\n    page = 1\n    durl = \"https://www.foxla.com/\"\n    url = 'https://www.foxla.com/tag/crime-publicsafety'\n    f = open('history.json')\n    data = json.load(f)\n    while True:\n        if (index > 19):\n            print(\"stop\")\n            time.sleep(2)\n            #If the page is done scraping go to the second page.\n            page = page + 1\n            url = 'https://www.foxla.com/tag/crime-publicsafety'\n            url = url + \"?page=\" + str(page)\n            index = 0\n\n        else:\n            try:\n                response = requests.get(url)\n                soup = BeautifulSoup(response.text, 'html.parser')\n                #Finding title of webpage\n                print((soup.findAll('h3', attrs={\"class\": \"title\"})[index].string))\n                x = (soup.findAll('h3', attrs={\"class\": \"title\"})[index])\n                for a in x.find_all('a', href=True):\n                    print(\"Found the URL:\", a['href'])\n                    _durl = a['href']\n                url2 = durl + _durl\n                #Gets Url\n                response = requests.get(url2)\n                #Once Url is obtained goes to story url\n                soup = BeautifulSoup(response.text, 'html.parser')\n                name = soup.find('span', {\"class\": \"dateline\"}).text[:-3]\n                #Name var is the name of the city in California that the crime was comitted in\n                print((name))\n                index = index + 1\n                #if name was found in data add 1 else set it as 1\n                if name in data:\n                    d = ({name: data[name] + 1})\n                else:\n                    d = {name: 1}\n                #dumps data in json file\n                data.update(d)\n                data2 = json.dumps(data)\n                print(data2)\n                jsonFile = open(\"history.json\", \"w+\")\n                jsonFile.write(json.dumps(data))\n                jsonFile.close()\n            except:\n                #If the webpage cannot find a crime go to the next story\n                index = index + 1\nWebScrape()\n", "281": "import csv\nimport dateutil.parser as date\nimport jinja2\nimport numpy as np\nimport os\nimport pathlib\nimport pandas as pd\nimport sqlite3 as sql\nfrom fuzzywuzzy import fuzz\nfrom pyne import nucname as nn\n\n\ndef get_cursor(file_name):\n    \"\"\" Connects and returns a cursor to an sqlite output file\n\n    Parameters\n    ----------\n    file_name: str\n        name of the sqlite file\n\n    Returns\n    -------\n    sqlite cursor3\n    \"\"\"\n    con = sql.connect(file_name)\n    con.row_factory = sql.Row\n    return con.cursor()\n\n\ndef import_pris(pris_link):\n    \"\"\" Opens pris_csv using Pandas. Adds Latitude and Longitude\n    columns\n\n    Parameters\n    ----------\n    pris_link: str\n        path to reactors_pris_2016.original.csv file\n\n    Returns\n    -------\n    pris: pd.Dataframe\n        pris database\n    \"\"\"\n    pris = pd.read_csv(pris_link,\n                       delimiter=',',\n                       encoding='iso-8859-1',\n                       skiprows=20,\n                       )\n\n    pris = pris.rename(columns={pris.columns[2]: 'Country'})\n    pris = pris[['Country', 'Unit', 'Current Status', 'Type',\n                 'Model', 'Operator', 'Reactor Supplier', 'Const. Date',\n                 'Grid Date', 'Shutdown Date', 'RUP [MWe]']]\n    pris.insert(11, 'Latitude', np.nan)\n    pris.insert(12, 'Longitude', np.nan)\n    pris = pris[pris.Unit.notnull()]\n    pris = pris[pris.Unit != 'Unit']\n    pris = pris.replace(np.nan, '')\n    return pris\n\n\ndef import_webscrape_data(scrape_link):\n    \"\"\" Returns sqlite content of webscrape by performing an\n    sqlite query\n\n    Parameters\n    ----------\n    scrape_link: str\n        path to webscrape.sqlite file\n\n    Returns\n    -------\n    coords: sqlite cursor\n        sqlite cursor containing webscrape data\n    \"\"\"\n    cur = get_cursor(scrape_link)\n    coords = cur.execute(\"SELECT name, long, lat FROM reactors_coordinates\")\n    return coords\n\n\ndef get_edge_cases():\n    \"\"\" Returns a dictionary of edge cases that fuzzywuzzy is\n    unable to catch. This could be because PRIS database stores\n    reactor names and Webscrape database fetches power plant names,\n    or because PRIS reactor names are abbreviated.\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n    others: dict\n        dictionary of edge cases with \"key=pris_reactor_name, and\n        value=webscrape_plant_name\"\n    \"\"\"\n    others = {'OHI-': '\u014ci',\n              'ASCO-': 'Asc\u00f3',\n              'ROVNO-': 'Rivne',\n              'SHIN-KORI-': 'Kori',\n              'ANO-': 'Arkansas One',\n              'HANBIT-': 'Yeonggwang',\n              'FERMI-': 'Enrico Fermi',\n              'BALTIC-': 'Kaliningrad',\n              'COOK-': 'Donald C. Cook',\n              'HATCH-': 'Edwin I. Hatch',\n              'HARRIS-': 'Shearon Harris',\n              'SHIN-WOLSONG-': 'Wolseong',\n              'ST. ALBAN-': 'Saint-Alban',\n              'LASALLE-': 'LaSalle County',\n              'ZAPOROZHYE-': 'Zaporizhzhya',\n              'ROBINSON-': 'H. B. Robinson',\n              'SUMMER-': 'Virgil C. Summer',\n              'FARLEY-': 'Joseph M. Farley',\n              'ST. LAURENT ': 'Saint-Laurent',\n              'HADDAM NECK': 'Connecticut1 Yankee',\n              'FITZPATRICK': 'James A. FitzPatrick',\n              'HIGASHI DORI-1 (TOHOKU)': 'Higashid\u014dri',\n              }\n    return others\n\n\ndef sanitize_webscrape_name(name):\n    \"\"\" Sanitizes webscrape powerplant names by removing unwanted\n    strings (listed in blacklist), applying lower case, and deleting\n    trailing whitespace.\n\n    Parameters\n    ----------\n    name: str\n        webscrape plant name\n\n    Returns\n    -------\n    name: str\n        sanitized name for use with fuzzywuzzy\n    \"\"\"\n    blacklist = ['nuclear', 'power',\n                 'plant', 'generating',\n                 'station', 'reactor', 'atomic',\n                 'energy', 'center', 'electric']\n    name = name.lower()\n    for blacklisted in blacklist:\n        name = name.replace(blacklisted, '')\n    name = name.strip()\n    name = ' '.join(name.split())\n    return name\n\n\ndef sanitize_pris_name(name):\n    pris_name = name.lower()\n    if pris_name.find('-') != -1 and is_int(pris_name[-1]):\n        if pris_name[pris_name.find('-') + 1:].find('-') != -1:\n            idx = pris_name.find('-')\n            idx += pris_name[pris_name.find('-') + 1:].find('-')\n            pris_name = pris_name[:idx]\n        else:\n            pris_name = pris_name[:pris_name.find('-')]\n    return pris_name\n\n\ndef is_int(str):\n    \"\"\" Checks if input string is a number rather than a letter\n\n    Parameters\n    ----------\n    str: str\n        string to test\n\n    Returns\n    -------\n    answer: bool\n        returns True if string is a number; False if string is not\n    \"\"\"\n    answer = False\n    try:\n        int(str)\n    except ValueError:\n        return answer\n    answer = True\n    return answer\n\n\ndef merge_coordinates(pris_link, scrape_link, data_year):\n    \"\"\" Obtains coordinates from webscrape.sqlite and\n    writes them to matching reactors in PRIS reactor file.\n\n    Parameters\n    ----------\n    pris_link: str\n        path and name of pris reactor text file\n    scrape: str\n        path and name of webscrape sqlite file\n    data_year: int\n        year the data is pulled from\n\n    Returns\n    -------\n    null\n        Writes pris text file with coordinates\n    \"\"\"\n    others = get_edge_cases()\n    pris = import_pris(pris_link)\n    coords = import_webscrape_data(scrape_link)\n    for web in coords:\n        for i, prs in pris.iterrows():\n            webscrape_name = sanitize_webscrape_name(web['name'])\n            pris_name = sanitize_pris_name(prs[1])\n            if fuzz.ratio(webscrape_name, pris_name) > 78:\n                prs['Latitude'] = web['lat']\n                prs['Longitude'] = web['long']\n            else:\n                for other in others.keys():\n                    edge_case_key = other.lower()\n                    edge_case_value = others[other].lower()\n                    if fuzz.ratio(pris_name, edge_case_key) > 80:\n                        if fuzz.ratio(webscrape_name, edge_case_value) > 75:\n                            prs['Latitude'] = web['lat']\n                            prs['Longitude'] = web['long']\n    pris.to_csv(\n        '../database/reactors_pris_' +\n        str(data_year) +\n        '.csv',\n        index=False,\n        sep=',')\n\n\ndef save_output(pris, data_year):\n    \"\"\" Saves updated PRIS database as 'reactors_pris_2016.csv'\n\n    Parameters\n    ----------\n    pris: pd.DataFrame\n        updated PRIS database with latitude and longitude info\n    data_year: int\n        year the data is pulled from\n\n    Returns\n    -------\n\n    \"\"\"\n    pris.to_csv('../database/reactors_pris_' + str(data_year) + '.csv',\n                index=False,\n                sep=',',\n                )\n\n\ndef import_csv(in_csv, delimit=','):\n    \"\"\" Imports contents of a csv text file to a list of\n    lists.\n\n    Parameters\n    ---------\n    in_csv: str\n        path and name of input csv file\n    delimit: str\n        delimiter of the csv file\n\n    Returns\n    -------\n    data_list: list\n        list of lists containing the csv data\n    \"\"\"\n    with open(in_csv, encoding='utf-8') as source:\n        sourcereader = csv.reader(source, delimiter=delimit)\n        data_list = []\n        for row in sourcereader:\n            data_list.append(row)\n    return data_list\n\n\ndef load_template(in_template):\n    \"\"\" Returns a jinja2 template from file.\n\n    Parameters\n    ---------\n    in_template: str\n        path and name of jinja2 template\n\n    Returns\n    -------\n    output_template: jinja template object\n    \"\"\"\n    with open(in_template, 'r') as default:\n        output_template = jinja2.Template(default.read())\n    return output_template\n\n\ndef get_composition_fresh(in_list, burnup):\n    \"\"\" Returns a dictionary of isotope and composition (in mass fraction)\n    using vision_recipes for fresh UOX fuel.\n\n    Parameters\n    ---------\n    in_list: list\n        list containing vision_recipes\n    burnup: int\n        burnup\n\n    Returns\n    -------\n    data_dict: dict\n        dictionary with key=[isotope],\n        and value=[composition]\n    \"\"\"\n    data_dict = {}\n    for i in range(len(in_list)):\n        if i > 1:\n            if burnup == 33:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][1])})\n            elif burnup == 51:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][3])})\n            else:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][5])})\n    return data_dict\n\n\ndef get_composition_spent(in_list, burnup):\n    \"\"\" Returns a dictionary of isotope and composition (in mass fraction)\n    using vision_recipes for spent nuclear fuel\n\n    Parameters\n    ---------\n    in_list: list\n        list containing vision_recipes data\n    burnup: int\n        burnup\n\n    Returns\n    -------\n    data_dict: dict\n        dictionary with key=[isotope],\n        and value=[composition]\n    \"\"\"\n    data_dict = {}\n    for i in range(len(in_list)):\n        if i > 1:\n            if burnup == 33:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][2])})\n            elif burnup == 51:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][4])})\n            else:\n                data_dict.update({nn.id(in_list[i][0]):\n                                  float(in_list[i][6])})\n    return data_dict\n\n\ndef write_recipes(fresh_dict, spent_dict, in_template,\n                  burnup, out_path):\n    \"\"\" Renders jinja template using fresh and spent fuel composition.\n\n    Parameters\n    ---------\n    fresh_dict: dict\n        dictionary with key=[isotope], and\n        value=[composition] for fresh UOX\n    spent_dict: dict\n        dictionary with key=[isotope], and\n        value=[composition] for spent fuel\n    in_template: jinja template object\n        jinja template object to be rendered\n    burnup: int\n        amount of burnup\n    out_path: str\n        output path for recipe files\n\n    Returns\n    -------\n    null\n        generates recipe files for cyclus.\n    \"\"\"\n    pathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n    rendered = in_template.render(fresh=fresh_dict,\n                                  spent=spent_dict)\n    with open(out_path + '/uox_' + str(burnup) + '.xml', 'w') as output:\n        output.write(rendered)\n\n\ndef produce_recipes(in_csv, recipe_template, burnup, out_path):\n    \"\"\" Generates commodity composition xml input for cyclus.\n\n    Parameters\n    ---------\n    in_csv: str\n        path and name of recipe file\n    recipe_template: str\n        path and name of recipe template\n    burnup: int\n        amount of burnup\n    out_path: str\n        output path for recipe files\n\n    Returns\n    -------\n    null\n        Generates commodity composition xml input for cyclus.\n    \"\"\"\n    recipe = import_csv(in_csv, ',')\n    write_recipes(get_composition_fresh(recipe, burnup),\n                  get_composition_spent(recipe, burnup),\n                  load_template(recipe_template), burnup, out_file)\n\n\ndef confirm_deployment(row, start_year):\n    \"\"\" Confirms if reactor is to be deployed for CYCLUS by\n    checking if the capacity > 400 and if the commercial date\n    is a proper date format.\n\n    Parameters\n    ----------\n    date_str: str\n        the commercial date string from PRIS data file\n    capacity: str\n        capacity in MWe from PRIS data file\n    start_year: int\n        start year of the simulation\n    Returns\n    -------\n    is_deployed: bool\n            determines whether the reactor will be deployed\n            in CYCLUS\n    \"\"\"\n    is_deployed = False\n    capacity = row['RUP [MWe]']\n    start_date = str(row['Grid Date'])\n    end_date = str(row['Shutdown Date'])\n    if len(start_date) > 4 and float(capacity) > 400:\n        if end_date == 'nan':\n            try:\n                date.parse(start_date)\n                is_deployed = True\n            except BaseException:\n                pass\n        elif date.parse(end_date).year > start_year:\n            try:\n                date.parse(start_date)\n                is_deployed = True\n            except BaseException:\n                pass\n    return is_deployed\n\n\ndef select_region(in_dataframe, region, start_year):\n    \"\"\" Returns a list of reactors that will be deployed for\n    CYCLUS by checking the capacity and commercial date\n\n    Parameters\n    ----------\n    in_dataframe: DataFrame\n        imported csv file in DataFrame format\n    region: str\n        name of the region\n    start_year: int\n        start year of simulation\n\n    Returns\n    -------\n    reactor_df: DataFrame\n            DataFrame of reactors from PRIS\n    \"\"\"\n    ASIA = {'IRAN, ISLAMIC REPUBLIC OF', 'JAPAN',\n            'KAZAKHSTAN',\n            'BANGLADESH', 'CHINA', 'INDIA',\n            'UNITED ARAB EMIRATES', 'VIETNAM',\n            'PAKISTAN', 'PHILIPPINES', 'KOREA, REPUBLIC OF',\n            'KAZAKHSTAN', 'ARMENIA', 'TAIWAN, CHINA'\n            }\n    UNITED_STATES = {'UNITED STATES OF AMERICA'}\n    SOUTH_AMERICA = {'ARGENTINA', 'BRAZIL'}\n    NORTH_AMERICA = {'CANADA', 'MEXICO', 'UNITED STATES OF AMERICA'}\n    EUROPE = {'UKRAINE', 'UNITED KINGDOM',\n              'POLAND', 'ROMANIA', 'RUSSIA',\n              'BELARUS', 'BELGIUM', 'BULGARIA',\n              'GERMANY', 'ITALY', 'NETHERLANDS',\n              'SWEDEN', 'SWITZERLAND', 'TURKEY',\n              'SLOVENIA', 'SOVIET UNION', 'SPAIN',\n              'CZECH REPUBLIC', 'FINLAND', 'FRANCE',\n              'SLOVAKIA', 'HUNGARY', 'LITHUANIA'\n              }\n    AFRICA = {'EGYPT', 'MOROCCO', 'SOUTH AFRICA', 'TUNISIA'}\n    ALL = (SOUTH_AMERICA | NORTH_AMERICA |\n           EUROPE | ASIA | AFRICA | UNITED_STATES)\n    regions = {'ASIA': ASIA,\n               'AFRICA': AFRICA,\n               'EUROPE': EUROPE,\n               'SOUTH_AMERICA': SOUTH_AMERICA,\n               'NORTH_AMERICA': NORTH_AMERICA,\n               'UNITED_STATES': UNITED_STATES,\n               'ALL': ALL}\n    if region.upper() not in regions.keys():\n        raise ValueError(region + 'is not a valid region')\n    reactor_df = pd.DataFrame(columns=in_dataframe.columns)\n    for index, row in in_dataframe.iterrows():\n        country = row['Country']\n        if country.upper() in regions[region.upper()]:\n            if confirm_deployment(row, start_year):\n                reactor_df = reactor_df.append(\n                    in_dataframe.loc[index], ignore_index=True)\n    reactor_df = reactor_df.replace(np.nan, '')\n    reactor_df = reactor_df.astype(str)\n\n    return reactor_df\n\n\ndef get_lifetime(in_row, start_year):\n    \"\"\" Calculates the lifetime of a reactor using first\n    commercial date and shutdown date. Defaults to 720 months\n    if shutdown date is not available.\n\n    Parameters\n    ----------\n    in_row: list\n        single row from PRIS data that contains reactor\n        information\n    start_year: int\n        start year for the simulation\n\n    Returns\n    -------\n    lifetime: int\n        lifetime of reactor\n    \"\"\"\n    comm_date = in_row['Grid Date']\n    if date.parse(comm_date).year < start_year:\n        comm_date = str(start_year) + '-02-01'\n    shutdown_date = in_row['Shutdown Date']\n    if not shutdown_date.strip():\n        return 720\n    else:\n        n_days_month = 365.0 / 12\n        delta = (date.parse(shutdown_date) - date.parse(comm_date)).days\n        return int(delta / n_days_month)\n\n\ndef write_reactors(in_dataframe, out_path, reactor_template, start_year,\n                   cycle_time=18, refuel_time=1, capacity_factor=1):\n    \"\"\" Renders CYCAMORE::reactor specifications using jinja2.\n\n    Parameters\n    ----------\n    in_dataframe: DataFrame\n        DataFrame containing PRIS data\n    out_path: str\n        output path for reactor files\n    reactor_template: str\n        path to reactor template\n    start_year: int\n        start year of the simulation\n    cycle_time: int\n        cycle length of reactors in months\n    refuel_time: int\n        average refuel time in months\n    capacity_factor: float\n        capacity factor to apply to all reactors, as a decimal\n\n    Returns\n    -------\n    null\n        writes xml files with CYCAMORE::reactor config\n    \"\"\"\n    if out_path[-1] != '/':\n        out_path += '/'\n    pathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n    reactor_template = load_template(reactor_template)\n    for index, row in in_dataframe.iterrows():\n        capacity = float(row['RUP [MWe]'])\n        if capacity >= 400:\n            name = row[1].replace(' ', '_')\n            assem_per_batch = 0\n            assem_no = 0\n            assem_size = 0\n            reactor_type = row['Type']\n            latitude = row['Latitude'] if row['Latitude'] != '' else 0\n            longitude = row['Longitude'] if row['Longitude'] != '' else 0\n            if reactor_type in ['BWR', 'ESBWR']:\n                assem_no = 732\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 138000 / assem_no\n            elif reactor_type in ['GCR', 'HWGCR']:  # Need batch number\n                assem_no = 324\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 114000 / assem_no\n            elif reactor_type == 'HTGR':  # Need batch number\n                assem_no = 3944\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 39000 / assem_no\n            elif reactor_type == 'PHWR':\n                assem_no = 390\n                assem_per_batch = int(assem_no / 45)\n                assem_size = 80000 / assem_no\n            elif reactor_type == 'VVER':  # Need batch number\n                assem_no = 312\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 41500 / assem_no\n            elif reactor_type == 'VVER-1200':  # Need batch number\n                assem_no = 163\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 80000 / assem_no\n            else:\n                assem_no = 241\n                assem_per_batch = int(assem_no / 3)\n                assem_size = 103000 / assem_no\n            config = reactor_template.render(\n                name=name,\n                lifetime=get_lifetime(\n                    row,\n                    start_year),\n                cycletime=cycle_time,\n                refueltime=refuel_time,\n                assem_size=assem_size,\n                n_assem_core=assem_no,\n                n_assem_batch=assem_per_batch,\n                power_cap=row['RUP [MWe]'] *\n                capacity_factor,\n                lon=longitude,\n                lat=latitude)\n            with open(out_path + name.replace(' ', '_') + '.xml',\n                      'w') as output:\n                output.write(config)\n\n\ndef obtain_reactors(in_csv, region, reactor_template, out_path, start_year):\n    \"\"\" Writes xml files for individual reactors in a given\n    region.\n\n    Parameters\n    ----------\n    in_csv: str\n        csv file name\n    region: str\n        region name\n    reactor_template: str\n        path to CYCAMORE::reactor config template file\n    out_path: str\n        output path for reactor files\n    start_year: int\n        start year of the simulation\n\n    Returns\n    -------\n    null\n        Writes xml files for individual reactors in region.\n    \"\"\"\n    in_data = pd.read_csv(in_csv, ',')\n    reactor_list = select_region(in_data, region, start_year)\n    write_reactors(reactor_list, out_path, reactor_template, start_year)\n\n\ndef write_deployment(in_dict, out_path, deployinst_template,\n                     inclusions_template):\n    \"\"\" Renders jinja template using dictionary of reactor name and buildtime.\n    Outputs an xml file that uses xinclude to include the reactor xml files\n    located in cyclus_input/reactors.\n\n    Parameters\n    ---------\n    in_dict: dictionary\n        dictionary with key=[reactor name], and value=[buildtime]\n    out_path: str\n        output path for files\n    deployinst_template: str\n        path to deployinst template\n    inclusions_template: str\n        path to inclusions template\n\n    Returns\n    -------\n    null\n        generates input files that have deployment and xml inclusions\n    \"\"\"\n    if out_path[-1] != '/':\n        out_path += '/'\n    pathlib.Path(out_path).mkdir(parents=True, exist_ok=True)\n    deployinst_template = load_template(deployinst_template)\n    inclusions_template = load_template(inclusions_template)\n    country_list = {value[0] for value in in_dict.values()}\n    for nation in country_list:\n        temp_dict = {}\n        for reactor in in_dict.keys():\n            if in_dict[reactor][0].upper() == nation.upper():\n                temp_dict.update({reactor: in_dict[reactor][1]})\n        pathlib.Path(out_path + nation.replace(' ', '_') +\n                     '/').mkdir(parents=True, exist_ok=True)\n        deployinst = deployinst_template.render(reactors=temp_dict)\n        with open(out_path + nation.replace(' ', '_') +\n                  '/deployinst.xml', 'w') as output1:\n            output1.write(deployinst)\n    inclusions = inclusions_template.render(reactors=in_dict)\n    with open(out_path + 'inclusions.xml', 'w') as output2:\n        output2.write(inclusions)\n\n\ndef get_buildtime(in_list, start_year, path_list):\n    \"\"\" Calculates the buildtime required for reactor\n    deployment in months.\n\n    Parameters\n    ----------\n    in_list: list\n        list of reactors\n    start_year: int\n        starting year of simulation\n    path_list: list\n        list of paths to reactor files\n\n    Returns\n    -------\n    buildtime_dict: dict\n        dictionary with key=[name of reactor], and\n        value=[set of country and buildtime]\n    \"\"\"\n    buildtime_dict = {}\n    for index, row in in_list.iterrows():\n        comm_date = date.parse(row['Grid Date'])\n        start_date = [comm_date.year, comm_date.month, comm_date.day]\n        delta = ((start_date[0] - int(start_year)) * 12 +\n                 (start_date[1]) +\n                 round(start_date[2] / (365.0 / 12)))\n        if delta < 0:\n            delta = 1\n        for index, reactor in enumerate(path_list):\n            name = row['Unit'].replace(' ', '_')\n            country = row['Country']\n            file_name = (reactor.replace(\n                os.path.dirname(path_list[index]), '')).replace('/', '')\n            if (name + '.xml' == file_name):\n                buildtime_dict.update({name: (country, delta)})\n    return buildtime_dict\n\n\ndef deploy_reactors(in_csv, region, start_year, deployinst_template,\n                    inclusions_template, reactors_path, deployment_path):\n    \"\"\" Generates xml files that specify the reactors that will be included\n    in a CYCLUS simulation.\n\n    Parameters\n    ---------\n    in_csv: str\n        path to pris reactor database\n    region: str\n        region name\n    start_year: int\n        starting year of simulation\n    deployinst_template: str\n        path to deployinst template\n    inclusions_template: str\n        path to inclusions template\n    reactors_path: str\n        path containing reactor files\n    deployment_path: str\n        output path for deployinst xml\n\n    Returns\n    -------\n    buildtime_dict: dict\n        dictionary with key=[name of reactor], and\n        value=[set of country and buildtime]\n    \"\"\"\n    lists = []\n    if reactors_path[-1] != '/':\n        reactors_path += '/'\n    for files in os.listdir(reactors_path):\n        lists.append(reactors_path + files)\n    in_data = pd.read_csv(in_csv)\n    reactor_list = select_region(in_data, region, start_year)\n    buildtime = get_buildtime(reactor_list, start_year, lists)\n    write_deployment(buildtime, deployment_path, deployinst_template,\n                     inclusions_template)\n    return buildtime\n\n\ndef render_cyclus(cyclus_template, region, in_dict,\n                  out_path, start_year, duration=780, burn_up=50):\n    \"\"\" Renders final CYCLUS input file with xml base, and institutions\n    for each country\n\n    Parameters\n    ----------\n    cyclus_template: str\n        path to CYCLUS input file template\n    region: str\n        region chosen for CYCLUS simulation\n    in_dict: dictionary\n        in_dict should be buildtime_dict from get_buildtime function\n    out_path: str\n        output path for CYCLUS input file\n    start_year: int\n        start year for the simulation\n    duration: int\n        duration for CYCLUS simulation to last in months\n    burn_up: int\n        burnup in GWd/MTU\n\n    Returns\n    -------\n    null\n        writes CYCLUS input file in out_path\n    \"\"\"\n    if out_path[-1] != '/':\n        out_path += '/'\n    cyclus_template = load_template(cyclus_template)\n    country_list = {value[0].replace(' ', '_') for value in in_dict.values()}\n    rendered = cyclus_template.render(start_year=start_year,\n                                      duration=duration,\n                                      burnup=burn_up,\n                                      countries=country_list,\n                                      base_dir=os.path.abspath(out_path) + '/')\n    with open(out_path + region + '.xml', 'w') as output:\n        output.write(rendered)\n", "282": "import pandas as pd\nfrom io import BytesIO\nimport os\nfrom airflow import DAG\nfrom google.cloud import storage\nfrom airflow.operators.python import PythonOperator, task\nfrom airflow.operators.bash import BashOperator\nfrom google.oauth2.credentials import Credentials\nimport datetime as dt\nimport json\nfrom google.cloud import secretmanager\nclient = secretmanager.SecretManagerServiceClient()\nimport requests\n\n\nproject_id=\"peak-text-334821\"\n# secret_id=\"\"\n\n# Build the resource name of the secret version.\n# secret_name = f\"projects/{project_id}/secrets/test-secret/versions/1\"\n# Access the secret version.\n# secret_key = client.access_secret_version(request={\"name\": secret_name})\n# payload = secret_key.payload.data.decode(\"UTF-8\")\n\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/airflow/gcs/data/peak-text-334821-eb7c45a6619a.json\"\nstorage_client = storage.Client(project=\"peak-text-334821\")\nbucket = storage_client.get_bucket('news_articles_scraped')\n\n\ndef CNN_Politics(url=\"https://search.api.cnn.io/content?q=politics&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n    data=[]\n    with requests.Session() as req:\n        for item in range(1, 1000, 100):\n            try:\n                r = req.get(url.format(item)).json()\n                for a in r['result']:\n                    if 'headline' in a.keys():\n                        title=a['headline']\n                    else:\n                         title='Nan'   \n                    if 'body' in a.keys():\n                        body=a['body']\n                    else:\n                         title='Nan'\n                    if 'lastPublishDate' in a.keys():\n                        datetime=a['lastPublishDate']\n                    else:\n                         datetime='Nan'\n                    if 'url' in a.keys():\n                        url=a['url']\n                    else:\n                         url='Nan'\n                    if 'byLine' in a.keys():\n                        author=a['byLine']\n                    else:\n                         author='Nan'   \n                    data.append([title, body, datetime, author, url])\n            except:\n                pass\n    df_politics = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n    df_politics[\"category\"] = 'politics'\n    bucket.blob('CNN/politics.csv').upload_from_string(df_politics.to_csv(), 'text/csv')\n\n\ndef CNN_World(url=\"https://search.api.cnn.io/content?q=world&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n    data=[]\n    with requests.Session() as req:\n        for item in range(1, 1000, 100):\n            try:\n                r = req.get(url.format(item)).json()\n                for a in r['result']:\n                    if 'headline' in a.keys():\n                        title=a['headline']\n                    else:\n                         title='Nan'   \n                    if 'body' in a.keys():\n                        body=a['body']\n                    else:\n                         title='Nan'\n                    if 'lastPublishDate' in a.keys():\n                        datetime=a['lastPublishDate']\n                    else:\n                         datetime='Nan'\n                    if 'url' in a.keys():\n                        url=a['url']\n                    else:\n                         url='Nan'\n                    if 'byLine' in a.keys():\n                        author=a['byLine']\n                    else:\n                         author='Nan'   \n                    data.append([title, body, datetime, author, url])\n            except:\n                pass\n    df_world = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n    df_world[\"category\"] = 'world'\n    bucket.blob('CNN/world.csv').upload_from_string(df_world.to_csv(), 'text/csv')\n\n\n\ndef CNN_US(url=\"https://search.api.cnn.io/content?q=US&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n    data=[]\n    with requests.Session() as req:\n        for item in range(1, 1000, 100):\n            try:\n                r = req.get(url.format(item)).json()\n                for a in r['result']:\n                    if 'headline' in a.keys():\n                        title=a['headline']\n                    else:\n                         title='Nan'   \n                    if 'body' in a.keys():\n                        body=a['body']\n                    else:\n                         title='Nan'\n                    if 'lastPublishDate' in a.keys():\n                        datetime=a['lastPublishDate']\n                    else:\n                         datetime='Nan'\n                    if 'url' in a.keys():\n                        url=a['url']\n                    else:\n                         url='Nan'\n                    if 'byLine' in a.keys():\n                        author=a['byLine']\n                    else:\n                         author='Nan'   \n                    data.append([title, body, datetime, author, url])\n            except:\n                pass\n    df_us = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n    df_us[\"category\"] = 'us'\n    bucket.blob('CNN/us.csv').upload_from_string(df_us.to_csv(), 'text/csv')\n\n\ndef CNN_Opinion(url=\"https://search.api.cnn.io/content?q=opinion&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n    data=[]\n    with requests.Session() as req:\n        for item in range(1, 1000, 100):\n            try:\n                r = req.get(url.format(item)).json()\n                for a in r['result']:\n                    if 'headline' in a.keys():\n                        title=a['headline']\n                    else:\n                         title='Nan'   \n                    if 'body' in a.keys():\n                        body=a['body']\n                    else:\n                         title='Nan'\n                    if 'lastPublishDate' in a.keys():\n                        datetime=a['lastPublishDate']\n                    else:\n                         datetime='Nan'\n                    if 'url' in a.keys():\n                        url=a['url']\n                    else:\n                         url='Nan'\n                    if 'byLine' in a.keys():\n                        author=a['byLine']\n                    else:\n                         author='Nan'   \n                    data.append([title, body, datetime, author, url])\n            except:\n                pass\n    df_opinion = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n    df_opinion[\"category\"] = 'opinion'\n    bucket.blob('CNN/opinion.csv').upload_from_string(df_opinion.to_csv(), 'text/csv')\n\ndef CNN_Economy(url=\"https://search.api.cnn.io/content?q=economy&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n    data=[]\n    with requests.Session() as req:\n        for item in range(1, 1000, 100):\n            try:\n                r = req.get(url.format(item)).json()\n                for a in r['result']:\n                    if 'headline' in a.keys():\n                        title=a['headline']\n                    else:\n                         title='Nan'   \n                    if 'body' in a.keys():\n                        body=a['body']\n                    else:\n                         title='Nan'\n                    if 'lastPublishDate' in a.keys():\n                        datetime=a['lastPublishDate']\n                    else:\n                         datetime='Nan'\n                    if 'url' in a.keys():\n                        url=a['url']\n                    else:\n                         url='Nan'\n                    if 'byLine' in a.keys():\n                        author=a['byLine']\n                    else:\n                         author='Nan'   \n                    data.append([title, body, datetime, author, url])\n            except:\n                pass\n    df_economy = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n    df_economy[\"category\"] = 'economy'\n    bucket.blob('CNN/economy.csv').upload_from_string(df_economy.to_csv(), 'text/csv')\n\n\n\ndef CNN_Tech(url=\"https://search.api.cnn.io/content?q=tech&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n    data=[]\n    with requests.Session() as req:\n        for item in range(1, 1000, 100):\n            try:\n                r = req.get(url.format(item)).json()\n                for a in r['result']:\n                    if 'headline' in a.keys():\n                        title=a['headline']\n                    else:\n                         title='Nan'   \n                    if 'body' in a.keys():\n                        body=a['body']\n                    else:\n                         title='Nan'\n                    if 'lastPublishDate' in a.keys():\n                        datetime=a['lastPublishDate']\n                    else:\n                         datetime='Nan'\n                    if 'url' in a.keys():\n                        url=a['url']\n                    else:\n                         url='Nan'\n                    if 'byLine' in a.keys():\n                        author=a['byLine']\n                    else:\n                         author='Nan'   \n                    data.append([title, body, datetime, author, url])\n            except:\n                pass\n    df_tech = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n    df_tech[\"category\"] = 'tech'\n    bucket.blob('CNN/tech.csv').upload_from_string(df_tech.to_csv(), 'text/csv')\n\ndef CNN_Health(url=\"https://search.api.cnn.io/content?q=health&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}\"):\n    data=[]\n    with requests.Session() as req:\n        for item in range(1, 1000, 100):\n            try:\n                r = req.get(url.format(item)).json()\n                for a in r['result']:\n                    if 'headline' in a.keys():\n                        title=a['headline']\n                    else:\n                         title='Nan'   \n                    if 'body' in a.keys():\n                        body=a['body']\n                    else:\n                         title='Nan'\n                    if 'lastPublishDate' in a.keys():\n                        datetime=a['lastPublishDate']\n                    else:\n                         datetime='Nan'\n                    if 'url' in a.keys():\n                        url=a['url']\n                    else:\n                         url='Nan'\n                    if 'byLine' in a.keys():\n                        author=a['byLine']\n                    else:\n                         author='Nan'   \n                    data.append([title, body, datetime, author, url])\n            except:\n                pass\n    df_health = pd.DataFrame(data=data, columns=[\"title\", \"body\", \"datetime\", \"author\", \"url\"])\n    df_health[\"category\"] = 'health'\n    bucket.blob('CNN/health.csv').upload_from_string(df_health.to_csv(), 'text/csv')\n\n\nwith DAG(\n    \"gcs_cnn\",\n    start_date=dt.datetime(2021, 12, 17),\n    schedule_interval='@daily') as dag:\n\n\n    t2 = PythonOperator(\n        task_id=\"webscrape_CNN_Politics\",\n        python_callable=CNN_Politics\n    )\n    t4 = PythonOperator(\n        task_id=\"webscrape_CNN_World\",\n        python_callable=CNN_World,\n    )\n    t6 = PythonOperator(\n        task_id=\"webscrape_CNN_US\",\n        python_callable=CNN_US\n    )\n    t8 = PythonOperator(\n        task_id='webscrape_CNN_Opinion',\n        python_callable=CNN_Opinion\n    )\n    t10 = PythonOperator(\n        task_id='webscrape_CNN_Economy',\n        python_callable=CNN_Economy\n    )\n    t12 = PythonOperator(\n        task_id='webscrape_CNN_Tech',\n        python_callable=CNN_Tech\n    )\n    t14 = PythonOperator(\n        task_id='webscrape_CNN_Health',\n        python_callable=CNN_Health\n    )\n\n[t2, t4, t6, t8, t10, t12, t14]\n\n", "283": "# This file was generated by the Tkinter Designer by Parth Jadhav\r\n# https://github.com/ParthJadhav/Tkinter-Designer\r\n\r\nfrom pathlib import Path\r\n\r\nfrom tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage\r\nimport tkinter as tk\r\nimport threading\r\nimport pyautogui\r\nimport time\r\nimport cv2\r\nfrom pytesseract import *\r\nfrom PIL import Image\r\nfrom selenium import webdriver\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\nfrom selenium.webdriver.chrome.service import Service\r\nser = Service(executable_path=ChromeDriverManager().install())\r\ndriver = webdriver.Chrome(service=ser)\r\nfrom selenium.webdriver.common.by import By\r\nimport mss\r\nimport mss.tools\r\n\r\npytesseract.tesseract_cmd = r'...AppData/Local/Programs/Tesseract-OCR/tesseract.exe' # Download pytesseract and link PATH here\r\n\r\nOUTPUT_PATH = Path(__file__).parent\r\nASSETS_PATH = OUTPUT_PATH / Path(\"./assets\")\r\n\r\ndef relative_to_assets(path: str) -> Path:\r\n    return ASSETS_PATH / Path(path)\r\n\r\nif __name__ == \"__main__\":\r\n    \r\n    window = tk.Tk()\r\n    window.geometry(\"431x550\")\r\n    window.configure(bg = \"#ECECEC\")\r\n    window.title(\"HUSKYEL1TE'S WZ STATS BOT\")\r\n    window.iconbitmap(\"icon.ico\")\r\n    \r\n    run = False\r\n    lock = threading.Lock()\r\n    monitor = threading.Condition(lock)\r\n    \r\n    def webscrape(username, original):\r\n        t = time.time()\r\n    \r\n        options = webdriver.ChromeOptions()\r\n        options.add_argument('--headless')\r\n        options.add_experimental_option('excludeSwitches', ['enable-logging'])\r\n        driver = webdriver.Chrome(options=options)\r\n        driver.get(f'https://cod.tracker.gg/warzone/profile/atvi/{username}/overview')\r\n        page_title = driver.find_elements(By.CLASS_NAME, 'lead')\r\n        \r\n        if not page_title or page_title[0] == \"WARZONE STATS NOT FOUND\":\r\n            print(\"WARZONE STATS NOT FOUND - Private profile\")\r\n            usernameBox.delete(0, tk.END)\r\n            usernameBox.insert(0, \"WARZONE STATS NOT FOUND - Private profile\")\r\n        \r\n        else:\r\n            usernameBox.delete(0, tk.END)\r\n            usernameBox.insert(0, original)\r\n            search = driver.find_elements(By.CLASS_NAME, 'value')\r\n            \r\n            if len(search) > 4:\r\n                print(\"Wins:\", search[0].text)\r\n                winsBox.delete(0, tk.END)\r\n                winsBox.insert(0, search[0].text)\r\n                \r\n                print(\"Win %:\", search[1].text)\r\n                winPercentageBox.delete(0, tk.END)\r\n                winPercentageBox.insert(0, search[1].text)\r\n                \r\n                print(\"Kills:\", search[2].text)\r\n                killsBox.delete(0, tk.END)\r\n                killsBox.insert(0, search[2].text)\r\n                \r\n                print(\"K/D:\", search[3].text)\r\n                KD_Box.delete(0, tk.END)\r\n                KD_Box.insert(0, search[3].text)\r\n    \r\n                print(\"Score/min:\", search[4].text)\r\n                scoreMinBox.delete(0, tk.END)\r\n                scoreMinBox.insert(0, search[4].text)\r\n                \r\n            else:\r\n                print(\"Incorrect name or private profile\")\r\n                \r\n                usernameBox.delete(0, tk.END)\r\n                usernameBox.insert(0, original)\r\n                \r\n                winsBox.delete(0, tk.END)\r\n                winsBox.insert(0, \"-----\")\r\n                \r\n                winPercentageBox.delete(0, tk.END)\r\n                winPercentageBox.insert(0, \"-----\")\r\n                \r\n                killsBox.delete(0, tk.END)\r\n                killsBox.insert(0, \"-----\")\r\n                \r\n                KD_Box.delete(0, tk.END)\r\n                KD_Box.insert(0, \"-----\")\r\n                \r\n                scoreMinBox.delete(0, tk.END)\r\n                scoreMinBox.insert(0, \"-----\")\r\n                \r\n        elapsed = time.time() - t\r\n        print(elapsed, \"Time to webscrape\")\r\n        webscrapeBox.delete(0, tk.END)\r\n        webscrapeBox.insert(0, str(round(elapsed, 2)) + \" seconds\")\r\n        \r\n        driver.close() \r\n        driver.quit()\r\n    \r\n    def runBot():\r\n        global run\r\n        global lock\r\n        global monitor\r\n        print(\"Waiting for lock\")\r\n        lock.acquire()\r\n        monitor.wait()\r\n        lock.release()\r\n        print(\"Acquired the lock!\")\r\n        \r\n        time.sleep(0.5)\r\n        \r\n        while run:\r\n            coords = pyautogui.locateOnScreen('1ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n            coords1 = pyautogui.locateOnScreen('2ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n            coords2 = pyautogui.locateOnScreen('3ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n            coords3 = pyautogui.locateOnScreen('6ex.png', confidence = 0.24, grayscale = True, region = (696, 938, 339, 36))\r\n    \r\n            if coords or coords1 or coords2 or coords3:\r\n                with mss.mss() as sct:\r\n                    # The screen part to capture\r\n                    region = {'top': 938, 'left': 696, 'width': 339, 'height': 35}\r\n                \r\n                    # Grab the data\r\n                    img = sct.grab(region)\r\n                \r\n                    # Save to the picture file\r\n                    mss.tools.to_png(img.rgb, img.size, output='screenshot.png')\r\n                \r\n                # Enlarge image for more accurate results\r\n                img = cv2.imread('screenshot.png')\r\n                img = cv2.resize(img, dsize=(526, 66), interpolation=cv2.INTER_CUBIC)\r\n                \r\n                # Save image and remove any extra spaces\r\n                cv2.imwrite('screenshot.png', img)\r\n                result = pytesseract.image_to_string(img) # Result variable is what the program thinks the username is after conversion to string\r\n                result = result.rstrip()\r\n                \r\n                # If the username is empty or doesn't have a hashtag, keep scanning/try again\r\n                if result == '' or '#' not in result:\r\n                    continue\r\n                    \r\n                # Special condition for when the username has a clantag, remove the clantag\r\n                if ']' in result:\r\n                    slicing = result.find(']')\r\n                    newUsername = result[slicing+1:]\r\n                    newUsername = newUsername.replace('#', '%')\r\n                    \r\n                    # cod tracker adds a '23' to the links after the percent sign, do the same here\r\n                    pcent = newUsername.index('%')\r\n                    newUsername = newUsername[:pcent+1] + '23' + newUsername[pcent+1:]\r\n                    \r\n                    webscrape(newUsername, result)\r\n                \r\n                # If username doesn't have clantag then it's ready for webscraping\r\n                elif ']' not in result:\r\n                    newUsername = result.replace('#', '%')\r\n                    \r\n                    # cod tracker adds a '23' to the links after the percent sign, do the same here\r\n                    pcent = newUsername.index('%')\r\n                    newUsername = newUsername[:pcent+1] + '23' + newUsername[pcent+1:]\r\n                    \r\n                    webscrape(newUsername, result)\r\n                    \r\n            else:\r\n                continue\r\n            \r\n    threading.Thread(target=runBot).start()\r\n        \r\n    def stopBot():\r\n        global run\r\n        run = False\r\n        \r\n    def startBot():\r\n        global run\r\n        global lock\r\n        global monitor\r\n        print(\"Starting the bot!\")\r\n        run = True\r\n        lock.acquire()\r\n        monitor.notify()\r\n        lock.release()\r\n\r\n    canvas = Canvas(\r\n        window,\r\n        bg = \"#ECECEC\",\r\n        height = 550,\r\n        width = 431,\r\n        bd = 0,\r\n        highlightthickness = 0,\r\n        relief = \"ridge\"\r\n    )\r\n    \r\n    canvas.place(x = 0, y = 0)\r\n    button_image_1 = PhotoImage(\r\n        file=relative_to_assets(\"button_1.png\"))\r\n    stopButton = Button(\r\n        image=button_image_1,\r\n        borderwidth=0,\r\n        highlightthickness=0,\r\n        command=stopBot,\r\n        relief=\"flat\"\r\n    )\r\n    stopButton.place(\r\n        x=171.0,\r\n        y=440.0,\r\n        width=90.0,\r\n        height=40.0\r\n    )\r\n    \r\n    button_image_2 = PhotoImage(\r\n        file=relative_to_assets(\"button_2.png\"))\r\n    runButton = Button(\r\n        image=button_image_2,\r\n        borderwidth=0,\r\n        highlightthickness=0,\r\n        command=startBot,\r\n        relief=\"flat\"\r\n    )\r\n    runButton.place(\r\n        x=62.0,\r\n        y=440.0,\r\n        width=90.0,\r\n        height=40.0\r\n    )\r\n    \r\n    button_image_3 = PhotoImage(\r\n        file=relative_to_assets(\"button_3.png\"))\r\n    historyButton = Button(\r\n        image=button_image_3,\r\n        borderwidth=0,\r\n        highlightthickness=0,\r\n        command=lambda: print(\"button_3 clicked\"),\r\n        relief=\"flat\"\r\n    )\r\n    historyButton.place(\r\n        x=280.0,\r\n        y=440.0,\r\n        width=90.0,\r\n        height=40.0\r\n    )\r\n    \r\n    canvas.create_text(\r\n        100.0,\r\n        508.0,\r\n        anchor=\"nw\",\r\n        text=\"Created by marcosmaciel.tech\",\r\n        fill=\"#000000\",\r\n        font=(\"Roboto\", 16 * -1)\r\n    )\r\n    \r\n    entry_image_1 = PhotoImage(\r\n        file=relative_to_assets(\"entry_1.png\"))\r\n    entry_bg_1 = canvas.create_image(\r\n        216.0,\r\n        132.5,\r\n        image=entry_image_1\r\n    )\r\n    usernameBox = Entry(\r\n        bd=0,\r\n        bg=\"#FFFFFF\",\r\n        highlightthickness=0\r\n    )\r\n    usernameBox.place(\r\n        x=57.0,\r\n        y=111.0,\r\n        width=318.0,\r\n        height=41.0\r\n    )\r\n    \r\n    entry_image_2 = PhotoImage(\r\n        file=relative_to_assets(\"entry_4.png\"))\r\n    entry_bg_2 = canvas.create_image(\r\n        121.0,\r\n        215.5,\r\n        image=entry_image_2\r\n    )\r\n    winsBox = Entry(\r\n        bd=0,\r\n        bg=\"#FFFFFF\",\r\n        highlightthickness=0\r\n    )\r\n    winsBox.place(\r\n        x=57.0,\r\n        y=194.0,\r\n        width=128.0,\r\n        height=41.0\r\n    )\r\n    \r\n    entry_image_3 = PhotoImage(\r\n        file=relative_to_assets(\"entry_4.png\"))\r\n    entry_bg_3 = canvas.create_image(\r\n        121.0,\r\n        298.0,\r\n        image=entry_image_3\r\n    )\r\n    killsBox = Entry(\r\n        bd=0,\r\n        bg=\"#FFFFFF\",\r\n        highlightthickness=0\r\n    )\r\n    killsBox.place(\r\n        x=57.0,\r\n        y=276.0,\r\n        width=128.0,\r\n        height=42.0\r\n    )\r\n    \r\n    entry_image_4 = PhotoImage(\r\n        file=relative_to_assets(\"entry_4.png\"))\r\n    entry_bg_4 = canvas.create_image(\r\n        312.0,\r\n        215.5,\r\n        image=entry_image_4\r\n    )\r\n    winPercentageBox = Entry(\r\n        bd=0,\r\n        bg=\"#FFFFFF\",\r\n        highlightthickness=0\r\n    )\r\n    winPercentageBox.place(\r\n        x=249.0,\r\n        y=194.0,\r\n        width=126.0,\r\n        height=41.0\r\n    )\r\n    \r\n    entry_image_5 = PhotoImage(\r\n        file=relative_to_assets(\"entry_5.png\"))\r\n    entry_bg_5 = canvas.create_image(\r\n        121.0,\r\n        383.0,\r\n        image=entry_image_5\r\n    )\r\n    scoreMinBox = Entry(\r\n        bd=0,\r\n        bg=\"#FFFFFF\",\r\n        highlightthickness=0\r\n    )\r\n    scoreMinBox.place(\r\n        x=57.0,\r\n        y=361.0,\r\n        width=128.0,\r\n        height=42.0\r\n    )\r\n    \r\n    entry_image_6 = PhotoImage(\r\n        file=relative_to_assets(\"entry_6.png\"))\r\n    entry_bg_6 = canvas.create_image(\r\n        312.0,\r\n        383.0,\r\n        image=entry_image_6\r\n    )\r\n    webscrapeBox = Entry(\r\n        bd=0,\r\n        bg=\"#FFFFFF\",\r\n        highlightthickness=0\r\n    )\r\n    webscrapeBox.place(\r\n        x=249.0,\r\n        y=361.0,\r\n        width=126.0,\r\n        height=42.0\r\n    )\r\n    \r\n    entry_image_7 = PhotoImage(\r\n        file=relative_to_assets(\"entry_7.png\"))\r\n    entry_bg_7 = canvas.create_image(\r\n        312.0,\r\n        298.0,\r\n        image=entry_image_7\r\n    )\r\n    KD_Box = Entry(\r\n        bd=0,\r\n        bg=\"#FFFFFF\",\r\n        highlightthickness=0\r\n    )\r\n    KD_Box.place(\r\n        x=249.0,\r\n        y=276.0,\r\n        width=126.0,\r\n        height=42.0\r\n    )\r\n    \r\n    canvas.create_text(\r\n        53.0,\r\n        94.0,\r\n        anchor=\"nw\",\r\n        text=\"Username\",\r\n        fill=\"#000000\",\r\n        font=(\"Arial BoldMT\", 13 * -1)\r\n    )\r\n    \r\n    canvas.create_text(\r\n        53.0,\r\n        258.0,\r\n        anchor=\"nw\",\r\n        text=\"Number of Kills\",\r\n        fill=\"#000000\",\r\n        font=(\"Arial BoldMT\", 13 * -1)\r\n    )\r\n    \r\n    canvas.create_text(\r\n        53.0,\r\n        176.0,\r\n        anchor=\"nw\",\r\n        text=\"Wins\",\r\n        fill=\"#000000\",\r\n        font=(\"Arial BoldMT\", 13 * -1)\r\n    )\r\n    \r\n    canvas.create_text(\r\n        246.0,\r\n        259.0,\r\n        anchor=\"nw\",\r\n        text=\"K/D\",\r\n        fill=\"#000000\",\r\n        font=(\"Arial BoldMT\", 13 * -1)\r\n    )\r\n    \r\n    canvas.create_text(\r\n        246.0,\r\n        342.0,\r\n        anchor=\"nw\",\r\n        text=\"Time to Webscrape\",\r\n        fill=\"#000000\",\r\n        font=(\"Arial BoldMT\", 13 * -1)\r\n    )\r\n    \r\n    canvas.create_text(\r\n        246.0,\r\n        176.0,\r\n        anchor=\"nw\",\r\n        text=\"Win %\",\r\n        fill=\"#000000\",\r\n        font=(\"Arial BoldMT\", 13 * -1)\r\n    )\r\n    \r\n    canvas.create_text(\r\n        53.0,\r\n        342.0,\r\n        anchor=\"nw\",\r\n        text=\"Score/Min\",\r\n        fill=\"#000000\",\r\n        font=(\"Arial BoldMT\", 13 * -1)\r\n    )\r\n    \r\n    canvas.create_text(\r\n        70.0,\r\n        36.0,\r\n        anchor=\"nw\",\r\n        text=\"HUSKYEL1TE'S WZ STATS BOT\",\r\n        fill=\"#000000\",\r\n        font=(\"RobotoRoman Regular\", 20 * -1)\r\n    )\r\n    window.resizable(False, False)\r\n    window.mainloop()\r\n", "284": "from .scrape import WebScrape \r\nimport schedule \r\nfrom .service import DataProcess\r\nimport time\r\n\r\ndef processwebdata():\r\n\r\n    print(\"entering process webdata in scheduler\")\r\n    event_list = WebScrape.scrapeweb()\r\n    time.sleep(2)\r\n    DataProcess.saveeventdata(event_list)\r\n    \r\n\r\ndef deletePastEvents():\r\n\r\n    print('begin delete')\r\n\r\n    DataProcess.deletePastEvents()\r\n    print(\"finish delete\")\r\n\r\n\r\n\r\n\r\nschedule.every().day.at(\"00:45\").do(processwebdata)\r\nschedule.every().day.at(\"01:17\").do(deletePastEvents)\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef startScehdule():\r\n\r\n\r\n    while True: \r\n\r\n        # Checks whether a scheduled task  \r\n        # is pending to run or not \r\n        schedule.run_pending() \r\n        time.sleep(1)\r\n\r\n\r\n# deletion of past events scheduler\r\n\r\n\r\n\r\n\r\n", "285": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Sat Sep 11 16:40:57 2021\r\n\r\n@author: Kiran B\r\n\"\"\"\r\n\r\nimport streamlit as st\r\nimport spacy_streamlit as sts\r\n \r\nfrom PIL import Image\r\n\r\nfrom bs4 import BeautifulSoup #converts the contents of a page into a proper format\r\nimport requests #used to get the content from a web page\r\nimport spacy\r\n\r\nimport pandas as pd\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nimport seaborn as sns\r\n\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\nst.set_page_config(layout = 'wide')\r\nst.set_option('deprecation.showPyplotGlobalUse', False)\r\n\r\nst.title('EMAIL TEMPLATE GENERATION')\r\n\r\nst.write(\"Using Webscraping and NLP Techniques to Generate an E-mail Template\")\r\n\r\nimage = Image.open('WebScraping_EmailTemplate.png')\r\nst.image(image, caption='Email Template Generation by Web-Scraping')\r\n\r\ndef Webscrape_divID(URL, div_id):\r\n    '''This function scrapes the website from the URL given to it.\\\r\n    It collects the entire website data and stores the data in the html format\\\r\n        Also it extracts the data segment based on the div_id'''\r\n    \r\n    HEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\r\n    \r\n    # Making the HTTP Request\r\n    webpage = requests.get(URL, headers=HEADERS)\r\n  \r\n    # Creating the Soup Object containing all data\r\n    soup = BeautifulSoup(webpage.content, \"html.parser\")\r\n\r\n    results = soup.find(id=div_id)\r\n    st.text(\"Scraping Web......Done!\\n\")\r\n    st.write('**Exracted text to be analyzed:**')   \r\n    st.write(results.get_text())\r\n    \r\n    return results.get_text()\r\n\r\n\r\ndef Webscrape_Classname(URL, classname):\r\n    \r\n    '''This function scrapes the website from the URL given to it.\\\r\n    It collects the entire website data and stores the data in the html format \\\r\n    Also it extracts the data segment based on the classname'''\r\n    \r\n    HEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\r\n    \r\n    # Making the HTTP Request\r\n    webpage = requests.get(URL, headers=HEADERS)\r\n  \r\n    # Creating the Soup Object containing all data\r\n    soup = BeautifulSoup(webpage.content, \"html.parser\")\r\n   \r\n    results = soup.find(\"div\", class_= classname)\r\n    st.text(\"Scraping Web......Done!\\n\")\r\n    st.write('**Exracted text to be analyzed:**')   \r\n    st.write(results.get_text())\r\n    \r\n    return results.get_text()\r\n\r\n\r\ndef Word_Frequency(spacy_text):\r\n    '''Visualize the Noun and Verb frequencies in the extracted text'''\r\n    \r\n    #Filtering for nouns and verbs only\r\n    nouns_verbs = [token.text for token in spacy_text if token.pos_ in ('NOUN', 'VERB')]\r\n    \r\n    cv = CountVectorizer()\r\n    X = cv.fit_transform(nouns_verbs)\r\n    sum_words = X.sum(axis=0)\r\n    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\r\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\r\n    wf_df = pd.DataFrame(words_freq)\r\n    wf_df.columns = ['word', 'count']\r\n    \r\n    sns.barplot(x = 'count', y = 'word', data = wf_df, palette=\"GnBu_r\")\r\n    st.pyplot()\r\n    \r\n    st.write(\"**Word Count(Noun & Verb) of the Extracted Text:\\n**\")\r\n    st.write(wf_df)\r\n    \r\n\r\ndef POS_Tag(data):\r\n    '''Tag Parts of Speech to the Extracted data and visualize'''\r\n    \r\n    sts.visualize_parser(data)\r\n    sts.visualize_ner(data, labels=nlp.get_pipe(\"ner\").labels)\r\n    \r\n\r\n # Use case 1\r\ndef Replace_Content1(token):\r\n    '''Find and replace selected tokens for Usecase 1'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'John Dooley':\r\n        return '[Your Name]\\n'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'John':\r\n        return '\\n[Your Name]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jennifer':\r\n        return \"[Your Manager's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return '[Date]'\r\n    return token.text\r\n\r\ndef FindnReplace1(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content1, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\ndef Replace_Content1a(token):\r\n    '''Find and replace selected tokens for Usecase 1a'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'William J. Jones':\r\n        return '[Your Name]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'William Jones':\r\n        return '[Your Signature on the Hard copy]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'ORG':\r\n        return '[Your Company Name]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Perez':\r\n        return \"[Your Manager's Name]\"\r\n    if token.ent_iob !=0 and token.ent_type_ == 'DATE' and token.text == 'between now and September 1, 2013':\r\n        return \"before [Your task completion timeline]\"\r\n    if token.ent_iob !=0 and token.ent_type_ == 'DATE' and token.text == 'September 1, 2013 through September 21, 2013':\r\n        return \"[From Date] through [To Date]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'GPE':\r\n        return '[Place/Country name]'\r\n    if token.text == \"Mr.\":\r\n        return '\\b'\r\n    if token.text == \"cruise\":\r\n        return '\\b'\r\n    if token.text == \"wife\":\r\n        return \"[self/companion/friend]\"\r\n    \r\n    return token.text\r\n\r\ndef FindnReplace1a(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content1a, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n\r\n# Use Case 2\r\ndef Replace_Content2(token):\r\n    '''Find and replace selected tokens for Usecase 2'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and (token.text == 'Joe' or token.text == 'Joe Brown'):\r\n        return '[Your Name]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Steve':\r\n        return \"[Your Manager's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return '[Sickness Date]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'ORG':\r\n        return '[Hospital/Clinic Name]'\r\n    if token.text == 'Joejoe.brown765@email.com555':\r\n        return '\\n[Your Name]\\n[Your Email ID]'\r\n    if token.text == '555':\r\n        return '\\n[Your Contact'\r\n    if token.text == '5555':\r\n        return 'Number]'\r\n    return token.text\r\n\r\ndef FindnReplace2(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content2, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\ndef Replace_Content2a(token):\r\n    '''Find and replace selected tokens for Usecase 2a'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jane':\r\n        return '\\n[Your Name]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jane Doe':\r\n        return '[Your Name]'\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and (token.text == 'Patricia' or token.text == 'Tom'):\r\n        return \"[Your Colleague's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE' and token.text == 'Friday':\r\n        return \"on [Meeting day]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return '[Sickness Date]'\r\n    return token.text\r\n\r\ndef FindnReplace2a(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content2a, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n\r\n# Use Case 3\r\ndef Replace_Content3(token):\r\n    '''Find and replace selected tokens for Usecase 3'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Smith':\r\n        return \"[Your Colleague's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonny\\n':\r\n        return \"\\n [Your Name]\\n [Your Designation]\"\r\n    if token.text == \"Formal\":\r\n        return 'Formal Birthday Wishes'\r\n    if token.text == \"Mr.\":\r\n        return ''\r\n    return token.text\r\n\r\ndef FindnReplace3(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content3, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\ndef Replace_Content3a(token):\r\n    '''Find and replace selected tokens for Usecase 3a'''\r\n    \r\n    if token.text == \"company!Have\":\r\n        return 'company!\\n Have'\r\n    return token.text\r\n\r\ndef FindnReplace3a(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content3a, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n\r\n# Use Case 4\r\ndef Replace_Content4(token):\r\n    '''Find and replace selected tokens for Usecase 4'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Paul JonesPhoneEmail':\r\n        return \"\\n\\n Regards,\\n [Your Name]\\n [Your Contact No.]\\n [Your Email ID]\\n\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return \"[Years of Experience]\"\r\n    if token.text == \"Address\":\r\n        return \"\"\r\n    if token.text == \"store\":\r\n        return \"\\b\"\r\n    if token.text == \"retail\":\r\n        return \"\\b\"\r\n        \r\n    return token.text\r\n\r\ndef FindnReplace4(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content4, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\ndef Replace_Content4a(token):\r\n    '''Find and replace selected tokens for Usecase 4a'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and (token.text == 'Mary Garcia12' or token.text == 'Mary Garcia'):\r\n        return \"[Your Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Lee':\r\n        return \"[Hiring Manager's Name]\"\r\n    if token.text == \"Lee\":\r\n        return \"\\n [Hiring Manager's Name]\\n\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Franklin Lee':\r\n        return \"To: [Hiring Manager's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE' and token.text == 'five years':\r\n        return \"[Your experience in years]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return \"[Mailing Date]\\n\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'ORG' and token.text == 'CBI Industries39':\r\n        return \"[Company Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'ORG':\r\n        return \"[Your leaving Company name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'GPE' and token.text == 'AvenueTownville':\r\n        return \"[Building No., Street Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'GPE' and token.text == 'New Hampshire':\r\n        return \"\\n [Area Name, Town Name, Pincode]\\n\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'GPE':\r\n        return \"[Campus Name]\"\r\n    if token.text == 'Rogers':\r\n        return \"\\n\"\r\n    if token.text == \"Mr.\":\r\n        return \"\"\r\n    if token.text == \"03060\":\r\n        return \"\\n\"\r\n    if token.text == \"Sincerely\":\r\n        return \"\\n\\n Sincerely\"\r\n    if token.text == \"Signature\":\r\n        return \"\\n [Signature]\"\r\n    return token.text\r\n\r\ndef FindnReplace4a(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content4a, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n\r\n# Use Case 5\r\ndef Replace_Content5(token):\r\n    '''Find and replace selected tokens for Usecase 5'''\r\n    \r\n    if token.text == \",\":\r\n        return \"\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Amy':\r\n        return \"[Your Colleague's Name],\\n\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonathan':\r\n        return \"\\n\\n [Your Name]\\n [Your Contact number]\"\r\n    if token.text == \"Sincerely\":\r\n        return \"\\n Sincerely,\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return \"[Timeline] and [Reason for Appreciation]\"\r\n    return token.text\r\n\r\ndef FindnReplace5(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content5, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\ndef Replace_Content5a(token):\r\n    '''Find and replace selected tokens for Usecase 5a'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'John':\r\n        return \"[Your Colleague's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Samantha':\r\n        return \"\\n [Your Name]\\n [Your Designation]\"\r\n    if token.text == \"Best\":\r\n        return \"Best Regards\"\r\n    if token.text == 'project':\r\n        return \"[Work of Appreciation]\"\r\n    if token.text == \"Dear\":\r\n        return \"\\nDear\"\r\n    return token.text\r\n\r\ndef FindnReplace5a(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content5a, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n\r\n# Use Case 6\r\ndef Replace_Content6(token):\r\n    '''Find and replace selected tokens for Usecase 6'''\r\n    \r\n    if token.text == \",\":\r\n        return \"\"\r\n    if token.text == \"Hello\":\r\n        return \"Dear [Sender's Name],\\n\"\r\n    if token.text == \"COLLEAGUE\":\r\n        return \"[Your Colleague's Name]\"\r\n    if token.text == \"Regards\":\r\n        return \"\\n Regards,\"\r\n    if token.text == \"NAME\":\r\n        return \"\\n\\n [Your Name]\"\r\n    if token.text == \"do\":\r\n        return \"don't\"\r\n    if token.text == \"n\u2019t\":\r\n        return \"\\b\"\r\n    return token.text\r\n\r\ndef FindnReplace6(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content6, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\ndef Replace_Content6a(token):\r\n    '''Find and replace selected tokens for Usecase 6a'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jane Doe':\r\n        return \"[Your Colleague's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'John Smith':\r\n        return \"\\n [Your Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\r\n        return \"[Your Return Date]\"\r\n    if token.text == \"Thank\":\r\n        return \"Dear [Sender's Name],\\n\\n\\t Thank\"\r\n    if token.text == \"She\":\r\n        return \"He/She\"\r\n    return token.text\r\n\r\ndef FindnReplace6a(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content6a, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n\r\n# Use Case 7\r\ndef Replace_Content7(token):\r\n    '''Find and replace selected tokens for Usecase 7'''\r\n    \r\n    if token.text == \",\":\r\n        return \"\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Sam':\r\n        return \"[Your Partner's Name],\\n\\n\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'Jonathan':\r\n        return \"\\n\\n[Your Name]\\n[Your Contact Number]\"\r\n    if token.text == \"Please\":\r\n        return \"\\nPlease\"\r\n    if token.text == \"Thank\":\r\n        return \"\\nThank\"\r\n    if token.text == \"again!Sincerely\":\r\n        return \"\\b\\b, again!\\nSincerely,\"\r\n    return token.text\r\n\r\ndef FindnReplace7(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content7, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\ndef Replace_Content7a(token):\r\n    '''Find and replace selected tokens for Usecase 7a'''\r\n    \r\n    if token.ent_iob != 0 and token.ent_type_ == 'PERSON' and token.text == 'James':\r\n        return \"[Your Colleague's Name]\"\r\n    if token.ent_iob != 0 and token.ent_type_ == 'DATE' and token.text == 'a whole year':\r\n        return \"[Number of years]\"\r\n    if token.text == \"at\":\r\n        return \"\"\r\n    return token.text\r\n\r\ndef FindnReplace7a(nlp_doc):\r\n    with nlp_doc.retokenize() as retokenizer:\r\n        for ent in nlp_doc.ents:\r\n            retokenizer.merge(ent)\r\n    tokens = map(Replace_Content7a, nlp_doc)\r\n    return ' '.join(tokens)\r\n\r\n\r\n# Collect User Input    \r\nst.write(\"**Select your desrired category for the Email Template:**\")\r\noption = st.selectbox(\"Drop down options\",\\\r\n                      ('Vacation Leave Email Template', 'Sick Leave Email Template',\\\r\n                       'Birthday Wishes Email Template', 'Cover Letter Email Template',\\\r\n                           'Employee work appreciation Email Template', 'Out of Office Email Template',\\\r\n                               'Thank you note for Business Email Template'))\r\nst.write('\\n**You have selected**:', option)\r\n\r\nnlp = spacy.load('en_core_web_trf')\r\n\r\nif option == 'Vacation Leave Email Template':\r\n        \r\n    URL1 = \"https://www.thebalancecareers.com/formal-leave-of-absence-letter-request-example-2060597\"\r\n    div_id1 = \"mntl-sc-block-callout-body_1-0-3\"\r\n    URL2 = \"https://www.greatsampleresume.com/letters/personal-letters/vacation-leave\"\r\n    classname2 = \"letter-table\"\r\n    \r\n    def Process_URL1():\r\n        extract1 = Webscrape_divID(URL1, div_id1)\r\n        \r\n        phr = \"As we discussed\"\r\n        temp = extract1.replace(str(phr), \"\\n\\tAs we discussed\")\r\n        phr1 = \"Dear\"\r\n        temp2 = temp.replace(str(phr1), \"\\nDear\") \r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text1 = nlp(temp2)\r\n    \r\n        Word_Frequency(spacy_text1)\r\n        \r\n        POS_Tag(spacy_text1)\r\n        sts.visualize_tokens(spacy_text1, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n    \r\n        # Generate Template\r\n        temp2 = FindnReplace1(spacy_text1)\r\n        \r\n        phr2 = \"I plan\"\r\n        temp3 = temp2.replace(str(phr2), \"\\nI plan\")\r\n        phr3 = \"I would also\"\r\n        temp4 = temp3.replace(str(phr3), \"\\nI would also\")\r\n        phr4 = \"Thank you\"\r\n        temp5 = temp4.replace(str(phr4), \"\\nThank you\")\r\n        phr5 = \"Best\"\r\n        template1 = temp5.replace(str(phr5), \"\\n\\nBest\")\r\n        st.subheader(\"**Your Template for Vacation Leave Email**\\n\")\r\n        st.text(template1)\r\n    \r\n    \r\n    def Process_URL2():\r\n        extract2 = Webscrape_Classname(URL2, classname2)\r\n        \r\n        phr1 = \"Yours sincerely,\"\r\n        temp1 = extract2.replace(str(phr1), \"\\nYours sincerely,\\n\")\r\n        phr2 = \"(555)-555-5555\"\r\n        temp2 = temp1.replace(str(phr2), \"[Your Contact Number]\")\r\n        phr3 = \"my leave\"\r\n        temp3 = temp2.replace(str(phr3), \"\\b\\b\\b my leave\")\r\n        phr4 = \"I think you\"\r\n        temp4 = temp3.replace(str(phr4), \"\\nI think you\")\r\n        phr5 = \"I am planning\"\r\n        temp5 = temp4.replace(str(phr5), \"\\n\\tI am planning\")\r\n        phr6 = \"If there are\"\r\n        temp6 = temp5.replace(str(phr6), \"\\nIf there are\")\r\n        phr7 = \"I am writing\"\r\n        temp7 = temp6.replace(str(phr7), \"\\n\\tI am writing\")\r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text1a = nlp(temp7)\r\n    \r\n        Word_Frequency(spacy_text1a)\r\n        \r\n        POS_Tag(spacy_text1a)\r\n        sts.visualize_tokens(spacy_text1a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n           \r\n        # Generate Template\r\n        temp = FindnReplace1a(spacy_text1a)\r\n        \r\n        phr4 = \"Assistant Manager\"\r\n        template1a = temp.replace(str(phr4), \"[Your Designation]\")\r\n        \r\n        st.subheader(\"**Your Template for Vacation Leave Email**\\n\")\r\n        st.text(template1a)\r\n            \r\n    \r\n    st.write('**Please select the keyword set from the below options to generate the Template**')\r\n    keyword = st.radio(\"Keyword-set Selection\",('absence, discussed, approved, assistance',\\\r\n                                                'vacation, formally, endeavored, trip'))\r\n        \r\n    if(keyword == 'absence, discussed, approved, assistance'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL1()\r\n    \r\n    elif(keyword == 'vacation, formally, endeavored, trip'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL2()\r\n    \r\n    \r\nelif option == 'Sick Leave Email Template':\r\n    \r\n    URL1 = \"https://www.thebalancecareers.com/sample-sickness-absence-excuse-letter-2060603\"\r\n    div_id1 = \"mntl-sc-block-callout-body_1-0-3\"\r\n    URL2 = \"https://www.thebalancecareers.com/sample-sickness-absence-excuse-letter-2060603\"\r\n    div_id2 = \"mntl-sc-block-callout-body_1-0-4\"\r\n    \r\n    def Process_URL1():\r\n        extract1 = Webscrape_divID(URL1, div_id1)\r\n        \r\n        phr = \"Dear\"\r\n        temp = extract1.replace(str(phr), \"\\n\\nDear\") \r\n        phr1 = \"I am writing\"\r\n        temp1 = temp.replace(str(phr1), \"\\n\\t I am writing\")\r\n        phr2 = \"Please\"\r\n        temp2 = temp1.replace(str(phr2), \"\\nPlease\")\r\n        phr3 = \"Regards\"\r\n        temp2 = temp2.replace(str(phr3), \"\\n\\nRegards\")\r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text2 = nlp(temp2)\r\n    \r\n        Word_Frequency(spacy_text2)\r\n    \r\n        POS_Tag(spacy_text2)\r\n        sts.visualize_tokens(spacy_text2, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n        \r\n       # Generate Template\r\n        temp = FindnReplace2(spacy_text2)\r\n    \r\n        phrase = \"an acute attack of food poisoning\"\r\n        template2 = temp.replace(str(phrase),\"[Reason for Sickness]\")\r\n        st.subheader(\"**Your Template for Sick Leave Email**\\n\")\r\n        st.text(template2)\r\n        \r\n    def Process_URL2():\r\n        extract2 = Webscrape_divID(URL2, div_id2)\r\n        \r\n        phr = \"Dear\"\r\n        temp = extract2.replace(str(phr), \"\\n\\nDear\")\r\n        phr1 = \"Supervisor Name\"\r\n        temp1 = temp.replace(str(phr1), \"[Your Manager's Name]\")\r\n        phr2 = \"I've asked\"\r\n        temp2 = temp1.replace(str(phr2), \"\\nI've asked\")\r\n        phr3 = \"I've come\"\r\n        temp3 = temp2.replace(str(phr3), \"\\n\\tI've come\")\r\n        phr4 = \"I will try\"\r\n        temp4 = temp3.replace(str(phr4), \"\\nI will try\")\r\n        phr5 = \"Thank you\"\r\n        temp5 = temp4.replace(str(phr5), \"\\n\\nThank you\")\r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text2a = nlp(temp5)\r\n    \r\n        Word_Frequency(spacy_text2a)\r\n    \r\n        POS_Tag(spacy_text2a)\r\n        sts.visualize_tokens(spacy_text2a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n        \r\n        # Generate Template\r\n        template2a = FindnReplace2a(spacy_text2a)\r\n        st.subheader(\"**Your Template for Sick Leave Email**\\n\")\r\n        st.text(template2a)\r\n    \r\n    st.write('**Please select the keyword set from the below options to generate the Template**')\r\n    keyword = st.radio(\"Keyword-set Selection\",('document, absence, illness, treatment',\\\r\n                                                'flu, rest, recover, prepare'))\r\n        \r\n    if(keyword == 'document, absence, illness, treatment'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL1()\r\n    \r\n    elif(keyword == 'flu, rest, recover, prepare'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL2()\r\n    \r\n    \r\nelif option == 'Birthday Wishes Email Template':\r\n    \r\n    URL1 = \"https://www.targettraining.eu/happy-birthday-emails/\"\r\n    classname1 = \"avia-promocontent\"\r\n    URL2 = \"https://www.happybirthdaywisher.com/employee/\"\r\n    div_id2 = \"mensagem-1072\"\r\n    \r\n    def Process_URL1():\r\n        extract1 = Webscrape_Classname(URL1, classname1)\r\n        \r\n        phr1 = \"I am\"\r\n        temp1 = extract1.replace(str(phr1),\"\\n\\tI am\")\r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text3 = nlp(temp1)\r\n    \r\n        Word_Frequency(spacy_text3)\r\n    \r\n        POS_Tag(spacy_text3)\r\n        sts.visualize_tokens(spacy_text3, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n    \r\n        # Generate Template\r\n        template3 = FindnReplace3(spacy_text3)\r\n         \r\n        st.subheader(\"**Your Template for Birthday Wishes Email**\\n\")\r\n        st.text(template3)\r\n        \r\n        \r\n    def Process_URL2():\r\n        extract2 = Webscrape_divID(URL2, div_id2)\r\n    \r\n        phr1 = \"birthday!\"\r\n        temp1 = extract2.replace(str(phr1), \"birthday!\\n\\nSincerely,\\n[Your Name]\\n[Your Designation]\")\r\n        \r\n        phr2 = \"Happy\"\r\n        temp2 = temp1.replace(str(phr2), \"Dear [Your Colleague's Name],\\n\\tHappy\")\r\n        \r\n        phr3 = \"Your positivity\"\r\n        temp3 = temp2.replace(str(phr3), \"\\nYour positivity\")\r\n        \r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text3a = nlp(temp3)\r\n    \r\n        Word_Frequency(spacy_text3a)\r\n    \r\n        POS_Tag(spacy_text3a)\r\n        sts.visualize_tokens(spacy_text3a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n    \r\n        # Generate Template\r\n        template3a = FindnReplace3a(spacy_text3a)\r\n         \r\n        st.subheader(\"**Your Template for Birthday Wishes Email**\\n\")\r\n        st.text(template3a)\r\n        \r\n    \r\n    st.write('**Please select the keyword set from the below options to generate the Template**')\r\n    keyword = st.radio(\"Keyword-set Selection\",('writing, wish, enjoy, returns',\\\r\n                                                'wonderful, positivity, expertise, regarded'))\r\n        \r\n    if(keyword == 'writing, wish, enjoy, returns'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL1()\r\n    \r\n    elif(keyword == 'wonderful, positivity, expertise, regarded'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL2()\r\n        \r\n   \r\n    \r\nelif option == 'Cover Letter Email Template':\r\n    \r\n    URL1 = \"https://www.thebalancecareers.com/email-cover-letter-samples-2060246\"\r\n    div_id1 = \"mntl-sc-block-callout-body_1-0-1\"\r\n    URL2 = \"https://www.thebalancecareers.com/how-to-address-a-cover-letter-2060281\"\r\n    div_id2 = \"mntl-sc-block-callout-body_1-0-5\"\r\n    \r\n    def Process_URL1():\r\n        extract1 = Webscrape_divID(URL1, div_id1)\r\n    \r\n        phr1 = \"Store Manager Position\"\r\n        temp1 = extract1.replace(str(phr1),\"[Role you are applying for]\")\r\n        \r\n        phr2 = \"Your Name\"\r\n        temp2 = temp1.replace(str(phr2), \"[Your Name]\")\r\n        \r\n        phr3 = \"Store Manager position\"\r\n        temp3 = temp2.replace(str(phr3),\"[Role you are applying for]\")\r\n        \r\n        phr4 = \"Payroll management, scheduling, reports, and inventory control expertise\"\r\n        temp4 = temp3.replace(str(phr4),\"\")\r\n        \r\n        phr5 = \"Extensive work with visual standards and merchandising high-ticket items\"\r\n        temp5 = temp4.replace(str(phr5),\"\")\r\n        \r\n        phr6 = \"retail management\"\r\n        temp6 = temp5.replace(str(phr6),\"[Your previous role]\")\r\n        \r\n        phr7 = \"XYZ Company:\"\r\n        temp7 = temp6.replace(str(phr7),\"[Company name you are applying for]:\\n[Your Skill Set]...for example\")\r\n        \r\n        phr8 = \"I read\"\r\n        temp8 = temp7.replace(str(phr8), \"\\n\\tI read\")\r\n        \r\n        phr9 = \"Dear\"\r\n        temp9 = temp8.replace(str(phr9), \"\\n\\nDear\")\r\n        \r\n        phr10 = \"I can offer\"\r\n        temp10 = temp9.replace(str(phr10), \"\\nI can offer\")\r\n        \r\n        phr11 = \"Over\"\r\n        temp11 = temp10.replace(str(phr11), \"\\nOver\")\r\n        \r\n        phr12 = \"Ability\"\r\n        temp12 = temp11.replace(str(phr12), \"\\nAbility\")\r\n        \r\n        phr13 = \"In addition\"\r\n        temp13 = temp12.replace(str(phr13), \"\\n\\nIn addition\")\r\n        \r\n        phr14 = \"My broad\"\r\n        temp14 = temp13.replace(str(phr14), \"\\nMy broad\")\r\n        \r\n        phr15 = \"I look\"\r\n        temp15 = temp14.replace(str(phr15), \"\\nI look\")\r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text4 = nlp(temp15)\r\n        \r\n        Word_Frequency(spacy_text4)\r\n        \r\n        POS_Tag(spacy_text4)\r\n        sts.visualize_tokens(spacy_text4, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n        \r\n        # Generate Template\r\n        template4 = FindnReplace4(spacy_text4)\r\n             \r\n        st.subheader(\"**Your Template for Cover Letter Email**\\n\")\r\n        st.text(template4)\r\n        \r\n    def Process_URL2():\r\n        extract2 = Webscrape_divID(URL2, div_id2)\r\n        \r\n        phr = \"Dear\"\r\n        temp = extract2.replace(str(phr), \"\\nDear\")\r\n        \r\n        phr1 = \"03060555-555-5555mary.garcia@email.com\"\r\n        temp1 = temp.replace(str(phr1), \"[Your Contact Number and Email ID]\\n\")\r\n        \r\n        phr2 = \"operations assistant/associate\"\r\n        temp2 = temp1.replace(str(phr2), \"[Role in which you have experience]\")\r\n        \r\n        phr3 = \"operations assistant\"\r\n        temp3 = temp2.replace(str(phr3), \"[Role you are applying for]\")\r\n        \r\n        phr4 = \"orders, resolved customer issues, ordered supplies, and prepared reports\"\r\n        temp4 = temp3.replace(str(phr4), \"[Your responsibilities at your leaving company]\")\r\n        \r\n        phr5 = \"bookkeeping, data entry, and sales support\"\r\n        temp5 = temp4.replace(str(phr5), \"[Your prior job nature]\")\r\n        \r\n        phr6 = \"Strong communication skills, in person, in writing, and on the phone\"\r\n        temp6 = temp5.replace(str(phr6), \"\\n[Your Skillset]\")\r\n        \r\n        phr7 = \"Excellent attention to detail and organization skills\"\r\n        temp7 = temp6.replace(str(phr7), \"-\")\r\n        \r\n        phr8 = \"Top-notch customer service\"\r\n        temp8 = temp7.replace(str(phr8), \"-\")\r\n        \r\n        phr9 = \"Experience in the industry and passion for the product\"\r\n        temp9 = temp8.replace(str(phr9), \"-\")\r\n        \r\n        phr10 = \"Adept at all the usual professional software, including Microsoft Office Suite\"\r\n        temp10 = temp9.replace(str(phr10), \"-\")\r\n        \r\n        phr11 = \"I\u2019ve included\"\r\n        temp11 = temp10.replace(str(phr11), \"\\n\\nI\u2019ve included\")\r\n        \r\n        phr12 = \"Basically\"\r\n        temp12 = temp11.replace(str(phr12), \"\\nBasically\")\r\n        \r\n        phr13 = \"I was excited\"\r\n        temp13 = temp12.replace(str(phr13), \"\\n\\tI was excited\")\r\n        \r\n        phr14 = \"CBI Industries39 Main\"\r\n        temp14 = temp13.replace(str(phr14), \"\")\r\n        \r\n        phr15 = \"In my most\"\r\n        temp15 = temp14.replace(str(phr15), \"\\nIn my most\")\r\n        \r\n        phr16 = \"My other\"\r\n        temp16 = temp15.replace(str(phr16), \"\\nMy other\")\r\n        \r\n        phr17 = \"(hard copy letter)\"\r\n        temp17 = temp16.replace(str(phr17), \"\\n\")\r\n        \r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text4a = nlp(temp17)\r\n    \r\n        Word_Frequency(spacy_text4a)\r\n        \r\n        POS_Tag(spacy_text4a)\r\n        sts.visualize_tokens(spacy_text4a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n        \r\n        # Generate Template\r\n        template4a = FindnReplace4a(spacy_text4a)\r\n             \r\n        st.subheader(\"**Your Template for Cover Letter Email**\\n\")\r\n        st.text(template4a)\r\n        \r\n    st.write('**Please select the keyword set from the below options to generate the Template**')\r\n    keyword = st.radio(\"Keyword-set Selection\",('qualifications, seeking, gracious, superior',\\\r\n                                                'excited, smoothly, recent, previous'))\r\n        \r\n    if(keyword == 'qualifications, seeking, gracious, superior'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL1()\r\n    \r\n    elif(keyword == 'excited, smoothly, recent, previous'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL2()\r\n        \r\n\r\n\r\nelif option == 'Employee work appreciation Email Template':\r\n    \r\n    URL1 = \"https://talkroute.com/7-sample-thank-you-notes-for-business/\"\r\n    div_id1 = \"x-content-band-5\"\r\n    URL2 = \"https://www.thebalancecareers.com/appreciation-email-samples-2059555\"\r\n    div_id2 = \"mntl-sc-block-callout-body_1-0-4\"\r\n    \r\n    def Process_URL1():\r\n        extract1 = Webscrape_divID(URL1, div_id1)\r\n    \r\n        phr1 = \"You showed\"\r\n        temp1 = extract1.replace(str(phr1),\"\\nYou showed\")\r\n        \r\n        phr2 = \"I am\"\r\n        temp2 = temp1.replace(str(phr2),\"\\nI am\")\r\n        \r\n        phr3 = \"Thank you\"\r\n        temp3 = temp2.replace(str(phr3),\"\\n\\tThank you\")\r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text5 = nlp(temp3)\r\n        \r\n        Word_Frequency(spacy_text5)\r\n        \r\n        POS_Tag(spacy_text5)\r\n        sts.visualize_tokens(spacy_text5, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n        \r\n        # Generate Template    \r\n        template5 = FindnReplace5(spacy_text5)\r\n        \r\n        st.subheader(\"**Your Template for Employee Work Appreciation Email**\\n\")\r\n        st.text(template5)\r\n    \r\n    \r\n    def Process_URL2():\r\n        extract2 = Webscrape_divID(URL2, div_id2)\r\n    \r\n        phr1 = \"Subject Line: Thank You Very Much!\"\r\n        temp1 = extract2.replace(str(phr1), \"Subject Line: Thank You Very Much!\\n\")\r\n        \r\n        phr2 = \"I wanted\"\r\n        temp2 = temp1.replace(str(phr2), \"\\n\\tI wanted\")\r\n        \r\n        phr3 = \"I know how\"\r\n        temp3 = temp2.replace(str(phr3), \"\\nI know how\")\r\n        \r\n        phr4 = \"You are a\"\r\n        temp4 = temp3.replace(str(phr4), \"\\nYou are a\")\r\n        \r\n        phr5 = \"Best\"\r\n        temp5 = temp4.replace(str(phr5), \"\\n\\nBest\")\r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text5a = nlp(temp5)\r\n        \r\n        Word_Frequency(spacy_text5a)\r\n        \r\n        POS_Tag(spacy_text5a)\r\n        sts.visualize_tokens(spacy_text5a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n        \r\n        # Generate Template    \r\n        template5a = FindnReplace5a(spacy_text5a)\r\n        \r\n        st.subheader(\"**Your Template for Employee Work Appreciation Email**\\n\")\r\n        st.text(template5a)\r\n        \r\n    st.write('**Please select the keyword set from the below options to generate the Template**')\r\n    keyword = st.radio(\"Keyword-set Selection\",('tremendous, busy, depend, exceptional',\\\r\n                                                'appreciated, effort, satisfied, contributions'))\r\n        \r\n    if(keyword == 'tremendous, busy, depend, exceptional'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL1()\r\n    \r\n    elif(keyword == 'appreciated, effort, satisfied, contributions'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL2()\r\n    \r\n\r\nelif option == 'Out of Office Email Template':\r\n   \r\n    URL1 = \"https://www.ionos.com/digitalguide/e-mail/technical-matters/perfect-out-of-office-message-examples-and-templates/\"\r\n    div_id1 = \"c118391\"\r\n    URL2 = \"https://www.tenfold-security.com/en/outlook-out-of-office-different-user/\"\r\n    classname2 = \"fusion-reading-box-container reading-box-container-2\"\r\n    \r\n    def Process_URL1():\r\n        extract1 = Webscrape_divID(URL1, div_id1)\r\n    \r\n        phrase = \"Formal out of office reply with referral for customers\"\r\n        temp = extract1.replace(str(phrase),\"\")\r\n        \r\n        phr1 = \"Feel free\"\r\n        temp1 = temp.replace(str(phr1),\"\\nFeel free\")\r\n        \r\n        phr2 = \"You can\"\r\n        temp2 = temp1.replace(str(phr2),\"\\nYou can\")\r\n        \r\n        phr3 = \"Thank you\"\r\n        temp3 = temp2.replace(str(phr3),\"\\nThank you\")\r\n        \r\n        phr4 = \"Thank you for your message\"\r\n        temp4 = temp3.replace(str(phr4), \"\\tThank you for your message\")\r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text6 = nlp(temp4)\r\n        \r\n        Word_Frequency(spacy_text6)\r\n        \r\n        POS_Tag(spacy_text6)\r\n        sts.visualize_tokens(spacy_text6, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n        \r\n        # Generate Template\r\n        temp1 = FindnReplace6(spacy_text6)\r\n        \r\n        phr = \"MM / DD / YY\"\r\n        temp2 = temp1.replace(str(phr), \"[Your Date of Return]\")\r\n        \r\n        phr1 = \"( colleague@example.com )\"\r\n        temp3 = temp2.replace(str(phr1), \"[Your Colleague's Email ID]\")\r\n        \r\n        phr2 = \"( XXX - XXXX )\"\r\n        template6 = temp3.replace(str(phr2), \"[Your Colleague's Contact No.]\")\r\n             \r\n        st.subheader(\"**Your Template for Out of Office Email**\\n\")\r\n        st.text(template6)\r\n        \r\n    def Process_URL2():\r\n        extract2 = Webscrape_Classname(URL2, classname2)\r\n    \r\n        phr1 = \"(555-555-1234)\"\r\n        temp1 = extract2.replace(str(phr1), \"[Your Colleague's Phone Number]\")\r\n        \r\n        phr2 = \"(jane.doe@example.com)\"\r\n        temp2 = temp1.replace(str(phr2), \"[Your Colleague's Email ID]\")\r\n        \r\n        phr3 = \"In urgent\"\r\n        temp3 = temp2.replace(str(phr3), \"\\nIn urgent\")\r\n        \r\n        phr4 = \"Your message\"\r\n        temp4 = temp3.replace(str(phr4), \"\\nYour message\")\r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text6a = nlp(temp4)\r\n        \r\n        Word_Frequency(spacy_text6a)\r\n        \r\n        POS_Tag(spacy_text6a)\r\n        sts.visualize_tokens(spacy_text6a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n    \r\n        # Generate Template\r\n        template6a = FindnReplace6a(spacy_text6a)\r\n    \r\n        st.subheader(\"**Your Template for Out of Office Email**\\n\")\r\n        st.text(template6a)\r\n        \r\n    st.write('**Please select the keyword set from the below options to generate the Template**')\r\n    keyword = st.radio(\"Keyword-set Selection\",('access, represent, assist, understanding',\\\r\n                                                'respond, urgent, matters, forwarded'))\r\n        \r\n    if(keyword == 'access, represent, assist, understanding'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL1()\r\n    \r\n    elif(keyword == 'respond, urgent, matters, forwarded'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL2()\r\n        \r\n    \r\n\r\nelif option == 'Thank you note for Business Email Template':\r\n    \r\n    URL1 = \"https://talkroute.com/7-sample-thank-you-notes-for-business/\"\r\n    div_id1 = \"x-content-band-6\"\r\n    URL2 = \"https://talkroute.com/7-sample-thank-you-notes-for-business/\"\r\n    div_id2 = \"x-content-band-2\"\r\n    \r\n    def Process_URL1():\r\n        extract1 = Webscrape_divID(URL1, div_id1)\r\n        \r\n        phr1 = \"I\u2019m very\"\r\n        temp1 = extract1.replace(str(phr1), \"\\tI\u2019m very\")\r\n    \r\n        # Parse the text with spaCy\r\n        spacy_text7 = nlp(temp1)\r\n        \r\n        Word_Frequency(spacy_text7)\r\n        \r\n        POS_Tag(spacy_text7)\r\n        sts.visualize_tokens(spacy_text7, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n        \r\n        # Generate Template\r\n        template7 = FindnReplace7(spacy_text7)\r\n             \r\n        st.subheader(\"**Your Template for Business Deal Closure Email**\\n\")\r\n        st.text(template7)\r\n        \r\n    def Process_URL2():\r\n        extract2 = Webscrape_divID(URL2, div_id2)\r\n    \r\n        phr1 = \"great day!Sincerely\"\r\n        temp1 = extract2.replace(str(phr1), \"great day!\\nSincerely\")\r\n        phr2 = \"Your friends\"\r\n        temp2 = temp1.replace(str(phr2), \"\\n\\n[Your Name]\")\r\n        phr3 = \"I\u2019m delighted\"\r\n        temp3 = temp2.replace(str(phr3), \"\\n\\n\\tI\u2019m delighted\")\r\n        phr4 = \"We would\"\r\n        temp4 = temp3.replace(str(phr4), \"\\nWe would\")\r\n        phr5 = \"You could\"\r\n        temp5 = temp4.replace(str(phr5), \"\\nYou could\")\r\n        phr6 = \"(your business)\"\r\n        temp6 = temp5.replace(str(phr6), \"\\n[Your Company Name]\")\r\n        \r\n        # Parse the text with spaCy\r\n        spacy_text7a = nlp(temp6)\r\n        \r\n        Word_Frequency(spacy_text7a)\r\n        \r\n        POS_Tag(spacy_text7a)\r\n        sts.visualize_tokens(spacy_text7a, attrs=[\"text\", \"pos_\", \"dep_\", \"ent_type_\"])\r\n        \r\n        # Generate Template\r\n        template7a = FindnReplace7a(spacy_text7a)\r\n             \r\n        st.subheader(\"**Your Template for Business Deal Closure Email**\\n\")\r\n        st.text(template7a)\r\n        \r\n    st.write('**Please select the keyword set from the below options to generate the Template**')\r\n    keyword = st.radio(\"Keyword-set Selection\",('partnership, family, fruitful, business',\\\r\n                                                'delighted, customer, loyal, patronage'))\r\n        \r\n    if(keyword == 'partnership, family, fruitful, business'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL1()\r\n    \r\n    elif(keyword == 'delighted, customer, loyal, patronage'):\r\n        st.write('You selected:', keyword)\r\n        Process_URL2()\r\n\r\n\r\n", "286": "from selenium import webdriver\nfrom selenium.webdriver.support.ui import Select\nfrom time import sleep\nimport re\nimport operator\nfrom selenium.webdriver.common.keys import Keys\n\nfrom webdriver_manager.chrome import ChromeDriverManager\n\nimport pika\nimport sys\n\nimport requests\nimport bs4\nimport json\nimport datetime\n\nfrom apscheduler.schedulers.blocking import BlockingScheduler\n\nsched = BlockingScheduler()\nprint(\"Webscraping Scheduler started\")\n@sched.scheduled_job('interval', minutes=10)\ndef timed_job():\n    # %pip install selenium\n    # %pip install webdriver_manager\n    # %pip install pika\n    \n    print(\"Starting webscraper\")\n    \n    URL = 'https://www.steamspy.com/search.php'\n\n    browser = webdriver.Chrome(ChromeDriverManager().install())\n\n    games = ['Grand Theft Auto V', 'Portal 2', 'The Witcher 3: Wild Hunt', 'Tomb Raider (2013)', 'The Elder Scrolls V: Skyrim',\n             'Left 4 Dead 2', 'Borderlands 2', 'Fallout 4', 'PAYDAY 2', 'Grand Theft Auto IV', 'DOOM (2016)','BioShock',\n             'Half-Life 2', 'Red Dead Redemption 2', 'Limbo','Counter-Strike: Global Offensive', 'Life is Strange',\n             'Team Fortress 2', 'BioShock Infinite']\n\n    # games = ['Grand Theft Auto V', 'Portal-2']\n    # games = ['Portal-2']\n\n    gameInfos = []\n\n    for game in games:\n        browser.get(URL)\n    #     browser.implicitly_wait(1)\n        search_field = browser.find_element_by_xpath('/html/body/div[3]/div[2]/div[1]/div[2]/div/div/div/div/div/form/div/input')\n        search_field.send_keys(game)\n        search_field.submit()\n        getInfo = browser.find_element_by_xpath('/html/body/div[3]/div[2]/div[1]/div[2]/div[1]/div[1]/div/p').text\n\n        if getInfo.find(\"Price:\") != -1:\n            cleanData = getInfo[getInfo.find(\"Price:\"):].split('\\n')\n            del cleanData[1]\n            wat700 = \"\".join(cleanData).replace(\"Owners\", \";Owners\").replace(': ', \"=\")\n            dictionary = dict(subString.split('=') for subString in wat700.split(\";\")) \n\n            dictionary[\"Game\"] = game\n            gameInfos.append(dictionary)\n\n    browser.close()\n    jsonData = json.dumps(gameInfos)\n    print(jsonData)\n\n\n    connection = pika.BlockingConnection(\n        pika.ConnectionParameters(host='localhost'))\n    channel = connection.channel()\n\n    channel.exchange_declare(exchange='gamerevWebscraper', exchange_type='fanout')\n\n    channel.queue_declare(queue='webscrapeData')\n    channel.queue_declare(queue='webscrapeData2')\n\n    channel.queue_bind(exchange='gamerevWebscraper', queue=\"webscrapeData\")\n    channel.queue_bind(exchange='gamerevWebscraper', queue=\"webscrapeData2\")\n\n    channel.basic_publish(exchange='gamerevWebscraper', routing_key='', body=jsonData)\n    print(\" [x] Sent %r\" % jsonData)\n    connection.close()\n\n    print(\"Webscraper scheduler job has run at %s\" % datetime.datetime.now())\n\ntimed_job()\nsched.start()\n\n\n\n", "287": "from pandacode import datafile\nfrom seleniumcode import WebScrape\nimport re\n\ndef fill_data(business_name, address_list, source):\n    row_index = datafile.index[(datafile['Business Name'] == business_name)]\n    datafile.at[row_index, 'Source'] = source\n    datafile.at[row_index, \"Street Address\"] = address_list[0]\n    datafile.at[row_index, \"City\"] = address_list[1]\n    datafile.at[row_index, \"State\"] = address_list[2]\n    datafile.at[row_index, \"Zip\"] = address_list[3]\n\n#FIRST - We go to the California Business Search website.\ncalsearch = WebScrape()\ncorpwiki = WebScrape()\n\nfor business_name in datafile['Business Name']:\n    calsearch.get(\"https://businesssearch.sos.ca.gov/\")\n    if re.search(\"LLC\", business_name, re.IGNORECASE) or re.search(\"LP\", business_name, re.IGNORECASE):\n        calsearch.click_button(\"LLCNameOpt\")\n    else:\n        calsearch.click_button(\"CorpNameOpt\")\n\n    calsearch.search_box(\"SearchCriteria\", business_name)\n    address_list = calsearch.get_results(business_name)\n    if type(address_list) is list:\n        source = \"California Business Search\"\n        fill_data(business_name, address_list, source)\n    else: #it returns 1\n        #go to Corp Wiki and check\n        corpwiki.get(\"https://www.corporationwiki.com/\")\n        corpwiki.search_box(\"keywords\", business_name)\n        address_list = corpwiki.get_results2(business_name)\n        if type(address_list) is list:\n            source = \"Corporation Wiki\"\n            fill_data(business_name, address_list, source)\n        else:\n            empty_list = [\" \", \" \", \" \", \" \"]\n            fill_data(business_name, empty_list, source)\n    \ncalsearch.quit()\ncorpwiki.quit()\n\nprint(datafile.head)\n\n", "288": "import requests.exceptions\nfrom web_scrape import WebScrape\nfrom web_processing import WebProcess\nfrom web_word_counter import WordCounter\nfrom bar_chart_plot import BarChart\nfrom pie_chart_plot import PieChart\n\n\nweb_scrape = WebScrape()\nweb_process = WebProcess()\nword_counter = WordCounter()\nbar_chart = BarChart()\npie_chart = PieChart()\n\n\nclass Framework:\n    \"\"\"Below is where the various classes already instantiated are called \"\"\"\n    def call(self):\n        try:\n            URL = input(\"Enter a website to analyze: \")\n            URL_scrape = web_scrape.scrape(URL)\n            URL_process = web_process.text_process(URL_scrape)\n            word_count = word_counter.word_counter(URL_process)\n            bar_chart.plot_bar_chart(word_count)\n            try:\n                pie_chart.plot_pie_chart(word_count)\n            except ValueError:\n                return\n            URL_store = open(\"log.csv\", \"a\")\n            URL_store.write(f\"{URL}\\n\")\n            URL_store.close()\n            \"\"\"EXCEPTIONS\"\"\"\n        except requests.exceptions.MissingSchema:\n            print(f\"Invalid Web URL Format, you entered '{URL}'\")\n        except requests.exceptions.ConnectTimeout:\n            print(\"Sorry Network issues\")\n        except requests.exceptions.InvalidSchema:\n            print(f\"Invalid Web URL Format, you entered '{URL}'\")\n        except requests.exceptions.InvalidURL:\n            print(f\"You entered {URL},  Enter a valid website\")\n        except requests.exceptions.ConnectionError:\n            print(f\"You entered {URL},  Enter a valid website\")\n            \n        \n\n\n", "289": "from bs4 import BeautifulSoup\r\nimport requests\r\nimport csv\r\nimport re\r\n\r\nedicoes = [1527]\r\nlinks = []\r\nissues_gravados = 0\r\narticles_gravados = 0 \r\n\r\ndef webscrape_issues(numeros):\r\n    global issues_gravados\r\n    url = f'http://www.uel.br/revistas/uel/index.php/informacao/issue/view/{numeros}'\r\n    source = requests.get(url).text\r\n    soup = BeautifulSoup(source, 'lxml')\r\n    lidos = 0\r\n   \r\n    for texto in soup.find_all('table', class_='tocArticle'):\r\n        link = texto.find('a', attrs={'href': re.compile(\"http://\")})\r\n        link2 = str(link.get('href')[65:70])\r\n        titulo = texto.find('div', class_='tocTitle').text.strip()\r\n        autor = texto.find('div', class_='tocAuthors').text.strip().replace('\\t', '')\r\n        pages = texto.find('div', class_='tocPages').text.strip().replace('\\t', '')\r\n        if autor is not '' and pages is not 'i':\r\n            links.append(link2)\r\n        print(f'T\u00edtulo do trabalho: {titulo}')\r\n        print(f'Autores: {autor}')\r\n        print(f'Link: {link2}')\r\n        print(f'P\u00e1ginas: {pages}')\r\n        print()\r\n        lidos += 1\r\n        issues_gravados += 1\r\n    return lidos > 0\r\n\r\ndef webscrape_articles(numeros):\r\n    global articles_gravados\r\n    url = f'http://www.uel.br/revistas/uel/index.php/informacao/article/view/{numeros}'\r\n    source = requests.get(url).text\r\n    soup = BeautifulSoup(source, 'lxml')\r\n    lidos = 0\r\n   \r\n    for texto in soup.find_all('div', id='content'):\r\n        titulo = texto.find('div', id='articleTitle').text.strip()\r\n        autor = texto.find('div', id='authorString').text.strip()\r\n        resumo = texto.find('div', id='articleAbstract').text.strip().replace('Resumo', '').replace('\\n', '')\r\n        keywords = texto.find('div', id='articleSubject').text.strip().replace('Palavras-chave', '').replace('\\n', '')\r\n        print(f'T\u00edtulo do trabalho: {titulo}')\r\n        print(f'Autores: {autor}')\r\n        print(f'Resumo: {resumo}')\r\n        print(f'Palavras-Chave: {keywords}')\r\n        print()\r\n        csv_writer.writerow([titulo, autor, resumo, keywords])\r\n        lidos += 1\r\n        articles_gravados += 1\r\n    return lidos > 0\r\n\r\n#realiza opera\u00e7\u00f5es com arquivos csv\r\narquivo_csv = open('InfoeInfo_articles.csv', 'w', encoding='utf-8')\r\ncsv_writer = csv.writer(arquivo_csv)\r\ncsv_writer.writerow(['titulo', 'autor','resumo','keywords'])\r\n\r\nfor item in edicoes:\r\n    webscrape_issues(item)\r\n\r\nprint(f'Total de artigos analisados: {issues_gravados}')\r\nprint(links)\r\nfor item in links:\r\n    webscrape_articles(item)\r\n\r\narquivo_csv.close()\r\nprint(f'Total de artigos reunidos: {articles_gravados}')\r\n\r\n\r\n\r\n\r\n", "290": "from bs4 import BeautifulSoup\r\nimport requests\r\nimport csv\r\nimport re\r\n\r\nedicoes = [1527]\r\nlinks = []\r\nissues_gravados = 0\r\narticles_gravados = 0 \r\n\r\ndef webscrape_issues(numeros):\r\n    global issues_gravados\r\n    url = f'http://www.uel.br/revistas/uel/index.php/informacao/issue/view/{numeros}'\r\n    source = requests.get(url).text\r\n    soup = BeautifulSoup(source, 'lxml')\r\n    lidos = 0\r\n   \r\n    for texto in soup.find_all('table', class_='tocArticle'):\r\n        link = texto.find('a', attrs={'href': re.compile(\"http://\")})\r\n        link2 = str(link.get('href')[65:70])\r\n        titulo = texto.find('div', class_='tocTitle').text.strip()\r\n        if titulo in 'Informa\u00e7\u00e3o':\r\n            print(titulo)\r\n        autor = texto.find('div', class_='tocAuthors').text.strip().replace('\\t', '')\r\n        print(f'T\u00edtulo do trabalho: {titulo}')\r\n        print(f'Autores: {autor}')\r\n        print(f'Link: {link2}')\r\n        print()\r\n        lidos += 1\r\n        issues_gravados += 1\r\n    return lidos > 0\r\n\r\ndef webscrape_articles(numeros):\r\n    global articles_gravados\r\n    url = f'http://www.uel.br/revistas/uel/index.php/informacao/article/view/{numeros}'\r\n    source = requests.get(url).text\r\n    soup = BeautifulSoup(source, 'lxml')\r\n    lidos = 0\r\n   \r\n    for texto in soup.find_all('div', id='content'):\r\n        resumo = 'None'\r\n        keywords = 'None'\r\n        titulo = texto.find('div', id='articleTitle').text.strip()\r\n        autor = texto.find('div', id='authorString').text.strip()\r\n        resumo = texto.find('div', id='articleAbstract').text.strip().replace('Resumo', '').replace('\\n', '')\r\n        keywords = texto.find('div', id='articleSubject').text.strip().replace('Palavras-chave', '').replace('\\n', '')\r\n        print(f'T\u00edtulo do trabalho: {titulo}')\r\n        print(f'Autores: {autor}')\r\n        print(f'Resumo: {resumo}')\r\n        print(f'Palavras-Chave: {keywords}')\r\n        print()\r\n        csv_writer.writerow([titulo, autor, resumo, keywords])\r\n        lidos += 1\r\n        articles_gravados += 1\r\n    return lidos > 0\r\n\r\n# realiza opera\u00e7\u00f5es com arquivos csv\r\narquivo_csv = open('InfoeInfov2.csv', 'w', encoding='utf-8')\r\ncsv_writer = csv.writer(arquivo_csv)\r\ncsv_writer.writerow(['titulo', 'autor','resumo','keywords'])\r\n\r\nfor item in edicoes:\r\n    webscrape_issues(item)\r\n\r\nprint(f'Total de artigos analisados: {issues_gravados}')\r\n\r\nfor item in links:\r\n    webscrape_articles(item)\r\n\r\narquivo_csv.close()\r\nprint(f'Total de artigos reunidos: {articles_gravados}')\r\n\r\n\r\n\r\n\r\n", "291": "import unittest\nimport sys\nsys.path.append('src/')\nfrom src import ws_nbc, ner_model\n\n'''\nA simple and basic unit test of model and web scrapping.\n'''\n\nclass Testing(unittest.TestCase):\n\n    def test_0_get_unique_results(self):\n        a = {'NAME': ['Kevin', 'Joe'], 'ORGANIZATION': ['Apple','Amazon'], 'LOCATION': ['Canada', 'USA']}\n        test = 'My name is Kevin and my friends name is Joe.  I work for Apple and my friend works for Amazon.  I live in Canada and my friend lives in USA'\n        model = ner_model.Model()\n        ner_results = model.ner(test)\n        b = ner_model.get_unique_results(ner_results)\n        self.assertEqual(a,b)\n\n    def test_1_web_scraper(self):\n        url = 'https://www.nbcnews.com/politics/biden-says-considering-gas-tax-holiday-rcna34419'\n        scraper = ws_nbc.WebScrape(url)\n        content = scraper.scrape_news_article\n        self.assertIsNotNone(content)\n\n    def test_2_n_scraper(self):\n        url = 'https://www.nbcnews.com/'\n        scraper = ws_nbc.WebScrape(url)\n        content = scraper.scrape_n_articles(num_articles=1)\n        self.assertIsNotNone(content)\n\nif __name__ == '__main__':\n    unittest.main()", "292": "import urllib2\nfrom bs4 import BeautifulSoup\nimport sys\nfrom operator import itemgetter\nimport random\n\n#------------------------For webscrape 1\npage_count_acm=1\n#------------------------For webscrape 1\n\ndictionary = {}\ndkeys={}\nsearchvalue = []\nconcat_string =''\n\n#------------------------For webscrape 2\npage_count_indeed = 0\nc_indeed = 1\ntest_indeed = 0\ncount_total_indeed = 0\ncounter_indeed = 1\ncal_page_indeed = 0\nval_indeed = 0\n#------------------------For webscrape 2\n\n#------------------------For webscrape 3\npage_count_ieee = 1\n#------------------------For webscrape 3\n\n#------------------------Dictionary count\ndic_count = 1\n#------------------------Dictionary count\n\n\nclass Environment:\n\t#Scrape ACM website and calculate jacquard for each job\n\tdef webscrape(self,stream,area,field,location):\n\t\tag = Agent()\n\t\tglobal page_count_acm\n\t\tglobal dictionary\n\t\tglobal dic_count\n\t\tif area == \"\":\n\t\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'?page='+str(page_count_acm)\n\t\telif stream == \"\": \n\t\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+area+'?page='+str(page_count_acm)\n\t\telse: quote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'/'+area+'?page='+str(page_count_acm)\n\t\tprint quote_page_acm,'\\n'\n\t\tpage_acm = urllib2.urlopen(quote_page_acm)\n\t\tsoup_acm = BeautifulSoup(page_acm, 'html.parser')\n\t\tbox_acm = soup_acm.find_all('div',attrs={'class':'aiResultsMainDiv'})\n\t\ttemp_acm = soup_acm.find('span',attrs={'class':'aiPageTotalTop'}) \n\t\tif  temp_acm == None:\n\t\t\tself.webscrape1(stream,area,field,location)\n\t\tcount_total_acm = int(temp_acm.get_text())\n\t\tfor bx_acm in box_acm:\n\t\t\ttitle_acm = str(bx_acm.find('div',attrs={'class':'aiResultTitle'}).get_text().strip().encode('utf-8'))\n\t\n\t\t\turl_acm = 'http://jobs.acm.org'+str(bx_acm.find('div',attrs={'class':'aiResultTitle'}).find('h3').find('a').get('href').encode('utf-8'))\n\t\t\n\t\t\tdetails_acm = bx_acm.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find_all('li')\n\t\t\n\t\t\tcompany_acm = str(details_acm[0].get_text().strip().encode('utf-8'))\n\t\t\n\t\t\tlocation_acm = str(details_acm[1].get_text().strip().encode('utf-8'))\n\t\t\n\t\t\tdate_acm = str(details_acm[2].get_text().strip().encode('utf-8'))\n\t\t\n\t\t\tif bx_acm.find('li',attrs={'id':'searchResultsCategoryDisplay'}) != None:\n\t\t\t\tcategory_acm = str(details_acm[3].get_text().strip().encode('utf-8'))\n\t\t\telse :\n\t\t\t\tcategory_acm = 'None'\n\t\t\tif bx_acm.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}) == None:\n\t\t\t\tdescription_acm = str(bx_acm.find('div',attrs={'class':'aiResultsDescription'}).get_text().strip().encode('utf-8'))\n\t\t\telse :\n\t\t\t\tdescription_acm = str(bx_acm.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip().encode('utf-8'))\n\t\t\n\t\t\n\n\t\t\tdictionary[dic_count] = [title_acm,company_acm,location_acm,date_acm,category_acm,description_acm,url_acm,0,'x']\n\t\t\tag.jacard(field,location,dic_count)\n\t\t\tdic_count = dic_count+1\n\n\t\t\n\t\tif (page_count_acm < count_total_acm):\n\t\t\tpage_count_acm = page_count_acm + 1\n\t\t\tself.webscrape(stream,area,field,location)\n\t\n\t\n\n\n\t#scrape Indeed website and calculate jacquard for each job\n\tdef webscrape1(self,stream,area,field,location):\n\t\tag = Agent()\n\t\tglobal page_count_indeed\n\t\tglobal c_indeed\n\t\tglobal test_indeed\n\t\tglobal count_total_indeed \n\t\tglobal counter_indeed\n\t\tglobal cal_page_indeed\n\t\tglobal val_indeed\n\t\tglobal dictionary\n\t\tglobal dic_count\n\t\t\n\t\tquote_page_indeed = 'https://www.indeed.com/jobs?q='+stream+'&l='+area+'&start='+str(page_count_indeed)\n\t\tprint quote_page_indeed,'\\n'\n\t\tpage_indeed = urllib2.urlopen(quote_page_indeed)\n\t\tsoup_indeed = BeautifulSoup(page_indeed, 'html.parser')\n\t\t\n\t\t\n\t\tbox_indeed = soup_indeed.find_all('div',attrs={'class':'row'})\n\t\ttemmp_indeed = soup_indeed.find('div',attrs={'id':'searchCount'})\n\t\tif temmp_indeed == None:\n\t\t\tself.webscrape2(stream,area,field,location)\n\t\tif test_indeed == 0:\n\t\t\tcount_total_indeed = int(temmp_indeed.get_text().split()[5].replace(',',''))\n\t\t\twhile val_indeed <= count_total_indeed:\n\t\t\t\tcal_page_indeed = cal_page_indeed + 1\n\t\t\t\tval_indeed = 25 * cal_page_indeed\n\t\t\ttest_indeed = 1\n\t\t\tcal_page_indeed = cal_page_indeed + 1\n\t\t\tif cal_page_indeed > 50:\n\t\t\t\tcal_page_indeed = 50\n\t\t\n\t\tfor bx_indeed in box_indeed:\n\t\t\ttitle_indeed = str(bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get_text().strip().encode('utf-8'))\n\t\t\t\n\t\t\tcompany_indeed = str(bx_indeed.find('span',attrs={'class':'company'}).get_text().strip().encode('utf-8'))\n\t\t\t\n\t\t\tlocation_indeed = str(bx_indeed.find('span',attrs={'class':'location'}).get_text().strip().encode('utf-8'))\n\t\t\t\n\t\t\tdescription_indeed = str(bx_indeed.find('span',attrs={'class':'summary'}).get_text().strip().encode('utf-8'))\n\t\t\t\n\t\t\tdate_indeed = 'None'\n\t\t\tcategory_indeed = 'None'\n\t\t\turl_indeed = str(bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get('href').encode('utf-8'))\n\t\t\n\t\t\tdictionary[dic_count] = [title_indeed,company_indeed,location_indeed,date_indeed,category_indeed,description_indeed,url_indeed,0,'x']\n\t\t\tag.jacard(field,location,dic_count)\n\t\t\tdic_count = dic_count +1\n\n\n\t\t\n\t\tif (counter_indeed", "293": "from channels.consumer import SyncConsumer\nfrom time import sleep\nfrom tasks.webscrape_li import linkedin_search\nfrom tasks.webscrape_search import definition_search\nfrom tasks.word_collect import word_collection\n\nclass BackgroundTaskConsumer(SyncConsumer):\n\n    # This is here for testing purposes\n    def test_wait(self, message):\n        if 'wait' not in message.keys():\n            raise ValueError('message must include wait key')\n\n        if not isinstance(message['wait'], int):\n            raise ValueError('wait value must be an integer')\n\n        print(f\"Task: test_wait has begun with wait value: {message['wait']}\")\n        sleep(message['wait'])\n        print(f\"Task: test_wait has ended\")\n\n    # This will be used to scrape Linkedin\n    def scrape_linkedin(self, message):\n\n        if 'url' not in message.keys():\n            raise ValueError('message must include url key.')\n\n        print(f\"Task: Linkedin search scraping has begun with url value: {message['url']}\")\n        linkedin_search(URL=message['url'], limit=message['limit'])\n        print(f\"Task: Linkedin search scraping has ended.\")\n\n# This will be used to scrape google for a word definition\n    def scrape_definition(self, message):\n\n        if 'limit' not in message.keys():\n            raise ValueError('message must include limit key.')\n\n        print(f\"Task: Definition search scraping has begun with limit: {message['limit']}\")\n        definition_search(word_id=message['word_id'], limit=message['limit'])\n        print(f\"Task: Definition search scraping has ended.\")\n\n    # This will be used to scrape Linkedin\n    def word_collect(self, message):\n\n        if 'limit' not in message.keys():\n            raise ValueError('message must include limit key.')\n\n        print(f\"Task: Word collection has begun with limit: {message['limit']}\")\n        word_collection(limit=message['limit'])\n        print(f\"Task: Word collection has ended.\")\n\n\n", "294": "#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\n# Uncomment these if you are running this for the first time\n\n#curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n\n#python get-pip.py\n\n\n# In[2]:\n\n\n#/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n\n\n# In[3]:\n\n\n#IF ON MAC, UNCOMMENT\n\n#brew cask install chromedriver\n\n\n# In[4]:\n\n\nimport bs4\nfrom bs4 import BeautifulSoup as soup\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# In[5]:\n\n\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.keys import Keys\n\n\n# In[6]:\n\n\nprint(' ')\nprint('Enter NAICS Code(s):','(enter codes with comma in between and no spapes)')\nnaics_input = input('Code(s): ')\nnaics_str = str(naics_input)\n\nprint(' ')\nprint('Enter Keyword(s):','(enter keywords with comma in between and no spapes)')\nkeyword_input = input('Keyword(s): ')\nkeyword = str(keyword_input)\n\nif ',' in keyword:\n    keyword = keyword.split(',')\nelse:\n    keyword = [keyword]\nprint(keyword)\n\ndates = {'Inactive Date': 0, 'Published Date': 0, 'Updated Date': 0}\n\ndef set_dates():\n    for date in dates:\n        print(' ')\n        print(date + ': (enter number for option you want)')\n        print('[0] Any Time')\n        print('[1] Past Day')\n        print('[2] Past 2 Days')\n        print('[3] Past 3 Days')\n        print('[4] Past Week')\n        print('[5] Past Month')\n        print('[6] Past 3 Months')\n        print('[7] Past Year')\n        try:\n            dates[date] += int(input('Time Period: '))\n        except:\n            print('ERROR: You must enter a number 0-7')\n            dates[date] += int(input('Time Period: '))\n\nset_dates()\ninactive_date, published_date, updated_date = [dates[i] for i in dates]\n\n# In[8]:\n\ntoday = datetime.today().strftime('%m/%d/%y')\nm_today, d_today, y_today = today.split('/')\ntoday_list = today.split('/')\n\nprint(' ')\nprint('Response/Date Offers Due: (enter number for option you want)')\nprint('[0] Any Time')\nprint('[1] Custom Day')\ncustom_pref = input('Due Date Preference: ')\n\nif int(custom_pref):\n    print(' ')\n    print('What would you like the last possible due date be (pick a date in the future, give answer as \"MM/DD/YY\")')\n    custom_day = input('Custom Date: ')\n    if len(custom_day) != 8 or len(custom_day.split('/')) !=3:\n        print('ERROR: Incorrect Format, Try Again')\n        custom_day = input('Custom Date: ')\n\n    m_custom, d_custom, y_custom = custom_day.split('/')\n\n    today_time_dict = {'day': [d_today], 'month': [m_today], 'year': ['20' + y_today]}\n    custom_time_dict = {'day': [d_custom], 'month': [m_custom], 'year': ['20' + y_custom]}\n\nelse:\n    custom_time_dict = 0\n\noption = webdriver.ChromeOptions()\n\noption.add_argument('--ignore-certificate-errors')\noption.add_argument('--incognito')\noption.add_argument('--headless')\n\ndriver = webdriver.Chrome(options=option)\n\n# In[7]:\n\nwait = WebDriverWait(driver, 10)\n\n# In[9]:\n\n\noption_pairs = [[0, 'Any Time'], [1, 'Past day'], [2, 'Past 2 days'], [3, 'Past 3 days'], [4, 'Past week'],\n               [5, 'Past month'], [6, 'Past 3 months'], [7, 'Past year']]\n\n\n# In[10]:\n\n\ntime_filters = {0: inactive_date, 1: published_date, 2: updated_date} #, 3: due_data}\n\n\noption_pairs = {0: 'Any Time', 1: 'Past day', 2: 'Past 2 days', 3: 'Past 3 days', 4: 'Past week',\n               5: 'Past month', 6: 'Past 3 months', 7: 'Past year'}\n\n\ntime_filters = {your_key: int(time_filters[your_key]) for your_key in time_filters if time_filters[\n    your_key] != 0}\n\n\ndef run_filters(link, filters, keywords, custom_dict):\n    driver.get(link)\n    if not filters and not keywords and not custom_dict:\n        return soup(driver.page_source, 'html5lib')\n    for i in filters:\n        select = Select(driver.find_elements_by_xpath('//select[@id=\"time span\"]')[i])\n        select.select_by_visible_text(option_pairs[filters[i]])\n        if i == 0:\n            submit_button = driver.find_elements_by_xpath(\n                '//button[@class=\"sam button tertiary small ng-star-inserted\"]')[2]\n        else:\n            submit_button = driver.find_elements_by_xpath(\n                '//button[@class=\"sam button tertiary small ng-star-inserted\"]')[1]\n\n        submit_button.click()\n\n        try:\n            wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'row')))\n        except:\n            print(' ')\n            print('No Results For Given Search')\n            return\n\n        #keyword search\n\n    if type(custom_dict) == dict:\n\n        select = Select(driver.find_elements_by_xpath('//select[@id=\"time span\"]')[3])\n        select.select_by_visible_text('Custom Date')\n\n        wait.until(EC.element_to_be_clickable((By.XPATH, '//div[@class = \"usa-date-of-birth date-group\"]')))\n\n        driver.find_elements_by_xpath('//div[@class = \"usa-date-of-birth date-group\"]')[0].click()\n\n        for time in today_time_dict:\n            driver.find_elements_by_xpath(\"//input[@name = 'date \" + time +  \"']\")[0].send_keys(today_time_dict[time])\n\n        driver.find_elements_by_xpath('//div[@class = \"usa-date-of-birth date-group\"]')[1].click()\n\n        for time in custom_dict:\n            driver.find_elements_by_xpath(\"//input[@name = 'date \" + time +  \"']\")[1].send_keys(custom_dict[time])\n\n        which_box = len(filters) + 5\n\n        filter_button = driver.find_elements_by_xpath(\n           '//button[@class=\"sam button tertiary small ng-star-inserted\"]')[which_box]\n\n        filter_button.click()\n\n        try:\n            print(driver.page_source)\n            wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'row')))\n        except:\n            print(' ')\n            print('No Results For Given Search')\n            return\n\n    for kw in keywords:\n        wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'textarea#keywordSearchName-ac-textarea'))).click()\n        wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'textarea#keywordSearchName-ac-textarea'))).send_keys(kw)\n        driver.find_element_by_id(\"keywordSearchName-ac-textarea\").send_keys(Keys.RETURN)\n\n        print(' ')\n\n        try:\n            wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'row')))\n        except:\n            print(' ')\n            print('No Results For Given Search')\n            return\n\n    #due date\n    #selected_source = driver.page_source\n    #selected_soup = soup(selected_source, 'html5lib')\n    return #selected_soup\n\n\n# In[15]:\n\n\n#dd = 'https://beta.sam.gov/search?index=opp&naics='+ naics_str +'&page=1'\n#selected_soup = run_filters(dd, time_filters)\n\n\n# In[16]:\n\n\ndef webscrape(naics, counter_start = 0): #source, counter_start = 0):\n\n    naics = int(naics)\n\n    wait\n\n    try:\n        wait.until(EC.presence_of_element_located((By.XPATH, '//list-results-message')))\n        selected_source = driver.page_source\n        selected_soup = soup(selected_source, 'html5lib')\n        source = selected_soup\n        results_shown = source.find('list-results-message').text\n\n    except:\n        print(' ')\n        print('No Results For Given Search')\n        return\n\n        print(driver.current_url)\n        print('FAILED, TRY AGAIN')\n        #run_filters(dd, time_filters)\n        return 'FAILED, TRY AGAIN'#webscrape(naics, counter_start)\n\n    if ' ' in results_shown[12:14]:\n        first_results =  int(results_shown[12])\n        total_results = int(results_shown[17])\n        num_pages = 1\n    else:\n        first_results =  int(results_shown[12:14])\n        total_results = int(results_shown[18:21])\n        if (int(results_shown[18:21]) % 10) != 0:\n            num_pages = total_results // first_results + 1\n        else:\n            num_pages = total_results // first_results\n\n    scraped = pd.DataFrame({'Contract Name': [], 'NAICS': [], 'Contract Link':[],\n                            'Description': [], 'Department/Ind. Agency': [], 'Sub-tier': [], 'Office': [],\n                            'Notice ID': [], 'Current Date Offers Due': [], 'Last Updated Date': [],\n                            'Last Published Date': [],'Type': [], 'Current Response Date': [], 'Awardee': [],\n                            'Product Service Code': []\n                           })\n\n    counter = counter_start\n\n    print('Total of ' + str(total_results) + ' results.')\n    print(' ')\n\n    for page in range(1, num_pages + 1):\n\n        page_num_len = len(str(page))\n        beta_sam = driver.current_url.split('page=')[0] + 'page=' + str(page) + driver.current_url.split(\n            'page=')[1][page_num_len:]\n\n        driver.get(beta_sam)\n\n        wait.until(EC.presence_of_element_located(\n            (By.XPATH, '//list-results-message[@class=\"ng-tns-c3-1 ng-star-inserted\"]')))\n        sam_source = driver.page_source\n        sam_soup = soup(sam_source, 'html5lib')\n\n        rows = sam_soup.findAll(class_ = 'sam-ui grid')\n\n        print(str(round(100 * (page - .5)/num_pages, 2)) + '% Done with ' + str(\n            naics) + ', total of ' + str(num_pages) + ' pages')\n\n        for row in rows:\n\n            #outer page\n            scraped.loc[counter, 'Contract Name'] = row.div.div.h3.a.text\n            scraped.loc[counter, 'Contract Link'] = ('https://beta.sam.gov' + row.div.div.h3.a['href']).split('?index=')[0]\n\n            description = [i.text for i in row.div.div.span.findAll('p')]\n\n            if len(description) == 1:\n                description = description.pop()\n            elif len(description) == 0:\n                description = None\n            else:\n                description = ', '.join(description)\n\n            scraped.loc[counter, 'Description'] = description\n\n            for i in row.findAll(class_ = 'sam-ui small list')[0].findAll('li'):\n                if i.a:\n                    scraped.loc[counter, i.strong.text.strip()] = i.a.text.strip()\n                else:\n                    scraped.loc[counter, i.strong.text.strip()] = i.span.text.strip()\n\n            scraped.loc[counter, 'Office'] = row.div.div.ul.findAll('span')[0].text\n\n            for j in row.findAll(class_ = 'four wide column')[0].findAll('li'):\n                if j.span.text.strip() != 'Contract Opportunities':\n                    scraped.loc[counter, j.strong.text.strip()] = j.span.text.strip()\n\n            #inner page\n            driver.get(scraped.loc[counter, 'Contract Link'])\n\n            WebDriverWait(driver, 10).until(EC.presence_of_element_located(\n                (By.XPATH, '//div[@class=\"sam-ui padded raised segment\"]')))\n            inner_source = driver.page_source\n            inner_soup = soup(inner_source, 'html5lib')\n\n            scraped.loc[counter, 'Product Service Code'] = inner_soup.find(\"li\", {\n                \"id\": \"classification-classification-code\"}).text[22:]\n            scraped.loc[counter, 'NAICS'] = inner_soup.find(\"li\", {\n                \"id\": \"classification-naics-code\"}).text[12:]\n            scraped.loc[counter, 'Primary Point of Contact (PPOC)'] = inner_soup.find(\"li\", {\n                \"id\": \"contact-primary-poc-full-name\"}).text\n            scraped.loc[counter, 'PPOC Email'] = inner_soup.find(\"li\", {\n                \"id\": \"contact-primary-poc-email\"}).a['href']\n            try:\n                scraped.loc[counter, 'PPOC Phone'] = inner_soup.find(\"li\", {\n                    \"id\": \"contact-primary-poc-phone\"}).text[16:]\n            except:\n                scraped.loc[counter, 'PPOC Phone'] = np.nan\n\n            driver.back()\n\n            counter += 1\n\n        #progress report\n        print(str(round(100 * page/num_pages, 2)) + '% Done with ' + str(naics) + ', total of ' + str(\n            num_pages) + ' pages')\n\n    #if it is too short\n    if scraped.shape[0] != int(total_results):\n        print('ERROR: Trying again now')\n        print('Length is ' + str(scraped.shape[0]) + ', should be ' + total_results)\n        return webscrape(naics, counter_start)\n    scraped.fillna('None Given', inplace = True)\n    return scraped\n\n# In[17]:\n\n\ndef run_webscrape(naics):\n    master_scraped = pd.DataFrame({'Contract Name': [], 'NAICS': [], 'Contract Link':[],\n                                    'Description': [], 'Department/Ind. Agency': [], 'Sub-tier': [], 'Office': [],\n                                    'Notice ID': [], 'Current Date Offers Due': [], 'Last Updated Date': [],\n                                    'Last Published Date': [],'Type': [], 'Current Response Date': [], 'Awardee': []\n                                   })\n    if ',' in naics:\n        naics = [int(i) for i in naics.split(',')]\n        for code in naics:\n            print(' ')\n            print('Scraping ' + str(code))\n            dd = 'https://beta.sam.gov/search?index=opp&naics='+ str(code) +'&page=1'\n            selected_soup = run_filters(dd, time_filters, keyword, custom_time_dict)\n            wait\n            master_scraped = master_scraped.append(\n            webscrape(code, selected_soup, master_scraped.shape[0]))\n        return master_scraped\n    else:\n        dd = 'https://beta.sam.gov/search?index=opp&naics='+ naics +'&page=1'\n        selected_soup = run_filters(dd, time_filters, keyword, custom_time_dict)\n        return webscrape(naics)#, selected_soup)\n\n\n# In[18]:\n\n\nscraped = run_webscrape(naics_str)\nscraped\n\n\n# In[ ]:\n\ntry:\n    scraped.loc[1,:]\n    with pd.ExcelWriter('webscraped.xlsx') as writer:\n        scraped.to_excel(writer, sheet_name='Sales Data')\n\nexcept:\n    None\n# In[ ]:\n", "295": "'''\nCreated on Aug 7, 2021\n\n@author: 4king\n'''\n\nfrom urllib.request import urlopen, Request\nimport textwrap\nfrom webscrape.searchresults.categorydetails.albumdetail.albumparse.album_parser import album_parser\nfrom webscrape.searchresults.categorydetails.albumdetail.albumdisplay import album_user_reviews\nfrom webscrape import clear\n\ndef view_album_details(album_link):\n    #clear command line\n    clear()\n    #obtain the html for the album page\n    req = Request(album_link, headers={'User-Agent': 'Mozilla/5.0'})\n    page = urlopen(req).read()\n    html_album = page.decode(\"utf-8\")\n    \n    #create the parser and parse each element\n    parser = album_parser(html_album)\n    name = parser.get_name()\n    \n    summary = parser.get_summary()\n    summary = summary.replace('', '\\n')\n    credit = parser.get_credit()\n    \n    #print the results to the command line\n    print(name)\n    #create the formatting for the summary\n    body = '\\n\\n'.join(['\\n'.join(textwrap.wrap(line, 100,\n                 break_long_words=False, replace_whitespace=False))\n                 for line in summary.splitlines() if line.strip() != ''])\n    print(body)\n    print(\"\\n\" + credit + \"\\n\\n\")\n    \n    \n    #prompt the user to view the reviews of the album\n    answer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n    \n    #while the answer is r\n    while (answer == \"r\"):\n        \n        album_user_reviews.view_album_user_reviews(album_link + \"/user-reviews\")\n        #clear command line output\n        clear()\n        #print the results to the command line\n        print(name)\n        print(body)\n        print(\"\\n\" + credit + \"\\n\\n\")\n        \n        #prompt the user to view the reviews of the movie\n        answer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n    \n    \n    ", "296": "'''\nCreated on Aug 7, 2021\n\n@author: Jacob Summers\n'''\n\nfrom urllib.request import urlopen, Request\nimport textwrap\nfrom webscrape.searchresults.categorydetails.gamedetail.gameparse.game_parser import game_parser\nfrom webscrape.searchresults.categorydetails.gamedetail.gamedisplay import game_user_reviews\nfrom webscrape import clear\n\ndef view_game_details(game_link):\n    \n    #clear command line\n    clear()\n    \n    #obtain the html code for the game page\n    req = Request(game_link, headers={'User-Agent': 'Mozilla/5.0'})\n    page = urlopen(req).read()\n    html_game = page.decode(\"utf-8\")\n    \n    \n    #create the parser and parse each element\n    parser = game_parser(html_game)\n    name = parser.get_name()\n    \n    summary = parser.get_summary()\n    summary = summary.replace('', '\\n')\n    credit = parser.get_credit()\n    \n    #print the results to the command line\n    print(name)\n    #create the formatting for the summary\n    body = '\\n\\n'.join(['\\n'.join(textwrap.wrap(line, 100,\n                 break_long_words=False, replace_whitespace=False))\n                 for line in summary.splitlines() if line.strip() != ''])\n    print(body)\n    print(\"\\n\" + credit + \"\\n\\n\")\n    \n    \n    #prompt the user to view the reviews of the movie\n    answer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n    \n    #while the answer is r\n    while (answer == \"r\"):\n        \n        game_user_reviews.view_game_user_reviews(game_link + \"/user-reviews\")\n        #clear the command line\n        clear()\n        #print the results to the command line\n        print(name)\n        print(body)\n        print(\"\\n\" + credit + \"\\n\\n\")\n        \n        #prompt the user to view the reviews of the movie\n        answer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n    \n    ", "297": "#import scrappy\nimport scrapy\nfrom ..items import WebscrapeItem\nfrom scrapy.loader import ItemLoader\n# Import the CrawlerProcess: for running the spider\nfrom scrapy.crawler import CrawlerProcess\n\nclass OpenDeltaCrawler(scrapy.Spider):\n    name = \"forbeCrawler\"\n\n    def start_requests(self):\n        url = \"https://www.forbes.com/sites/billybambrough/?sh=28b3aa666a89\"\n        #headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n        yield scrapy.Request(url, callback = self.parse_blog)\n\n    def parse_blog(self,response):\n\n        # look for more articles button\n        more_articles = response.css('').attrib['href']\n\n\n        # Go to the blocks that contain blog posts\n        blog_posts = response.xpath('//h3[contains(@class,\"title list-title m0005\")]')\n        # Go to the blog links\n        blog_links = blog_posts.xpath('./a/@href')\n        print(blog_links)\n        # Extract the links (as a list of strings)\n        links_to_follow = blog_links.extract()\n\n        # look for more articles button\n        more_articles = response.css('').attrib['href']\n\n        # Follow the links in the next parser\n        for url in links_to_follow:\n            #headers = {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n            yield response.follow(url=url, callback=self.parse_pages)\n\n    def parse_pages(self, response):\n        i = ItemLoader(item=WebscrapeItem(), selector=response)\n        i.add_css('blog_title','h1.article-headline')\n        i.add_css('blog_excerpt','div.article-excerpt')\n        i.add_xpath('blog_author','//div[@class=\"article-meta grey-text\"]/span[1]/span')\n        i.add_xpath('blog_publish_date','//div[@class=\"article-meta grey-text\"]/span[2]')\n        i.add_xpath('blog_text','//p')\n        i.add_value('blog_url',response.url)\n        yield i.load_item()\n\n\n        # item = WebscrapeItem()\n        # # Direct to the blog title text\n        # blog_title = response.css('h1.article-headline::text')\n        # # Exctract and clean the blog title text\n        # blog_title_clean = blog_title.extract_first().strip()\n        # # Direct to the Exceprt\n        # blog_excerpt = response.css('div.article-excerpt::text')\n        # # Extract and clean the blog exceprt\n        # blog_excerpt_clean = blog_excerpt.extract_first().strip()\n        # # Direct to Blog Author\n        # blog_author = response.xpath('//div[@class=\"article-meta grey-text\"]/span[1]/span/text()')\n        # # Extract and clean the author\n        # blog_author_clean = blog_author.extract_first().strip()\n        # # Direct to publish\n        # blog_publish_date = response.xpath('text()')\n        # # Extract and clean the publish date\n        # blog_publish_date_clean = blog_publish_date.extract_first().strip()\n        # print(blog_publish_date_clean)\n        # print(blog_author_clean)\n        # # code to parse blog posts\n\n\n", "298": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri May 29 15:07:07 2020\n\n@author: Hayden Rampadarath (haydenrampadarath@gmail.com)\n\nScript to webscrape My Anime List\n\"\"\"\n\n\nimport urllib\nimport requests\nimport bs4\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re\nfrom tqdm import tqdm\nfrom time import sleep\nfrom bs4.element import NavigableString, Tag\n\n\n\ndef extractNavigableStrings(context):\n    \"\"\" from https://stackoverflow.com/questions/29110820/how-to-scrape-between-span-tags-using-beautifulsoup\"\"\"\n    strings = []\n    for e in context.children:\n        if isinstance(e, NavigableString):\n            strings.append(e)\n        if isinstance(e, Tag):\n            strings.extend(extractNavigableStrings(e))\n    return strings\n\n\n\ndef parse_MAl(url):\n    \"\"\"\n    Parameters\n    ----------\n    url : string\n        myanimelist.net url string \n\n    Returns\n    -------\n    df : DataFrame\n        returns a dataframe with columns \"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"\n\n    \"\"\"\n    html = requests.get(url)\n    soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n    results = soup.find_all(class_= \"ranking-list\")\n    \n    df = pd.DataFrame(columns=[\"name\",\"english_name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\", \"url\"])\n    i = 0\n    for result in results:\n        #print(i)\n        url_= result.find(class_=\"hoverinfo_trigger fl-l fs14 fw-b\")[\"href\"]\n        html_ = requests.get(url_)\n        soup_ = BeautifulSoup(html_.content, 'html.parser', from_encoding=\"utf-8\")\n        \n        t1name = extractNavigableStrings(soup_.find(class_=\"h1-title\"))\n        if len(t1name) == 1:\n            name = t1name[0]\n            english_name = None\n        elif len(t1name) >= 2:\n            name=t1name[0]\n            english_name=t1name[1]\n        else:\n            name = None\n            english_name = None\n            \n        Type, Dates, members = result.find(class_=\"information di-ib mt4\").text.strip().splitlines()\n        try:\n            members = float(\"\".join(members.split()[0].split(\",\")))\n        except:\n            members = None\n            \n        [Type_, eps, n] = [\", \".join(x.split()) for x in re.split(r'[()]',Type)]\n        \n        try:\n            eps = float(eps.split(\",\")[0])\n        except:\n            eps = None\n        \n        try:\n            genres = [genre.text.strip() for genre in soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"genre\")]\n            \n        except:\n            genres = None\n        \n        try:\n            score = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingValue\")[0].text.strip())\n        except:\n            score = None\n        #try:\n        #    score_members = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingCount\")[0].text.strip())\n        #except:\n         #   score_members = None\n        \n        df = df.append({\n            \"name\": name,\n            \"english_name\":english_name,\n            \"type\": Type_,\n            \"episodes\": eps,\n            \"members\": members,\n            #\"score_members\": score_members,\n            \"rating\": score,\n            \"genre\": genres,\n            \"dates\": Dates,\n            \"url\": url_\n        },ignore_index=True)\n        \n        i+=1\n    return df\n\n\ndef webscrape_MAl(anime_limit=16750, start=0):\n    url_template = \"https://myanimelist.net/topanime.php?limit={}\"\n    df = pd.DataFrame(columns=[\"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"])\n    for limit in tqdm(range(start,anime_limit, 50)): # iterate in steps of 50\n        url = url_template.format(limit)\n        df_temp = parse_MAl(url)\n        if df_temp[\"name\"].isnull().sum() >= 40:\n            print(\"Number of missing names, for limit {} = {}\".format(limit, df_temp[\"name\"].isnull().sum()))\n            print(\"--------Halting---------\")\n            raise SystemExit()\n        save_mal_temp(df_temp, limit)\n        \n        # I think MAL has a limit on the number of conenctions per minute/second/hour\n        # and after 200-400, the site blocks access. Adding the pause for 1 minute below soleves the issue\n        sleep(60) # pause the loop for 1 minute. \n        \n\n\n\n\n\ndef save_mal_temp(df, limit):\n    csvTemp = \"temp/MAL_start_{}.csv\".format(limit)\n    df.to_csv(csvTemp)\n        \n    #print(\"Number of missing names, for limit {} = {}\".format(limit, df[\"name\"].isnull().sum()))\n    \n    \n\nwebscrape_MAl(anime_limit = 16750, start = 16550)", "299": "\"\"\" Module to scrape wikipedia using wikipedia wikipediaapi\"\"\"\n\nfrom utils import Scraper\nfrom utils import wikiSectionTitles\nimport wikipediaapi as wiki\n\n\n# ------------------Classes\n\nclass wikiScraper(Scraper):\n    \"\"\"class holding input variables and retrieving results from wikipedia\"\"\"\n\n    def __str__(self):\n        attrDict = str(self.__dict__)\n        return f'wiki Scraper object named {self.journalName} with the following attributes {attrDict}'\n\n    def webscrapeWikipedia(self):\n        \"\"\"wrapping function to scrape wikipedia\"\"\"\n        self.checkForMainAttr()\n        if self.journalName == 'wikipedia':\n            self.wikiPage = self.buildWikiPage(self.plantName)\n            self.sectionContents = self.getWikiSections()\n            if \"Summary\" in self.sectionTitles:\n                self.sectionContents[\"Summary\"] = self.wikiPage.summary\n            else:\n                pass\n        else:\n            print(\"journal is not wikipedia\")\n\n\n    def buildWikiPage(self, plantName):\n        \"\"\"build wikipage using wikip. api with checks\"\"\"\n        plantName= self.plantName\n        wiki_wiki = wiki.Wikipedia('en')\n        page_py = wiki_wiki.page(plantName)\n        if page_py.exists():\n            return page_py\n        else:\n            print(f'could not find wikipediaPage for {plantName}')\n            return False\n\n    def getWikiSections(self):\n        \"\"\"extract specific section from wiki page\"\"\"\n        wikiOnlineSections = [i for i in self.wikiPage.sections if i.title.title() in self.sectionTitles]\n        dic = {}\n        for v in wikiOnlineSections:\n            k = v.title\n            dic[k] = v.text\n        return dic\n\nif __name__ == \"__main__\":\n    test = wikiScraper(\"wikipedia\", 'Parthenium', wikiSectionTitles)\n    print(test.__dict__)\n    test.webscrapeWikipedia()\n    print(test.sectionContents)\n    print(test)\n", "300": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Nov 13 11:57:02 2018\n\nBlackbox Webscrape\n\n@author: kyrie\n\"\"\"\n\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport numpy as np\n\n\ndef blackbox_webscrape(sim_num, endstep,intervals):\n    current_step = 1 # starting step #\n\n    url = \"https://www.informatics.indiana.edu/jbollen/I501F18/blackbox/BlackBox_N.php\" # Blackbox URL\n    driver = webdriver.Chrome('C:/Users/Kyrie/GitHub/I501Blackbox/chromedriver')  # Location of chrome driver on Kyrie's Desktop\n    #driver = webdriver.Chrome('C:/Users/kyrie/OneDrive/Documents/GitHub/I501Blackbox/chromedriver')  # Location of chrome driver on Kyrie's Laptop\n    #driver = webdriver.Firefox(executable_path='/usr/local/bin/geckodriver') #this is for Becca's Macbook. Comment this out and do the other drivers for Kyrie.\n    \n    driver.get(url) # Get URL\n    \n    driver.find_element_by_name(\"cycles\").clear() # clear current interval amount\n    driver.find_element_by_name(\"cycles\").send_keys(intervals) # Set to chosen interval amount\n        \n    while current_step < endstep: # Run until end step \n\n        soup = BeautifulSoup(driver.page_source) # read page source\n        \n        table1 = soup.find_all('table',id=\"system\")[0] # find blackbox table in source\n        \n        find_step = soup.find_all('p') # find location of current step in source\n        current_step = int(find_step[3].contents[0][13:]) # Finds current step in integer format\n        \n        #Find all of the numbers in the 20 x 20 grid in the page source and store to my_table\n        rows = table1.findChildren(['th', 'tr'])\n        \n        my_table = []\n        i = 0\n        j = 0\n\n        for row in rows:\n            i += 1\n            cells = row.findChildren('td')\n            for cell in cells:\n                j += 1\n                if j == 21: # Reset to 1 after 20\n                    j= 1\n                value = cell.string\n                my_table.append(value)\n        \n        my_table2 = np.reshape(my_table, (20,20)).astype(int) # Reshape to 20 x 20 array\n        \n        # Save array to text file with simulation # and step # in filename\n        output_file = 'sim' +str(sim_num)+ '_step' + str(current_step) + '_int' + str(intervals) + '.txt'\n        np.savetxt(output_file, my_table2, delimiter=',',fmt= '%d') \n        \n        # Print status\n        print('Simulation ' + str(sim_num) + ': Step ' + str(current_step) + ' file saved.')\n        \n        time.sleep(5) # wait 5 seconds to allow for saving file\n        \n        # Click next button on page\n        button = driver.find_element(By.XPATH, '//button[text()=\"Next n Step\"]')\n        button.click()\n        \n        time.sleep(5) # wait 5 seconds to wait after clicking Next Step\n        \n\n# Initiates the blackbox function (can tweak the inputs of simulation number, end step and the intervals)            \nblackbox_webscrape(7, 7000, 1)    # blackbox_webscrape(sim_num, endstep,intervals)          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n\n\n", "301": "import logging\nimport os\n\nfrom flask import (\n    Flask,\n    abort,\n    after_this_request,\n    current_app,\n    jsonify,\n    make_response,\n    render_template,\n    request,\n    send_file,\n    send_from_directory,\n)\n\nfrom webscraper.selenium_scrape import setup_webdriver, selenium_clean_up\nfrom webscraper.webscraper import single_site_scrape, mass_webscrape\nfrom webscraper.data_manipulation import file_management as fm\n\napp_name = \"prospect_webscraper\"\nlogging.basicConfig(\n    level=logging.INFO\n    # ,format=f'%asctime)s %(levelname)s %(name)s %(threadName)s : %(message)s' # customise the log formatting here\n)  # initialise the python logger configs and default logging level to none\nlogger = logging.getLogger(app_name)  # creates the app logger\napp = Flask(app_name)\napp.config[\"UPLOAD_EXTENSIONS\"] = [\".csv\", \".xls\", \".xlsx\"]\n\n\n@app.route(\"/\")\ndef index():\n    return render_template(\"index.html\")\n\n\n@app.get(\"/single_submission\")\ndef single_submission():\n    return render_template(\"single_submission.html\")\n\n\n@app.get(\"/multi_submission\")\ndef multi_submission():\n    return render_template(\"multi_submission.html\")\n\n\n@app.route(\"/api/single_submission/\", methods=[\"GET\"])\ndef single_api_submission():\n    base_url = request.args.get(\"base_url\")\n    logging.info(f\"Searching for {base_url}\")\n    selenium_driver = setup_webdriver()\n    output = jsonify(single_site_scrape(base_url, selenium_driver))\n    selenium_clean_up(selenium_driver)\n    return make_response(output, 200)\n\n\n@app.route(\"/upload\", methods=[\"POST\"])\ndef upload_file():\n    # check if the post request has the file part\n    uploaded_file = request.files[\"file\"]\n    file = uploaded_file.filename\n    file_name = os.path.splitext(file)[0]\n    if file != \"\":  # this is done on the renderer side too but just for precaution\n        file_extension = os.path.splitext(file)[1]\n        if file_extension not in current_app.config[\"UPLOAD_EXTENSIONS\"]:\n            abort(400)\n        uploaded_file.save(file)\n    df = fm.read_excel_get_url_series(file)\n    # output = fm.data_dict_to_pandas(webscraper.dry_run(df))  # for testing, add this in and remove the mass webscrape\n    os.remove(file)  # remove the uploaded file\n\n    # do the scraping\n    selenium_driver = setup_webdriver()\n    output = fm.data_dict_to_pandas(mass_webscrape(df, selenium_driver))\n    selenium_clean_up(selenium_driver)\n\n    # generate the output\n    file_to_download = f\"output/{file_name}_output{file_extension}\"\n    fm.output_excel_file(\n        f\"{file_to_download}\", output\n    )  # generate the file for the user to download\n    return render_template(\n        \"dataframe_template.html\",\n        tables=output.to_html(),\n        titles=file_name,\n        file_to_download=file_to_download,\n    )\n\n\n@app.route(\"/download/output/\", methods=[\"GET\", \"POST\"])\ndef download_file(file_to_download):\n    @after_this_request\n    def remove_file(response):\n        # FIXME: if the file isn't downloaded then this stays in the output folder\n        try:\n            os.remove(f\"output/{file_to_download}\")\n        except Exception as e:\n            app.logger.error(\"Error removing or closing downloaded file handle\", e)\n        return response\n\n    try:\n        path = f\"output/{file_to_download}\"\n        return send_file(path, download_name=file_to_download, as_attachment=True)\n    except FileNotFoundError:\n        pass\n        return \"\"\"\n        \n         I've seen data you people wouldn't believe... Unstructured data in production databases... I watched users abuse\n         column typing using only varchar. Like the file you requested a second time, those moments will be lost in\n          time, like tears in rain\n          Go back to the home page\n               \n        \n        \"\"\"\n    except Exception as error:\n        return app.logger.error(\"Unable to serve the requested file\", error)\n\n\n@app.get(\"/health\")\ndef health():\n    response = {\"healthcheck\": \"I'm working here\"}\n    return make_response(jsonify(response), 200)\n\n\n@app.route(\"/favicon.ico\")\ndef favicon():\n    return send_from_directory(\n        os.path.join(app.root_path, \"static\"),\n        \"favicon.ico\",\n        mimetype=\"image/vnd.microsoft.icon\",\n    )\n\n\nif __name__ == \"__main__\":\n    app.run(port=8888, debug=True)\n", "302": "import webscrape_module as d\nfrom bs4 import BeautifulSoup as soup\n\nclass CentrisScraper(d.WebScrape):\n    def __init__(self, url, searched):\n        super().__init__(url, searched)\n    \n    def findApparts(self):\n\n        file_rows, address_column, price_column, link_column, img_column = [], [], [], [], []\n        link_start = \"https://centris.ca\"\n        page = self.parsePage()\n        apparts = page.findAll(\"div\", {\"class\":\"shell\"})\n\n        for appart in apparts:\n\n            address = appart.find(\"span\", {\"class\":\"address\"}).text.strip()\n            address_column.append(address)\n            price = appart.find(\"span\", {\"itemprop\":\"price\"}).text.strip()\n            price_column.append(price)\n            link = link_start+appart.find(\"a\")[\"href\"]\n            link_column.append(link)\n            img = appart.find(\"img\", {\"itemprop\":\"image\"})[\"src\"]\n            img_column.append(img)\n\n            print(\"--------------------------------\")\n            print(f\"{address}\")\n            print(f\"Prix: {price}\")\n            print(f\"Lien: {link}\")\n            print(f\"Image: {img}\")\n            print(\"--------------------------------\")\n        \n        \n        file_rows.append(address_column)\n        file_rows.append(price_column)\n        file_rows.append(link_column)\n        file_rows.append(img_column)\n\n        return file_rows", "303": "from django.apps import AppConfig\n\n\nclass WebscrapeAppConfig(AppConfig):\n    name = 'webscrape_app'\n", "304": "from multiprocessing import Process\r\nimport requests\r\nfrom bs4 import BeautifulSoup as soup\r\nimport language_check\r\nimport time\r\nimport random\r\n\r\ntool = language_check.LanguageTool('en-US')\r\n#Declare the file we want to store to\r\n\r\ndef WebScrape(startPage, endPage):\r\n    # fileName = 'super_magic_god_of_harry_potter_langCheck'+ str(startPage) + '_' + str(endPage) +'.txt'\r\n    f= open(\"super_magic_god_of_harry_potter_langCheck751_796.txt\",\"a+\",encoding=\"utf-8\")\r\n\r\n    myUrl = 'http://lnmtl.com/chapter/super-magic-god-of-harry-potter-chapter-'\r\n\r\n\r\n    for page in range(startPage, endPage):\r\n        #traverse the Site\r\n        print(page)\r\n        srhUrl =  myUrl + str(page) + \"/\"\r\n        try:\r\n            uClient =  requests.get(srhUrl)\r\n            uClient.encoding = \"utf-8\"\r\n            html_content = soup(uClient.content, 'html.parser')\r\n\r\n            Body = html_content.findAll(\"div\",{\"class\":'chapter-body'})\r\n\r\n            translatedBody = Body[0].findAll(\"sentence\",{\"class\":'translated'})\r\n            count = 0\r\n            for transdiv in translatedBody:\r\n                temp  = transdiv.text\r\n                temp = temp.replace(\"\\t\", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")\r\n                temp = temp.replace(\"\u201e\",\"\\\"\").replace(\"\u201d\",\"\\\"\")\r\n                matches = tool.check(temp)\r\n                #print(len(matches))\r\n                if(len(matches) > 0):\r\n                    temp = language_check.correct(temp, matches)\r\n                #print(temp)\r\n                if count == 0:\r\n                    f.write(temp)\r\n                    f.write(\"\\n\")\r\n                    count += 1\r\n                else:\r\n                    f.write(temp)\r\n                f.write(\"\\n\")\r\n\r\n            f.write(\"\\n\\n\")\r\n            # f1.write(\"\\n\\n\")\r\n            print(\"Completed page\")\r\n        except:\r\n            continue\r\n        num = random.randint(5, 8)\r\n\r\n        time.sleep(num)\r\n\r\n    f.close()\r\n\r\n\r\nif __name__ == '__main__':\r\n        srtPage = 772\r\n        endPage = 796\r\n        WebScrape(srtPage,endPage)\r\n        \r\n", "305": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Feb  9 20:57:29 2020\n\n@author: Mishaun_Bhakta\n\nThe purpose of this program is to automate the process of preparing and closing \nBureau of Land Management (BLM) federal oil and gas lease sales\n\nThe first phase of the program will visit the website, scrape the lease information\nfor each tract, and parse the information into a template sale note spreadsheet\nIt will also download the shapefile of the sale, which is needed for evaluating tracts on\nDrillingInfo.com\n\nOnce the spreadsheet is prepared and the sale takes place, the program will \nscrape the webpage again and gather information regarding won parcels.\nThe scrape will get the bonus bid for each parcel we won by evaluating our bidder number\n\nFinally, a dataframe will be created based on won lots and the information will be \npassed into a pdf fill form function to create paperwork needed to send to \nBureau of Land Management\n\n\"\"\"\nimport os, re, shutil, time\n\n#sale parameters\nstate = \"Wyoming\"\nstinitials = \"WY\"\ndate = \"Dec 15, 2020\"\nbidder = '3'\nurl = 'https://www.energynet.com/govt_listing.pl?sg=5359'\nbidDuration = 1 #amount of hours the parcel is open for bidding\n\n#Navigate to energynet/govt sale and get sale page\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\n\n#getting filepath of this file\nfilepath = os.path.dirname(__file__)\n\n#driver will be used based on operating system - windows or mac\ntry:\n    driver = webdriver.Chrome(filepath + \"/chromedriver.exe\")\nexcept:\n    driver = webdriver.Chrome(filepath + \"/chromedriver\")\n\n#selenium driver object will go to url of sale\ndriver.implicitly_wait(30)\ndriver.get(url)\n\n#finding the show dropdown menu\nshowButton = driver.find_elements_by_css_selector('#DataTables_Table_0_length > label > select')[0]\n                                                   \n#sending keys to select all lots\nshowButton.send_keys(\"All\")\nshowButton.send_keys(Keys.RETURN)\ntime.sleep(2)\n                                                 \n\n#storing html content in variable after reaching target sale page\nsalehtml = BeautifulSoup(driver.page_source, \"html.parser\")\n\ndef webscrape_presale(parsepage):\n    '''This function will take a page and scrape its data for sale lot information\n    It will also download sale shapefile and move it to directory of this script file\n    '''\n\n    #webscrape sale page - gathering lot serial numbers from html \n    serialnums = parsepage.find_all(\"span\", \"lot-name\")\n    #ist comprehension - appending text into serialnums\n    serialnums = [i.text for i in serialnums]\n    \n    #storing all data from tag 'td's with clas name \"lot-legal\n    #this html container/tag has 3 pieces of information\n    legalinfo = parsepage.find_all(\"td\", \"lot-legal\")\n    \n    lotclosings = parsepage.find_all(\"td\", \"lot-closing\")\n    \n    \n    closingInfo = []\n    for item in lotclosings:\n        #finding date from lot closing element\n        closingDate = re.search(\"\\d+/\\d+/\\d+\", item.text)[0]\n        \n        #finding opening time from lot closing element - then calculate closing time\n        openingTime = re.search(\"\\d+:\\d+\", item.text)[0]\n        #calculating closing time \n        closingTime = str(int(openingTime.split(\":\")[0]) + bidDuration)+\":\" + openingTime.split(\":\")[1]\n        \n        dateAndTime = closingDate + \" \" + closingTime\n        closingInfo.append(dateAndTime)\n    \n    #initializing empty arrays\n    acres = []\n    desc = []\n    county = []\n    \n    for item in legalinfo:\n        county.append(item.contents[0].text)\n        desc.append(item.contents[1].text)\n        #getting acres by splitting at : and blankspace to get string of numerical value - taking out a comma if above 1000 in order to convert to float\n        acres.append(float(re.split(\":\\W\",item.contents[2].text)[1].replace(',','')))\n\n    ##getting shapefile from webpage  \n    #clicking link of where shapefile is stored on sale page  \n    try:\n        driver.find_element_by_link_text(\"GIS Data WGS84\").click()\n        time.sleep(2)\n    except:\n        print(\"shapefile download unable to be clicked from webscraper\")\n        \n    try:\n        driver.find_element_by_link_text(\"Notice of Competitive Oil and Gas Internet-Based Lease Sale\").click()\n    except:\n        print(\"Sale notice pdf was unable to be clicked\")\n    #getting list of filenames in downloads\n    try:\n        downloaddir = \"/Users/Mishaun_Bhakta/Downloads/\"\n        downloads = os.listdir(downloaddir)\n    except:\n        downloaddir = \"C:/Users/mishaun/Downloads/\"\n        downloads = os.listdir(downloaddir) \n        \n    #pattern will find downloaded file name of shapefile\n    pattern = \"BLM\"+ stinitials + \"\\S*.zip\"\n    \n    try:\n        #searching through filenames in downlaods folder\n        finds = []\n        for file in downloads:\n            if re.findall(pattern, file):\n                finds.append(file)\n                break\n            \n        #moving file from downloads folder to directory of this script file - then renaming it to a cleaner name\n        shutil.copy(downloaddir + finds[0], filepath)\n    except:\n        print(\"moving shapefile unable to be moved due to not finding file and/or not able to be clicked\")\n        \n        \n    try:\n        os.rename(finds[0], \"BLM \" + stinitials + \" \" + date + \" Shapefile.\" + finds[0].split(\".\")[1])\n    except:\n       pass\n   \n    return acres, desc, county, serialnums, closingInfo\n\n#storing global variables of web scrape information\nacres, descriptions, counties, serials, closing = webscrape_presale(salehtml)\n\n#Open sale template and update insert information from webscrape\nimport openpyxl\n\ndef fillexcel():\n    '''\n    This function will take scraped (global) values for lots and insert into sale spreadsheet \n    '''    \n    \n    #opening template sale notebook for modifications\n    #preserving vba to keep formatting of workbook preserved - also keeping formulas \n    wb = openpyxl.load_workbook(\"BLM Sale Notes Template.xlsm\", keep_vba = True)\n    sheet = wb.active\n    \n    #updating sheet title to sale title\n    sheet[\"B6\"] = \"BLM {} {} Sale Notes\".format(stinitials, date)\n    \n    #inserting values from webscrape into spreadsheet -8th row is where data rows begin\n    for i in range(0,len(serials)):\n        sheet.cell(row = 8+i, column = 2, value = serials[i])\n        sheet.cell(row = 8+i, column = 5, value = closing[i])\n        sheet.cell(row = 8+i, column = 6, value = acres[i])\n        sheet.cell(row = 8+i, column = 7, value = counties[i])\n        sheet.cell(row = 8+i, column = 8, value = descriptions[i])\n    \n    #checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\n    if os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n        print(\"File already exists - Preventing overwrite of changes in excel file\")\n    else:\n        wb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n        wb.close()\n\n#checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\nif os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n    print(\"File already exists - Preventing overwrite of changes in excel file\")\nelse:\n    fillexcel()\n\nbidtags = salehtml.find_all(\"td\", \"lot-bid\")\nourwinnings = {}\n\nfor i in range (0,len(bidtags)):\n    textCont = bidtags[i].text\n    \n    #extracting bidder number from lot bid tags - try statement prevents break if no bids were received\n    try:\n        winBidder = re.findall('#\\d+', textCont)[0].replace(\"#\",'')\n    except:\n        print(\"no bids received for parcel: \" + serials[i])\n        winBidder = type(None)\n        pass\n    \n    #if we won the bid, then capture the winning bid amount\n    if bidder == winBidder:\n        winAmount = re.findall('\\$\\d+', textCont)[0]\n        winAmount = winAmount.replace('$','')\n    \n        ourwinnings[i] = winAmount\n\ndef fillwinnings():\n    '''This function will take ourwinnings dictionary and add values to created spreadsheet\n    '''\n    \n    #### insert our winnings into sale spreadsheet\n    wb = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), keep_vba = True)\n    sheet = wb.active\n    \n    for i in range(0,len(ourwinnings)):\n        #row 8 is the starting row for parcels in teh spreadsheet, inserting data relative to 8th row by adding parcel number of sale\n        sheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 17, value = ourwinnings[list(ourwinnings.keys())[i]])\n        sheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 16, value = 'Y')\n    \n    wb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n    wb.close()\n\n#create dataframe for completed sale sheet\nimport pandas as pd\n\n#use pdf reader to fill in form\n# conda install -c conda-forge pdfrw\nimport pdfrw\n\n#copied code and function from article: https://bostata.com/how-to-populate-fillable-pdfs-with-python/\n##############################################################################\nANNOT_KEY = '/Annots'\nANNOT_FIELD_KEY = '/T'\nANNOT_VAL_KEY = '/V'\nANNOT_RECT_KEY = '/Rect'\nSUBTYPE_KEY = '/Subtype'\nWIDGET_SUBTYPE_KEY = '/Widget'\n\ndef write_fillable_pdf(input_pdf_path, output_pdf_path, data_dict):\n    '''\n    This function will fill in pdf's forms based on a form pdf\n    '''\n    \n    template_pdf = pdfrw.PdfReader(input_pdf_path)\n    annotations = template_pdf.pages[0][ANNOT_KEY]\n    for annotation in annotations:\n        if annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n            if annotation[ANNOT_FIELD_KEY]:\n                key = annotation[ANNOT_FIELD_KEY][1:-1]\n                if key in data_dict.keys():\n                    annotation.update(\n                        pdfrw.PdfDict(V='{}'.format(data_dict[key]))\n                    )\n    pdfrw.PdfWriter().write(output_pdf_path, template_pdf)\n##############################################################\n\n\ndef wonlotsDF():\n    '''\n    This function will create a dataframe of the won lots by reading information\n    from completed sale note spreadsheet\n    The dataframe will then be used to parse pdf's \n    '''\n    #using openpyxl in order to read formulated values from spreadsheet\n    # NOTE: have to manually open excel and save sheet for formulated cells to read after filling in values\n    data_onlyWB = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), data_only = True, keep_vba = True)\n    dataSheet = data_onlyWB.active\n    \n    #covnerting spreadsheet into dataframe\n    df = pd.DataFrame(dataSheet.values)\n    \n    #slicing the dataframe to get only relevant data\n    df = df.iloc[6:,1:25]\n    #setting columns to first row of dataframe\n    df.columns = df.iloc[0]\n    #dropping the repeated row with column names\n    df = df.drop(index =[6])\n    \n    #filtering data frame with values only won by magnum\n    wonlotsdf = df[df[\"Magnum Won (Y/N)\"] == 'Y']\n    return wonlotsdf\n\n\ndef createBidSheets():\n    '''\n    This function will take a template pdf and generate pdf's based on wonlots dataframe\n    '''\n    #calling funciton wonlotsDF in ordre for bid sheets to be created \n    wonlotsdf = wonlotsDF()\n\n    templatePDF = 'bidsheet template.pdf'\n    \n    for i in range(0,len(wonlotsdf.index)):\n    \n        OutputPath = filepath +\"/Bid Sheets/\" + wonlotsdf.iloc[i][\"Serial numbers\"] + \" Bid Sheet.pdf\"\n        \n        fields = {\n                \"State\": stinitials,\n                \"Date of Sale\": date,\n                'Check Box for Oil and Gas' : \"x\",\n                \"Oil and Gas/Parcel No\" : wonlotsdf.iloc[i][\"Serial numbers\"],\n                \"TOTAL BID FOR Oil and Gas Lease\" : wonlotsdf.iloc[i][\"Total Bid (Number on BLM Bid Sheet)\"],\n                \"PAYMENT SUBMITTED WITH BID for Oil and Gas\" : wonlotsdf.iloc[i][\"Min Due\"],\n                \"Print or Type Name of Lessee\" : \"R&R Royalty, LTD\",\n                \"Address of Lessee\": \"500 N Shoreline Blvd, Ste 322\",\n                \"City\" : \"Corpus Christi\",\n                \"State_2\": \"TX\",\n                \"Zip Code\" : \"78401\"\n                }\n        \n        write_fillable_pdf(templatePDF, OutputPath, fields)\n\ndef openDI():\n    '''This function will open up DrillingInfo and log user in\n    '''\n    \n    driver = webdriver.Chrome(filepath + \"/chromedriver\")\n    wait = WebDriverWait(driver, 20)\n        \n    driver.get(\"https://app.drillinginfo.com/gallery/\")\n    userfield = driver.find_element_by_name(\"username\")\n    passfield = driver.find_element_by_name(\"password\")\n    \n    userfield.click()\n    userfield.send_keys(\"mbhaktamgm\")\n    passfield.click()\n    passfield.send_keys(\"itheCwe\")\n    passfield.send_keys(Keys.RETURN)\n    \n    myworkspaces = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\\\"workspaces-section\\\"]/div[1]/div[1]')))\n    myworkspaces.click()\n    \n    default_workspace = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"workspaces-section\\\"]/div[3]/di-carousel/section/div[2]/table/tbody/tr[2]/a/span[2]/span\")))\n    default_workspace.click()\n    \n#splitting counties in counties variable at the comma+space to formulate string \n#for drilling info paste and filter\n    \nsplitCounties = [item.split(\", \") for item in counties]\n\nDIcounties =  []\nfor item in splitCounties:\n    #taking \"County\" out of word\n    temp = item[0].upper().replace(\"COUNTY\", \"\")\n    formattedCounty = temp + \"(\" + item[1] + \")\"\n    DIcounties.append(formattedCounty)\n\n\n\n        \n    \n", "306": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Feb  9 20:57:29 2020\n\n@author: Mishaun_Bhakta\n\nThe purpose of this program is to automate the process of preparing and closing \nBureau of Land Management (BLM) federal oil and gas lease sales\n\nThe first phase of the program will visit the website, scrape the lease information\nfor each tract, and parse the information into a template sale note spreadsheet\nIt will also download the shapefile of the sale, which is needed for evaluating tracts on\nDrillingInfo.com\n\nOnce the spreadsheet is prepared and the sale takes place, the program will \nscrape the webpage again and gather information regarding won parcels.\nThe scrape will get the bonus bid for each parcel we won by evaluating our bidder number\n\nFinally, a dataframe will be created based on won lots and the information will be \npassed into a pdf fill form function to create paperwork needed to send to \nBureau of Land Management\n\n\"\"\"\nimport os, re, shutil, time\n\n#sale parameters\nstate = \"Montana\"\nstinitials = \"MT\"\ndate = \"Sep 22, 2020\"\nbidder = '3'\nurl = 'https://www.energynet.com/govt_listing.pl?sg=5314'\nbidDuration = 1 #amount of hours the parcel is open for bidding\n\n#Navigate to energynet/govt sale and get sale page\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\n\n#getting filepath of this file\nfilepath = os.path.dirname(__file__)\n\n#driver will be used based on operating system - windows or mac\ntry:\n    driver = webdriver.Chrome(filepath + \"/chromedriver.exe\")\nexcept:\n    driver = webdriver.Chrome(filepath + \"/chromedriver\")\n\n#selenium driver object will go to url of sale\ndriver.implicitly_wait(30)\ndriver.get(url)\n\n#finding the show dropdown menu\nshowButton = driver.find_elements_by_css_selector('#DataTables_Table_0_length > label > select')[0]\n                                                   \n#sending keys to select all lots\nshowButton.send_keys(\"All\")\nshowButton.send_keys(Keys.RETURN)\n                                                 \n\n#storing html content in variable after reaching target sale page\nsalehtml = BeautifulSoup(driver.page_source, \"html.parser\")\n\ndef webscrape_presale(parsepage):\n    '''This function will take a page and scrape its data for sale lot information\n    It will also download sale shapefile and move it to directory of this script file\n    '''\n\n    #webscrape sale page - gathering lot serial numbers from html \n    serialnums = parsepage.find_all(\"span\", \"lot-name\")\n    #ist comprehension - appending text into serialnums\n    serialnums = [i.text for i in serialnums]\n    \n    #storing all data from tag 'td's with clas name \"lot-legal\n    #this html container/tag has 3 pieces of information\n    legalinfo = parsepage.find_all(\"td\", \"lot-legal\")\n    \n    lotclosings = parsepage.find_all(\"td\", \"lot-closing\")\n    \n    \n    closingInfo = []\n    for item in lotclosings:\n        #finding date from lot closing element\n        closingDate = re.search(\"\\d+/\\d+/\\d+\", item.text)[0]\n        \n        #finding opening time from lot closing element - then calculate closing time\n        openingTime = re.search(\"\\d+:\\d+\", item.text)[0]\n        #calculating closing time \n        closingTime = str(int(openingTime.split(\":\")[0]) + bidDuration)+\":\" + openingTime.split(\":\")[1]\n        \n        dateAndTime = closingDate + \" \" + closingTime\n        closingInfo.append(dateAndTime)\n    \n    #initializing empty arrays\n    acres = []\n    desc = []\n    county = []\n    \n    for item in legalinfo:\n        county.append(item.contents[0].text)\n        desc.append(item.contents[1].text)\n        #getting acres by splitting at : and blankspace to get string of numerical value - taking out a comma if above 1000 in order to convert to float\n        acres.append(float(re.split(\":\\W\",item.contents[2].text)[1].replace(',','')))\n\n    ##getting shapefile from webpage  \n    #clicking link of where shapefile is stored on sale page    \n    driver.find_element_by_link_text(\"GIS Data WGS84\").click()\n    time.sleep(2)\n    try:\n        driver.find_element_by_link_text(\"Notice of Competitive Oil and Gas Internet-Based Lease Sale\").click()\n    except:\n        print(\"Sale notice pdf was unable to be clicked\")\n    #getting list of filenames in downloads\n    try:\n        downloaddir = \"/Users/Mishaun_Bhakta/Downloads/\"\n        downloads = os.listdir(downloaddir)\n    except:\n        downloaddir = \"C:/Users/mishaun/Downloads/\"\n        downloads = os.listdir(downloaddir) \n        \n    #pattern will find downloaded file name of shapefile\n    pattern = \"BLM\"+ stinitials + \"\\S*.zip\"\n    \n    #searching through filenames in downlaods folder\n    finds = []\n    for file in downloads:\n        if re.findall(pattern, file):\n            finds.append(file)\n            break\n        \n    #moving file from downloads folder to directory of this script file - then renaming it to a cleaner name\n    shutil.copy(downloaddir + finds[0], filepath)\n    \n    try:\n        os.rename(finds[0], \"BLM \" + stinitials + \" \" + date + \" Shapefile.\" + finds[0].split(\".\")[1])\n    except:\n       pass\n   \n    return acres, desc, county, serialnums, closingInfo\n\n#storing global variables of web scrape information\nacres, descriptions, counties, serials, closing = webscrape_presale(salehtml)\n\n#Open sale template and update insert information from webscrape\nimport openpyxl\n\ndef fillexcel():\n    '''\n    This function will take scraped (global) values for lots and insert into sale spreadsheet \n    '''    \n    \n    #opening template sale notebook for modifications\n    #preserving vba to keep formatting of workbook preserved - also keeping formulas \n    wb = openpyxl.load_workbook(\"BLM Sale Notes Template.xlsm\", keep_vba = True)\n    sheet = wb.active\n    \n    #updating sheet title to sale title\n    sheet[\"B6\"] = \"BLM {} {} Sale Notes\".format(stinitials, date)\n    \n    #inserting values from webscrape into spreadsheet -8th row is where data rows begin\n    for i in range(0,len(serials)):\n        sheet.cell(row = 8+i, column = 2, value = serials[i])\n        sheet.cell(row = 8+i, column = 5, value = closing[i])\n        sheet.cell(row = 8+i, column = 6, value = acres[i])\n        sheet.cell(row = 8+i, column = 7, value = counties[i])\n        sheet.cell(row = 8+i, column = 8, value = descriptions[i])\n    \n    #checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\n    if os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n        print(\"File already exists - Preventing overwrite of changes in excel file\")\n    else:\n        wb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n        wb.close()\n\n#checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\nif os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n    print(\"File already exists - Preventing overwrite of changes in excel file\")\nelse:\n    fillexcel()\n\nbidtags = salehtml.find_all(\"td\", \"lot-bid\")\nourwinnings = {}\n\nfor i in range (0,len(bidtags)):\n    textCont = bidtags[i].text\n    \n    #extracting bidder number from lot bid tags - try statement prevents break if no bids were received\n    try:\n        winBidder = re.findall('#\\d+', textCont)[0].replace(\"#\",'')\n    except:\n        print(\"no bids received for parcel: \" + serials[i])\n        winBidder = type(None)\n        pass\n    \n    #if we won the bid, then capture the winning bid amount\n    if bidder == winBidder:\n        winAmount = re.findall('\\$\\d+', textCont)[0]\n        winAmount = winAmount.replace('$','')\n    \n        ourwinnings[i] = winAmount\n\ndef fillwinnings():\n    '''This function will take ourwinnings dictionary and add values to created spreadsheet\n    '''\n    \n    #### insert our winnings into sale spreadsheet\n    wb = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), keep_vba = True)\n    sheet = wb.active\n    \n    for i in range(0,len(ourwinnings)):\n        #row 8 is the starting row for parcels in teh spreadsheet, inserting data relative to 8th row by adding parcel number of sale\n        sheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 17, value = ourwinnings[list(ourwinnings.keys())[i]])\n        sheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 16, value = 'Y')\n    \n    wb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n    wb.close()\n\n#create dataframe for completed sale sheet\nimport pandas as pd\n\n#use pdf reader to fill in form\n# conda install -c conda-forge pdfrw\nimport pdfrw\n\n#copied code and function from article: https://bostata.com/how-to-populate-fillable-pdfs-with-python/\n##############################################################################\nANNOT_KEY = '/Annots'\nANNOT_FIELD_KEY = '/T'\nANNOT_VAL_KEY = '/V'\nANNOT_RECT_KEY = '/Rect'\nSUBTYPE_KEY = '/Subtype'\nWIDGET_SUBTYPE_KEY = '/Widget'\n\ndef write_fillable_pdf(input_pdf_path, output_pdf_path, data_dict):\n    '''\n    This function will fill in pdf's forms based on a form pdf\n    '''\n    \n    template_pdf = pdfrw.PdfReader(input_pdf_path)\n    annotations = template_pdf.pages[0][ANNOT_KEY]\n    for annotation in annotations:\n        if annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n            if annotation[ANNOT_FIELD_KEY]:\n                key = annotation[ANNOT_FIELD_KEY][1:-1]\n                if key in data_dict.keys():\n                    annotation.update(\n                        pdfrw.PdfDict(V='{}'.format(data_dict[key]))\n                    )\n    pdfrw.PdfWriter().write(output_pdf_path, template_pdf)\n##############################################################\n\n\ndef wonlotsDF():\n    '''\n    This function will create a dataframe of the won lots by reading information\n    from completed sale note spreadsheet\n    The dataframe will then be used to parse pdf's \n    '''\n    #using openpyxl in order to read formulated values from spreadsheet\n    # NOTE: have to manually open excel and save sheet for formulated cells to read after filling in values\n    data_onlyWB = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), data_only = True, keep_vba = True)\n    dataSheet = data_onlyWB.active\n    \n    #covnerting spreadsheet into dataframe\n    df = pd.DataFrame(dataSheet.values)\n    \n    #slicing the dataframe to get only relevant data\n    df = df.iloc[6:,1:25]\n    #setting columns to first row of dataframe\n    df.columns = df.iloc[0]\n    #dropping the repeated row with column names\n    df = df.drop(index =[6])\n    \n    #filtering data frame with values only won by magnum\n    wonlotsdf = df[df[\"Magnum Won (Y/N)\"] == 'Y']\n    return wonlotsdf\n\n\ndef createBidSheets():\n    '''\n    This function will take a template pdf and generate pdf's based on wonlots dataframe\n    '''\n    #calling funciton wonlotsDF in ordre for bid sheets to be created \n    wonlotsdf = wonlotsDF()\n\n    templatePDF = 'bidsheet template.pdf'\n    \n    for i in range(0,len(wonlotsdf.index)):\n    \n        OutputPath = filepath +\"/Bid Sheets/\" + wonlotsdf.iloc[i][\"Serial numbers\"] + \" Bid Sheet.pdf\"\n        \n        fields = {\n                \"State\": stinitials,\n                \"Date of Sale\": date,\n                'Check Box for Oil and Gas' : \"x\",\n                \"Oil and Gas/Parcel No\" : wonlotsdf.iloc[i][\"Serial numbers\"],\n                \"TOTAL BID FOR Oil and Gas Lease\" : wonlotsdf.iloc[i][\"Total Bid (Number on BLM Bid Sheet)\"],\n                \"PAYMENT SUBMITTED WITH BID for Oil and Gas\" : wonlotsdf.iloc[i][\"Min Due\"],\n                \"Print or Type Name of Lessee\" : \"R&R Royalty, LTD\",\n                \"Address of Lessee\": \"500 N Shoreline Blvd, Ste 322\",\n                \"City\" : \"Corpus Christi\",\n                \"State_2\": \"TX\",\n                \"Zip Code\" : \"78401\"\n                }\n        \n        write_fillable_pdf(templatePDF, OutputPath, fields)\n\ndef openDI():\n    '''This function will open up DrillingInfo and log user in\n    '''\n    \n    driver = webdriver.Chrome(filepath + \"/chromedriver\")\n    wait = WebDriverWait(driver, 20)\n        \n    driver.get(\"https://app.drillinginfo.com/gallery/\")\n    userfield = driver.find_element_by_name(\"username\")\n    passfield = driver.find_element_by_name(\"password\")\n    \n    userfield.click()\n    userfield.send_keys(\"mbhaktamgm\")\n    passfield.click()\n    passfield.send_keys(\"itheCwe\")\n    passfield.send_keys(Keys.RETURN)\n    \n    myworkspaces = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\\\"workspaces-section\\\"]/div[1]/div[1]')))\n    myworkspaces.click()\n    \n    default_workspace = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"workspaces-section\\\"]/div[3]/di-carousel/section/div[2]/table/tbody/tr[2]/a/span[2]/span\")))\n    default_workspace.click()\n    \n#splitting counties in counties variable at the comma+space to formulate string \n#for drilling info paste and filter\n    \nsplitCounties = [item.split(\", \") for item in counties]\n\nDIcounties =  []\nfor item in splitCounties:\n    #taking \"County\" out of word\n    temp = item[0].upper().replace(\"COUNTY\", \"\")\n    formattedCounty = temp + \"(\" + item[1] + \")\"\n    DIcounties.append(formattedCounty)\n\n\n\n        \n    \n", "307": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Feb  9 20:57:29 2020\n\n@author: Mishaun_Bhakta\n\nThe purpose of this program is to automate the process of preparing and closing \nBureau of Land Management (BLM) federal oil and gas lease sales\n\nThe first phase of the program will visit the website, scrape the lease information\nfor each tract, and parse the information into a template sale note spreadsheet\nIt will also download the shapefile of the sale, which is needed for evaluating tracts on\nDrillingInfo.com\n\nOnce the spreadsheet is prepared and the sale takes place, the program will \nscrape the webpage again and gather information regarding won parcels.\nThe scrape will get the bonus bid for each parcel we won by evaluating our bidder number\n\nFinally, a dataframe will be created based on won lots and the information will be \npassed into a pdf fill form function to create paperwork needed to send to \nBureau of Land Management\n\n\"\"\"\nimport os, re, shutil, time\n\n#sale parameters\nstate = \"Utah\"\nstinitials = \"UT\"\ndate = \"Sep 29, 2020\"\nbidder = '3'\nurl = 'https://www.energynet.com/govt_listing.pl?sg=5320'\nbidDuration = 0.5 #amount of hours the parcel is open for bidding\n\n#Navigate to energynet/govt sale and get sale page\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\n\n#getting filepath of this file\nfilepath = os.path.dirname(__file__)\n\n#driver will be used based on operating system - windows or mac\ntry:\n    driver = webdriver.Chrome(filepath + \"/chromedriver.exe\")\nexcept:\n    driver = webdriver.Chrome(filepath + \"/chromedriver\")\n\n#selenium driver object will go to url of sale\ndriver.implicitly_wait(30)\ndriver.get(url)\n\n#finding the show dropdown menu\nshowButton = driver.find_elements_by_css_selector('#DataTables_Table_0_length > label > select')[0]\n                                                   \n#sending keys to select all lots\nshowButton.send_keys(\"All\")\nshowButton.send_keys(Keys.RETURN)\n                                                 \n\n#storing html content in variable after reaching target sale page\nsalehtml = BeautifulSoup(driver.page_source, \"html.parser\")\n\ndef webscrape_presale(parsepage):\n    '''This function will take a page and scrape its data for sale lot information\n    It will also download sale shapefile and move it to directory of this script file\n    '''\n\n    #webscrape sale page - gathering lot serial numbers from html \n    serialnums = parsepage.find_all(\"span\", \"lot-name\")\n    #ist comprehension - appending text into serialnums\n    serialnums = [i.text for i in serialnums]\n    \n    #storing all data from tag 'td's with clas name \"lot-legal\n    #this html container/tag has 3 pieces of information\n    legalinfo = parsepage.find_all(\"td\", \"lot-legal\")\n    \n    lotclosings = parsepage.find_all(\"td\", \"lot-closing\")\n    \n    \n    closingInfo = []\n    for item in lotclosings:\n        #finding date from lot closing element\n        closingDate = re.search(\"\\d+/\\d+/\\d+\", item.text)[0]\n        \n        #finding opening time from lot closing element - then calculate closing time\n        openingTime = re.search(\"\\d+:\\d+\", item.text)[0]\n        #calculating closing time \n        closingTime = str(int(openingTime.split(\":\")[0]) + bidDuration)+\":\" + openingTime.split(\":\")[1]\n        \n        dateAndTime = closingDate + \" \" + closingTime\n        closingInfo.append(dateAndTime)\n    \n    #initializing empty arrays\n    acres = []\n    desc = []\n    county = []\n    \n    for item in legalinfo:\n        county.append(item.contents[0].text)\n        desc.append(item.contents[1].text)\n        #getting acres by splitting at : and blankspace to get string of numerical value - taking out a comma if above 1000 in order to convert to float\n        acres.append(float(re.split(\":\\W\",item.contents[2].text)[1].replace(',','')))\n\n    ##getting shapefile from webpage  \n    #clicking link of where shapefile is stored on sale page    \n    driver.find_element_by_link_text(\"GIS Data WGS84\").click()\n    time.sleep(2)\n    try:\n        driver.find_element_by_link_text(\"Notice of Competitive Oil and Gas Internet-Based Lease Sale\").click()\n    except:\n        print(\"Sale notice pdf was unable to be clicked\")\n    #getting list of filenames in downloads\n    try:\n        downloaddir = \"/Users/Mishaun_Bhakta/Downloads/\"\n        downloads = os.listdir(downloaddir)\n    except:\n        downloaddir = \"C:/Users/mishaun/Downloads/\"\n        downloads = os.listdir(downloaddir) \n        \n    #pattern will find downloaded file name of shapefile\n    pattern = \"BLM\"+ stinitials + \"\\S*.zip\"\n    \n    #searching through filenames in downlaods folder\n    finds = []\n    for file in downloads:\n        if re.findall(pattern, file):\n            finds.append(file)\n            break\n        \n    #moving file from downloads folder to directory of this script file - then renaming it to a cleaner name\n    shutil.copy(downloaddir + finds[0], filepath)\n    \n    try:\n        os.rename(finds[0], \"BLM \" + stinitials + \" \" + date + \" Shapefile.\" + finds[0].split(\".\")[1])\n    except:\n       pass\n   \n    return acres, desc, county, serialnums, closingInfo\n\n#storing global variables of web scrape information\nacres, descriptions, counties, serials, closing = webscrape_presale(salehtml)\n\n#Open sale template and update insert information from webscrape\nimport openpyxl\n\ndef fillexcel():\n    '''\n    This function will take scraped (global) values for lots and insert into sale spreadsheet \n    '''    \n    \n    #opening template sale notebook for modifications\n    #preserving vba to keep formatting of workbook preserved - also keeping formulas \n    wb = openpyxl.load_workbook(\"BLM Sale Notes Template.xlsm\", keep_vba = True)\n    sheet = wb.active\n    \n    #updating sheet title to sale title\n    sheet[\"B6\"] = \"BLM {} {} Sale Notes\".format(stinitials, date)\n    \n    #inserting values from webscrape into spreadsheet -8th row is where data rows begin\n    for i in range(0,len(serials)):\n        sheet.cell(row = 8+i, column = 2, value = serials[i])\n        sheet.cell(row = 8+i, column = 5, value = closing[i])\n        sheet.cell(row = 8+i, column = 6, value = acres[i])\n        sheet.cell(row = 8+i, column = 7, value = counties[i])\n        sheet.cell(row = 8+i, column = 8, value = descriptions[i])\n    \n    #checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\n    if os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n        print(\"File already exists - Preventing overwrite of changes in excel file\")\n    else:\n        wb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n        wb.close()\n\n#checking to see whether or not excel file already exists - if it does it'll prevent overwriting of changes\nif os.path.exists(filepath+ \"/\" + \"BLM {} {} Sale Notes.xlsm\".format(stinitials, date)):\n    print(\"File already exists - Preventing overwrite of changes in excel file\")\nelse:\n    fillexcel()\n\nbidtags = salehtml.find_all(\"td\", \"lot-bid\")\nourwinnings = {}\n\nfor i in range (0,len(bidtags)):\n    textCont = bidtags[i].text\n    \n    #extracting bidder number from lot bid tags - try statement prevents break if no bids were received\n    try:\n        winBidder = re.findall('#\\d+', textCont)[0].replace(\"#\",'')\n    except:\n        print(\"no bids received for parcel: \" + serials[i])\n        winBidder = type(None)\n        pass\n    \n    #if we won the bid, then capture the winning bid amount\n    if bidder == winBidder:\n        winAmount = re.findall('\\$\\d+', textCont)[0]\n        winAmount = winAmount.replace('$','')\n    \n        ourwinnings[i] = winAmount\n\ndef fillwinnings():\n    '''This function will take ourwinnings dictionary and add values to created spreadsheet\n    '''\n    \n    #### insert our winnings into sale spreadsheet\n    wb = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), keep_vba = True)\n    sheet = wb.active\n    \n    for i in range(0,len(ourwinnings)):\n        #row 8 is the starting row for parcels in teh spreadsheet, inserting data relative to 8th row by adding parcel number of sale\n        sheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 17, value = ourwinnings[list(ourwinnings.keys())[i]])\n        sheet.cell(row = 8 + list(ourwinnings.keys())[i], column = 16, value = 'Y')\n    \n    wb.save(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date))\n    wb.close()\n\n#create dataframe for completed sale sheet\nimport pandas as pd\n\n#use pdf reader to fill in form\n# conda install -c conda-forge pdfrw\nimport pdfrw\n\n#copied code and function from article: https://bostata.com/how-to-populate-fillable-pdfs-with-python/\n##############################################################################\nANNOT_KEY = '/Annots'\nANNOT_FIELD_KEY = '/T'\nANNOT_VAL_KEY = '/V'\nANNOT_RECT_KEY = '/Rect'\nSUBTYPE_KEY = '/Subtype'\nWIDGET_SUBTYPE_KEY = '/Widget'\n\ndef write_fillable_pdf(input_pdf_path, output_pdf_path, data_dict):\n    '''\n    This function will fill in pdf's forms based on a form pdf\n    '''\n    \n    template_pdf = pdfrw.PdfReader(input_pdf_path)\n    annotations = template_pdf.pages[0][ANNOT_KEY]\n    for annotation in annotations:\n        if annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n            if annotation[ANNOT_FIELD_KEY]:\n                key = annotation[ANNOT_FIELD_KEY][1:-1]\n                if key in data_dict.keys():\n                    annotation.update(\n                        pdfrw.PdfDict(V='{}'.format(data_dict[key]))\n                    )\n    pdfrw.PdfWriter().write(output_pdf_path, template_pdf)\n##############################################################\n\n\ndef wonlotsDF():\n    '''\n    This function will create a dataframe of the won lots by reading information\n    from completed sale note spreadsheet\n    The dataframe will then be used to parse pdf's \n    '''\n    #using openpyxl in order to read formulated values from spreadsheet\n    # NOTE: have to manually open excel and save sheet for formulated cells to read after filling in values\n    data_onlyWB = openpyxl.load_workbook(\"BLM {} {} Sale Notes.xlsm\".format(stinitials, date), data_only = True, keep_vba = True)\n    dataSheet = data_onlyWB.active\n    \n    #covnerting spreadsheet into dataframe\n    df = pd.DataFrame(dataSheet.values)\n    \n    #slicing the dataframe to get only relevant data\n    df = df.iloc[6:,1:25]\n    #setting columns to first row of dataframe\n    df.columns = df.iloc[0]\n    #dropping the repeated row with column names\n    df = df.drop(index =[6])\n    \n    #filtering data frame with values only won by magnum\n    wonlotsdf = df[df[\"Magnum Won (Y/N)\"] == 'Y']\n    return wonlotsdf\n\n\ndef createBidSheets():\n    '''\n    This function will take a template pdf and generate pdf's based on wonlots dataframe\n    '''\n    #calling funciton wonlotsDF in ordre for bid sheets to be created \n    wonlotsdf = wonlotsDF()\n\n    templatePDF = 'bidsheet template.pdf'\n    \n    for i in range(0,len(wonlotsdf.index)):\n    \n        OutputPath = filepath +\"/Bid Sheets/\" + wonlotsdf.iloc[i][\"Serial numbers\"] + \" Bid Sheet.pdf\"\n        \n        fields = {\n                \"State\": stinitials,\n                \"Date of Sale\": date,\n                'Check Box for Oil and Gas' : \"x\",\n                \"Oil and Gas/Parcel No\" : wonlotsdf.iloc[i][\"Serial numbers\"],\n                \"TOTAL BID FOR Oil and Gas Lease\" : wonlotsdf.iloc[i][\"Total Bid (Number on BLM Bid Sheet)\"],\n                \"PAYMENT SUBMITTED WITH BID for Oil and Gas\" : wonlotsdf.iloc[i][\"Min Due\"],\n                \"Print or Type Name of Lessee\" : \"R&R Royalty, LTD\",\n                \"Address of Lessee\": \"500 N Shoreline Blvd, Ste 322\",\n                \"City\" : \"Corpus Christi\",\n                \"State_2\": \"TX\",\n                \"Zip Code\" : \"78401\"\n                }\n        \n        write_fillable_pdf(templatePDF, OutputPath, fields)\n\ndef openDI():\n    '''This function will open up DrillingInfo and log user in\n    '''\n    \n    driver = webdriver.Chrome(filepath + \"/chromedriver\")\n    wait = WebDriverWait(driver, 20)\n        \n    driver.get(\"https://app.drillinginfo.com/gallery/\")\n    userfield = driver.find_element_by_name(\"username\")\n    passfield = driver.find_element_by_name(\"password\")\n    \n    userfield.click()\n    userfield.send_keys(\"mbhaktamgm\")\n    passfield.click()\n    passfield.send_keys(\"itheCwe\")\n    passfield.send_keys(Keys.RETURN)\n    \n    myworkspaces = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\\\"workspaces-section\\\"]/div[1]/div[1]')))\n    myworkspaces.click()\n    \n    default_workspace = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"workspaces-section\\\"]/div[3]/di-carousel/section/div[2]/table/tbody/tr[2]/a/span[2]/span\")))\n    default_workspace.click()\n    \n#splitting counties in counties variable at the comma+space to formulate string \n#for drilling info paste and filter\n    \nsplitCounties = [item.split(\", \") for item in counties]\n\nDIcounties =  []\nfor item in splitCounties:\n    #taking \"County\" out of word\n    temp = item[0].upper().replace(\"COUNTY\", \"\")\n    formattedCounty = temp + \"(\" + item[1] + \")\"\n    DIcounties.append(formattedCounty)\n\n\n\n        \n    \n", "308": "from selenium import webdriver \nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport csv\nimport urllib.request\nfrom lxml import html\nimport time\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.chrome.options import Options\nfrom pprint import pprint as pp\nfrom tabulate import tabulate\n\nscrape_file = r'Classic-Imports-and-Design\\python webscrape\\url-lists\\jonathan-charles-sku.csv'\nlogin_url = 'https://catalog.jonathancharlesfurniture.com/jcusa/e/mobile/login'\n\nwith open(scrape_file, encoding='UTF-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    scrape_file = list(reader)\n\nwritefile = open('Classic-Imports-and-Design\\python webscrape\\output.csv', 'w+', encoding='UTF8', newline='')\noutput_file = csv.writer(writefile, delimiter=\",\")\n\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option('excludeSwitches', ['enable-logging'])\ndriver = webdriver.Chrome('Classic-Imports-and-Design\\python webscrape\\chromedriver.exe', options=options)\n\n#login\nprint('mklatsky')\nprint('23Monday!01')\ndriver.get(login_url)\nprint(\"Please log into the wholesale account.\")\ninput()\n\nfile_header = ['sku', 'Name', 'Brand', 'Categories', 'Wholesale', 'Retail', 'Description', 'Images']\noutput_file.writerow(file_header)\nfails = []\n\nfor product in scrape_file:\n    try:\n        product[0] = product[0].strip()\n        product[1] = product[1].strip()\n        data = []\n        driver.get('https://catalog.jonathancharlesfurniture.com/jcusa/e/mobile/products?query=' + product[0])  \n        try:\n            element_present = EC.presence_of_element_located((By.CLASS_NAME, 'catalog-item-number'))\n            WebDriverWait(driver, 6).until(element_present)\n        except:\n            fails.append(product[0] + ' Does not exist')\n            continue\n        #click to product page\n        driver.find_element(By.CLASS_NAME, 'catalog-item-number').click()\n        time.sleep(3)\n        #sku\n        data.append(product[0])\n        #name\n        try:\n            product_name = driver.find_element(By.CLASS_NAME, 'item-name').find_element(By.TAG_NAME, 'h1').text\n            data.append(product_name)\n        except:\n            fails.append(product[0] + ' has NO NAME')\n            continue\n        #brand\n        data.append('Jonathan Charles')\n        #catagory\n        data.append(product[1])\n        #wholesale\n        try:\n            wholesale = float( driver.find_element(By.CLASS_NAME, 'item-price').text.lstrip('$').replace(',','') )\n            data.append(wholesale)\n        #retail\n            data.append(wholesale*2.2)\n        except:\n            data.append(0)\n            data.append(0)\n            fails.append(product[0] + ' has NO PRICE')\n        #description\n        try:\n            material = ''\n            other_mat = ''\n            info_list = driver.find_element(By.CLASS_NAME, 'label-data-list-inline').find_elements(By.TAG_NAME, 'li')\n            for element in info_list:\n                if 'Dim. - IN' in element.find_element(By.CLASS_NAME, 'label').text:\n                    dimensions = element.find_element(By.CLASS_NAME, 'data').text + '\\n'\n                if 'Materials' == element.find_element(By.CLASS_NAME, 'label').text:\n                    material = element.find_element(By.CLASS_NAME, 'data').text + '\\n'\n                if 'Other Materials' in element.find_element(By.CLASS_NAME, 'label').text:\n                    other_mat = element.find_element(By.CLASS_NAME, 'data').text + '\\n'   \n            description = driver.find_element(By.CSS_SELECTOR, 'p.story-full').text\n            data.append(dimensions + material + other_mat + description)\n        except:\n            data.append('NA')\n            fails.append(product[0] + 'has NO DESCRIPTION')\n        #images\n        try:\n            images = driver.find_elements(By.CSS_SELECTOR, 'div.item-image-wrapper')\n            file_names = []\n            c = 1\n            for element in images:\n                file_name = product[0] + '-' + str(c)\n                image_url = element.find_element(By.TAG_NAME, 'a').get_attribute('href')\n                print(image_url)\n                urllib.request.urlretrieve(image_url, 'Classic-Imports-and-Design/python webscrape/product-images/' + file_name + '.jpg') \n                c = c + 1\n                file_names.append(file_name)\n            data.append(','.join(file_names))\n        except:\n            fails.append(product[0] + ' has no IMAGES')\n            data.append('NA')\n\n        pp(fails)\n        print_table = data.copy()\n        print_table[6] = 'desc...'\n        print_table[7] = 'images...'\n        print(tabulate([file_header] + [print_table]))\n        print(data[6])\n        print(data[7] + '\\n')\n        output_file.writerow(data)\n    except:\n        fails.append(product[0] + ' FAILED DUE TO UNKNOWN ERROR')\n\npp(fails)\nwritefile.close()\n\n#element_present = EC.presence_of_element_located((By.CLASS_NAME, 'product-item-link'))\n#WebDriverWait(driver, 6).until(element_present)\n#driver.get(url)\n#time.sleep()\n#content = driver.page_source\n#soup = BeautifulSoup(content, \"html.parser\")\n#tree = html.fromstring(driver.page_source) \n\n#driver.find_elements(By.XPATH, XPATH)\n#driver.find_elements(By.CLASS_NAME, CLASSNAME)\n#soup.find(\"TAG-TYPE\", class_=\"CLASS-NAME\").text\n\n#urllib.request.urlretrieve(IMG-URL, FILENAME) \n\n#output_file.writerow(data)", "309": "import time\nimport requests\nfrom bs4 import BeautifulSoup\nimport sys\nimport json\n#function to webscrape data\ndef webscrape(link):\n    try:\n        page = requests.get(link)\n    except Exception as e:\n        error_type,error_obj,error_info = sys.exc_info()\n        print(\"error\")\n    \n    soup = BeautifulSoup(page.content,'html.parser')\n    body  = soup.find('script',attrs={'id':'__NEXT_DATA__'})\n    pydict = json.loads(body.text)\n    restaurant = pydict['props']['pageProps']['initialMenuState']['restaurant']\n    result = {}\n    result[\"restaurant_name\"]=restaurant[\"name\"]\n    result[\"restaurant_logo\"] = restaurant['logo']\n    result[\"latitude\"] = float(restaurant['latitude'])\n    result[\"longitude\"] = float(restaurant['longitude'])\n    result[\"cuisine_tags\"] = restaurant['cuisineString'].split(\", \")\n    menu_items = pydict['props']['pageProps']['initialMenuState']['menuData']['items']\n    items = []\n    for i in range(len(menu_items)):\n        item = menu_items[i]\n        item_name = item[\"name\"]\n        item_price = float(item[\"price\"])\n        item_description = item[\"description\"]\n        item_image = item[\"image\"]\n        items.append({\"item_name\":item_name,\"item_description\":item_description,\"item_price\":item_price,\"item_image\":item_image})\n    result[\"menu_items\"]= items\n    return result\n#function compeleted\n\n#given links in data sample_json links\ngiven_links = [\n    \"https://www.talabat.com/uae/restaurant/621133/ginos-deli-jlt?aid=1308\",\n    \"https://www.talabat.com/uae/restaurant/645430/pasta-della-nona-jlt-jumeirah-lakes-towers?aid=1308\",\n    \"https://www.talabat.com/uae/restaurant/50445/pizzaro-marina-3?aid=1308\",\n    \"https://www.talabat.com/uae/restaurant/605052/the-pasta-guyz-dubai-marina?aid=1308\",\n    \"https://www.talabat.com/uae/restaurant/621796/pizza-di-rocco-jumeirah-lakes-towers--jlt?aid=1308\"\n]\n\n#5 urls randomly chosen\nmy_urls = [\n'https://www.talabat.com/uae/restaurant/648008/jaffer-bhais-restaurant-al-barsha-1?aid=1308',\n'https://www.talabat.com/uae/restaurant/41433/yin-yang-restaurant-jumeriah-lakes-towers--jlt?aid=1308',\n'https://www.talabat.com/uae/restaurant/643032/oregano-dubai-media-city-dubai-internet-city--dic?aid=1308',\n'https://www.talabat.com/uae/restaurant/603280/carluccios-restaurant-cafe-dubai-marina?aid=1308',\n'https://www.talabat.com/uae/restaurant/606838/everyday-roastery-coffee-dubai-marina?aid=1308'\n]\n\n#all urls \nlinks = given_links + my_urls\n\n\noutput = {}\ncount = 0\nfor i in links:\n    y = webscrape(i)\n    output[str(count)] = y\n    count += 1\n  \n#response is output in json format, where output is in python\nresponse = json.dumps(output)\nprint(response)\n", "310": "import scrapy\nfrom ..items import WebscrapeItem\n\nclass AmazonSpiderSpider(scrapy.Spider):\n    name = 'amazon'\n    start_urls = ['https://www.amazon.com/s?k=masks+50pcs&ref=nb_sb_noss_1']\n    pageNumber = 2\n\n    def parse(self, response):\n        items = WebscrapeItem()\n\n        product_name = response.css('.a-color-base.a-text-normal').css('::text').extract()\n        product_price = response.css('.a-price-whole::text').extract()\n\n        items['product_name'] = product_name\n        items['product_price'] = product_price\n\n        yield items\n\n        nextPage = 'https://www.amazon.com/s?k=masks+50pcs&page=' + str(AmazonSpiderSpider.pageNumber)\n\n        if AmazonSpiderSpider.pageNumber <= 100:\n            AmazonSpiderSpider.pageNumber += 1\n            yield response.follow(nextPage, callback = self.parse)", "311": "import json\n\nwith open('../../webScrape/ucitele.json') as file:\n    ucitele = json.loads(file.readline())\n\nwith open('../../webScrape/ucitele_fzp.json') as file:\n    ucitele_fzp = json.loads(file.readline())\n\n\nucitele.update(ucitele_fzp)\n\n\nucitele_dicts = {}\nr_id = 1\nfor workplace_str in ucitele.keys():\n    for j in ucitele[workplace_str]:\n        data_str, workplaces_str = j.split(' - ')\n        if ucitele_dicts.get(data_str):\n            ucitele_dicts[data_str]['workplaces'].append(workplace_str.split(',')[0])\n        else:\n            data = data_str.split(', ')\n            firstname = data[1]\n            surname = data[0]\n            titles1 = data[2].split(' ') if len(data) > 2 else []\n            if 'ak.' in titles1:\n                index = titles1.index('ak.')\n                titles1[index] = titles1[index] + ' ' +  titles1[index+1]\n                titles1.pop(index+1)\n            titles2 = data[3].split(' ') if len(data) > 3 else []\n            ucitele_dicts[data_str] = {\n                'id': r_id,\n                'firstname': firstname,\n                'surname': surname,\n                'titles1': titles1,\n                'titles2': titles2,\n                'workplaces': [workplace_str.split(',')[0]]\n            }\n            r_id += 1\n            \nucitele_out = list(ucitele_dicts.values())\n\n# zadny ucitel podle stagu neni zamestnancem vice pracovist\nprint(ucitele_out)\n\nwith open('../../webScrape/ucitele2.json', 'w') as file:\n    file.write(json.dumps(ucitele_out))\n\nwith open(f\"../data/data_Borrowers.csv\", \"w\") as outfile:\n    for ucitel in ucitele_out:\n        outfile.write(str(ucitel['id'])+';')\n        outfile.write(str(ucitel['firstname']) + ';')\n        outfile.write(str(ucitel['surname']) + ';')\n\n        outfile.write(\"\\n\")", "312": "import pickle\n\nif __name__ == '__main__':\n    #from scrape import webscrape\n    #webscrape(num_enqueues=5,src_file='./sam_entities.json')\n\n    from preprocess import submit_for_processing, vectorize_unseen_data\n    submit_for_processing()\n    #vectorize_unseen_data(vocab_src=\"./vocabularies/vocabulary_100k_docfreq_100_hq.csv\")\n\n    #from predict import predict_unseen\n    #loaded_model = pickle.load(open('pima.pickle.dat', 'rb'))\n    #predict_unseen(loaded_model)\n", "313": "# CST 205\n# Charlie Nguyen\n# 12/4/20\n# need to pip install request\n# need to pip install BeatifulSoup4\n# This class allows us to webscrape data off of tastespotting.com\n# citation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nwebsite_url = 'http://www.tastespotting.com/'\nwebsite_page = requests.get(website_url)\nwebsite_soup = BeautifulSoup(website_page.content, 'html.parser')\n\n# print(website_soup.prettify())\nwebsite_recipe_file = open(\"webscrape_recipe_file.py\", \"w\")\nwebsite_recipe_file.write(\"website_recipe_info = [\\n\")\n\n# used so we can figure out when the end of the list is\nrecipe_test = website_soup.find_all('div', 'trendspotted-item')\nrecipe_card_count = len(recipe_test)\n\nfor recipe_card in website_soup.find_all('div', 'trendspotted-item'):\n    recipe_card_count = recipe_card_count - 1\n    website_recipe_file.write(\"\\t{\\n\")\n\n    # adding a title for ease\n    website_recipe_file.write(\"\\t\\t\\\"title\\\" : \")\n    website_recipe_file.write(\"\\\"image_\" + str(recipe_card_count) + \"\\\"\")\n    website_recipe_file.write(\",\\n\")\n\n    recipe_text = str(recipe_card.find('h3'))\n\n\n    # locating where the first link is at for\n    # the recipe URL\n    i = 30\n    target_index = recipe_text.index(\"target\")\n    n = target_index - 2\n    recipe_url = recipe_text[i:n]\n    target_index = recipe_url.index(\"&amp\")\n    recipe_url = recipe_url[:target_index]\n    website_recipe_file.write(\"\\t\\t\\\"recipe_url\\\" : \")\n    website_recipe_file.write(\"\\\"\" + recipe_url + \"\\\"\")\n    website_recipe_file.write(\",\\n\")\n    #print(recipe_url)\n\n\n    # locating search words\n    end_of_tags_words = recipe_text.index(\" height\")\n    start_of_tags_Words = recipe_text.index(\"alt=\")\n    i = start_of_tags_Words + 5\n    n = end_of_tags_words - 1\n    snippet = recipe_text[i:]\n    quote_index = snippet.index(\"\\\"\")\n    tag_words = snippet[:quote_index]\n    website_recipe_file.write(\"\\t\\t\\\"tags\\\" : \")\n    website_recipe_file.write(\"\\\"\" + tag_words + \"\\\"\")\n    website_recipe_file.write(\",\\n\")\n\n    # locating image location\n    start_of_image = recipe_text.index(\"src\")\n    end_of_image = recipe_text.index(\"width\")\n    i = start_of_image + 5\n    n = end_of_image - 2\n    snippet = recipe_text[i:]\n    quote_index = snippet.index(\"\\\"\")\n    image_url = snippet[:quote_index]\n    website_recipe_file.write(\"\\t\\t\\\"image_url\\\" : \")\n    website_recipe_file.write(\"\\\"\" + image_url + \"\\\"\")\n    website_recipe_file.write(\"\\n\")\n    if(recipe_card_count == 0) :\n        website_recipe_file.write(\"\\t}\\n\")\n    else :\n        website_recipe_file.write(\"\\t},\\n\")\n\nwebsite_recipe_file.write(\"]\")", "314": "import sys\nimport numpy as np\nimport pandas as pd\nimport requests \nimport matplotlib.pyplot as plt \nclass webscrape:\n    def __init__(self):\n        pass\n    \n    def scape_url(self, url):\n        resp = requests.get(url)\n        data = resp.json()\n        issues = pd.DataFrame(data, columns=['number', 'title',\n                                     'labels', 'state'])\n        print (data)\n        \n\ndef main():\n    url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n    scraper = webscrape()\n    scraper.scape_url(url)\n    \nif __name__ == '__main__':\n    main( )", "315": "#Imports\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport json\nimport pandas as pd\n\n#Opens the link to the web page to be scraped\ntest_data = 'http://econpy.pythonanywhere.com/ex/001.html'\npage = urllib.request.urlopen(test_data)\n\n#Import page into BeautifulSoup\nsoup = BeautifulSoup(page, \"html.parser\")\n\n#Use BeautifulSoup to search dataset for div tags\ninfo = soup.find_all('div', title = 'buyer-info')\n\n#Generate lists\nA=[]\nB=[]\n\n#for statement that sorts data by tags and appends them to the generated lists\nfor row in info:\n    buyer_names = row.find_all('div', title = 'buyer-name')\n    item_price = row.find_all('span', class_ =  'item-price')\n    A.append(buyer_names[0].find(text = True))\n    B.append(item_price[0].find(text = True))\n\n#Combines lists A and B and formats them as JSON\njson_data =  json.dumps([{'Buyer Name': buyer_name, 'Item Price': item_price} for buyer_name, item_price in zip(A, B)])\n\n#Writes the json_data to a file called 'output.webscrape_to_JSON.json'\nwith open('output.webscrape_to_JSON.json', 'w') as outfile:\n    json.dump(json_data, outfile)\n\n#use pandas to convert list to data frame\ndf=pd.DataFrame()\ndf['Buyer Name']=A\ndf['Item Price']=B\n\n#Display dataframe\nprint(df)", "316": "import requests\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n\nfrom nba_py import player\nimport webscrape\n\ndef shot(s_type):\n    if \"jump shot\" in s_type.lower():\n        return \"jump\"\n    elif \"layup shot\" in s_type.lower():\n        return \"layup\"\n    else:\n        return \"else\"\n    \ndef shot_dist(dist):\n    if dist < 8:\n        return \"less than 8\"\n    elif dist < 16:\n        return \"8-16\"\n    elif dist < 24:\n        return \"16-24\"\n    else:\n        return \"24+\"\n    \ndef def_dist(dist):\n    if dist < 2:\n        return \"0-2\"\n    elif dist < 4:\n        return \"2-4\"\n    elif dist < 6:\n        return \"4-6\"\n    else:\n        return \"6+\"\n\ndef transform_web(data):\n    data = data.copy()\n    shot_type = pd.get_dummies(data[\"Shot Type\"].apply(shot))\n    data[\"Shot Dist.\"] = data[\"Shot Dist.\"].apply(lambda x : float(x.replace(\"ft.\", \"\")))\n    shot_clock = data[\"Shot Clock\"].apply(lambda x: float(x))\n    touch_time = data[\"Touch Time\"].apply(lambda x: float(x))\n    drib = data[\"Drib.\"].apply(lambda x: int(x))\n    data[\"Def Dist.\"] = data[\"Def Dist.\"].apply(lambda x: float(x))\n\n    def_dist_c = pd.get_dummies(data[\"Def Dist.\"].apply(def_dist))\n\n    \n    shot_dist_c = pd.get_dummies(data[\"Shot Dist.\"].apply(shot_dist))\n    # if \"24+\" not in shot_dist_c.columns:\n    #     shot_dist_c[\"24+\"] = 0\n    \n    con = [shot_type, shot_clock, data[\"Shot Dist.\"],touch_time, drib, data[\"Def Dist.\"],def_dist_c, shot_dist_c, (data[\"Made?\"]==\"Yes\").astype(int)]\n    new_shot_chart = pd.concat(con , axis=1)\n\n    # pred = ['16-24', '24+', '8-16', 'less than 8', 'else', 'jump', 'layup', 'Made?']\n    pred = ['Shot Dist.','Def Dist.', 'else', 'jump', 'layup', 'Made?']\n\n    return new_shot_chart[pred]\n\n\ndef predictor(athlete, season):\n\n    web_df = webscrape.getData(athlete, season)\n    transformed_web = transform_web(web_df)\n    logistic = LogisticRegression()\n\n    predictors = transformed_web.columns[:-1]\n\n    logistic.fit(transformed_web[predictors], transformed_web[\"Made?\"])\n    return logistic\n\n\ndef transform_big(data):\n\n    data = data.copy()\n    shot_type = pd.get_dummies(data[\"Shot Type\"].apply(shot))\n    data[\"Shot Dist.\"] = data[\"Shot Dist.\"].apply(lambda x : x.replace(\"ft.\", \"\"))\n    data[\"Shot Dist.\"] = data[\"Shot Dist.\"].apply(lambda x : 0 if x== \"\" else float(x))\n    \n    # shot_clock = data[\"Shot Clock\"].apply(lambda x: 0 if x == \"\" else float(x))\n    # touch_time = data[\"Touch Time\"].apply(lambda x: float(x))\n    # drib = data[\"Drib.\"].apply(lambda x: int(x))\n    data[\"Def Dist.\"] = data[\"Def Dist.\"].apply(lambda x: float(x))\n\n    # def_dist_c = pd.get_dummies(data[\"Def Dist.\"].apply(def_dist))\n    \n    player_c = pd.get_dummies(data[\"Player\"])\n\n    shot_dist_c = pd.get_dummies(data[\"Shot Dist.\"].apply(shot_dist))\n    \n    con = [player_c, shot_type , data[\"Def Dist.\"],\n            shot_dist_c, data[\"Shot Dist.\"],(data[\"Made?\"]==\"Yes\").astype(int)]\n    \n#     con = [player_c, shot_type, shot_clock, touch_time, drib, \n#            shot_dist_c, data[\"Shot Dist.\"],(data[\"Made?\"]==\"Yes\").astype(int)]\n        \n    new_shot_chart = pd.concat(con , axis=1)\n\n    pred = player_c.columns[:len(player_c)].tolist()+ ['Shot Dist.', 'Def Dist.', 'else', 'jump', 'layup', 'Made?']\n\n    return new_shot_chart[pred]\n\ndef getDataFrame(lst):\n    app = []\n    for player in lst:\n        app.append(webscrape.getData(player, \"2014\"))\n    print \"done\"\n    df_a = pd.concat(app)\n    df_a.reset_index(drop=True, inplace =True)\n    return transform_big(df_a)\n\ndef large_model(lst):\n    df_guards = getDataFrame(lst)\n    \n    logistic = LogisticRegression()\n\n    predictors = df_guards.columns[:-1]\n\n    logistic.fit(df_guards[predictors], df_guards[\"Made?\"])\n    return logistic, predictors\n    ", "317": "from bs4 import BeautifulSoup\nimport os\nimport pandas as pd\nimport numpy as np\nimport sys\nimport requests\nfrom datetime import datetime\n\nspeeches_URL = 'https://speeches.byu.edu'\n\nspeakers = list(pd.read_csv('Talk_Quotes_Data.csv')['Speaker'].unique())\nspeakers_format = [spkr.lower().replace(' ','-').replace('.','') for spkr in speakers]\n\nall_speaker_links = [speeches_URL + '/speakers/' + spkr + '/' for spkr in speakers_format]\n\nspeeches = pd.DataFrame(columns = ['Speaker', 'Date', 'Title', 'URL'])\n\nescapes = ''.join([chr(char) for char in range(1, 32)])\ntranslator = str.maketrans('', '', escapes)\n\nfor spkr_link in all_speaker_links:\n    spkr_page = requests.get(spkr_link)\n    spkr_soup = BeautifulSoup(spkr_page.content, 'lxml')\n    all_talks = spkr_soup.find_all('article', 'card card--reduced')\n    this_speaker = spkr_soup.find('h1', 'speaker-listing__name').text\n    for talk in all_talks:\n        this_Date = datetime.strptime(talk\n                    .find('div', 'card__bylines card__bylines--reduced')\n                    .text\n                    .translate(translator), \"%B %d, %Y\")\n        this_URL = talk.find('a')['href']\n        this_Title = talk.find('h2').text.translate(translator)\n        this_speech = pd.DataFrame({'Speaker':[this_speaker],\n                                    'Date':[this_Date],\n                                    'Title':[this_Title],\n                                    'URL':[this_URL]})\n        speeches = speeches.append(this_speech, ignore_index = True)\n      \n\nall_content = []\nunavail_text = 'The text for this speech is unavailable.'\nint_prop = 'Intellectual Reserve'\nrights = 'All rights reserved.'\n\nrows_to_rm = []\n\nfor speech_row in range(np.shape(speeches)[0]):\n    perc_comp = speech_row / np.shape(speeches)[0] * 100\n    sys.stdout.write(f\"\\rWebscrape Progress: {int(np.round(perc_comp))}%\")\n    speech_URL = speeches['URL'][speech_row]\n    speech_page = requests.get(speech_URL)\n    speech_soup = BeautifulSoup(speech_page.content, 'lxml')\n    all_paragraphs = speech_soup.findChildren('p', \n                                              recursive = True, \n                                              attrs={'class': None})\n    all_text = [p.text.translate(translator) for p in all_paragraphs]\n    if len(all_text) > 2:\n        end_idx = np.max(np.where([' amen.' in text \n                                   or ' Amen.' in text \n                                   or int_prop in text\n                                   or unavail_text in text\n                                   or rights in text\n                                   for text in all_text]))\n        if unavail_text in all_text[end_idx] or int_prop in all_text[end_idx] or rights in all_text[end_idx]:\n            end_idx += -1\n            all_text = [text for text in all_text if 'Speech highlights' not in text]\n        content = all_text[:(end_idx+1)]\n        all_content += [content]\n    else:\n        rows_to_rm += [speech_row]\n    sys.stdout.flush()\n\nspeeches = speeches[~speeches.index.isin(rows_to_rm)]\n\nsys.stdout.write(\"\\rWebscrape Progress: 100%\\n\")\nsys.stdout.write(\"Webscrape complete!\\n\")\n\nquotes = pd.DataFrame(columns = list(speeches.columns))\n\nfor i in range(np.shape(speeches)[0]):\n    perc_comp = i / np.shape(speeches)[0] * 100\n    sys.stdout.write(f\"\\rData Compiling Progress: {int(np.round(perc_comp))}%\")\n    num_quotes = len(all_content[i])\n    speech_speaker = [speeches.iloc[i,0] for x in range(num_quotes)]\n    speech_date = [speeches.iloc[i,1] for x in range(num_quotes)]\n    speech_title = [speeches.iloc[i,2] for x in range(num_quotes)]\n    speech_URL = [speeches.iloc[i,3] for x in range(num_quotes)]\n    \n    speech_quote_info = pd.DataFrame({'Speaker':speech_speaker,\n                                     'Date':speech_date,\n                                     'Title':speech_title,\n                                     'URL':speech_URL,\n                                     'Content':all_content[i]})\n    quotes = quotes.append(speech_quote_info, ignore_index = True)\nsys.stdout.write(\"\\rData Compiling Progress: 100%\\n\")\nsys.stdout.write(\"Compilation complete!\\n\")\n\nquotes.to_csv('Speech_Quotes_Data.csv', index = False)\n", "318": "from multiprocessing import Process\r\nimport requests\r\nfrom bs4 import BeautifulSoup as soup\r\nimport language_check\r\nimport time\r\nimport random\r\n\r\ntool = language_check.LanguageTool('en-US')\r\n#Declare the file we want to store to\r\n\r\ndef WebScrape(startPage, endPage):\r\n    fileName = 'super_magic_god_of_harry_potter_langCheck'+ str(startPage) + '_' + str(endPage) +'.txt'\r\n    f= open(fileName,\"w+\",encoding=\"utf-8\")\r\n\r\n    myUrl = 'http://lnmtl.com/chapter/super-magic-god-of-harry-potter-chapter-'\r\n\r\n\r\n    for page in range(startPage, endPage):\r\n        #traverse the Site\r\n        print(page)\r\n        srhUrl =  myUrl + str(page) + \"/\"\r\n        uClient =  requests.get(srhUrl)\r\n        uClient.encoding = \"utf-8\"\r\n        html_content = soup(uClient.content, 'html.parser')\r\n\r\n        Body = html_content.findAll(\"div\",{\"class\":'chapter-body'})\r\n\r\n        translatedBody = Body[0].findAll(\"sentence\",{\"class\":'translated'})\r\n        count = 0\r\n        for transdiv in translatedBody:\r\n            temp  = transdiv.text\r\n            temp = temp.replace(\"\\t\", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")\r\n            temp = temp.replace(\"\u201e\",\"\\\"\").replace(\"\u201d\",\"\\\"\")\r\n            matches = tool.check(temp)\r\n            #print(len(matches))\r\n            if(len(matches) > 0):\r\n                temp = language_check.correct(temp, matches)\r\n            #print(temp)\r\n            if count == 0:\r\n                f.write(temp)\r\n                f.write(\"\\n\")\r\n                count += 1\r\n            else:\r\n                f.write(temp)\r\n            f.write(\"\\n\")\r\n\r\n\r\n        f.write(\"\\n\\n\")\r\n        # f1.write(\"\\n\\n\")\r\n        print(\"Completed page\")\r\n\r\n        num = random.randint(5, 8)\r\n\r\n        time.sleep(num)\r\n\r\n    f.close()\r\n\r\n\r\nif __name__ == '__main__':\r\n    proc = []\r\n    i=1\r\n    while i < 796:\r\n        srtPage = 0\r\n        endPage = 0\r\n        if i > 750:\r\n            srtPage = i\r\n            endPage = i + 45\r\n        else:\r\n            srtPage = i\r\n            endPage = i + 50\r\n        print(str(srtPage) + '-' + str(endPage))\r\n        p = Process(target=WebScrape, args=(srtPage,endPage))       \r\n        proc.append(p)\r\n        i += 50 \r\n        \r\n    for p in proc: \r\n        print(p)\r\n        p.start()\r\n        p.join()\r\n", "319": "import os\r\nimport requests\r\nfrom bs4 import BeautifulSoup as soup\r\nimport language_check\r\nimport time\r\nimport random\r\n\r\ntool = language_check.LanguageTool('en-US')\r\n#Declare the file we want to store to\r\n\r\ndef WebScrape( myUrl, fileName, startPage, endPage):\r\n\r\n    if os.path.exists(fileName):\r\n        print(\"Append TO existing file\")\r\n        f= open(fileName,\"a+\",encoding=\"utf-8\")\r\n    else:\r\n        print(\"Create new file\")\r\n        f= open(fileName,\"w+\",encoding=\"utf-8\")\r\n    #myUrl = 'https://comrademao.com/mtl/all-attributes-martial-path/all-attributes-martial-path-chapter-'\r\n\r\n    for page in range(startPage, endPage):\r\n        #traverse the Site\r\n        print(page)\r\n        srhUrl =  myUrl + str(page) + \"/\"\r\n        try:\r\n            uClient =  requests.get(srhUrl)\r\n            uClient.encoding = \"utf-8\"\r\n            html_content = soup(uClient.content, 'html.parser')\r\n\r\n            Body = html_content.findAll(\"article\",{\"class\":'status-publish'})\r\n\r\n            translatedBody = Body[0].findAll(\"p\")\r\n            transaltedText = translatedBody[0].find_all(\"p\",class_=False)\r\n            count = 0\r\n            for transdiv in transaltedText:\r\n                temp  = transdiv.text\r\n                temp = temp.replace(\"\\t\", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")\r\n                temp = temp.replace(\"\u201e\",\"\\\"\").replace(\"\u201d\",\"\\\"\").replace(\"\u201c\",\"\\\"\").replace(\"\u201d\",\"\\\"\")\r\n                matches = tool.check(temp)\r\n                #print(len(matches))\r\n                if(len(matches) > 0):\r\n                    temp = language_check.correct(temp, matches)\r\n                #print(temp)\r\n                if count == 0:\r\n                    f.write(temp)\r\n                    f.write(\"\\n\")\r\n                    count += 1\r\n                else:\r\n                    f.write(temp)\r\n                f.write(\"\\n\")\r\n\r\n            f.write(\"\\n\\n\")\r\n            print(\"Completed page\")\r\n        except:\r\n            continue\r\n        num = random.randint(5, 8)\r\n\r\n        time.sleep(num)\r\n\r\n    f.close()\r\n\r\n\r\nif __name__ == '__main__':\r\n        srtPage = 201\r\n        endPage = 593\r\n        fileName = \"all_attributes_martial_path.txt\"\r\n        myUrl = 'https://comrademao.com/mtl/all-attributes-martial-path/all-attributes-martial-path-chapter-'\r\n        WebScrape(myUrl, fileName, srtPage, endPage)\r\n        \r\n", "320": "#from . import itemToData as i2d\nfrom webscraper import webscrapeGymgrossisten as wsg\nfrom webscraper import webscrapeMatsmart as wsm\nimport json\n\n\nclass proteinProduct:\n    def __init__(self, name, desc='', proteinPercentage=0, proteinPerItem=0, price=0, ppk=0, itemsInBatch=0, site=''):\n        self.name = name\n        self.description = desc\n        self.priceTotal = priceTotal\n        self.proteinPercentage = proteinPercentage\n        self.proteinPerItem = proteinPerItem\n        self.itemsInBatch = itemsInBatch\n        self.proteinTotal = self.ItemsInBatch * self.proteinPerItem\n        self.ppk = self.proteinTotal / self.priceTotal\n        self.site = site\n\n        # ppk = proteinTotal / priceTotal\n        # ProteinTotal = ItemsInBatch * proteinPerItem\n\n        # * Name, Description(desc, proteininneh\u00e5ll, j\u00e4mf\u00f6r med gainomax bar), price, protein per krona\n\n\ndef ppk():\n    pass\n\n\ndef wsgToJson():\n    jsona = json.dumps([vars(item) for item in wsg.getProducts()], indent=4)\n    f = open(\"data/gymgrossisten.json\", \"w\")\n    f.write(jsona)\n    f.close()\n\n\ndef wsmToJson():\n    jsona = json.dumps([vars(item) for item in wsm.getProducts()], indent=4)\n    f = open(\"data/matsmart.json\", \"w\")\n    f.write(jsona)\n    f.close()\n\n\ndef bigFunc():\n    pass\n    # WHAT WE WANT:\n    # * pull json (A)\n    # * pull scrape (B)\n    # * Turn all in A which is in B \"active\". All in A which is not in B \"inactive\"\n    # * All in B which are not in A, write to file with basic values\n\n    # WHAT INFORMATION DO WE WANT IN BIG FILE:\n    # * Name, Description(desc, proteininneh\u00e5ll, j\u00e4mf\u00f6r med gainomax bar), price, protein per krona\n", "321": "# -*- mode: python ; coding: utf-8 -*-\n\nblock_cipher = None\n\n\na = Analysis(['Webscrape.py'],\n             pathex=['C:\\\\Users\\\\albcy\\\\PycharmProjects\\\\untitled3\\\\venv'],\n             binaries=[],\n             datas=[],\n             hiddenimports=[],\n             hookspath=[],\n             runtime_hooks=[],\n             excludes=[],\n             win_no_prefer_redirects=False,\n             win_private_assemblies=False,\n             cipher=block_cipher,\n             noarchive=False)\npyz = PYZ(a.pure, a.zipped_data,\n             cipher=block_cipher)\nexe = EXE(pyz,\n          a.scripts,\n          a.binaries,\n          a.zipfiles,\n          a.datas,\n          [],\n          name='Webscrape',\n          debug=False,\n          bootloader_ignore_signals=False,\n          strip=False,\n          upx=True,\n          upx_exclude=[],\n          runtime_tmpdir=None,\n          console=False )\n", "322": "\"\"\"\nRequest trail data from all resorts\n\"\"\"\n\nimport os\nimport time\nfrom datetime import date\n\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom tqdm import tqdm\n\n\nclass WebscrapeTrails:\n    def __init__(self):\n\n        self.CURRENT_DIRECTORY = os.getcwd()\n\n        self.MAIN_URL = \"https://jollyturns.com/resort/united-states-of-america\"\n\n        self.ALPINE_MEADOWS_URL = f\"{self.MAIN_URL}/alpine-meadows/\"\n        self.ARAPAHOE_BASIN_URL = f\"{self.MAIN_URL}/arapahoe-basin/\"\n        self.ASPEN_SNOWMASS_URL = f\"{self.MAIN_URL}/aspen-snowmass/\"\n        self.BALD_MOUNTAIN_URL = f\"{self.MAIN_URL}/bald-mountain/\"\n        self.BEAVER_CREEK_URL = f\"{self.MAIN_URL}/beaver-creek-resort/\"\n        self.COPPER_URL = f\"{self.MAIN_URL}/copper-mountain-resort/\"\n        self.CRESTED_BUTTE_URL = f\"{self.MAIN_URL}/crested-butte-mountain-resort/\"\n        self.DIAMOND_PEAK_URL = f\"{self.MAIN_URL}/diamond-peak/\"\n        self.ELDORA_URL = f\"{self.MAIN_URL}/eldora-mountain-resort/\"\n        self.JACKSON_HOLE_URL = f\"{self.MAIN_URL}/jackson-hole/\"\n        self.LOVELAND_URL = f\"{self.MAIN_URL}/loveland-ski-area/\"\n        self.MONARCH_URL = f\"{self.MAIN_URL}/monarch-ski-area/\"\n        self.STEAMBOAT_URL = f\"{self.MAIN_URL}/steamboat-ski-resort/\"\n        self.TAOS_URL = f\"{self.MAIN_URL}/taos-ski-valley/\"\n        self.TELLURIDE_URL = f\"{self.MAIN_URL}/telluride-ski-resort/\"\n        self.VAIL_URL = f\"{self.MAIN_URL}/vail-ski-resort/\"\n        self.WINTER_PARK_URL = f\"{self.MAIN_URL}/winter-park-resort/\"\n        self.WOLF_CREEK_URL = f\"{self.MAIN_URL}/wolf-creek-ski-area/\"\n\n        self.URLs = [\n            self.ALPINE_MEADOWS_URL,\n            self.ARAPAHOE_BASIN_URL,\n            self.ASPEN_SNOWMASS_URL,\n            self.BALD_MOUNTAIN_URL,\n            self.BEAVER_CREEK_URL,\n            self.COPPER_URL,\n            self.CRESTED_BUTTE_URL,\n            self.DIAMOND_PEAK_URL,\n            self.ELDORA_URL,\n            self.JACKSON_HOLE_URL,\n            self.LOVELAND_URL,\n            self.MONARCH_URL,\n            self.STEAMBOAT_URL,\n            self.TAOS_URL,\n            self.TELLURIDE_URL,\n            self.VAIL_URL,\n            self.WINTER_PARK_URL,\n            self.WOLF_CREEK_URL,\n        ]\n\n        self.browser_options = webdriver.ChromeOptions()\n        self.browser_options.add_argument(\"--no-sandbox\")\n        self.browser_options.add_argument(\"--headless\")\n        self.browser_options.add_argument(\"--disable-gpu\")\n\n        self.browser = webdriver.Chrome(options=self.browser_options)\n\n        self.lst_run_difficulty = [\n            \"skiruns-green\",\n            \"skiruns-blue\",\n            \"skiruns-black\",\n            \"skiruns-double-black\",\n        ]\n\n        self.blank_value = \"__NA__\"\n\n    def make_tables(self, URL: str) -> pd.core.frame.DataFrame:\n        \"\"\"\n        Inputs:\n            URL from URLs (str)\n        Outputs:\n            Pandas DataFrame of ski resort information\n        \"\"\"\n\n        self.browser.get(URL)\n\n        time.sleep(3)\n\n        soup = BeautifulSoup(self.browser.page_source, \"html.parser\")\n\n        X_web_trail = soup.select(\"table.table-striped tr\")\n\n        lst_rows = [x.text.strip() for x in X_web_trail]\n        lst_rows = [i.replace(\"  \", \"|\") for i in lst_rows]\n        lst_rows = [i.replace(\" ft\", \"|\") for i in lst_rows]\n        lst_rows = [i.replace(\" mi\", \"|\") for i in lst_rows]\n\n        lst_cols = [\n            \"Name Bottom Top Vertical rise\",\n            \"Name Bottom Top Vertical drop Length\",\n            \"Name Elevation\",\n        ]\n\n        # Indices where headers start, separating runs by difficulty\n        idx_headers = [i for i, j in enumerate(lst_rows) if j in lst_cols]\n\n        # Create DataFrame from rows\n        df_trails = pd.DataFrame(lst_rows, columns=[\"trail_data\"]).reset_index(\n            drop=True\n        )\n\n        # Expand DataFrame values into separate columns\n        df_trails = df_trails[\"trail_data\"].str.split(\"|\", expand=True)\n        df_trails.columns = [\n            \"Trail Name\",\n            \"Bottom Elev (ft)\",\n            \"Top Elev (ft)\",\n            \"Vertical Drop (ft)\",\n            \"Length (mi)\",\n            \"Blank\",\n        ]\n\n        df_trails.drop(\"Blank\", axis=1, inplace=True)\n\n        lst_difficulty = soup.select(\"h4\")\n        lst_difficulty = [l.text.strip() for l in lst_difficulty]\n        lst_difficulty = [i.replace(\"Ski runs: \", \"\") for i in lst_difficulty]\n\n        df_difficulties = pd.DataFrame(lst_difficulty, index=idx_headers)\n\n        df_combined = pd.merge(\n            df_trails, df_difficulties, left_index=True, right_index=True, how=\"outer\"\n        )\n\n        df_combined.rename(columns={0: \"Difficulty\"}, inplace=True)\n\n        df_combined[\"Difficulty\"].fillna(method=\"ffill\", inplace=True)\n\n        # Remove rows which are not trail names\n        df_combined = df_combined[\n            ~df_combined[\"Trail Name\"].isin(lst_cols)\n        ].reset_index(drop=True)\n\n        # Remove runs with no name and trails with __NA__ value\n        df_combined = df_combined[df_combined[\"Trail Name\"].notnull()].reset_index(\n            drop=True\n        )\n        df_combined = df_combined[\n            df_combined[\"Trail Name\"] != self.blank_value\n        ].reset_index(drop=True)\n\n        # Remove Lifts, Restaurants, and Terrain Park\n        df_combined = df_combined[\n            ~df_combined[\"Difficulty\"].isin([\"Lifts\", \"Restaurants\", \"terrain park\"])\n        ].reset_index(drop=True)\n\n        # Add URL\n        df_combined[\"URL\"] = URL\n\n        return df_combined\n\n    def rename_resorts(self, df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n        \"\"\"\n        Rename resorts for recommendation system\n\n        INPUT\n            df: Pandas DataFrame\n        OUTPUT\n            Pandas DataFrame, with resort_name column altered\n        \"\"\"\n\n        df[\"Resort\"] = df[\"URL\"].str.split(\"united-states-of-america/\", 1, expand=True)[\n            1\n        ]\n        df[\"Resort\"] = df[\"Resort\"].str.split(\"/\", 1, expand=True)[0]\n\n        dict_webscrape_trail_names = {\n            \"alpine-meadows\": \"Alpine Meadows\",\n            \"arapahoe-basin\": \"Arapahoe Basin\",\n            \"aspen-snowmass\": \"Aspen Snowmass\",\n            \"bald-mountain\": \"Bald Mountain\",\n            \"beaver-creek-resort\": \"Beaver Creek\",\n            \"copper-mountain-resort\": \"Copper\",\n            \"crested-butte-mountain-resort\": \"Crested Butte\",\n            \"diamond-peak\": \"Diamond Peak\",\n            \"eldora-mountain-resort\": \"Eldora\",\n            \"jackson-hole\": \"Jackson Hole\",\n            \"loveland-ski-area\": \"Loveland\",\n            \"monarch-ski-area\": \"Monarch\",\n            \"steamboat-ski-resort\": \"Steamboat\",\n            \"taos-ski-valley\": \"Taos\",\n            \"telluride-ski-resort\": \"Telluride\",\n            \"vail-ski-resort\": \"Vail\",\n            \"winter-park-resort\": \"Winter Park\",\n            \"wolf-creek-ski-area\": \"Wolf Creek\",\n        }\n\n        df[\"Resort\"] = df[\"Resort\"].map(dict_webscrape_trail_names).fillna(df[\"Resort\"])\n\n        # Drop URL column\n        df.drop(\"URL\", axis=1, inplace=True)\n\n        # Reset index\n        df = df.reset_index(drop=True)\n\n        return df\n\n    def save_trail_data(self, df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n        \"\"\"\n        Save trail data to Parquet file\n        \"\"\"\n\n        current_date = date.today().strftime(\"%Y%m%d\")\n\n        df.to_parquet(\n            f\"{self.CURRENT_DIRECTORY}/data/trail_data_{current_date}.parquet\",\n            index=False,\n        )\n\n\nif __name__ == \"__main__\":\n\n    ws = WebscrapeTrails()\n\n    # Request trail data from all ski resorts\n    # TODO: Request resorts in parallel?\n    lst_trail_data = [ws.make_tables(URL=url) for url in tqdm(ws.URLs)]\n\n    # Combine trail data\n    df_resorts = pd.concat(lst_trail_data)\n\n    # Capitalize trail difficulty\n    df_resorts[\"Difficulty\"] = df_resorts[\"Difficulty\"].str.title()\n\n    # Get resort name for trails\n    df_resorts = ws.rename_resorts(df=df_resorts)\n\n    # Format trail values\n    lst_formatted_cols = [\n        \"Bottom Elev (ft)\",\n        \"Top Elev (ft)\",\n        \"Vertical Drop (ft)\",\n        \"Length (mi)\",\n    ]\n    df_resorts[lst_formatted_cols] = df_resorts[lst_formatted_cols].astype(\"float64\")\n\n    # Convert run distance from miles to feet\n    df_resorts.loc[df_resorts[\"Length (mi)\"] < 1, \"Length (mi)\"] = (\n        df_resorts[\"Length (mi)\"] * 5280\n    )\n\n    # Format trail values as integers\n    df_resorts[lst_formatted_cols] = df_resorts[lst_formatted_cols].astype(int)\n\n    # Rename Length column\n    df_resorts.rename(columns={\"Length (mi)\": \"Slope Length (ft)\"}, inplace=True)\n\n    # Calculate average steepness\n    df_resorts[\"Average Steepness\"] = (\n        df_resorts[\"Vertical Drop (ft)\"] / df_resorts[\"Slope Length (ft)\"]\n    ).round(2)\n\n    # Save trail data\n    # ws.save_trail_data(df=df_resorts)\n", "323": "'''\nCreated on Aug 5, 2021\n\n@author: Jacob Summers\n'''\nfrom urllib.request import urlopen, Request\nimport textwrap\nfrom webscrape.searchresults.categorydetails.moviedetail.movieparse.movie_parser import movie_parser\nfrom webscrape.searchresults.categorydetails.moviedetail.moviedisplay import movie_user_reviews\nfrom webscrape import clear\n\n#views the details of the movie that are parsed out by the movie_parser object\ndef view_movie_details(movie_link):\n    #clear the command line output\n    clear()\n    #obtain the html code for the movie page\n    req = Request(movie_link, headers={'User-Agent': 'Mozilla/5.0'})\n    page = urlopen(req).read()\n    html_movie = page.decode(\"utf-8\")\n    \n    #create the parser and parse each element\n    parser = movie_parser(html_movie)\n    name = parser.get_name()\n    cast = parser.get_cast()\n    summary = parser.get_summary()\n    summary = summary.replace('', '\\n')\n    credit = parser.get_credit()\n    \n    #print the results to the command line\n    print(name)\n    print(cast + \"\\n\")\n    #create the formatting for the summary\n    body = '\\n\\n'.join(['\\n'.join(textwrap.wrap(line, 100,\n                 break_long_words=False, replace_whitespace=False))\n                 for line in summary.splitlines() if line.strip() != ''])\n    \n    print(body)\n    print(\"\\n\" + credit + \"\\n\\n\")\n    \n    #prompt the user to view the reviews of the movie\n    answer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n    \n    #while the\n    while (answer == \"r\"):\n        \n        movie_user_reviews.view_movie_user_reviews(movie_link + \"/user-reviews\")\n        #clear the command line output\n        clear()\n        #print the results to the command line\n        print(name)\n        print(cast + \"\\n\")\n        print(body)\n        print(\"\\n\" + credit + \"\\n\\n\")\n        \n        #prompt the user to view the reviews of the movie\n        answer = input(\"Type r to view user reviews. Type anything else to go back to results page\\n\")\n    \n    \n    \n    \n    ", "324": "import streamlit as st\r\nimport json\r\nimport requests\r\n\r\n\r\n\r\nst.title(\"Productionizing the Pipeline\")\r\n\r\nmenu = [\"Home\", \"Login\", \"API2\",\"API3 & API4\",\"SignUp\"]\r\nchoice = st.sidebar.selectbox(\"Menu\",menu)\r\n\r\nif choice == \"Home\":\r\n     st.subheader(\"Home\")\r\n\r\nelif choice == \"Login\":\r\n    st.subheader(\"Login Section \")\r\n\r\n    username = st.sidebar.text_input(\"User Name\")\r\n    password = st.sidebar.text_input(\"Password\", type='password')\r\n    if st.sidebar.checkbox(\"Login\"):\r\n        if password=='12345':\r\n            st.success(\"Logged in as {}\".format(username))\r\n\r\n            # Connecting with API Gateway to invoke lambda functions\r\n            # #API 1\r\n            st.subheader(\"API1 - Scraping Data\")\r\n            scrape_data = st.text_input(\"Enter the url to scrape data\")\r\n            get_url = scrape_data\r\n\r\n            if scrape_data:\r\n                gateway_url1 = 'https://7bbv59k23f.execute-api.us-east-1.amazonaws.com/Test1/webscrape'\r\n                #convert into json format\r\n                json_payload1 = json.dumps({ \"url\": get_url})\r\n                response1 = requests.post(gateway_url1, data=json_payload1)\r\n\r\n                data1 = response1.json()\r\n                display_data1 = st.json(data1)\r\n\r\n        else:\r\n            st.warning(\"Incorrect Username/Passwrod\")\r\n#API2\r\nelif choice == \"API2\":\r\n    st.subheader(\"API2 - Named Entity Recognition \")\r\n    gateway_url2 = 'https://sjys10yiqj.execute-api.us-east-1.amazonaws.com/dev/entitiesdetection'\r\n    # convert into json format\r\n    json_payload2 = json.dumps({'key': 'ScrapedFolder/webscrape.txt', 'decision': 'Hello'})\r\n    response2 = requests.post(gateway_url2, data=json_payload2)\r\n\r\n    data2 = response2.json()\r\n    display_data2 = st.json(data2)\r\n\r\n#API 3 & API4\r\nelif choice == \"API3 & API4\":\r\n    st.subheader(\"API3 & API4 - Anonymization & De-Anonymization Entities\")\r\n\r\n    radio_list = st.radio(\r\n        \"How do u wanna view your data?\",\r\n        ('Masked View', 'De-identify View'))\r\n\r\n    if radio_list == 'Masked View':\r\n        st.write('Your data is masked')\r\n\r\n\r\n        gateway_url3 = 'https://sjys10yiqj.execute-api.us-east-1.amazonaws.com/dev/dataanonymization'\r\n\r\n        json_payload3 = json.dumps({ 'filelocation': 'ScrapedFolder/webscrape.txt', 'decision':'anonymize'})\r\n\r\n        response3 = requests.post(gateway_url3, data=json_payload3)\r\n\r\n        data3 = response3.json()\r\n        display_data3 = st.text_area(data3)\r\n\r\n\r\n    else:\r\n        st.write('Your data is not masked')\r\n\r\n        gateway_url4 = 'https://sjys10yiqj.execute-api.us-east-1.amazonaws.com/dev/dataanonymization'\r\n\r\n        json_payload4 = json.dumps({ 'filelocation': 'ScrapedFolder/webscrape.txt', 'decision': 'deidentify'})\r\n\r\n        response4 = requests.post(gateway_url4, data=json_payload4)\r\n\r\n        data4 = response4.json()\r\n        display_data4 = st.text_area(data4)\r\n\r\n\r\nelif choice == \"SignUp\":\r\n    st.subheader(\"Create New Account\")\r\n    new_user = st.text_input(\"Username\")\r\n    new_password = st.text_input(\"Password\",type='password')\r\n\r\n    if st.button(\"SignUp\"):\r\n        st.success(\"You have successfully created a valid account\")\r\n        st.info(\"Go to Login Menu to Login\")\r\n\r\n", "325": "#!/usr/bin/env python3\n\nimport sys\nimport requests, bs4, click\n\n# Generate search url from given arguments\ndef get_search_url(args):\n    args = \"+\".join(args)\n    search_url = f\"https://www.imdb.com/find?q={args}&s=tt&ttype=ft\"\n    return search_url\n\n\n# Store search page html\ndef get_search_html(url):\n    res = requests.get(url)\n    res.raise_for_status()\n    search_html = bs4.BeautifulSoup(res.text, features='html.parser')\n    return search_html\n\n    \n# Webscrape search page & return list of title names\ndef get_search_titles(html):\n    title_elems = html.select(\"#main td.result_text\")\n    titles = []\n    for title in (title_elems):\n        titles.append(title.getText().strip())\n    return titles\n\n\n# Webscrape search page & return list of title IDs\ndef get_search_ids(html):\n    id_elems = html.select(\"#main td.result_text a\")\n    ids = []\n    for id_elem in (id_elems):\n        title_id = id_elem.attrs['href']    \n        ids.append(title_id)\n    return ids \n\n\n# Webscrape parental guidance page & store nudity list\ndef get_nudity_list(title_id):\n    parents_guide_url = f'https://www.imdb.com{title_id}parentalguide'\n    res = requests.get(parents_guide_url)\n    res.raise_for_status()\n    parents_guide_html = bs4.BeautifulSoup(res.text, features='html.parser')\n    nudity_elems = parents_guide_html.select('#advisory-nudity li.ipl-zebra-list__item')\n    nudity_list = []\n    for _, list_item in enumerate(nudity_elems):\n        nudity_list.append(list_item.getText().strip().rstrip('Edit').strip())\n    return nudity_list\n\n\n# Print out movie title and nudity info to stdout\ndef show_nudity_info(nudity_list, title):\n    click.echo(title + \"\\n\")\n    for _, item in enumerate(nudity_list):\n        click.echo(\"* \" + item)\n\n\n# List search titles & prompt for title selection \ndef show_title_list(search_titles):\n    while True:\n        selections = [\"q\"]\n        print()\n        for i, title in enumerate(search_titles[:10]):\n            print(f\"{i+1}) {title}\")\n            selections.append(str(i+1))\n        print()\n        title_selection = input(\"Enter [title #] or [q] to quit: \")\n        if title_selection in selections:\n            break  \n\n    # Validate selection choice\n    if title_selection.lower() == \"q\":\n        exit()\n    else:\n        return (int(title_selection)-1)\n    \n\n@click.command()\n@click.argument('movie_title', nargs=-1, required=True)\n@click.option('-l', is_flag=True, help='Select from title list.')\ndef main(movie_title, l):\n    \"\"\"Display nudity information for given movie title.\"\"\"\n\n    search_url = get_search_url(movie_title)\n    search_html = get_search_html(search_url)\n    search_titles = get_search_titles(search_html)\n    search_ids = get_search_ids(search_html)\n\n    if l:\n        title_index = show_title_list(search_titles)\n    else:\n        title_index = 0\n\n    nudity_list = get_nudity_list(search_ids[title_index])\n    show_nudity_info(nudity_list, search_titles[title_index])\n\n\nif __name__ == '__main__':\n    main()\n", "326": "import requests\nimport numpy as np\nimport pandas as pd\nimport argparse\nfrom bs4 import BeautifulSoup\n\nclass WebScrape:\n    '''\n    This web scrapping class allows you to scrape a single news article of your choice,\n    or you can scrape multiple news article and store it to a dataframe.  You need to initialize the \n    class with the specific url and depending on your choice, run to scrape one article or more than one. \n    Keep in mind that the scrape_N_articles() also allows you to scrape one article, however it is a\n    random article from the landing page of NBC News.\n\n    def __init__()              ---->   Initialize requests and obtains content from landing page of URL\n\n    def scrape_news_article()   ---->   Web Scrapping of a single news article\n\n    def scrap_N_articles()      ---->   Web Scrapping of multipl articles of the NBC home page\n\n    '''\n    def __init__(self, url):\n        self.url = url\n        self.request = requests.get(self.url)\n\n        # We'll save in coverpage the cover page content\n        self.coverpage = self.request.content\n\n        # Soup creation\n        self.soup = BeautifulSoup(self.coverpage, 'html5lib')\n\n    def scrape_news_article(self):\n        title = self.soup.find('h1').get_text()\n        x = self.soup.find_all('p', {'class':['','endmark']})\n\n        # Unifying the paragraphs\n        list_paragraphs = []\n        for p in np.arange(0, len(x)):\n            paragraph = x[p].get_text()\n            list_paragraphs.append(paragraph)\n            final_article = \" \".join(list_paragraphs)\n\n        article_dict = {'article link': self.url, 'article title' : title, 'article content': final_article}\n\n        return article_dict\n\n    def scrape_n_articles(self, num_articles=1):\n        # News identification\n        coverpage_news = []\n        for tag in self.soup.find_all('h2', class_='styles_headline__ice3t'):\n            for anchor in tag.find_all('a'):\n                coverpage_news.append(anchor)\n\n        print('Number of articles found: {}'.format(len(coverpage_news)))\n\n        ## Let's extract the text from the article\n        # Empty lists for content, links and titles\n        news_contents = []\n        list_links = []\n        list_titles = []\n\n        for n in np.arange(0, num_articles):\n                \n            # Getting the link of the article\n            link = coverpage_news[n]['href']\n            list_links.append(link)\n            \n            # Getting the title\n            title = coverpage_news[n].get_text()\n            list_titles.append(title)\n            \n            # Reading the content (it is divided in paragraphs)\n            article = requests.get(link)\n            article_content = article.content\n            soup_article = BeautifulSoup(article_content, 'html5lib')\n            x = soup_article.find_all('p', {'class':['','endmark']})\n            \n            # Unifying the paragraphs\n            list_paragraphs = []\n            final_article = \"\"\n            for p in np.arange(0, len(x)):\n                paragraph = x[p].get_text()\n                list_paragraphs.append(paragraph)\n                final_article = \" \".join(list_paragraphs)\n                \n            news_contents.append(final_article)\n\n        # df_show_info\n        nbc_articles = pd.DataFrame({\n            # 'Article Title': list_titles,\n            'article link': list_links,\n            'article content': news_contents})\n\n        return nbc_articles\n\nif __name__ == '__main__':\n\n    parser= argparse.ArgumentParser()\n    parser.add_argument('--nbc_url', type=str, default='https://www.nbcnews.com/')\n    parser.add_argument('--nbc_article_url', type=str, default='https://www.nbcnews.com/politics/biden-says-considering-gas-tax-holiday-rcna34419')\n    parser.add_argument('--save_path', type=str, default='../data/ws_data/')\n    parser.add_argument('--num_articles', type=int, default=5)\n    args = parser.parse_args()\n\n    # Scrape a single article\n    nbc_article = WebScrape(args.nbc_article_url)\n    article = nbc_article.scrape_news_article()\n\n    # Scrape N articles\n    nbc_news = WebScrape(args.nbc_url)\n    nbc_articles = nbc_news.scrape_n_articles(num_articles=args.num_articles)\n\n    # Save to dataframe for visibility of output\n    nbc_articles.to_csv(args.save_path + 'ws_nbc.csv')", "327": "import sys\r\n\r\nSearchPattern = 'error'\r\nSearchColumn = 4\r\n\r\nSearchPattern = 'Cardiac disorders'\r\nSearchColumn = 3\r\n\r\n\r\nyp = ''\r\nmp = ''\r\ndp = ''\r\nc1 = 0\r\nc2 = 0\r\nc3 = 0\r\n\r\nfi = open( 'DAEN_webscrape_medsummary.txt', 'r' )\r\nline = fi.readline()\r\nfo = open( 'DAEN_webscrape_medsummary_' + SearchPattern + '_' + str( SearchColumn ) + '.txt', 'w' )\r\nfo.write( 'Year\\tMonth\\tDay\\tSearchColumn\\tSearchPattern\\tNumber of cases\\tNumber of cases with a single suspected medicine\\tNumber of cases where death was a reported outcome\\n' )\r\nfo2 = open( 'DAEN_webscrape_medsummary_' + SearchPattern + '_' + str( SearchColumn ) + '_each.txt', 'w' )\r\nfo2.write( 'Year\\tMonth\\tDay\\tMedDRA system organ class\\tMedDRA reaction term\\tNumber of cases\\tNumber of cases with a single suspected medicine\\tNumber of cases where death was a reported outcome\\n' )\r\nwhile( 1 ):\r\n    line = fi.readline().strip()\r\n    if( line == '' ):\r\n        break\r\n    linesp = line.split( '\\t' )\r\n    if( ( yp != linesp[ 0 ] ) or ( mp != linesp[ 1 ] ) or ( dp != linesp[ 2 ] ) ):\r\n        if( yp != '' ):\r\n            # save it\r\n            fo.write( yp + '\\t' +\\\r\n                      mp + '\\t' +\\\r\n                      dp + '\\t' +\\\r\n                      str( SearchColumn ) + '\\t' +\\\r\n                      SearchPattern + '\\t' +\\\r\n                      str( c1 ) + '\\t' +\\\r\n                      str( c2 ) + '\\t' +\\\r\n                      str( c3 ) + '\\n' )\r\n        # put new values in yp, mp, dp\r\n        yp = linesp[ 0 ]\r\n        mp = linesp[ 1 ]\r\n        dp = linesp[ 2 ]\r\n        c1 = 0\r\n        c2 = 0\r\n        c3 = 0\r\n        \r\n    if( linesp[ SearchColumn ].find( SearchPattern ) >= 0 ):\r\n        fo2.write( line + '\\n' )\r\n        c1 += int( linesp[ 5 ] )\r\n        c2 += int( linesp[ 6 ] )\r\n        c3 += int( linesp[ 7 ] )\r\n        \r\nfo.write( yp + '\\t' +\\\r\n          mp + '\\t' +\\\r\n          dp + '\\t' +\\\r\n          str( SearchColumn ) + '\\t' +\\\r\n          SearchPattern + '\\t' +\\\r\n          str( c1 ) + '\\t' +\\\r\n          str( c2 ) + '\\t' +\\\r\n          str( c3 ) + '\\n' )\r\n\r\nfi.close()\r\nfo.close()\r\nfo2.close()\r\n                                \r\n", "328": "\"\"\"\r\nclass and methods to webscrape historical flights data in the US airspace network\r\nfrom https://www.transtats.bts.gov using Selenium.\r\n\"\"\"\r\n\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.keys import Keys\r\nfrom selenium.webdriver.support.ui import Select\r\nimport time\r\n\r\n\r\nclass flights_webscrape:\r\n\r\n    def __init__(self,url,years):\r\n        self.url  = url\r\n        self.years = years\r\n        \r\n    def get_chrome_driver(self,driver_path):\r\n        \"\"\"\r\n        create a chrome driver object for Selenium from the specified driver file\r\n        \"\"\"\r\n        driver = webdriver.Chrome(driver_path)\r\n        return driver\r\n    \r\n    def create_param_list(self):\r\n        \"\"\"\r\n        create list of parameters to pull from the url \r\n        \"\"\"\r\n        # Time Period Parameters\r\n        Tparam = ['Year','Month','DayofMonth','DayOfWeek','FlightDate']\r\n        # Airline Parameters\r\n        Aparam = ['Reporting_Airline','Tail_Number','Flight_Number_Reporting_Airline']\r\n        # Origin Parameters\r\n        Oparam = ['OriginAirportID','Origin','OriginCityName','OriginStateName','OriginWac']\r\n        # Destination Parameters\r\n        Dparam = ['DestAirportID','Dest','DestCityName','DestStateName','DestWac']\r\n        # Departure Performance\r\n        DPfparam = ['CRSDepTime','DepTime','DepDelay','DepartureDelayGroups','TaxiOut','WheelsOff']\r\n        # Arrival Performance\r\n        APfparam = ['CRSArrTime','ArrTime','ArrDelay','ArrivalDelayGroups','TaxiIn','WheelsOn']\r\n        # Cancellations\r\n        Cparam = ['Cancelled','CancellationCode','Diverted']\r\n        # Flight Summary\r\n        Fparam = ['AirTime','Flights','Distance']\r\n        # Cause of Delay\r\n        CDparam = ['CarrierDelay','WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay']\r\n        # Gate Return Delay\r\n        Gparam = ['FirstDepTime','TotalAddGTime','LongestAddGTime']\r\n        # Diverted Landings\r\n        Dvparam = ['DivAirportLandings']\r\n        \r\n        param_list = Tparam + Aparam + Oparam + Dparam + DPfparam + APfparam + Cparam + Fparam + CDparam + Gparam + Dvparam\r\n        return param_list\r\n    \r\n    def download_datasets(self,driver,param_list):\r\n        \"\"\"\r\n        Select the list of parameters and year on the url and download the flights data for given year\r\n        \"\"\"\r\n        ## go to specified web url\r\n        driver.get(self.url)\r\n\r\n        ## select all the parameters of interest on the css selector\r\n        for param in param_list:\r\n            schk = driver.find_element_by_css_selector('input[Title='+param+']')\r\n            webdriver.ActionChains(driver).move_to_element(schk).click(schk).perform()\r\n        \r\n        ## scroll up to top of window\r\n        driver.execute_script(\"window.scrollTo(0, 0)\")\r\n\r\n        ## Pull Data by Year\r\n        for year in self.years:\r\n            for month in np.arange(1,13):\r\n                selectY = Select(driver.find_element_by_id('XYEAR'))\r\n                selectY.select_by_value(year)\r\n                selectM = Select(driver.find_element_by_id('FREQUENCY'))\r\n                selectM.select_by_value(str(month))\r\n                ## click the download button (files go to browser downloads folder)\r\n                sbtn = driver.find_element_by_css_selector('button[onclick=\"tryDownload()\"]')\r\n                sbtn.click()\r\n                time.sleep(45)\r\n        \r\n        ## close the driver once download is complete\r\n        driver.close()\r\n\r\n    def write_parquet(self,read_path,write_path):\r\n        \"\"\"\r\n        convert csvs in zip files to parquet for faster spark queries\r\n        \"\"\"\r\n        import pandas as pd\r\n        import os\r\n        import pyarrow as pa\r\n        import pyarrow.parquet as pq\r\n        \r\n        filelist = [f for f in os.listdir(read_path) if 'ONTIME_REPORTING' in f]\r\n        \r\n        for file in filelist:\r\n            temp = pd.read_csv(read_path + file,compression='zip')\r\n            table = pa.Table.from_pandas(temp)\r\n            # Local dataset write\r\n            pq.write_to_dataset(table, root_path=write_path,partition_cols=['YEAR', 'MONTH'])\r\n        return\r\n\r\n\r\n    \r\n\r\n", "329": "from flask import Blueprint, render_template, request\nfrom base64 import encode\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nimport time\nimport datetime\n\nwebscrape_blueprint = Blueprint(\"webscrape\", __name__)\n\n#Opens the duunitori web page\nurl = \"https://duunitori.fi/\"\ndriver = webdriver.Edge()\ndriver.maximize_window()\ndriver.get(url)\n\n#Waits until the element is located on the page and then clicks it if it's present, quit if the element can't be located. \n#This eliminates possible \"element not interactable exceptions\"\ntry:\n    cookies = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"gdpr-close\")))\n    cookies.click()\nexcept:\n    driver.quit()\n\n#Finds the search bar, inputs and submits the job you want to search for\ntry:\n    search = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"search-autocomplete\"]/ul/li/input')))\n    search.send_keys(\"Software Engineer\")\n    search.send_keys(Keys.RETURN)\nexcept:\n    driver.quit()\n\n\n#Gets the first job posting on the page, saves the company name and clicks to open up the job posting\npost = driver.find_elements(By.CLASS_NAME, \"job-box__hover\").get_attribute(\"data-position\")\n\nprint(post)\n\ncompanyName = post.get_attribute(\"data-company\")\nprint(companyName)\n    \npost.click()\n\n#Finds the paragraph where the date info is located, it'll be in the form of \"Julkaistu 17.2.\" for example\n#Adds the current year into the end of the string and then trims the \"Julkaistu\" from the beginning of the string\npostDate = driver.find_element(By.XPATH, \"/html/body/div[6]/div/div[1]/div[1]/div[1]/div[3]/p[2]/span[1]\").text + datetime.datetime.today().strftime(\"%Y\")\npostDate = postDate[10:]\nprint(postDate)\n\nlocation = driver.find_element(By.XPATH, \"/html/body/div[6]/div/div[1]/div[1]/div[1]/div[3]/p[1]/a[2]/span\").text\nprint(location)\n\nrole = driver.find_element(By.XPATH, \"/html/body/div[6]/div/div[1]/div[1]/div[1]/div[3]/h1\").text\nprint(role)\n\ndriver.quit()\n\n", "330": "import pandas as pd\nimport numpy\nimport csv\nimport regex\nfrom bs4 import BeautifulSoup as soup\nimport requests\n\n#This file was created to webscrape data from the purr site to find how many results there\n#Are for each of the subject tags\n\ninfo = pd.read_excel(\"PURRSubjectTags_wColleges.xlsx\") #Read subjects tags / colleges from excel sheet\nnumpyinfo = info.to_numpy() #Convert info to a numpy array\ncollegesdict = dict() #Create a dictionary where colleges will be the keys and departments the items\nfor i in numpyinfo:\n    collegesdict[i[1]] = list() #Create a list as the item for each college in the dictionary\nfor i in numpyinfo:\n    collegesdict[i[1]].append(i[0]) #Append department names to each list\n\ndepttags = [numpyinfo[i][0] for i in range(len(numpyinfo))] # Create a list with department tags\ndepttagsdict = dict() #Create a dictionary with department tags as the keys, and website links and number of results as the items\nfor i in depttags:\n    depttagsdict[i] = list() #Establish keys\nfor i in depttagsdict.keys():\n    depttagsdict[i].append(\"https://purr.purdue.edu/publications/browse?tag=\"+i) #add link to search for each key\n    site_raw = requests.get(\"https://purr.purdue.edu/publications/browse?tag=\"+i) #webscrape\n    site_soup = soup(site_raw.text, 'html.parser') #parse data\n    site_info = site_soup.find(\"li\", {\"class\":\"counter\"}) #locate count of results\n    results = regex.compile(\"()\\s+((Results[\\s\\w-]+)|(No record found\\s+))(<\\/li>)\") #regex that finds count of results\n    match = results.search(str(site_info)) #look for the regex in site_info\n    gtwo = match.group(3) #the group containing the number of results\n    flag = True\n    if(gtwo == None):\n        gtwo = match.group(4)\n        flag = False #if there are no results, set flag to false\n    numresults = regex.compile(\"of ([\\d]+)\")\n    if(flag):\n        nummatch = numresults.search(gtwo)\n        depttagsdict[i].append(int(nummatch.group(1))) #add number of results to dict\n    else:\n        depttagsdict[i].append(0) #add 0 to dict if there are no results\nfid = open(\"table.csv\", 'w') #put all the stored info in table.csv\nwriter=csv.writer(fid)\nheader = [\"Department Tag\", \"College\", \"Number of PURR results\"] #heading\nwriter.writerow(header)\nfor i in collegesdict.keys():\n    for j in collegesdict[i]:\n        writer.writerow([j, i, depttagsdict[j][1]]) #add dictionary below heading\nfid.close() #close file", "331": "from bs4 import BeautifulSoup\r\nimport requests\r\nimport csv\r\n\r\n# * This is an example of python program that collects data from a website. In this example medical infomation is\r\n# *  collected from a medical website and put in a excel CVS file.\r\n\r\n\r\n# * Creates to parse through pages on a website\r\npage = 0\r\ncounter = 97\r\n\r\n# * This loops through all the pages on the website\r\nfor page in range(97, 123):\r\n    page = chr(counter)\r\n    link = \"https://www.drugs.com/alpha/\" + str(page) + \".html\"\r\n\r\n    # * This stores the link in a variable that BeautifulSoup can access\r\n    source = requests.get(link).text\r\n\r\n    # * This scrapes the websites html through a  parser\r\n    scrape = BeautifulSoup(source, 'html.parser')\r\n\r\n    # * This creates an excel file to write data onto.\r\n    csv_file = open('cms_scrape2.csv', 'a', newline='')\r\n\r\n    # * This creates the heading on the file for codes and diseases.\r\n    csv_writer = csv.writer(csv_file)\r\n    csv_writer.writerow(['Drug', 'Description'])\r\n\r\n    # * This gets the ul that contains all the drugs\r\n    webScrape = scrape.find('ul', class_='ddc-list-column-2')\r\n\r\n    # * This loops through the li's that contain all the different individual drugs\r\n    for drugs in webScrape.find_all('li'):\r\n        print(drugs.text)\r\n        csv_writer.writerow([drugs.text])\r\n\r\n    # * This is a counter for the ascii value to change the webpage.\r\n    counter = counter + 1\r\n\r\ncsv_file.close()\r\n", "332": "import sqlite3\nimport webscrape\n\n\ndef make_tables():\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    command = (\"CREATE TABLE IF NOT EXISTS users(\"\n               + \"id integer PRIMARY KEY,\"\n               + \"username text,\"\n               + \"email text,\"\n               + \"password text);\")\n    cursor.execute(command)\n    command = (\"CREATE TABLE IF NOT EXISTS wishlists(\"\n               + \"id integer PRIMARY KEY,\"\n               + \"user_id integer,\"\n               + \"book1 text DEFAULT null,\"\n               + \"book2 text DEFAULT null,\"\n               + \"book3 text DEFAULT null,\"\n               + \"book4 text DEFAULT null,\"\n               + \"book5 text DEFAULT null,\"\n               + \"book6 text DEFAULT null,\"\n               + \"book7 text DEFAULT null,\"\n               + \"book8 text DEFAULT null,\"\n               + \"book9 text DEFAULT null,\"\n               + \"book10 text DEFAULT null,\"\n               + \"book11 text DEFAULT null,\"\n               + \"book12 text DEFAULT null,\"\n               + \"book13 text DEFAULT null,\"\n               + \"book14 text DEFAULT null,\"\n               + \"book15 text DEFAULT null);\")\n    cursor.execute(command)\n    conn.commit()\n\n\ndef register(username, email, password):\n    if username_exists(username) or email_exists(email):\n        return False\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    command = (\"INSERT INTO users(username, email, password)\"\n               + \"VALUES(?, ?, ?);\")\n    cursor.execute(command, (username, email, password,))\n    conn.commit()\n    return True\n\n\n# Returns tuple of booleans\n# First value True if username found\n# Second value True if password matches\ndef login(username, password):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    command = (\"SELECT * FROM users WHERE username=?\")\n    cursor.execute(command, (username,))\n    row = cursor.fetchall()\n    if len(row) == 0:\n        return (False, False)\n    if row[0][3] == password:\n        return (True, True)\n    return (True, False)\n\n\ndef username_exists(username):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    command = (\"SELECT * FROM users WHERE username=?;\")\n    cursor.execute(command, (username,))\n    rows = cursor.fetchall()\n    if len(rows) == 0:\n        return False\n    return True\n\n\ndef email_exists(email):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    command = (\"SELECT * FROM users WHERE email=?;\")\n    cursor.execute(command, (email,))\n    rows = cursor.fetchall()\n    if len(rows) == 0:\n        return False\n    return True\n\n\n# To be used if user forgets username/password\ndef update_user(user_id, email, username, password):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    command = (\"UPDATE users\"\n               + \"SET email=?,\"\n               + \"username=?,\"\n               + \"password=?\"\n               + \"WHERE id=?;\")\n    cursor.execute(command, (email, username, password, user_id,))\n    conn.commit()\n\n\ndef get_user_id(email='', username=''):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    if not username_exists(username) and not email_exists(email):\n        return False\n    if email != '':\n        command = (\"SELECT * FROM users WHERE email=?\")\n        cursor.execute(command, (email,))\n        row = cursor.fetchall()[0]\n        return row[0]\n    command = (\"SELECT * FROM users WHERE username=?\")\n    cursor.execute(command, (username,))\n    row = cursor.fetchall()[0]\n    return row[0]\n\n\n# Returns first open space in wishlist if it exists\n# Returns True if full\ndef wishlist_is_full(user_id):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    command = (\"SELECT * FROM wishlists WHERE user_id=?;\")\n    cursor.execute(command, (user_id,))\n    row = cursor.fetchall()[0]\n    for i in range(len(row)):\n        if row[i] is None:\n            return i\n    return True\n\n\ndef create_wishlist(user_id):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    command = (\"INSERT INTO wishlists(user_id)\"\n               + \"VALUES(?);\")\n    cursor.execute(command, (user_id,))\n    conn.commit()\n\n\ndef insert_book(user_id, title):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    index = wishlist_is_full(user_id)\n    if index is True:\n        return False\n    book_num = \"book\" + str(index-1)\n    command = (\"UPDATE wishlists \"\n               + \"SET \" + book_num + \"=? \"\n               + \"WHERE user_id=?\")\n    print(command)\n    cursor.execute(command, (title, user_id,))\n    conn.commit()\n\n\ndef get_index(user_id, title):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    command = (\"SELECT * FROM wishlists WHERE user_id=?;\")\n    cursor.execute(command, (user_id,))\n    row = cursor.fetchall()[0]\n    for i in range(len(row)):\n        if row[i] == title:\n            return i-1\n    return 16\n\n\ndef delete_book(user_id, title):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    index = get_index(user_id, title)\n    book_num = \"book\" + str(index)\n    print(book_num)\n    command = (\"UPDATE wishlists \"\n               + \"SET \" + book_num + \"=null \"\n               + \"WHERE user_id=?;\")\n    cursor.execute(command, (user_id,))\n    conn.commit()\n\n\ndef get_wishlist(user_id):\n    conn = sqlite3.connect('database.db')\n    cursor = conn.cursor()\n    command = (\"SELECT * FROM wishlists WHERE user_id=?\")\n    cursor.execute(command, (user_id,))\n    wishlist = list(cursor.fetchall()[0])[2:]\n    while None in wishlist:\n        wishlist.remove(None)\n    books = set()\n    for book in wishlist:\n        books.update(webscrape.cheapest_textbooks(book))\n        books.update(webscrape.thriftbooks(book))\n    return (wishlist, books)\n", "333": "from ast import parse\r\nfrom re import A\r\nimport sqlite3\r\nfrom urllib import response\r\nfrom urllib.request import ProxyDigestAuthHandler\r\nfrom flask import Flask\r\nfrom flask import render_template\r\nfrom flask import request\r\n#from flask_sqlalchemy import SQLAlchemy\r\nfrom datetime import datetime\r\n\r\nimport scrapy\r\nfrom tech import *\r\n\r\n\r\napp = Flask(__name__)\r\n\r\n@app.route('/', methods= ['GET','POST'])\r\ndef root():\r\n    if request.method=='GET':\r\n            return render_template('home.html')\r\n    elif request.method=='POST':\r\n        startspider()\r\n        con = sqlite3.connect(r\"C:\\Users\\VARUN\\Desktop\\flask\\Web Scraper\\Webscrape\\Webscrape\\mydata.db\")\r\n        cur = con.cursor()\r\n        cur.execute(\"SELECT * FROM data_tb\")\r\n        data = cur.fetchall()\r\n        #d=data['Title']\r\n        #print(d)\r\n        return render_template('home.html', data=data)\r\n        \r\n        #p=['1','2','3','4','5']\r\n        \r\n        #return  render_template('home.html',proxies=p)\r\n\r\n\r\nif __name__=='__main__':\r\n    app.run(debug=True, threaded=True)", "334": "import sqlite3\nimport requests\n\ndef create():\n    con=sqlite3.connect('sql.db')\n    cur=con.cursor()\n    cur.execute('''CREATE TABLE IF NOT EXISTS CLASSES(\n        SUBJECT TEXT NOT NULL,\n        DAY INTEGER NOT NULL,\n        TIME INTEGER NOT NULL,\n        SKIP INTEGER DEFAULT 0,\n        REASON TEXT);''')\n    con.commit()\n\n    cur.execute('''CREATE TABLE IF NOT EXISTS ASSIGNMENTS(\n        SUBJECT TEXT NOT NULL,\n        DATE INTEGER NOT NULL,\n        MONTH INTEGER NOT NULL,\n        YEAR INTEGER NOT NULL,\n        TIME INTEGER NOT NULL,\n        DESCRIPTION TEXT);''')\n    con.commit()\n\n    cur.execute('''CREATE TABLE IF NOT EXISTS EVENTS(\n        EVENT TEXT NOT NULL,\n        ETYPE TEXT NOT NULL,\n        DATE INTEGER NOT NULL,\n        MONTH INTEGER NOT NULL,\n        YEAR INTEGER NOT NULL,\n        TIME INTEGER NOT NULL,\n        DESCRIPTION TEXT);''')\n    con.commit()\n\n    con.close()\n    return\n\n\ndef skip():\n    skipp=0\n    reasons=[\"MASS BUNK\",\"CANCELLATION OF CLASS\",\"HOLIDAY\"]\n    reason=\"\"\n    con=sqlite3.connect('sql.db')\n    cur=con.cursor()\n    cur.execute('''SELECT rowid,* FROM CLASSES''')\n    output = cur.fetchall()\n    for row in output:\n        print(f'{row[0]} : {row[1]} at {row[3]} and Status is {row[4]}')\n    con.commit()\n    assgi = input(\"Select The Class To Skip \") # 30 seconds to reply\n    rowi=int(assgi)\n    reas = input(\"Select The Reason To Skip \") # 30 seconds to reply\n    reason=reasons[int(reas)]\n    cur.execute('''UPDATE CLASSES set SKIP = 1, REASON=? where rowid = ?''',(reason,rowi))\n    con.commit()\n    con.close()\n    \n\ndef delc():\n    skipp=0\n    con=sqlite3.connect('sql.db')\n    cur=con.cursor()\n    cur.execute('''SELECT rowid,* FROM CLASSES''')\n    output = cur.fetchall()\n    for row in output:\n        print(f'{row[0]} : {row[1]} at {row[3]} and Status is {row[4]}')\n    con.commit()\n    assgi = input(\"Select The Class To Delete \") # 30 seconds to reply\n    rowi=int(assgi)\n    cur.execute('''DELETE FROM CLASSES WHERE rowid = ?''',(rowi,))\n    con.commit()\n    con.close()\n\ndef addc():\n    con=sqlite3.connect('sql.db')\n    cur=con.cursor()\n    subject=\"\"\n    day=\"\"\n    time=\"\"\n    subjecti = input(\"Enter the Subject \") # 30 seconds to reply\n    subject=subjecti\n\n    dayi = input(\"What is the Day? \") # 30 seconds to reply\n    day=int(dayi)\n\n    timei = input(\"What is the Time(HH) ? \") # 30 seconds to reply\n    time=int(timei)\n    \n    \n    cur.execute('''INSERT INTO CLASSES(SUBJECT,DAY,TIME) VALUES(\n        ?,?,?)''',(subject,day,time))\n    con.commit()\n    con.close()\n\ndef classes():\n    con=sqlite3.connect('sql.db')\n    cur=con.cursor()\n    cur.execute('''SELECT rowid,* FROM CLASSES''')\n    output = cur.fetchall()\n    for row in output:\n        print(f'{row[0]} : {row[1]} at {row[3]} and Status is {row[4]}')\n    con.commit()\n    con.close()\n\n\n# def webscrape():\n#     baseurl=\"https://codeforces.com/api/user.status?handle=\"\n#     handle=\"Aayush5sep\"\n#     count=25\n#     finalurl=baseurl+handle+\"&count=\"+str(count)\n#     print(finalurl)\n#     response = requests.get(finalurl)\n#     data = response.json()\n#     print(data)\n\n# webscrape()\n\n# assig_loop.start()\n# classes.start()\n# keep_alive()\n", "335": "from json import loads\nfrom json.decoder import JSONDecodeError\nfrom time import sleep\nfrom typing import Any\n\nimport requests\nfrom requests.exceptions import HTTPError, RequestException\n\nfrom scrape.log import logger  # type: ignore\n\n\ndef webscrape_results(\n    target_url: str, querystring: str | None = None\n) -> Any:\n    \"\"\"webscrape_results takes a target_url, run_beautiful_soup,\n    and querystring to extract results for further parsing purposes.\n\n    Args:\n        - target_url (str): a website to be scraped\n        - run_beautiful_soup (bool): an ID that determines the output.\n            False is for JSON scraping.\n            True is for HTML scraping with BeautifulSoup.\n        Defaults to False.\n        - querystring (Optional[str], optional):\n        A querystring to govern the requested results. Defaults to None.\n\n    Returns:\n        Any: Is either text from JSON, text from BeautifulSoup, or None if no results were found.\n    \"\"\"\n    sleep(1.5)\n    try:\n        response = requests.get(target_url, params=querystring)\n        logger.debug(response.status_code)\n        return loads(response.text)\n    except (JSONDecodeError, RequestException, HTTPError, AttributeError) as exception:\n        logger.error(\n            f\"An error has occurred, moving to next item in sequence.\\\n            Cause of error: {exception}\"\n        )\n", "336": "from bs4 import BeautifulSoup\nimport os\nimport pandas as pd\nimport numpy as np\nimport sys\nimport requests\nfrom datetime import datetime\n\nchurch_URL = 'https://www.churchofjesuschrist.org/'\n\nescapes = ''.join([chr(char) for char in range(1, 32)])\ntranslator = str.maketrans('', '', escapes)\n\nall_speakers_page = requests.get(church_URL + 'general-conference/speakers?lang=eng')\nall_speaker_soup = BeautifulSoup(all_speakers_page.content, 'lxml')\n\nall_speaker_links_soup = all_speaker_soup.find_all('div', \"lumen-tile__title\")\nall_links = [speaker.find('a')['href'][1:] for speaker in all_speaker_links_soup]\nlinks_to_use = all_links[:15]\n\npage_num = str(1)\n\ntalks = pd.DataFrame(columns = ['Speaker', 'Date', 'Title', 'URL'])\n\nfor speaker_link in links_to_use:\n    page = requests.get(church_URL + speaker_link)\n    \n    scripture_soup = BeautifulSoup(page.content, 'lxml')\n    all_links = scripture_soup.find_all('a', 'pages-nav__list-item__anchor')\n    if len(all_links) > 0:\n        link_texts = [int(link.text) if len(link.text) == 1 else 0 for link in all_links]\n        num_pages = np.max(link_texts)\n    else:\n        num_pages = 1\n\n    for this_page_num in range(1,num_pages+1):\n        this_URL = church_URL + speaker_link + \"&page=\" + str(this_page_num)\n        this_page = requests.get(this_URL)\n        speaker_soup = BeautifulSoup(this_page.content, 'lxml')\n        \n        this_speaker = speaker_soup.find('h1').get_text()\n        \n        talk_titles_soup = speaker_soup.find_all('div', \"lumen-tile__title\")\n        these_titles = [title.get_text().translate(translator) for title in talk_titles_soup]\n    \n        talk_dates_soup = speaker_soup.find_all('div', \"lumen-tile__content\")\n        these_dates_str = [date.get_text().translate(translator) for date in talk_dates_soup]\n        these_dates = [datetime.strptime(date_str, \"%B %Y\") for date_str in these_dates_str]\n    \n        talk_links_soup = speaker_soup.find_all('a', \"lumen-tile__link\")\n        these_URLs = [church_URL + link['href'][1:] for link in talk_links_soup]\n        \n        these_talks = pd.DataFrame({'Speaker':[this_speaker for i in these_titles],\n                      'Date':these_dates,\n                      'Title':these_titles,\n                      'URL':these_URLs})\n        \n        talks = talks.append(these_talks, ignore_index = True)\n\nall_content = []\n\nfor talk_row in range(np.shape(talks)[0]):\n    perc_comp = talk_row / np.shape(talks)[0] * 100\n    sys.stdout.write(f\"\\rWebscrape Progress: {int(np.round(perc_comp))}%\")\n    talk_URL = talks['URL'][talk_row]\n    talk_page = requests.get(talk_URL)\n    talk_soup = BeautifulSoup(talk_page.content, 'lxml')\n    ids = [p.get('id') for p in talk_soup.find_all('p')]\n    paragraphs = [id[0] == 'p' and id[1].isdigit()  if id != None else False for id in ids]\n    paragraph_ids = [ids[paragraph_index] for paragraph_index in list(np.where(paragraphs)[0])]\n    \n    content = [talk_soup.find(id = this_id).get_text().translate(translator) for this_id in paragraph_ids]\n    all_content += [content]\n    sys.stdout.flush()\nsys.stdout.write(\"\\rWebscrape Progress: 100%\\n\")\nsys.stdout.write(\"Webscrape complete!\\n\")\n\nquotes = pd.DataFrame(columns = list(talks.columns))\n\nfor i in range(np.shape(talks)[0]):\n    perc_comp = i / np.shape(talks)[0] * 100\n    sys.stdout.write(f\"\\rData Compiling Progress: {int(np.round(perc_comp))}%\")\n    num_quotes = len(all_content[i])\n    talk_speaker = [talks.iloc[i,0] for x in range(num_quotes)]\n    talk_date = [talks.iloc[i,1] for x in range(num_quotes)]\n    talk_title = [talks.iloc[i,2] for x in range(num_quotes)]\n    talk_URL = [talks.iloc[i,3] for x in range(num_quotes)]\n    \n    talk_quote_info = pd.DataFrame({'Speaker':talk_speaker,\n                                     'Date':talk_date,\n                                     'Title':talk_title,\n                                     'URL':talk_URL,\n                                     'Content':all_content[i]})\n    quotes = quotes.append(talk_quote_info, ignore_index = True)\nsys.stdout.write(\"\\rData Compiling Progress: 100%\\n\")\nsys.stdout.write(\"Compilation complete!\\n\")\n\nquotes.to_csv('Talk_Quotes_Data.csv', index = False)", "337": "# Handling imports\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport numpy as np\nimport re\nimport sys\n\n# Set up the dataframe\ndf = pd.read_csv(r\"C:\\Users\\chave\\Desktop\\Duke\\APL2021_DataPlus\\Data_Extraction\\MA_Webscrape\\Massachusetts_MEA_Database_Text_FileName.csv\")\n# print(df.columns)\n# Edit the dataframe\ndf.drop(['Unnamed: 0', 'Text'], axis=1, inplace=True)\ndf.rename(columns={'URL - to be filled in':'URL','File Name':'Filename'}, inplace=True)\n\ndef get_text(row):\n    # Get URL\n    url = row['URL']\n    questionable_stuff = ['doc', 'pdf']\n    \n    if not pd.isnull(url) and not any(x in url for x in questionable_stuff):\n        try:\n            # Extract from website\n            source = requests.get(url).text\n            soup = BeautifulSoup(source, 'lxml')\n\n            # Parse through text of site\n            paragraph =  soup.find_all('div', class_ =\"main-content main-content--two\")[1]\n            #print(paragraph.prettify())\n\n            lines = paragraph.find_all(class_ = \"ma__rich-text\")[1]\n            text = lines.get_text(separator=' ')\n            text= re.sub(r\"(\\n( ?))+\", \"\\n\", text)\n            text = re.sub(r\"   +\", \"  \", text)\n            text = text.replace(\"\\n\",\" \")\n            return text\n\n        except Exception as exception:\n            print(url)\n            return exception.__class__.__name__\n    return np.nan\n\ndf['Text'] = df.apply(lambda row : get_text(row), axis=1)\nnp.set_printoptions(threshold=sys.maxsize)\ndf.to_csv(r\"C:\\Users\\chave\\Desktop\\Duke\\APL2021_DataPlus\\Data_Extraction\\MA_Webscrape\\Massachusetts_MEA_Database_Text.csv\")", "338": "from unittest import TestCase\nfrom web_scrape import WebScrape\nfrom web_processing import WebProcess\nfrom web_word_counter import WordCounter\n\n\n\nclass TestTask(TestCase):\n    def setUp(self):\n        self.web_scrape = WebScrape()\n        self.web_process = WebProcess()\n        self.count = WordCounter()\n\n    def test_web_scrape(self):\n        result =  self.web_scrape.scrape(\"https://www.python.org\")\n        self.assertIsNotNone(result)\n        self.assertIsInstance(result, str)\n        self.assertNotIsInstance(result, dict)\n        self.assertNotIsInstance(result, list)\n        self.assertTrue(result)\n        self.assertGreater(len(result), 1)\n\n    def test_web_processing(self):\n        result = self.web_process.text_process(\"python software is a 1 2 3 4 class\")\n        self.assertIsNotNone(result)\n        self.assertIsInstance(result, list)\n        self.assertNotIsInstance(result, dict)\n        self.assertNotIsInstance(result, dict)\n        self.assertTrue(result)\n        self.assertEqual(result, [\"Python\" , \"Software\", \"Class\"])\n\n    def test_web_counter(self):\n        result = self.count.word_counter([\"python\", \"software\", \"train\", \"sound\", \"music\", \"punch\", \"dance\"])\n        self.assertIsNotNone(result)\n        self.assertIsInstance(result, dict)\n        self.assertNotIsInstance(result, list)\n        self.assertNotIsInstance(result, str)\n        self.assertEqual(len(result), 7)\n        self.assertEqual(result, {\"python\": 1, \"software\": 1, \"train\" : 1, \"sound\" : 1, \"music\" : 1, \"punch\" : 1, \"dance\" : 1})\n        self.assertEqual(sum(list(result.values())), 7)\n        self.assertEqual(list(result.keys()), [\"python\", \"software\", \"train\", \"sound\", \"music\", \"punch\", \"dance\"])\n\n    def tearDown(self):\n        self.web_scrape = None\n        self.web_process = None\n        self.count = None\n    \n       \n\n\n\n\n\n\n\n\n", "339": "#!/usr/bin/env python\nimport os\nimport sys\n\nif __name__ == \"__main__\":\n    os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"config.settings.local\")\n\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError:\n        # The above import may fail for some other reason. Ensure that the\n        # issue is really that Django is missing to avoid masking other\n        # exceptions on Python 2.\n        try:\n            import django  # noqa\n        except ImportError:\n            raise ImportError(\n                \"Couldn't import Django. Are you sure it's installed and \"\n                \"available on your PYTHONPATH environment variable? Did you \"\n                \"forget to activate a virtual environment?\"\n            )\n\n        raise\n\n    # This allows easy placement of apps within the interior\n    # webscrape directory.\n    current_path = os.path.dirname(os.path.abspath(__file__))\n    sys.path.append(os.path.join(current_path, \"webscrape\"))\n\n    execute_from_command_line(sys.argv)\n", "340": "import rpa as r\nimport urllib as urllib\nimport re\nimport datetime\nimport csv\nfrom multiprocessing import Process\nimport multiprocessing\n\n\nclass WebScrape:\n    def GoToWeb(StartingIValue):\n        r.init()\n\n        i = StartingIValue\n        chunk = 100\n        RunUntil = i + chunk\n\n        GTG = True\n        listPigs = []\n\n        while GTG == True:\n\n            try:\n                newUrl = f'https://www.cpspedigrees.com/poland/pigs/show/{i}'\n                r.url(newUrl)\n\n                pigInstance = Pig()\n\n                pigInfo = r.read('ul')\n\n                if pigInfo == '':\n                    #GTG = False\n                    i += 1\n                    continue\n\n                regNum = re.search('Registration#:\\n(.*)\\n', pigInfo)\n                regNum = regNum.group(1).strip()\n                pigInstance.regNum = regNum\n\n                Sex = re.search('Sex:\\n(.*)\\n', pigInfo)\n                Sex = Sex.group(1).strip()\n                pigInstance.Sex = Sex\n\n                pigInstance.FullName = r.read('show-pig-fullname')\n\n                DOB = re.search('Farrow Date:\\n(.*)\\n', pigInfo)\n                DOB = DOB.group(1).strip()\n                pigInstance.DOB = DOB\n\n                Owner = re.search('Owner:\\n(.*)\\n', pigInfo)\n                Owner = Owner.group(1).strip()\n                pigInstance.Owner = Owner\n\n                Breeder = re.search('Breeder:\\n(.*)\\n', pigInfo)\n                Breeder = Breeder.group(1).strip()\n                pigInstance.Breeder = Breeder\n\n                TotalBorn = re.search('Total Born:\\n(.*)\\n', pigInfo)\n                TotalBorn = TotalBorn.group(1).strip()\n                pigInstance.TotalBorn = TotalBorn\n\n                BornAlive = re.search('Born Alive:\\n(.*)\\n', pigInfo)\n                BornAlive = BornAlive.group(1).strip()\n                pigInstance.BornAlive = BornAlive\n\n                SireRegNum = r.read(\n                    '/html/body/div[1]/div[4]/div/div[2]/div[2]/div/div/strong[1]')\n                pigInstance.SireRegNum = SireRegNum\n\n                DamRegNum = r.read(\n                    '/html/body/div[1]/div[4]/div/div[2]/div[6]/div/div/strong[1]')\n                pigInstance.DamRegNum = DamRegNum\n                pigInstance.webID = i\n\n                listPigs.append(pigInstance)\n\n                if i >= RunUntil:\n                    ExportPigListToCSV(listPigs, \"Pigs.csv\")\n                    listPigs.clear()\n                    RunUntil = i + chunk\n                i += 1\n\n            except Exception:\n                GTG == False\n        try:\n            ExportPigListToCSV(listPigs, \"Pigs.csv\")\n        except BaseException as e:\n            print(f'BaseException: {e}')\n        else:\n            print('Data has been loaded successfully !')\n\n\ndef ExportPigListToCSV(listPigs, filename):\n    with open(filename, 'a') as f:\n        writer = csv.writer(f)\n        j = 0\n        for item in listPigs:\n            j += 1\n            writer.writerow([item.webID, item.FullName, item.regNum, item.Sex, item.DOB, item.Owner,\n                            item.Breeder, item.TotalBorn, item.BornAlive, item.SireRegNum, item.DamRegNum])\n    return filename\n\n\ndef StringToFile(text, output):\n    f = open(output, \"a\")\n    f.write(text)\n    f.close()\n\n\ndef IngestWebContent(urlPath):\n    fp = urllib.urlopen(urlPath)\n    myBytes = fp.read()\n\n    myStr = myBytes.decode('utf8')\n\n    fp.close()\n    StringToFile(myStr, )\n\n    ###main entry point###\n\n\nclass Pig:\n    def __init__(self):\n        self.regNum = 0\n        self.Sex = \"\"\n        self.DOB = datetime.datetime.now()\n        self.Owner = \"\"\n        self.Breeder = \"\"\n        self.TotalBorn = 0\n        self.BornAlive = 0\n        self.SireRegNum = 0\n        self.DamRegNum = 0\n        self.FullName = \"\"\n        self.webID = 0\n###KEEP AT BOTTOM###\n\n\ndef main():\n    WebScrape.GoToWeb(302982)\n    # argumentList = [100, 200, 300]\n    # procs = []\n\n    # for argument in argumentList:\n    #     proc = multiprocessing.Process(\n    #         target=WebScrape.GoToWeb, args=(argument,))\n    #     procs.append(proc)\n    #     proc.start()\n\n    # for proc in procs:\n    #     proc.join()\n\n\nif __name__ == \"__main__\":\n    main()\n##End of main entry###\n", "341": "import physician\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport math\nimport random\nimport time\n\ndef convert_to_date(date: str) -> str:\n    '''\n    Converts date in format 'Jan 03, 2021' to 1/3/21\n    '''\n    formatted_date = ''\n    date_conversion_dict = {'Jan' : 1, 'Feb' : 2, 'Mar' : 3, 'Apr' : 4,\n                            'May' : 5, 'Jun' : 6, 'Jul' : 7, 'Aug' : 8,\n                            'Sep' : 9, 'Oct' : 10, 'Nov' : 11, 'Dec' : 12}\n    date = date.split()\n\n    if date[0] not in date_conversion_dict:\n        raise AttributeError(f'{date[0]} not in date_conversion_dict')\n    else:\n        formatted_date += str(date_conversion_dict[date[0]]) + '/'\n        formatted_date += str(int(date[1][:-1])) + '/'\n        formatted_date += date[2][2:]\n    \n    return formatted_date\n    \n\ndef set_source(physician: physician.Physician, source: str) -> None:\n    '''\n    Sets source during webscrape\n    '''\n    physician.set_source(source)\n\ndef set_tier(physician: physician.Physician) -> None:\n    '''\n    Sets tier based on residency\n    '''\n    tier_dict = {'T20_Rsrch' : ['Harvard', 'Harvard University', 'Harvard Medical School', 'Grossman', 'Columbia', 'Hopkins', 'Johns Hopkins Hospital', 'Johns Hopkins University', 'Francisco', \n                                'Duke', 'Duke University Health System', 'Duke University Hospital', 'Perelman', 'Stanford', 'Stanford University', 'University of Washington', 'Washington University/Barnes Hospital', 'University of Washington Medical Center', \n                                'Yale', 'Icahn', 'Icahn School of Medicine at Mount Sinai', 'Washington University', 'Washington University School of Medicine in St. Louis', 'Washington University in St. Louis', 'Washington University in St. Louis', 'Vanderbilt', 'Vanderbilt University', \n                                'Cornell', 'Mayo', 'Mayo Medical School', 'University of Pittsburgh', 'University of Pittsburg', 'University of Pittsburgh Medical Center', 'University Pittsburgh Medical Center Hospitals', 'Northwestern', 'McGaw Medical Center of Northwestern University', \n                                'Northwestern University, Feinberg School of Medicine', 'University of Michigan', 'University of Michigan Health System', 'Los Angeles', 'San Diego', 'Pritzker'],\n                'T50_Rsrch' : ['Baylor', 'Baylor College of Medicine', 'Baylor University Medical Center', 'Emory', 'Case Western', 'Case Western University', 'University of North Carolina', 'University of Michigan Hospitals and Health Centers', \n                                'Southwestern', 'Colorado', 'Southern California', 'University of Southern California', 'USC Medical Center', 'Maryland', 'LAC + USC Medical Center',  \n                                'Ohio State', 'The Ohio State Wexner Medical Center', 'University of Virginia', 'Boston', 'Oregon Health and Science', 'Oregon Health and Science University', 'Oregon Health Sciences University',  \n                                'Alabama', 'University of Alabama at Birmingham', 'Brown', 'University of Utah', 'University of Utah, Salt Lake City', 'University of Utah Medical Center', 'Albert Einstein', \n                                'University of Florida', 'University of Rochester', 'University of Rochester School of Medicine', 'University of Wisconsin', \n                                'Indiana University', 'University of Iowa', 'University of Iowa Hospitals and Clinics', 'University of Iowa, College of Medicine', 'University of Cincinnati', \n                                'University of Miami', 'University of Minnesota', 'University of South Florida', 'University of South Florida Morsani School of Medicine, Mayo School of Graduate Medical Education', 'USF College of Medicine', 'University of South Florida College of Medicine', \n                                'Dartmouth', 'University of Massachusetts', 'Texas Health and Science', \n                                'Wake Forest', 'Davis'],\n                'T20_PC' : ['University of Washington', 'Francisco', 'Minnesota', 'Oregon Health and Science', \n                            'University of North Carolina', 'University of North Carolina - Chapel Hill', 'University of North Carolina at Chapel Hill', 'University of Colorado', 'University of Colorado School of Medicine', 'University of Nebraska', 'University of Nebraska Medical Center', \n                            'Davis', 'Harvard', 'University of Kansas', 'University of Kansas Medical Center', 'University of Massachusetts', \n                            'University of Pittsburgh', 'University of Pittsburgh School of Medicine', 'University of Pittsburgh School of Dental Medicine', 'University of Pittsburg Medical Center', 'Los Angeles', 'Brown', 'Maryland', 'University of Maryland Medical Center', 'Baylor', 'Iowa', \n                            'New Mexico', 'Texas Southwestern', 'University of Michigan', 'University of Michicagn', 'University of Pennsylvania', 'University of Pennsylvania School of Medicine', 'Hospital of the University of Pennsylvania', \n                            'University of Pennsylvania Health System', 'Hospital of The University of PA', 'University of Wisconsin'],\n                'T50_PC' : ['Indiana University', 'University of Hawaii', 'University of Utah', 'East Carolina University', \n                            'University of Alabama', 'University of Alabama Medical Center', 'University of Rochester', 'University of Tennessee', 'University of Tennessee Health Science Center', 'Stanford', \n                            'Pritzker', 'Ohio State', 'Ohio State UMC', 'San Diego', 'University of California, San Diego', 'University of California - San Diego', 'University of Vermont', 'The University of Vermont Medical Center', 'Virginia', 'Boston University', 'Boston Medical Center',\n                            'Dartmouth', 'Mayo', 'University of Arkansas', 'University of North Texas', \n                            'University of Texas Health Science Center', 'University of Texas Medical Branch', 'Emory', 'Emory University', 'Northwestern', 'Vanderbilt', 'Cornell', \n                            'Tufts', 'Boston University/Tufts University', 'University of Oklahoma', 'University of Oklahoma, University of Colorado Health Science Center', 'University Of Oklahoma College of Medicine', 'Grossman', 'Texas Tech University Health Sciences Center', \n                            'University of Florida', 'Virginia Commonwealth', 'Tufts University School of Medicine'],\n                'T20_Resi' : ['Memorial Sloan', 'Massachusetts General', 'Hopkins', 'Mayo', 'San Fran', 'University of California, San Francisco', 'Penn Presbyterian', \n                                'Ohio State', 'University of Michigan', 'Vanderbilt', 'MD Anderson', 'Los Angeles', 'UCLA', 'UCLA Medical Center',  'University of California, Los Angeles School of Medicine', 'University of California, Los Angeles School of Medicine', \n                                'Stanford', 'MUSC Health', 'OHSU', 'Presbyterian', 'University of Kansas', 'Cedars-Sinai', \n                                'Brigham', 'Barnes-Jewish', 'University of California, Los Angeles School of Medicine']\n                }\n\n    if physician.get_residency() == 'N/A':\n        physician.set_tier('N/A')\n        print('1')\n        return\n\n    found = False\n    for key, _ in tier_dict.items():\n        if physician.get_residency() in tier_dict[key]:\n            physician.set_tier(key)\n            found = True\n            break\n    \n    if found == False and physician.get_residency() not in ['University Hospital of Louvain at Mont-Godinne', 'Thomas Jefferson University Hospitals']:\n        physician.set_tier('N/A')\n        print(3)\n    \n\n\ndef set_overall_rating_healthgrades(physician: physician.Physician, soup: BeautifulSoup) -> None:\n    '''\n    Sets overall rating during webscrape\n    '''\n    overall_rating = soup.select('div.overall-rating-wrapper strong')\n    try:\n        physician.set_overall_rating('Healthgrades', float(overall_rating[0].text.strip())) # Converts the html to float\n    except IndexError:\n        physician.set_overall_rating('Healthgrades', 'N/A')\n\ndef set_comment_healthgrades(physician: physician.Physician, soup: BeautifulSoup) -> None:\n    '''\n    Sets the comment as a 3-tuple of (date, rating, comment)\n    '''\n    # API GET request\n    url = 'https://www.healthgrades.com/api4/providerprofile/comments'\n    pwid = physician.get_link('Healthgrades')[-5:]\n    page_num = 1\n    data = {'currentPage': page_num, 'includeAllAnswers': True, 'perPage': 5, 'pwid': pwid, 'sortOption': 1}\n    headers = {\"accept\": \"*/*\",\n    \"accept-language\": \"en-US,en;q=0.9\",\n    \"content-type\": \"application/x-www-form-urlencoded\",\n    \"sec-ch-ua\": \"\\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Chromium\\\";v=\\\"101\\\", \\\"Google Chrome\\\";v=\\\"101\\\"\",\n    \"sec-ch-ua-mobile\": \"?0\",\n    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n    \"sec-fetch-dest\": \"empty\",\n    \"sec-fetch-mode\": \"cors\",\n    \"sec-fetch-site\": \"same-origin\",\n    \"Referer\": physician.get_link('Healthgrades'),\n    \"Referrer-Policy\": \"strict-origin-when-cross-origin\"}\n\n    request = requests.post(url, data = data, headers=headers)\n\n    json_data = json.loads(request.text)\n\n    total_comment_count = json_data['totalCommentCount']\n    num_loops = math.ceil(total_comment_count / 5)\n\n    for i in range(0, num_loops):\n        # time.sleep(random.random()*10)\n        page_num += 1\n\n        data = {'currentPage': page_num, 'includeAllAnswers': True, 'perPage': 5, 'pwid': pwid, 'sortOption': 1}\n        headers = {\"accept\": \"*/*\",\n        \"accept-language\": \"en-US,en;q=0.9\",\n        \"content-type\": \"application/x-www-form-urlencoded\",\n        \"sec-ch-ua\": \"\\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Chromium\\\";v=\\\"101\\\", \\\"Google Chrome\\\";v=\\\"101\\\"\",\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-origin\",\n        \"Referer\": physician.get_link('Healthgrades'),\n        \"Referrer-Policy\": \"strict-origin-when-cross-origin\"}\n        \n        for j in range(0, len(json_data['results'])):\n            comment_text = json_data['results'][j]['commentText'].strip()\n            comment_date = convert_to_date(json_data['results'][j]['submittedDate'])\n            comment_rating = json_data['results'][j]['overallScore']\n            useful_count = json_data['results'][j]['helpfulCount']\n\n            physician.add_comment((comment_date, 'Healthgrades', comment_rating, comment_text, useful_count))\n        \ndef webscrape_healthgrades(physician: physician.Physician) -> None:\n    '''\n    Webscrapes healthgrades.com for the given physician\n    '''\n    url = physician.get_link('Healthgrades')\n\n    if url != 'N/A':\n        set_source(physician, 'Healthgrades')\n        # set_tier(physician)\n\n        request = requests.get(url) # Gets HTML of website\n        soup = BeautifulSoup(request.content, 'html.parser')\n\n        set_overall_rating_healthgrades(physician, soup)\n        set_comment_healthgrades(physician, soup)\n\ndef set_overall_rating_vitals(physician: physician.Physician, soup: BeautifulSoup) -> None:\n    '''\n    Sets overall rating during webscrape\n    '''\n    time.sleep(random.random()*10)\n    overall_rating = soup.select('#app > div.top-section > div.profile-header-container.loc-vs-prvdr > header > div > div > div.header-info > div > div.name-info > div > div.rating-section > a.ratings > span.rating-score')\n    try:\n        physician.set_overall_rating('Vitals', float(overall_rating[0].text.strip())) # Converts the html to float\n    except IndexError:\n        physician.set_overall_rating('Vitals', 'N/A')\n\ndef webscrape_vitals(physician: physician.Physician) -> None:\n    '''\n    Webscrapes vitals.com for the given physician\n    '''\n    url = physician.get_link('Vitals')\n    page = 1\n\n    # Redirects url to the reviews page\n    if '.html' in url:\n        url = url[:-5]\n        url += f'/reviews?page={page}&sort=updated_at_dt%20desc'\n\n    if url != 'N/A':\n        set_source(physician, 'Vitals')\n\n    #     headers = {'authority': 'www.vitals.com',\n    #                 'method': 'GET',\n    #                 'path': '/doctors/Dr_Adam_Luginbuhl/reviews',\n    #                 'scheme': 'https',\n    #                 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n    #                 'accept-encoding': 'gzip, deflate, br',\n    #                 'accept-language': 'en-US,en;q=0.9',\n    #                 'cache-control': 'max-age=0',\n    #                 'cookie': 'gtinfo={\"ct\":\"Irvine\",\"c\":\"Orange\",\"cc\":\"6059\",\"st\":\"CA\",\"sc\":\"5\",\"z\":\"92697\",\"lat\":\"33.65\",\"lon\":\"-117.84\",\"dma\":\"803\",\"cntr\":\"usa\",\"cntrc\":\"840\",\"tz\":null,\"ci\":\"169.234.45.66\"}; notice_behavior=implied,us; _ga=GA1.1.280728103.1653438674; AMCVS_16AD4362526701720A490D45%40AdobeOrg=1; usprivacy=1YNN; s_cc=true; aam_uuid=91305347862693971774061348772953748111; _ibp=0:l3kup39v:f95c0d05-b5f2-4e1c-aea8-40a51ae5865c; TapAd_DID=7b3647c3-c14a-43c4-8917-d452f5028cec; notice_preferences=2:; notice_gdpr_prefs=0,1,2:; aam=aam%3D999996%2C529440%2C32964%2C32920%2C318069%2C663590%2C32539%2C617784%2C18091421%2C18292951%2C21558705%2C22156106%2C22833965%2C22876027%2C23269831%2C23376301%2C23421578%2C24060393; initial_url_path={%22url%22:%22%2Fdoctors%2FDr_Adam_Luginbuhl%2Freviews%22}; s_sq=%5B%5BB%5D%5D; __cfruid=4ddc06e72654592012cac55754a9a775651e3c9e-1653707609; __cf_bm=5cc70kut0gXZ76qYiSMZCN9f7y7GGEhnimld8nfb2fQ-1653707610-0-ARwjyKakQlDYoM83NBqkbqJpH0QvQAMTn2Iy+VhQwb97pO7i4UO24ttK1sod7/mimw3HqBT20E+dvOXVHqUiysyzp62o48BjOzaA1V+hUTwAn5OAZQ9W6OFEbKhBwCcpyLHSOoc/gHVasL0z6IYc+BW5eMOPbpEqzBHlnRISBohbSyGe04cI7LbHs1WPjPFULg==; mnet_session_depth=1%7C1653707611077; _ga_3ZVJC9H4TB=GS1.1.1653707611.5.0.1653707611.0; ui={%22expmatch%22:1%2C%22vtime%22:27561793}; fpci={%22iafValue%22:1%2C%22url%22:%22www.vitals.com%2Fdoctors%2FDr_Adam_Luginbuhl%2Freviews%22}; AMCV_16AD4362526701720A490D45%40AdobeOrg=-432600572%7CMCIDTS%7C19141%7CMCMID%7C91511255161485384144041336488334773784%7CMCAAMLH-1654312411%7C9%7CMCAAMB-1654312411%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1653714811s%7CNONE%7CMCAID%7CNONE%7CvVersion%7C4.5.2; _ibs=0:l3kup39x:87126a4d-080b-40f0-8864-c106f5a314d6',\n    #                 'referer': 'https://www.vitals.com/doctors/Dr_Adam_Luginbuhl.html',\n    #                 'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\", \"Google Chrome\";v=\"101\"',\n    #                 'sec-ch-ua-mobile': '?0',\n    #                 'sec-ch-ua-platform': \"Windows\",\n    #                 'sec-fetch-dest': 'document',\n    #                 'sec-fetch-mode': 'navigate',\n    #                 'sec-fetch-site': 'same-origin',\n    #                 'sec-fetch-user': '?1',\n    #                 'upgrade-insecure-requests': '1',\n    #                 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36}'\n    #                 }   \n        \n    #     request = requests.post(url, headers=headers)\n    #     json_data = json.loads(request.text)\n\n\n        # with requests.session() as s:\n\n        #     # load cookies:\n        #     s.get(url, headers=headers)\n\n        #     # get data:\n        #     data = s.get(url, headers=headers).json()\n\n\n        # headers = {'User-agent': 'Mozilla/5.0'}\n\n        # request = requests.get('https://www.vitals.com/doctors/Dr_Adam_Luginbuhl/reviews', headers = headers) # Gets HTML of website\n        # page = urllib.request.urlopen(request)\n        # soup = BeautifulSoup(request.content, 'html.parser')\n\n        # set_overall_rating_vitals(physician, soup)\n        # set_comment_vitals(physician, soup)\n        \ndef webscrape_ratemds(physician: physician.Physician) -> None:\n    '''\n    Webscrapes ratemds.com for the given physician\n    '''\n    page_num = 1\n    url = physician.get_link('RateMDs') + f'/?page={page_num}'\n\n    if url != 'N/A':\n        set_source(physician, 'RateMDs')\n        # set_tier(physician)\n        time.sleep(random.random()*10)\n\n        request = requests.get(url) # Gets HTML of website\n        soup = BeautifulSoup(request.content, 'html.parser')\n\n        # set_overall_rating_healthgrades(physician, soup)\n        # set_comment_healthgrades(physician, soup)\n\ndef set_overall_rating_yelp(physician: physician.Physician, soup: BeautifulSoup) -> None:\n    '''\n    Sets overall rating during webscrape\n    '''\n    overall_rating = soup.select('#main-content > div.margin-b3__09f24__l9v5d.border-color--default__09f24__NPAKY > div > div > div.arrange__09f24__LDfbs.gutter-1-5__09f24__vMtpw.vertical-align-middle__09f24__zU9sE.margin-b2__09f24__CEMjT.border-color--default__09f24__NPAKY > div:nth-child(1) > span > div')\n\n    try:\n        physician.set_overall_rating('Yelp', float(overall_rating[0]['aria-label'][0])) # Converts the html to float\n    except IndexError:\n        physician.set_overall_rating('Yelp', 'N/A')\n\ndef set_comment_yelp(physician: physician.Physician, soup: BeautifulSoup) -> None:\n    '''\n    Sets comment during webscrape\n    '''\n    comments = soup.find_all('div', {'review__09f24__oHr9V border-color--default__09f24__NPAKY'})\n\n    for comment in comments:\n        comment_tag = comment.find('span', {'raw__09f24__T4Ezm'})\n        comment_text = comment_tag.text\n\n\n\n        rating_tag = comment.find('div', {'i-stars__09f24__M1AR7 i-stars--regular-5__09f24__tKNMk border-color--default__09f24__NPAKY overflow--hidden__09f24___ayzG', \n                                            'i-stars__09f24__M1AR7 i-stars--regular-1__09f24__o88Iy border-color--default__09f24__NPAKY overflow--hidden__09f24___ayzG',\n                                            'i-stars__09f24__M1AR7 i-stars--regular-2__09f24__mq_AY border-color--default__09f24__NPAKY overflow--hidden__09f24___ayzG',\n                                            'i-stars__09f24__M1AR7 i-stars--regular-4__09f24__qui79 border-color--default__09f24__NPAKY overflow--hidden__09f24___ayzG',\n                                            'i-stars__09f24__M1AR7 i-stars--regular-3__09f24__sRNTp border-color--default__09f24__NPAKY overflow--hidden__09f24___ayzG'})\n        rating = rating_tag['aria-label'][0]\n\n        comment_date = comment.find('span', {'css-chan6m'}).text\n\n\n\n        try:\n            useful_count = comment.find('span', {'css-1lr1m88'}).text.strip()\n            physician.add_comment((comment_date, 'Yelp', rating, comment_text, useful_count))\n        except AttributeError:\n            useful_count = 0\n            physician.add_comment((comment_date, 'Yelp', rating, comment_text, useful_count))\n\n    \ndef webscrape_yelp(physician: physician.Physician) -> None:\n    '''\n    Webscrapes yelp.com for the given physician\n    '''\n    time.sleep(random.random()*10)\n    num_queries = 0\n    url = physician.get_link('Yelp')\n    # url = f'https://www.yelp.com/biz/chatime-irvine-irvine-2?start={num_queries}'\n    request = requests.get(url) # Gets HTML of website\n    soup = BeautifulSoup(request.content, 'html.parser')\n    total_num_queries = soup.select('#main-content > div.margin-b3__09f24__l9v5d.border-color--default__09f24__NPAKY > div > div > div.arrange__09f24__LDfbs.gutter-1-5__09f24__vMtpw.vertical-align-middle__09f24__zU9sE.margin-b2__09f24__CEMjT.border-color--default__09f24__NPAKY > div.arrange-unit__09f24__rqHTg.arrange-unit-fill__09f24__CUubG.border-color--default__09f24__NPAKY.nowrap__09f24__lBkC2 > span')\n    \n    if len(total_num_queries) > 0:\n        num_queries_left = int(total_num_queries[0].text[0])    \n    else:\n        num_queries_left = 1\n\n    while num_queries_left > 0:\n        # url = f'https://www.yelp.com/biz/chatime-irvine-irvine-2?start={num_queries}'\n        url = physician.get_link('Yelp') + f'?start={num_queries}'\n\n        if url != 'N/A':\n            set_source(physician, 'Yelp')\n            # set_tier(physician)\n\n            request = requests.get(url) # Gets HTML of website\n            soup = BeautifulSoup(request.content, 'html.parser')\n\n            set_overall_rating_yelp(physician, soup)\n            set_comment_yelp(physician, soup)\n        \n        num_queries_left -= 10\n        num_queries += 10\n\ndef webscrape_links(physician: physician.Physician) -> None:\n    '''\n    Calls the webscrape on each link in the physican\n    '''\n    # if physician.get_link('Healthgrades') != 'N/A':\n    #     webscrape_healthgrades(physician)\n\n    # if physician.get_link('Vitals') != 'N/A': # change .html to /reviews?page=1&sort=updated_at_dt%20desc\n    #     webscrape_vitals(physician)\n\n    # if physician.get_link('RateMDs') != 'N/A': # Does by page num in link\n    #     webscrape_ratemds(physician)\n\n    # if physician.get_link('Yelp') != 'N/A': # Does by num queries in link\n    #     webscrape_yelp(physician)\n    \n\ndef webscrape_list(list_of_physicians: list[physician.Physician]) -> None:\n    '''\n    Calls webscrape on all physicians\n    '''\n    for physician in list_of_physicians:\n        set_tier(physician)\n        webscrape_links(physician)\n\n", "342": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Nov 24 14:38:30 2018\n\n@author: Kyrie\n\"\"\"\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Nov 13 11:57:02 2018\n\nBlackbox Webscrape\n\n@author: kyrie\n\"\"\"\n\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport numpy as np\nfrom datetime import datetime\nfrom threading import Timer\n\n\n# Calculates time until 12:01am \nx=datetime.today()\ny=x.replace(day=x.day+1, hour=0, minute=1, second=0, microsecond=0)\n#y=x.replace(day=x.day+0, hour=14, minute=47, second=0, microsecond=0)\ndelta_t=y-x\nsecs=delta_t.seconds+1\n\n\ndef blackbox_webscrape():\n    current_step = 1 # starting step #\n    \n    # Manually adjust simulation number, endsteps and intervals here\n    sim_num = 8\n    endstep = 7000\n    intervals = 1\n    \n    print(\"Starting to run code. The date/time is: \" + str(x))\n\n    url = \"https://www.informatics.indiana.edu/jbollen/I501F18/blackbox/BlackBox_N.php\" # Blackbox URL\n    driver = webdriver.Chrome('C:/Users/Kyrie/GitHub/I501Blackbox/chromedriver')  # Location of chrome driver on Kyrie's Desktop\n    #driver = webdriver.Chrome('C:/Users/kyrie/OneDrive/Documents/GitHub/I501Blackbox/chromedriver')  # Location of chrome driver on Kyrie's Laptop\n    #driver = webdriver.Firefox(executable_path='/usr/local/bin/geckodriver') #this is for Becca's Macbook. Comment this out and do the other drivers for Kyrie.\n    \n    driver.get(url) # Get URL\n    \n    driver.find_element_by_name(\"cycles\").clear() # clear current interval amount\n    driver.find_element_by_name(\"cycles\").send_keys(intervals) # Set to chosen interval amount\n        \n    while current_step < endstep: # Run until end step \n\n        soup = BeautifulSoup(driver.page_source) # read page source\n        \n        table1 = soup.find_all('table',id=\"system\")[0] # find blackbox table in source\n        \n        find_step = soup.find_all('p') # find location of current step in source\n        current_step = int(find_step[3].contents[0][13:]) # Finds current step in integer format\n        \n        #Find all of the numbers in the 20 x 20 grid in the page source and store to my_table\n        rows = table1.findChildren(['th', 'tr'])\n        \n        my_table = []\n        i = 0\n        j = 0\n\n        for row in rows:\n            i += 1\n            cells = row.findChildren('td')\n            for cell in cells:\n                j += 1\n                if j == 21: # Reset to 1 after 20\n                    j= 1\n                value = cell.string\n                my_table.append(value)\n        \n        my_table2 = np.reshape(my_table, (20,20)).astype(int) # Reshape to 20 x 20 array\n        \n        # Save array to text file with simulation # and step # in filename\n        output_file = 'sim' +str(sim_num)+ '_step' + str(current_step) + '_int' + str(intervals) + '.txt'\n        np.savetxt(output_file, my_table2, delimiter=',',fmt= '%d') \n        \n        # Print status\n        print('Simulation ' + str(sim_num) + ': Step ' + str(current_step) + ' file saved.')\n        \n        time.sleep(5) # wait 5 seconds to allow for saving file\n        \n        # Click next button on page\n        button = driver.find_element(By.XPATH, '//button[text()=\"Next n Step\"]')\n        button.click()\n        \n        time.sleep(5) # wait 5 seconds to wait after clicking Next Step\n              \n            \n# Initiates the blackbox function to start at 1 minute after midnight (can tweak the inputs of simulation number, end step and the intervals)          \nt = Timer(secs, blackbox_webscrape) # t = Timer(secs, webscrape_blackbox)\nprint('Will run Blackbox webscraping code in ' + str(secs) + ' seconds.')\nt.start()            \n            \n# Close web browser if need to cancel code            \n            \n            \n            \n            \n            \n\n\n", "343": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\" Webscraping tasks and functions \"\"\"\n\nfrom luigi import ExternalTask, Parameter, build\nimport dask.dataframe as dd\nimport yfinance as yf\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nfrom csci_utils.Validation.validater import valid_type\nfrom csci_utils.luigi.task import Requirement, Requires, TargetOutput\nfrom csci_utils.luigi.dask.target import ParquetTarget\n\nfrom .utils import get_range\n\n\ndef scrape_summary_data(symbol):\n    \"\"\"Uses BeautifulSoup and Requests to webscrape summary data of stock from Yahoo Finance\n\n    Parameters:\n    symbol: The stock symbol (str)\n\n    Returns:\n    Two Pandas Dataframe in a Tuple with up to date summary statistics of stock\n    \"\"\"\n    # Check symbol is a (str), raise type error otherwise\n    valid_type(symbol, str, True)\n\n    try:\n        # Request data from yahoo finance\n        r = requests.get(f\"https://finance.yahoo.com/quote/{symbol}?p={symbol}\")\n\n        soup = BeautifulSoup(r.text, \"lxml\")\n\n        # Create summary stats dictionary and add current price\n        summary = {\n            \"Price\": soup.find_all(\"div\", {\"class\": \"My(6px) Pos(r) smartphone_Mt(6px)\"})[0]\n            .find(\"span\")\n            .text\n        }\n\n    except:\n        # Raise error if stock symbol input is not valid\n        raise ValueError(\"Did not enter valid stock symbol\")\n\n    # Use html class strings from inspecting yahoo finance website\n    class1_str = (\n        \"D(ib) W(1/2) Bxz(bb) Pend(12px) Va(t) ie-7_D(i) smartphone_D(b) smartphone_W(100%)\"\n        + \" smartphone_Pend(0px) smartphone_BdY smartphone_Bdc($seperatorColor)\"\n    )\n\n    # Locate summary statistics and add to summary dictionary\n    tablesoup = soup.find_all(\"div\", {\"class\": class1_str})[0].find_all(\"td\")\n    for i in range(7):\n        summary[tablesoup[2 * i].text] = tablesoup[2 * i + 1].text\n\n    # Return Pandas dataframe of summary stats\n    return pd.DataFrame(summary, index=[0])\n\n\nclass GetHistoricalData(ExternalTask):\n    \"\"\"Uses the yfinance package to webscrape stock data from yahoo finance\n    read in as a dask dataframe, and writes to parquet.\n\n        Parameters:\n        symbol: The stock symbol (str)\n        interval: The unit of time per row (str)\n    \"\"\"\n\n    # Task Parameters\n    symbol = Parameter(default=\"AAPL\")\n    interval = Parameter(default=\"1d\")\n\n    # Target Output as descriptor\n    output = TargetOutput(\n        file_pattern=\"data/{symbol}/{interval}/rawdata/\", target_class=ParquetTarget\n    )\n\n    def run(self):\n        stock = yf.Ticker(self.symbol)\n        df = stock.history(\n            interval=self.interval, period=get_range(self.interval)\n        ).iloc[:, :5]\n        df = df.dropna()\n        ddf = dd.from_pandas(df, chunksize=500)\n        self.output().write_dask(ddf, compression=\"gzip\")\n\n\n", "344": "from bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\nimport sqlite3\n\nconn = sqlite3.connect('vantest.db')\nc = conn.cursor()\n\n#c.execute('''CREATE TABLE vans(title TEXT, price TEXT, spec TEXT, link TEXT)''')\nbaseurl = 'https://www.autotrader.co.uk'\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36', \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"en-US,en;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"\n    }\n\n\nvans = []\n\ndef webscrape():\n    for page in range(1, 10):\n        fuel_type = 'Petrol'\n        distance = 40\n        r = requests.get(f'https://www.autotrader.co.uk/van-search?sort=relevance&postcode=ig26sl&radius={distance}&include-delivery-option=on&fuel-type={fuel_type}&page={page}', headers=headers)\n        soup = BeautifulSoup(r.text, 'lxml')\n        #print(r.status_code)\n\n        vanlist = soup.find_all('li', class_='search-page__result')\n        \n        for van in vanlist:\n            for link in van.find_all('a', class_='js-click-handler listing-fpa-link tracking-standard-link', href=True):\n                \n                van_title = van.find('h3', class_='product-card-details__title').text.strip()\n                van_price = van.find('div', class_='product-card-pricing__price').text.strip()\n                van_specs = van.find('p', class_='product-card-details__subtitle').text.strip()\n                van_link = baseurl + link['href']\n                van = {\n                'name': van_title,\n                'price': van_price,\n                'specs': van_specs,\n                'link': van_link\n                }\n                c.execute('''INSERT INTO vans VALUES(?,?,?,?)''', (van['name'], van['price'], van['specs'], van['link']))\n\n                vans.append(van)\n                print('Saving: ', van['name'])\nwebscrape()                \nconn.commit()\nprint('complete')\n\n#c.execute('''SELECT * FROM vans''')\n#results = c.fetchall()\n#print(results))\n\ndf = pd.read_sql_query(\"SELECT * FROM vans\", conn)\nprint(df)\n\nconn.close()\n\n", "345": "import webscrape_module as d\nfrom bs4 import BeautifulSoup as soup\n\nclass KijijiScraper(d.WebScrape):\n    def __init__(self, url, searched):\n        super().__init__(url, searched)\n    \n    def findApparts(self):\n        link_start = \"https://kijiji.ca\"\n        file_rows, title_column, price_column, description_column, location_column, img_column, link_column = [], [], [], [], [], [], []\n        page = self.parsePage()\n        apparts = page.findAll(\"div\", {\"class\":\"search-item\"})\n\n        for appart in apparts:    \n            title = appart.find(\"div\", {\"class\":\"title\"}).text.strip()\n            title_column.append(title)\n            link = link_start + appart.find(\"div\", {\"class\":\"title\"}).find(\"a\")[\"href\"]\n            link_column.append(link)\n            price = appart.find(\"div\", {\"class\":\"price\"}).text.strip()\n            price_column.append(price)\n            description = appart.find(\"div\", {\"class\":\"description\"}).text.strip()\n            description_column.append(description)\n            address = appart.find(\"div\", {\"class\":\"location\"}).text.strip()\n            location_column.append(address)\n            img = appart.find(\"div\", {\"class\":\"image\"}).find(\"img\")[\"src\"]\n            img_column.append(img)\n\n            print(\"--------------------------------\")\n            print(f\"Location: {address}\")\n            print(f\"Lien: {link}\")\n            print(f\"Prix: {price}\")\n            print(f\"Description: {description}\")\n            print(f\"Image: {img}\")\n            print(\"--------------------------------\")\n        \n    \n        file_rows.append(title_column)\n        file_rows.append(link_column)\n        file_rows.append(price_column)\n        file_rows.append(description_column)\n        file_rows.append(location_column)\n        file_rows.append(img_column)\n\n        return file_rows\n        \n\n\n\n\n            \n\n        \n\n\n\n\n    \n    \n", "346": "import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport webscrape_script as wss\nfrom datetime import date\n\ndf_urls = pd.read_csv(\"Input/urls_scrape.csv\")\ntry:\n    df_data = pd.read_csv(\"Output/data.csv\")\nexcept Exception as e:\n    df_data = pd.DataFrame()\n\n# The process is executed in two stages. In the first stage we load all the content and save it into\n# HTMML file. In the second Stage we perform the webscraping on the downloaded html. This makes eassier\n# the webscraping process and avoid problems with the internet connection.\n\n\ndef update_urlfile(df_urls, index_row):\n    \"\"\"\n    After downloading html or scraping content the process status has to be updated.\n    \"\"\"\n    df_urls.loc[index] = index_row\n    df_urls.to_csv(\"Input/urls_scrape.csv\", index=False)\n\n\nfor index in tqdm(range(len(df_urls))):\n    index_row = df_urls.loc[index]\n    territory = df_urls\n    if index_row[\"status\"] != True:\n        url = index_row[\"url\"]\n        if index_row[\"html_loaded\"] != True:\n            print(\"\\t\\tFIRST STAGE: Load info + dowload html\")\n            html_name = \"htmls/\" + url.split(\"/\")[-1] + \".html\"\n            wss.load_all_content(url, html_name, 30, 7)\n\n            # Update  html download process stautus\n            today_date = date.today()\n            today_date = today_date.strftime(\"%d/%m/%Y\")\n            index_row[\"scrap_date\"] = today_date\n            index_row[\"html_loaded\"] = True\n            index_row[\"html_name\"] = html_name\n\n            update_urlfile(df_urls, index_row)\n\n        else:\n            html_name = index_row[\"html_name\"]\n\n        print(\"\\t\\tSECOND STAGE: Scrape\")\n        df_scrape = wss.scrape_olx(html_name, url)\n\n        if len(df_data) == 0:\n            df_data = df_scrape\n        else:\n            df_data = df_data.append(df_scrape)\n        df_data[\"territory\"] = territory\n        df_data.to_csv(\"Output/data.csv\")\n        # Update  webscrape process stautus\n        index_row[\"number_obs\"] = df_scrape.shape[0]\n        index_row[\"status\"] = True\n        update_urlfile(df_urls, index_row)\n\n    else:\n        print(\"\\turl {} has been downloaded and scraped\".format(\n            index_row[\"url\"]))\n", "347": "# scraper.py includes functions that scrape the website dealerrater.com\n# to find the most positive reviews for a given dealership\n\n\n# Import modules\nimport requests # to request website data\nfrom bs4 import BeautifulSoup # to parse HTML and find content\nfrom dictionary import dictionary\n\n\n# Import local dictionary of positive and negative words\nnegdict, posdict, negdictstrip, posdictstrip = dictionary.dictionary()\n\n\n# 'pages' uses string manipulation to take an input and find the first 5 pages of reviews\ndef pages(URL, numpages): #takes URL, numpages, returns pagelist of numpages URL's of reviews\n\n    pagelist = []\n\n    for i in range(len(URL)):\n        if URL[i] == 'e' and URL[i+1] == '1':\n            var = i+1\n    for j in range(numpages):\n        before = URL[0:(var)]\n        after = URL[(var+1):]\n        pagenum = str(j+1)\n        page = before + pagenum + after\n        #print(pagenum)\n        pagelist.append(page)\n\n    return pagelist\n\n\n# 'reviewlist' uses the BeautifulSoup module to webscrape the given dealership's reviews\n# to create a list of all reviews\ndef reviewlist(pagelist):\n\n    reviewlist = []\n    for i in range(len(pagelist)):\n        \n        URL = pagelist[i]\n        page = requests.get(URL)\n        status = page.status_code #if 200, page is accessible\n        contents = page.text\n        soup = BeautifulSoup(contents, 'html.parser')\n        \n        main = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n        #finds specific reviews 2nd review starts at 2...index of reviews\n        review = main.find_all('div', recursive=False) \n        \n\n        for j in range(len(review)):\n            k = j+1\n            if k > 10:  #if k is greater than the number of reviews on page\n                break\n            else:\n                review = main.find_all('div', recursive=False)[k]\n                body = review.find_all('p')[0]\n                string = body.text\n                string2 = string\n                reviewlist.append(string2)\n\n    return reviewlist\n\n\n# 'generalrating' uses the BeautifulSoup module to webscrape the given dealership's reviews\n# to create a list of the reviews general ratings out of 5 stars\ndef generalrating(pagelist):\n    \n    rating_list = []\n\n    for i in range(len(pagelist)):\n        \n        URL = pagelist[i]\n        page = requests.get(URL)\n        status = page.status_code #if 200, page is accessible\n        contents = page.text\n        soup = BeautifulSoup(contents, 'html.parser')\n        \n        main = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n        #finds specific reviews 2nd review starts at 2...index of reviews\n        review = main.find_all('div', recursive=False) \n\n        for j in range(len(review)):\n            k = j+1\n            if k > 10:  #if k is greater than the number of reviews on page\n                break\n            else:\n                review = main.find_all('div', recursive=False)[k]\n                rating_1 = review.find_all('div')[4]\n                rating_2 = rating_1['class']\n\n                #[2] is the class review rating\n                rating_3 = rating_2[2]  #turning the rating class into a string\n                score = float(rating_3[7] + '.' + rating_3[8]) # score is review score out of 5\n                #index goes from 1-11 : 10 reviews per page\n\n                rating_list.append(score)\n            \n    return rating_list\n\n\n\n# 'specificrating' uses the BeautifulSoup module to webscrape the given dealership's reviews\n# to create a list of the reviews breakdown-ratings out of 5 stars and then averages them\ndef specificratings(pagelist):\n    breakdownlist = []\n    for i in range(len(pagelist)):\n        \n        #range(len(pagelist)):\n        URL = pagelist[i]\n        page = requests.get(URL)\n        status = page.status_code #if 200, page is accessible\n        contents = page.text\n        soup = BeautifulSoup(contents, 'html.parser')\n        \n        main = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n        #finds specific reviews 2nd review starts at 2...index of reviews\n        review = main.find_all('div', recursive=False) \n        for j in range(len(review)):\n            k = j+1\n            if k > 10:  #if k is greater than the number of reviews on page\n                break\n            else:\n                #finds specific reviews 2nd review starts at 2...index of reviews\n\n                review = main.find_all('div', recursive=False)[k]\n                rating_1 = review.find_all('div')[10] #rating breakdown\n\n                breakdownindex = [2, 5, 8, 11, 14] # index of location of specific breakdown scores\n                    # 2 'Customer Service\n                    # 5 'Quality of work\n                    # 8 'Friendliness\n                    # 11 is Pricing\n                    # 14 is overall excperience\n\n                avglist = []\n\n                for l in breakdownindex: # 5 is the number of specific ratings\n                    rating_2 = rating_1.find_all('div')[0] \n\n                    #finds individual rating sets\n                    rating_3 = rating_2.find_all('div')[l]\n                    \n                    rating_4 = rating_3['class']\n                    rating_5 = rating_4[1]\n                    score = float(rating_5[7] + '.' + rating_5[8]) # score is review score out of 5\n\n                    avglist.append(score)\n                \n                average = sum(avglist) / len(avglist)\n                breakdownlist.append(average)\n\n    return breakdownlist\n\n\n# 'positivewords' compares every word in every review to a dictionary of positive-sentiment words to \n# create a list of the number of positve words in each review\ndef positivewords(reviewlist):\n    \n    #initialize lists\n    sentimentscore, pos_words = ([] for i in range(2))\n    \n    for j in range(len(reviewlist)):\n\n        #initialize variables\n        pos_score = 0\n        rsp = []\n\n        ind_review = reviewlist[j]\n        splitwords = ind_review.split(' ')  #individual review list\n\n        for i in range(len(splitwords)):\n            for k in range(len(posdict)):\n\n                if (splitwords[i] == posdict[k]) or (splitwords[i] == posdictstrip[k]):\n\n                    pos_score +=1\n                    rsp.append(splitwords[i])\n\n        if pos_score == 0:\n            rsp.append(0)\n\n        pos_words.append(rsp)\n\n        sentimentscore.append(pos_score)\n\n    return pos_words", "348": "# scraper.py includes functions that scrape the website dealerrater.com\n# to find the most positive reviews for a given dealership\n\n\n# Import modules\nimport requests # to request website data\nfrom bs4 import BeautifulSoup # to parse HTML and find content\nfrom dictionary import dictionary\n\n\n# Import local dictionary of positive and negative words\nnegdict, posdict, negdictstrip, posdictstrip = dictionary.dictionary()\n\n\n# 'pages' uses string manipulation to take an input and find the first 5 pages of reviews\ndef pages(URL, numpages): #takes URL, numpages, returns pagelist of numpages URL's of reviews\n\n    pagelist = []\n\n    for i in range(len(URL)):\n        if URL[i] == 'e' and URL[i+1] == '1':\n            var = i+1\n    for j in range(numpages):\n        before = URL[0:(var)]\n        after = URL[(var+1):]\n        pagenum = str(j+1)\n        page = before + pagenum + after\n        #print(pagenum)\n        pagelist.append(page)\n\n    return pagelist\n\n\n# 'reviewlist' uses the BeautifulSoup module to webscrape the given dealership's reviews\n# to create a list of all reviews\ndef reviewlist(pagelist):\n\n    reviewlist = []\n    for i in range(len(pagelist)):\n        \n        URL = pagelist[i]\n        page = requests.get(URL)\n        status = page.status_code #if 200, page is accessible\n        contents = page.text\n        soup = BeautifulSoup(contents, 'html.parser')\n        \n        main = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n        #finds specific reviews 2nd review starts at 2...index of reviews\n        review = main.find_all('div', recursive=False) \n        \n\n        for j in range(len(review)):\n            k = j+1\n            if k > 10:  #if k is greater than the number of reviews on page\n                break\n            else:\n                review = main.find_all('div', recursive=False)[k]\n                body = review.find_all('p')[0]\n                string = body.text\n                string2 = string\n                reviewlist.append(string2)\n\n    return reviewlist\n\n\n# 'generalrating' uses the BeautifulSoup module to webscrape the given dealership's reviews\n# to create a list of the reviews general ratings out of 5 stars\ndef generalrating(pagelist):\n    \n    rating_list = []\n\n    for i in range(len(pagelist)):\n        \n        URL = pagelist[i]\n        page = requests.get(URL)\n        status = page.status_code #if 200, page is accessible\n        contents = page.text\n        soup = BeautifulSoup(contents, 'html.parser')\n        \n        main = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n        #finds specific reviews 2nd review starts at 2...index of reviews\n        review = main.find_all('div', recursive=False) \n\n        for j in range(len(review)):\n            k = j+1\n            if k > 10:  #if k is greater than the number of reviews on page\n                break\n            else:\n                review = main.find_all('div', recursive=False)[k]\n                rating_1 = review.find_all('div')[4]\n                rating_2 = rating_1['class']\n\n                #[2] is the class review rating\n                rating_3 = rating_2[2]  #turning the rating class into a string\n                score = float(rating_3[7] + '.' + rating_3[8]) # score is review score out of 5\n                #index goes from 1-11 : 10 reviews per page\n\n                rating_list.append(score)\n            \n    return rating_list\n\n\n\n# 'specificrating' uses the BeautifulSoup module to webscrape the given dealership's reviews\n# to create a list of the reviews breakdown-ratings out of 5 stars and then averages them\ndef specificratings(pagelist):\n    breakdownlist = []\n    for i in range(len(pagelist)):\n        \n        #range(len(pagelist)):\n        URL = pagelist[i]\n        page = requests.get(URL)\n        status = page.status_code #if 200, page is accessible\n        contents = page.text\n        soup = BeautifulSoup(contents, 'html.parser')\n        \n        main = soup.find('div', class_='col-xs-12 pad-top-lg mobile-hide')\n\n        #finds specific reviews 2nd review starts at 2...index of reviews\n        review = main.find_all('div', recursive=False) \n        for j in range(len(review)):\n            k = j+1\n            if k > 10:  #if k is greater than the number of reviews on page\n                break\n            else:\n                #finds specific reviews 2nd review starts at 2...index of reviews\n\n                review = main.find_all('div', recursive=False)[k]\n                rating_1 = review.find_all('div')[10] #rating breakdown\n\n                breakdownindex = [2, 5, 8, 11, 14] # index of location of specific breakdown scores\n                    # 2 'Customer Service\n                    # 5 'Quality of work\n                    # 8 'Friendliness\n                    # 11 is Pricing\n                    # 14 is overall excperience\n\n                avglist = []\n\n                for l in breakdownindex: # 5 is the number of specific ratings\n                    rating_2 = rating_1.find_all('div')[0] \n\n                    #finds individual rating sets\n                    rating_3 = rating_2.find_all('div')[l]\n                    \n                    rating_4 = rating_3['class']\n                    rating_5 = rating_4[1]\n                    score = float(rating_5[7] + '.' + rating_5[8]) # score is review score out of 5\n\n                    avglist.append(score)\n                \n                average = sum(avglist) / len(avglist)\n                breakdownlist.append(average)\n\n    return breakdownlist\n\n\n# 'positivewords' compares every word in every review to a dictionary of positive-sentiment words to \n# create a list of the number of positve words in each review\ndef positivewords(reviewlist):\n    \n    #initialize lists\n    sentimentscore, pos_words = ([] for i in range(2))\n    \n    for j in range(len(reviewlist)):\n\n        #initialize variables\n        pos_score = 0\n        rsp = []\n\n        ind_review = reviewlist[j]\n        splitwords = ind_review.split(' ')  #individual review list\n\n        for i in range(len(splitwords)):\n            for k in range(len(posdict)):\n\n                if (splitwords[i] == posdict[k]) or (splitwords[i] == posdictstrip[k]):\n\n                    pos_score +=1\n                    rsp.append(splitwords[i])\n\n        if pos_score == 0:\n            rsp.append(0)\n\n        pos_words.append(rsp)\n\n        sentimentscore.append(pos_score)\n\n    return pos_words", "349": "\"\"\"\nWSGI config for webscrape project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.2/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webscrape.settings')\n\napplication = get_wsgi_application()\n", "350": "\"\"\"\nASGI config for webscrape project.\n\nIt exposes the ASGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.2/howto/deployment/asgi/\n\"\"\"\n\nimport os\n\nfrom django.core.asgi import get_asgi_application\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webscrape.settings')\n\napplication = get_asgi_application()\n", "351": "# Define here the models for your spider middleware\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\n\n# useful for handling different item types with a single interface\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass AmazonWebscrapeSpiderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(self, response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, or item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(self, response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Request or item objects.\n        pass\n\n    def process_start_requests(self, start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n\n\nclass AmazonWebscrapeDownloaderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_request(self, request, spider):\n        # Called for each request that goes through the downloader\n        # middleware.\n\n        # Must either:\n        # - return None: continue processing this request\n        # - or return a Response object\n        # - or return a Request object\n        # - or raise IgnoreRequest: process_exception() methods of\n        #   installed downloader middleware will be called\n        return None\n\n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n", "352": "\"\"\"\nASGI config for webscrape_proj project.\n\nIt exposes the ASGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.2/howto/deployment/asgi/\n\"\"\"\n\nimport os\n\nfrom django.core.asgi import get_asgi_application\nfrom channels.routing import ProtocolTypeRouter,URLRouter\nfrom channels.auth import AuthMiddlewareStack\nfrom graph.routing import ws_urlpatterns\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'webscrape_proj.settings')\n\napplication = ProtocolTypeRouter({\n    'http': get_asgi_application(),\n    'websocket': AuthMiddlewareStack(URLRouter(ws_urlpatterns))\n})\n\n", "353": "# author: Bartlomiej \"furas\" Burek (https://blog.furas.pl)\n# date: 2022.05.29\n\n# [web scraping - How to webscrape an interactive webpage with python - Stack Overflow](https://stackoverflow.com/questions/72423199/how-to-webscrape-an-interactive-webpage-with-python/72425518#72425518)\n    \nimport requests\n    \nurl = 'http://chonos.ifop.cl/flow/stnclick'\n\nparams = {\n    'index': 0\n}\n\nfor number in range(10):\n    params['index'] = number\n    \n    response = requests.get(url, params=params)\n\n    data = response.json()\n\n    print('\\n---', data['name'], '---\\n')\n    \n    # show first 5 values\n    for item in data['series']['sim'][:5]:\n        print(item)\n    print('...')\n", "354": "#Scrape food websites for ingredients\n#look into hrecipe\n#http://microformats.org/wiki/hrecipe\n\nimport IPython\nfrom urllib.request import urlopen as uReq\nfrom urllib.request import Request\nfrom bs4 import BeautifulSoup as soup\nimport keys\nimport urllib\nimport json\nimport sys\n\n\ndef search(recipe):\n\tif recipe == '' or recipe == ' ':\n\t\traise ValueError(\"Error! Need recipe name.\")\n\t\t\n\telse:\n\t\t#Credit rjdang for google search code\n\t\tsearch = recipe\n\t\tsite = \"food network\"\n\t\tsearch_string = site + ' ' + search\n\t\turl = \"https://www.google.com/search?\" \n\t\tquery_encoded = urllib.parse.urlencode({\"q\":search_string})\n\t\tquery = url + query_encoded\n\t\t\n\t\t#spaces %20\n\t\t#apostrophe %27\n\n\t\turl_request = Request(query, headers = {\"User-Agent\": \"Mozilla/5.0\"})\n\t\tuclient = uReq(url_request)\n\t\tresponse = uclient.read()\n\t\tuclient.close()\n\t\tpage_soup = soup(response)\n\t\tred = page_soup.find_all('a')\n\t\t\n\t\t\n\t\tIPython.embed()\n\n\t\t\n\t\t\t\t\n\t\t\t\t\ndef webscrape(myurl):\n\t#open connection and grab page\n\tuClient = uReq(myurl)\n\tpage_html = uClient.read()\n\tuClient.close()\n\tpage_soup = soup(page_html, \"html.parser\")\n\tcontainer = page_soup.find(\"div\",{\"class\":\"o-Ingredients__m-Body\"})\n\thold = container.ul.findAll('li')\n\tingredients = []\n\tfor c in range(0,len(hold)):\n\t\tingredients.append(hold[c].label.string)\n\treturn ingredients\n\t# groceryList = []\n\t# quantity = []\n\t# for d in range(0,len(ingredients)):\n\t\t# ingredient = ingredients[d].split()\t\n\t\t# groceryList.append(ingredient[len(ingredient)-1])\n\t\t# amount = []\n\t\t# for y in range(0,len(ingredient)-1):\n\t\t\t# amount.append(ingredient[y])\n\t\t# quantity.append(amount)\n\t# return (groceryList, quantity)\n\t\nif __name__ == \"__main__\":\n\trecipe = \"shepherd's pie\"\n\tsearch(recipe)\n\t#vals = webscrape(keys.myurl)\n\t", "355": "import unittest\nfrom proj4 import *\n\n\nclass TestDataAccess(unittest.TestCase):\n\n    def test_Billboard_webscrape(self):\n        #1. test Billboard webscrape\n        scrape_inst = get_billboard_data()\n        self.assertEqual(len(scrape_inst), 100)\n\n    def test_Ticketmaster_Json(self):\n        #2. test Json data retrieval\n        api_inst = get_events_data('Ed Sheeran')\n        self.assertEqual(len(api_inst['_embedded']['events']), 10)\n\n\nclass TestClass(unittest.TestCase):\n\n    def testConstructor(self):\n        #3. test class constructor\n        a = TopArtists('Cardi B', 1, 6, 1, 39)\n        self.assertEqual(a.artist, 'Cardi B')\n        #4. test class constructor\n        self.assertEqual(a.current, 1)\n\n\n    def testString(self):\n        #5. test class string\n        a = TopArtists('Cardi B', 1, 6, 1, 39)\n        self.assertEqual(a.__str__(), 'Cardi B, 1, 6, 1, 39')\n\n\nclass TestDatabase(unittest.TestCase):\n\n    def test_artist_table(self):\n        conn = sqlite3.connect(DBNAME)\n        cur = conn.cursor()\n\n        sql = 'SELECT Artist FROM Artists'\n        results = cur.execute(sql)\n        result_list = results.fetchall()\n        artist_list=[]\n        for tuple in result_list:\n            artist_list.append(tuple[0])\n\n        #6. test length of Artist Database\n        self.assertEqual(len(result_list), 100)\n        #7. test artist in Database\n        self.assertIn('The Weeknd', artist_list)\n\n    def test_events_table(self):\n        conn = sqlite3.connect(DBNAME)\n        cur = conn.cursor()\n\n        sql = 'SELECT EventId FROM Events'\n        results = cur.execute(sql)\n        results_list = results.fetchall()\n\n        #8. test length of Events Database\n        self.assertEqual(len(results_list), 720)\n\n        sql = '''\n            SELECT Events.Artist, COUNT(Events.EventId)\n            FROM Events\n            WHERE Events.Artist = 'The Weeknd'\n        '''\n        results = cur.execute(sql)\n        results_list = results.fetchall()\n\n        #9. test event sql count in Database\n        self.assertEqual(len(results_list), 1)\n\n        sql = 'SELECT Artist FROM Events'\n        results = cur.execute(sql)\n        results_list = results.fetchall()\n        artist_list = []\n        for tuple in results_list:\n            artist_list.append(tuple[0])\n\n        #10. test lookup_events - filter out 2 artists with no concert/no referential integrity\n        self.assertNotIn('Cardi B', artist_list)\n\nclass TestDataProcessing(unittest.TestCase):\n\n    def test_commands(self):\n        results = process_command('artists')\n        #11 test process_command('artists') - bar chart\n        self.assertEqual(results[0], '21 Savage')\n\n    def test_bar(self):\n        #12 test bar graph\n        try:\n            bar_venue('CA')\n        except:\n            self.fail()\n\n    def test_map(self):\n        #13 test map\n        try:\n            map_concerts(3)\n        except:\n            self.fail()\n\n        #14 test map command, artist with no events\n        self.assertEqual(map_concerts(1), 0)\n\n    def test_pie(self):\n        #15 test pie chart\n        try:\n            pie_genre('US')\n        except:\n            self.fail()\n\n\n\n\n\nunittest.main()\n", "356": "\"\"\"\nDjango settings for webscrape_the_one project.\n\nGenerated by 'django-admin startproject' using Django 2.2.1.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.2/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/2.2/ref/settings/\n\"\"\"\n\nimport os\nfrom celery.schedules import crontab\nfrom .local_settings import FCM_SERVER_KEY, SECRET_KEY as sk\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/2.2/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = sk\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = ['0.0.0.0', '127.0.0.1', '192.168.42.195']\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'rest_framework',\n    'the_one',\n    'fcm_django',\n]\n\nFCM_DJANGO_SETTINGS = {\n    \"FCM_SERVER_KEY\": FCM_SERVER_KEY,\n    \"ONE_DEVICE_PER_USER\": True,\n    \"DELETE_INACTIVE_DEVICES\": False,\n}\n\n# PUSH_NOTIFICATIONS_SETTINGS = {\n#     'FCM_API_KEY':'AAAA1uniEsw:APA91bFi4H_zjEBQBTEqIYLaCx7NmPkpuUJ1yVex3rr59Cd3Vg8naXzcwF0_iDs3pvi-nB6WeUkEsTnWm63gVeByqiwiTzKJmbd2y3NukJ3LGDQjaXS7poXnUTMdRQVc72EdaoLg9B3u',\n#\n# }\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'webscrape_the_one.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'webscrape_the_one.wsgi.application'\n\n# Database\n# https://docs.djangoproject.com/en/2.2/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n# Password validation\n# https://docs.djangoproject.com/en/2.2/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n# Internationalization\n# https://docs.djangoproject.com/en/2.2/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/2.2/howto/static-files/\n\nSTATIC_URL = '/static/'\n\n# REDIS related settings\n# REDIS_HOST = 'localhost'\n# REDIS_PORT = '6379'\n# BROKER_URL = 'redis://' + REDIS_HOST + ':' + REDIS_PORT + '/0'\n# BROKER_TRANSPORT_OPTIONS = {'visibility_timeout': 3600}\n# CELERY_RESULT_BACKEND = 'redis://' + REDIS_HOST + ':' + REDIS_PORT + '/0'\n\nCELERY_BROKER_URL = 'redis://localhost:6379'\nCELERY_RESULT_BACKEND = 'redis://localhost:6379'\nCELERY_ACCEPT_CONTENT = ['application/json']\nCELERY_RESULT_SERIALIZER = 'json'\nCELERY_TASK_SERIALIZER = 'json'\nCELERY_BEAT_SCHEDULE = {\n    'scrape-every-minute': {\n        'task': 'the_one.tasks.scrape_periodically',\n        'schedule': crontab(),\n    }\n}\n", "357": "import requests\nfrom bs4 import BeautifulSoup\n\nclass Webscrape:\n    def __init__(self, price, type, trendIndicator, trendNum, source):\n        self.price = price\n        self.type = type\n        self.trendIndicator = trendIndicator\n        self.trendNum = trendNum\n        self.source = source\n        pass\n    def priceBitcoin(self):\n\n        url = \"https://coinmarketcap.com/currencies/bitcoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        self.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        return self.price\n    def typeBitcoin(self):\n        url = \"https://coinmarketcap.com/currencies/bitcoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        return coin\n\n    def trendIndicatorBitcoin(self):\n        url = \"https://coinmarketcap.com/currencies/bitcoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        return trendIndicator\n    def trendNumberBitcoin(self):\n        url = \"https://coinmarketcap.com/currencies/bitcoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        return trendNumber\n    def sourceBitcoin(self):\n        url = \"https://coinmarketcap.com/currencies/bitcoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        return source\n\n    def priceTrackBitcoin(self):\n\n        url = \"https://coinmarketcap.com/currencies/bitcoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        # if not down then list will be empty and it will have to be going up\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        ob = Webscrape(price,coin,trendIndicator,trendNumber,source)\n        return ob\n\n\n    def priceEterium(self):\n\n        url = \"https://coinmarketcap.com/currencies/ethereum/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        self.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        return self.price\n    def typeEterium(self):\n        url = \"https://coinmarketcap.com/currencies/ethereum/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        return coin\n\n    def trendIndicatorEterium(self):\n        url = \"https://coinmarketcap.com/currencies/ethereum/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        return trendIndicator\n    def trendNumberEterium(self):\n        url = \"https://coinmarketcap.com/currencies/ethereum/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        return trendNumber\n    def sourceEterium(self):\n        url = \"https://coinmarketcap.com/currencies/ethereum/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        return source\n    def priceTrackEthereum(self):\n\n        url = \"https://coinmarketcap.com/currencies/ethereum/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        # if not down then list will be empty and it will have to be going up\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        ob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n        return ob\n\n    def priceDodgecoin(self):\n\n        url = \"https://coinmarketcap.com/currencies/dogecoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        self.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        return self.price\n    def typeDodgecoin(self):\n        url = \"https://coinmarketcap.com/currencies/dogecoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        return coin\n\n    def trendIndicatorDodgecoin(self):\n        url = \"https://coinmarketcap.com/currencies/dogecoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        return trendIndicator\n    def trendNumberDodgecoin(self):\n        url = \"https://coinmarketcap.com/currencies/dogecoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        return trendNumber\n    def sourceDodgecoin(self):\n        url = \"https://coinmarketcap.com/currencies/dogecoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        return source\n    def priceTrackDodgecoin(self):\n\n        url = \"https://coinmarketcap.com/currencies/dogecoin/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        # if not down then list will be empty and it will have to be going up\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        ob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n        return ob\n\n    def priceTether(self):\n\n        url = \"https://coinmarketcap.com/currencies/tether/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        self.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        return self.price\n    def typeTether(self):\n        url = \"https://coinmarketcap.com/currencies/tether/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        return coin\n\n    def trendIndicatorTether(self):\n        url = \"https://coinmarketcap.com/currencies/tether/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        return trendIndicator\n    def trendNumberTether(self):\n        url = \"https://coinmarketcap.com/currencies/tether/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        return trendNumber\n    def sourceTether(self):\n        url = \"https://coinmarketcap.com/currencies/tether/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        return source\n    def priceTrackTether(self):\n\n        url = \"https://coinmarketcap.com/currencies/tether/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        # if not down then list will be empty and it will have to be going up\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        ob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n        return ob\n\n    def priceCatGirl(self):\n\n        url = \"https://coinmarketcap.com/currencies/catgirl/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        self.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        return self.price\n    def typeCatGirl(self):\n        url = \"https://coinmarketcap.com/currencies/catgirl/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        return coin\n\n    def trendIndicatorCatGirl(self):\n        url = \"https://coinmarketcap.com/currencies/catgirl/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        return trendIndicator\n    def trendNumberCatGirl(self):\n        url = \"https://coinmarketcap.com/currencies/catgirl/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        return trendNumber\n    def sourceCatGirl(self):\n        url = \"https://coinmarketcap.com/currencies/catgirl/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        return source\n    def priceTrackCatGirl(self):\n\n        url = \"https://coinmarketcap.com/currencies/catgirl/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        #if not down then list will be empty and it will have to be going up\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span',{'class': \"icon-Caret-down\"})\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        ob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n        return ob\n\n    def priceCelsius(self):\n\n        url = \"https://coinmarketcap.com/currencies/celsius/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        self.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        return self.price\n    def typeCelsius(self):\n        url = \"https://coinmarketcap.com/currencies/celsius/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        return coin\n\n    def trendIndicatorCelsius(self):\n        url = \"https://coinmarketcap.com/currencies/celsius/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        return trendIndicator\n    def trendNumberCelsius(self):\n        url = \"https://coinmarketcap.com/currencies/celsius/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        return trendNumber\n    def sourceCelsius(self):\n        url = \"https://coinmarketcap.com/currencies/celsius/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        return source\n    def priceTrackCelsius(self):\n\n        url = \"https://coinmarketcap.com/currencies/celsius/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        #if not down then list will be empty and it will have to be going up\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span',{'class': \"icon-Caret-down\"})\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        ob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n        return ob\n\n    def priceBitbook(self):\n\n        url = \"https://coinmarketcap.com/currencies/bitbook/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        self.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        return self.price\n    def typeBitbook(self):\n        url = \"https://coinmarketcap.com/currencies/bitbook/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        return coin\n\n    def trendIndicatorBitbook(self):\n        url = \"https://coinmarketcap.com/currencies/bitbook/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        return trendIndicator\n    def trendNumberBitbook(self):\n        url = \"https://coinmarketcap.com/currencies/bitbook/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        return trendNumber\n    def sourceBitbook(self):\n        url = \"https://coinmarketcap.com/currencies/bitbook/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        return source\n    def priceTrackBitbook(self):\n\n        url = \"https://coinmarketcap.com/currencies/bitbook/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        #if not down then list will be empty and it will have to be going up\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span',{'class': \"icon-Caret-down\"})\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        ob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n        return ob\n\n    def priceSandbox(self):\n\n        url = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        self.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        return self.price\n    def typeSandbox(self):\n        url = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        return coin\n\n    def trendIndicatorSandbox(self):\n        url = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        return trendIndicator\n    def trendNumberSandbox(self):\n        url = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        return trendNumber\n    def sourceSandbox(self):\n        url = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        return source\n    def priceTrackSandbox(self):\n\n        url = \"https://coinmarketcap.com/currencies/the-sandbox/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        #if not down then list will be empty and it will have to be going up\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span',{'class': \"icon-Caret-down\"})\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        ob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n        return ob\n\n    def priceM7v2(self):\n\n        url = \"https://coinmarketcap.com/currencies/m7v2/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        self.price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        return self.price\n    def typeM7v2(self):\n        url = \"https://coinmarketcap.com/currencies/m7v2/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        return coin\n\n    def trendIndicatorM7v2(self):\n        url = \"https://coinmarketcap.com/currencies/m7v2/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span', {'class': \"icon-Caret-down\"})\n        return trendIndicator\n    def trendNumberM7v2(self):\n        url = \"https://coinmarketcap.com/currencies/m7v2/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        return trendNumber\n    def sourceM7v2(self):\n        url = \"https://coinmarketcap.com/currencies/m7v2/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        return source\n    def priceTrackM7v2(self):\n\n        url = \"https://coinmarketcap.com/currencies/m7v2/\"\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        price = soup.find_all('div', {'class': \"priceValue\"})[0].find('span').text\n        source = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].find('img')['src']\n        coin = soup.find_all('div', {'class': \"sc-16r8icm-0 gpRPnR nameHeader\"})[0].text\n        #if not down then list will be empty and it will have to be going up\n        trendIndicator = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span',{'class': \"icon-Caret-down\"})\n        trendNumber = soup.find_all('div', {'class': \"sc-16r8icm-0 kjciSH priceTitle\"})[0].find_all('span')[1].text\n        ob = Webscrape(price, coin, trendIndicator, trendNumber, source)\n        return ob\n", "358": "from edtools import *\nfrom edtools.Scrape import WebScrape\n\n\n\n'''\n====================================================\n\n\tCLASS pcGamesDownloads EXTEND Web\n\n====================================================\n'''\n\n\n# Extend for https://pcgames-download.com/\nclass pcGamesPages( WebScrape.WebScrape ):\n\t\n\tdef __init__(self):\n\t\tcontinue\n\t\n\n\t#------------------------------------------------------\n\t#\tHTML EDITIONS\n\t#------------------------------------------------------\n\n\tdef deleteTags( self ):\n\t\t\n\t\tprint(\"Editing -> Deleting Tags... \")\n\n\t\tlistDelete = []\n\n\t\t# Delete scripts tags, analytics and pops BY TEXT inside tag\t\n\t\tlistDelete.append( [self.soup.find(lambda tag:tag.name==\"script\" and \"GoogleAnalyticsObject\" in tag.text)] )\n\t\tlistDelete.append( [self.soup.find(lambda tag:tag.name==\"script\" and \"_pop\" in tag.text)] )\n\n\t\t# Delete  TAG  google fonts CSS\n\t\tlistDelete.append( self.soup.select(\"link[id=baskerville_googleFonts-css]\") )\t\n\n\t\t# Delete Content Layout\n\t\tlistDelete.append( self.soup.findAll(\"h3\", {\"class\": \"blog-description\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"navigation\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"metaslider\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"sidebar\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"post-meta-container\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"comments\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"iframe\", {\"id\": \"mgiframe\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"footer\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"credits\"}) )\n\t\tlistDelete.append( self.soup.findAll(\"div\", {\"class\": \"yarpp-related\"}) )\t\t\n\t\tlistDelete.append( self.soup.findAll(\"img\", {\"class\": \"avatar\"}) )\n\n\t\t\n\t\t# DELETING TAGS\n\t\tfor listItems in listDelete:\n\t\t\tfor tag in listItems:\n\t\t\t\ttag.decompose()\n\n\n\n\n\t\t\n\n\tdef editNumNav( self ):\n\n\t\tprint(\"Editing -> Changing numbers nav menu links... \")\n\n\t\tnav = self.soup.find(\"div\", {\"class\": \"wp-pagenavi\"})\t\n\t\tbuttonsList = nav.findAll(\"a\")\n\n\t\tfor a in buttonsList:\t\t\t\n\n\t\t\tif not \"page\" in a['href']:\n\t\t\t\taUrl = self.folder + \"page1.html\"\n\t\t\telse:\n\t\t\t\taUrl = \"page\"+a['href'].split(\"/\")[-2] + \".html\"\n\n\t\t\ta['href'] = aUrl\n\n\n\n\tdef editPostUrl( self ):\n\t\t\n\t\tprint(\"Editing -> Changing post games links... \")\n\n\t\tpostList = self.soup.find(\"div\", {\"class\": \"posts\"})\n\t\tbuttonsPosts = postList.findAll(\"a\")\n\n\t\tfor a in buttonsPosts:\n\t\t\taUrl = a['href'][:-1] + \".html\"\n\t\t\ta['href'] = aUrl\n\t\t\n\t\n\n\t#------------------------------------------------------\n\t#\tGetters and Setters\n\t#------------------------------------------------------\n\n\tdef getUrlPosts( self ):\n\t\treturn self.soup.findAll(\"a\", {\"class\": \"more-link\"})\n\n\n\tdef setNumberPage( self, n):\n\t\tself.nPage = n\n\n\t\n\t#------------------------------------------------------\n\t#\tSaving Posts\n\t#------------------------------------------------------\n\n\tdef savePosts( self ):\n\n\t\tappPost = pcGamesPages( urlDomain = \"https://pcgames-download.com/\", folder = \"../../\")\n\n\t\tpagesList = self.getUrlPosts()\n\t\t\n\n\t\tfor gameUrlSrc in pagesList:\n\n\t\t\tgameUrl = gameUrlSrc[\"href\"]\t\t\t\n\t\t\tgameUrlData = gameUrl.replace(\"https://pcgames-download.com/\",\"\").split(\"/\")\n\t\t\t\n\t\t\tfolderPost = gameUrlData[0] + \"/\" + gameUrlData[1] + \"/\"\n\t\t\tnamePost = gameUrlData[2] + \".html\"\n\t\t\t\n\t\t\tprint( \"-\"*30+\"\\n[\"+self.nPage+\"] -> Downloading post: \" + namePost)\n\n\n\t\t\t# Create another instance for post pages\n\t\t\tappPost.getHtmlCode( gameUrl )\n\t\t\tappPost.deleteTags()\n\t\t\tappPost.saveHtml( folderPost, namePost )\t\n\t\t\tappPost.saveImages( )\t\n\n\t\t\tprint( \"-\"*30)\n\n\n\n'''\n====================================================\n\n\tEND  CLASS PcGamesDownloads\n\n====================================================\n'''\n\n\n\n\n\ndef initCustomPcGames( ini, fin, mult, folderPath):\n\n\n\tini = ini\n\tfin = fin\t\n\n\n\tdef pInit():\n\t\tprint(\"\\n\"*2 +\"=\"*80+\"\\n\" + \"\\n STARTED\\n\\n\" + \"=\"*80+ \"\\n\" )\n\t\n\tdef pEnd():\n\t\tprint(\"\\n\"*5 + \"=\"*80+\"\\n\" + \"\\n END\\n\\n\" + \"=\"*80 + \"\\n\")\n\n\tdef pPage(str):\n\t\tprint(\"\\n\"*3+\"=\"*80+\"\\n\" +  \"Working page: \" + str + \"\\n\" + \"=\"*80)\n\n\n\n\tdef customSoup(soup):\n\t\tsoupPosts = soup.find(\"div\", {\"class\": \"posts\"})\n\t\tsoupPosts.clear() # Delete content from sopupTag\n\t\treturn soupPosts\n\n\n\tdef appSinlePage():\n\n\n\t\tapp = pcGamesPages( urlDomain = \"https://pcgames-download.com/\", folder = folderPath )\n\n\n\t\tfor i in range(ini,fin+1):\n\t\t\n\t\t\ti = str(i)\n\t\t\turl = 'https://pcgames-download.com/page/' + i + '/'\n\t\t\tnamePage = \"page\" + i + \".html\"\n\n\t\t\tpPage(namePage)\t\n\n\n\t\t\t#-------------------------------------------------------\n\t\t\t# Scraping Procces\n\t\t\t#-------------------------------------------------------\n\n\t\t\t# Get html code from web\n\t\t\tapp.getHtmlCode( url )\t\t\t\n\n\t\t\t'''\n\t\t\t# Editing html\n\t\t\tapp.deleteTags()\t\t\n\t\t\tapp.editPostUrl()\n\t\t\tapp.editNumNav()\n\n\n\t\t\t# Save html and images\n\t\t\tapp.saveHtml( \"\", namePage)\n\t\t\tapp.saveImages()\n\n\t\t\t'''\n\t\t\t\n\t\t\t# Download all Post per page\n\t\t\t#app.resetSoup() # Reseting soup changes\n\t\t\tapp.setNumberPage(str(i))\t\t\t\n\t\t\tapp.savePosts()\n\n\n\t\t\t#-------------------------------------------------------\n\t\t\t# END Scraping Procces\n\t\t\t#-------------------------------------------------------\n\n\n\n\tdef appMultiplePageBy( mult ):\n\n\t\tmult = mult\n\t\t\n\t\tbigList = []\n\n\t\tapp = pcGamesPages( urlDomain = \"https://pcgames-download.com/\", folder = folderPath )\n\n\t\tfor i in range(ini,fin+1):\n\t\t\n\t\t\ti = str(i)\n\t\t\turl = 'https://pcgames-download.com/page/' + i + '/'\n\t\t\tnamePage = \"page\" + i + \".html\"\n\n\t\t\tpPage(namePage)\t\n\n\n\t\t\t\n\n\t\t\t#-------------------------------------------------------\n\t\t\t# Scraping Procces\n\t\t\t#-------------------------------------------------------\n\n\t\t\t# Get html code from web\n\t\t\tapp.getHtmlCode( url )\n\n\t\t\t# save images\n\t\t\tapp.saveImages()\n\n\t\t\t#-------------------------------------------------------\n\t\t\t# END Scraping Procces\n\t\t\t#-------------------------------------------------------\n\n\n\t\t\tbigList.append( app.getSoup().findAll(\"div\", {\"class\": \"post-container\"}) )\n\n\n\t\t\t# Multiplos de 10\n\t\t\tif (int(i)%mult) == 0 or int(i) == fin:\n\n\t\t\t\tprint(\"Editing html data...\")\n\n\t\t\t\tnamePage = \"big\" + i + \".html\"\n\t\t\t\tsoupPosts = app.getSoup().find(\"div\", {\"class\": \"posts\"})\n\t\t\t\tnav = app.getSoup().find(\"div\", {\"class\": \"wp-pagenavi\"})\n\n\t\t\t\tsoupPosts.clear() # Delete content from sopupTag\n\n\n\t\t\t\tfor itemBiglist in bigList:\n\t\t\t\t\tfor itemPageBotton in itemBiglist:\n\t\t\t\t\t\tsoupPosts.append(itemPageBotton)\n\n\n\n\t\t\t\tnav.clear() # Delete content from nav\n\n\t\t\t\tfor j in range(ini,fin+1):\n\n\t\t\t\t\tj = str(j)\n\n\t\t\t\t\tif (int(j)%mult) == 0 or int(j) == fin:\n\t\t\t\t\t\tif j == i:\n\t\t\t\t\t\t\tnav.append( BeautifulSoup(''+j+'', 'html.parser') )\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tnav.append( BeautifulSoup(''+j+'', 'html.parser') )\n\n\t\t\t\t\n\n\t\t\t\t# Editing html\n\t\t\t\tapp.deleteTags()\t\t\n\t\t\t\tapp.editPostUrl()\t\t\t\t\n\n\t\t\t\t# Save html and images\n\t\t\t\tapp.saveHtml( \"\", namePage)\n\n\t\t\t\t\n\t\t\t\tbigList = []\n\n\t\t\t\t\t\n\n\n\t\n\n\n\n\tpInit()\n\n\tappSinlePage()\n\t#appMultiplePageBy(mult)\n\n\tpEnd()\n\n\n\nlocalPath = \"c:/webscrape/pcgames-download.com/\"\n\n\ninitCustomPcGames(228,296,10, localPath ) #( inicio, final, multiplos ) => Multiplos sirve para unificar paginas\n\n", "359": "from ipaddress import ip_address\nfrom itertools import count\nimport json\nfrom re import T\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom datetime import date\nimport gspread\nimport logging\nimport yaml\nimport boto3\nimport os\nimport pandas as pd\nimport io\n\n\ndef get_child(html, pos):\n    return [i for i in html[pos].children][0]\n\ndef extract_books(page):\n\n    book_list = page.find_all('span', {'class' : 'zg-bdg-text'})\n\n    assert len(book_list) > 0, 'no books found'\n    books = {}\n\n    for book_ranking in book_list:\n        \n        book_rank_scrape = [i for i in book_ranking][0]\n\n        book = {\n            'book_rank_scrape' : book_rank_scrape\n        }\n\n        book_box = book_ranking.parent.parent.parent\n\n\n        book_info = [i for i in [i for i in [i for i in book_box.children][1]][0]]\n\n        if len(book_info) != 6:\n            if len(book_info) == 5:\n                # gonna guess that 5 means no star rating TODO improve later\n\n                img = book_info[0]\n                book_title = book_info[1]\n                author = book_info[2]\n                book_type = book_info[3]\n                price = book_info[4]\n\n                # book title\n                book['book_title_txt'] = [i for i in [i for i in [i for i in book_title.children][0].children][0].children][0]\n                book['author_txt'] = [i for i in [i for i in [i for i in author][0]][0]][0]\n                book['star_rating_txt' ] = 'unknown'\n                book['book_type_txt'] = [i for i in [i for i in book_type][0]][0]\n                book['price_txt'] = [i for i in [i for i in [i for i in [i for i in price][0]][0]][0]][0]\n\n            else:\n                # no idea jk :P but dict always needs 5 keys\n                book['book_title_txt'] = 'unknown'\n                book['author_txt'] = 'unknown'\n                book['star_rating_txt' ] = 'unknown'\n                book['book_type_txt'] = 'unknown'\n                book['price_txt'] = 'unknown'\n        else:\n            # expected 6 catorgories\n            img = book_info[0]\n            book_title = book_info[1]\n            author = book_info[2]\n            star_rating = book_info[3]\n            book_type = book_info[4]\n            price = book_info[5]\n\n            # book title\n            book['book_title_txt'] = [i for i in [i for i in [i for i in book_title.children][0].children][0].children][0]\n            book['author_txt'] = [i for i in [i for i in [i for i in author][0]][0]][0]\n            book['star_rating_txt' ]= [i for i in [i for i in [i for i in [i for i in [i for i in star_rating][0]][0]][0]][0]][0]\n            book['book_type_txt'] = [i for i in [i for i in book_type][0]][0]\n            book['price_txt'] = [i for i in [i for i in [i for i in [i for i in price][0]][0]][0]][0]\n\n        books[book_rank_scrape] = book\n\n    return books\n\n\n\n\ndef write_gs(df):\n\n    gc = gspread.service_account(\"webscrape-346716-e7082b6f73c5.json\")\n\n    sh = gc.open(\"Amazon Data\")\n\n    sheetName = date.today().strftime(\"%d_%m_%Y\") \n\n    try:\n        sh.add_worksheet(sheetName, rows = 20, cols = 10)\n    except:\n        sheetName = sheetName + '_1'\n        sh.add_worksheet(sheetName, rows = 20, cols = 10)\n\n    logging.info(f'Created excel sheetnamer was: {sheetName}')\n\n    worksheet = sh.worksheet(sheetName)\n\n    worksheet.update([df.columns.values.tolist()] + df.values.tolist())\n\n\ndef get_webpage(url):\n\n    api_key =  os.getenv('proxy_api')\n    assert api_key is not None, \"envrioment var proxy_api not found\"\n\n    api_url = \"https://api.webscrapingapi.com/v1\"\n\n    params = {\n        \"api_key\": api_key['api_key'],\n        \"url\": url\n    }\n\n    success_status = False\n    attept = 1\n    max_attepts = 10\n\n    while success_status is False and attept < max_attepts:\n        \n        try:\n            response = requests.request(\"GET\", api_url, params=params)\n            logging.info(f'response status {response.status_code}')\n\n            if response.status_code == 401:\n                raise Exception(\"401 error\")\n\n            if response.status_code == 422:\n                raise Exception(\"422 error\")\n\n            success_status = True\n        except:\n            attept += 1\n            pass\n\n    page = BeautifulSoup(response.content, features= \"html.parser\")\n\n    return page\n\n\n\ndef load_all_data():\n\n    gc = gspread.service_account(\"webscrape-346716-e7082b6f73c5.json\")\n\n    sh = gc.open(\"Amazon Data\")\n\n    worksheet_list = sh.worksheets()\n\n    sheet_dfs = {}\n\n    for sheet in worksheet_list:\n        dataframe = pd.DataFrame(sheet.get_all_records())\n        dataframe['date'] = sheet.title\n        sheet_dfs[sheet.title] = dataframe\n\n\n    all_data =  pd.concat(sheet_dfs.values(), ignore_index=True)\n    return all_data\n\n\ndef write_books_s3(book_dict, filename):\n    \n\n    aws_access_key = os.environ.get('access_key')\n    aws_secret_access = os.environ.get('secret')\n\n    if aws_access_key is None or aws_secret_access is None:\n        raise Exception(\"Missing envrioment variables to access s3- access_key and secret\")\n\n    s3 = boto3.client(\n        service_name = 's3',\n        region_name = 'eu-west-2',\n        aws_access_key_id = aws_access_key,\n        aws_secret_access_key = aws_secret_access\n    )\n\n    s3_bucket_name = 'books-webscrape'\n\n    response = s3.put_object(\n        Body = json.dumps(book_dict),\n        Bucket = s3_bucket_name,\n        Key = filename\n    )\n\n    status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n\n    if status == 200:\n        logging.info(f\"Successful S3 put_object response. Status - {status}\")\n    else:\n        logging.info(f\"Unsuccessful S3 put_object response. Status - {status}\")\n\n\ndef convert_to_pd(book_pages: Dict, date: chr):\n\n    books_dfs = {}\n\n    for cato in book_pages.keys():\n\n        cato_df = pd.DataFrame.from_dict(book_pages[cato], orient='index')\n        cato_df['catagory'] = cato\n\n        books_dfs[cato] = cato_df\n\n    day_df = pd.concat(books_dfs.values(), ignore_index=True)\n    day_df['date'] = date\n    \n    return day_df\n\ndef write_books_s3(book_pages_df, today):\n\n    aws_access_key = os.getenv('access_key')\n    aws_secret_access = os.getenv('secret')\n\n    if aws_access_key is None or aws_secret_access is None:\n        raise Exception(\"Missing envrioment variables to access s3- access_key and secret\")\n\n    s3 = boto3.client(\n        service_name = 's3',\n        region_name = 'eu-west-2',\n        aws_access_key_id = aws_access_key,\n        aws_secret_access_key = aws_secret_access\n    )\n\n    s3_bucket_name = 'books-webscrape'\n\n    response = s3.get_object(Bucket=s3_bucket_name, Key=\"books.csv\")\n\n    status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n\n    if status == 200:\n        logging.info(f\"Successful S3 get_object response. Status - {status}\")\n        books_df = pd.read_csv(response.get(\"Body\"))\n    else:\n        logging.info(f\"Unsuccessful S3 get_object response. Status - {status}\")\n\n\n    if today in books_df.date.unique():\n        logging.info(\"Data already exists, not saving\")\n        return\n    else:\n        logging.info(\"Data doesn't exists, adding to total\")\n        combined_data = pd.concat([books_df, book_pages_df])\n        \n        \n    with io.StringIO() as csv_buffer:\n        combined_data.to_csv(csv_buffer, index=False)\n\n        response = s3.put_object(\n            Bucket=s3_bucket_name, Key=\"books.csv\", Body=csv_buffer.getvalue()\n        )\n\n        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n\n        if status == 200:\n            logging.info(f\"Successful S3 put_object response. Status - {status}\")\n        else:\n            logging.info(f\"Unsuccessful S3 put_object response. Status - {status}\")\n", "360": "from selenium import webdriver \n #-*- coding: utf-8 -*-\nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport csv\nimport urllib.request\nfrom lxml import html\nimport time\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom pprint import pprint as pp\nfrom fuzzywuzzy import fuzz\n\nscrape_file = 'Classic-Imports-and-Design/python webscrape/url-lists/caracole-sku.csv'\nlogin_url = 'https://my.furnishweb.com/'\n\nwith open(scrape_file) as f:\n    reader = csv.reader(f, delimiter=',')\n    scrape_file = list(reader)\n\nwritefile = open('Classic-Imports-and-Design\\python webscrape\\output.csv', 'w+', encoding='UTF-8')\noutput_file = csv.writer(writefile, delimiter=\",\")\n\ndriver = webdriver.Chrome('Classic-Imports-and-Design\\python webscrape\\chromedriver.exe')\n\n#login\ndriver.get(login_url)\nprint(\"Please log into the wholesale account.\")\nprint('regencyantiqmd@aol.com')\nprint('23Friday!01')\ninput()\n\ndata = ['sku', 'Name', 'Brand', 'Categories', 'Wholesale', 'Retail', 'Description', 'Images', 'Length', 'Width', 'Height']\noutput_file.writerow(data)\nfails = []\n\nfor product in scrape_file:\n    pp(fails)\n    product[0] = product[0].strip()\n    product[1] = product[1].strip()\n    data = []\n    driver.get('https://my.furnishweb.com/index.cfm?go=products.show') \n    driver.execute_script('dsp_detail(\"' + product[0] + '\");')\n    time.sleep(3)\n\n    try:\n        product_status = driver.find_elements(By.XPATH, '/html/body/div[1]/div[2]/div/div/div[1]/div/div[3]/div/div[2]/div/div[1]/p')[0].text\n        if 'discontinued' in product_status.lower():\n            fails.append(product[0] + ' is discontinued')\n            continue\n    except:\n        fails.append(product[0] + ' could not find status')\n        continue\n    #sku\n    data.append(product[0])\n    #Name\n    try:\n        product_name = driver.find_elements(By.XPATH, '/html/body/div[1]/div[2]/div/div/div[1]/div/div[3]/div/div[2]/div/div[1]/h2[2]')[0].text \n        if product_name[0] == '*':\n            product_name = product_name.lstrip('* ')\n        data.append(product_name)\n    except:\n        fails.append(product[0] + ' has no name')\n        continue\n    #Brand\n    data.append('Caracole')\n    #Catagories\n    data.append(product[1])\n    #Wholesale\n    try:\n        price_table = driver.find_elements(By.CLASS_NAME, 'table')[0].text.split('\\n')\n        index = 0\n        for element in price_table:\n            if 'Greensboro' in element:\n                wholesale = float( price_table[index + 2].lstrip('$').replace(',','') )\n                price_found = True\n                data.append(wholesale)\n                break\n            else:\n                price_found = False\n                index = index + 1\n        if price_found == False:\n            fails.append(product[0] + ' has no Greensboro price')\n            continue\n        #Retail\n        data.append(wholesale * 2)\n    except:\n        data.append('0')\n        data.append('0')\n        fails.append(product[0] + ' does not have price table')\n    #Description\n    try:\n        divs = driver.find_elements(By.CLASS_NAME, 'productinfoheading')\n        product_overview = divs[1].find_elements(By.TAG_NAME, 'h3')\n        index = 0\n        for element in product_overview:\n            if 'OVERVIEW' in element.text:\n                break\n            else:\n                index = index + 1\n        product_desc = divs[1].find_elements(By.TAG_NAME, 'p')[index].text\n        product_desc = product_desc.replace('?', \"'\")\n        data.append(product_desc)\n    except:\n        data.append('NA')\n        fails.append(product[0] + ' has no description')\n    #Images\n    image_boxes = driver.find_elements(By.CLASS_NAME, 'col-xs-8')\n    try:\n        c = 1\n        img_filenames = []\n        for element in image_boxes:\n            #there are many col-xs-8 in HTML page. We only want ones for RGB image\n            if 'RGB' in element.text:    \n                #ELEMENT = IMAGE BOX\n                image_name = element.find_elements(By.TAG_NAME, 'strong')[0].text\n                #match img name and product name. if >60% match then it probably is a product image\n                if fuzz.partial_ratio(image_name.lower(), product_name.lower()) > 60:\n                    #find description of image, if RGB image then we download it\n                    image_details = element.find_elements(By.TAG_NAME, 'p')\n                    for line in image_details:\n                        if 'RGB' in line.text:\n                            use_image = True\n                            break\n                        else:\n                            use_image = False\n                    if use_image == True:\n                        dl_menu = element.find_elements(By.TAG_NAME, 'span')[0]\n                        dl_menu.click()\n                        time.sleep(1.25)\n                        dl_menu = driver.find_elements(By.ID, 'file-download-modal')[0]\n                        image_url = dl_menu.find_elements(By.TAG_NAME, 'a')[0].get_attribute('href')\n                        file_name = product[0] + '-' + str(c) + '.jpg'\n                        print('Downloading image:   ' + image_name + ' :  As > ' + file_name)\n                        urllib.request.urlretrieve(image_url, 'Classic-Imports-and-Design/python webscrape/product-images/' + file_name)\n                        img_filenames.append(file_name)                    \n                        driver.find_elements(By.CLASS_NAME, 'close')[0].click()\n                        time.sleep(.3)\n                        c = c+1\n                        use_image = False\n        data.append( ','.join(img_filenames) )\n    except Exception as err:\n        fails.append(product[0] + ' has no images: ' + str(err))\n        continue\n    #Dimensions\n    try:\n        index = 0\n        for element in product_overview:\n            if '(IN)' in element.text:\n                product_dim = divs[1].find_elements(By.TAG_NAME, 'p')[index].text\n                break\n            else:\n                index = index + 1\n        #L, W, H\n        #caracole has W, D, H\n        data.append(product_dim)\n    except:\n        fails.append(product[0] + 'has no dimensions')\n    print(data)\n\n    output_file.writerow(data)\n\n\n\npp(fails)\nwritefile.close()\n\n#driver.get(url)\n#time.sleep()\n#content = driver.page_source\n#soup = BeautifulSoup(content, \"html.parser\")\n#tree = html.fromstring(driver.page_source) \n\n#driver.find_elements(By.XPATH, XPATH)\n#driver.find_elements(By.CLASS_NAME, CLASSNAME)\n#soup.find(\"TAG-TYPE\", class_=\"CLASS-NAME\").text\n\n#urllib.request.urlretrieve(IMG-URL, FILENAME) \n\n#output_file.writerow(data)", "361": "from PIL import Image\nimport pytesseract as pt\nimport os\nimport glob\nimport json\n\n# Works only on Windows\npt.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n\n\n# FUNCTIONS\n# Add text from image to jsonElement\ndef addTextToJson(jsonElement, textJsonName, imagePath):\n    im = Image.open(str(imagePath))\n    content = pt.image_to_string(im, lang=\"nld\")\n    jsonElement[str(textJsonName)] = content\n    print(\"json text added from \" + imagePath)\n\n\ndef addCategoriesToJson(jsonElement, textJsonName, imagePath):\n    try:\n        im = Image.open(str(imagePath))\n        content = pt.image_to_string(im, lang=\"nld\")\n        # print(content)\n    except KeyError:\n        print(\"No linkedIn screenshot\")\n\n\n# SCRIPT\n# Remove old contents pdf folder\nfiles = glob.glob('./pdfFiles/*')\nfor f in files:\n    os.remove(f)\n\n# Load Json data\ndata = {}\nwith open('./data/WebscrapeData.json') as json_file:\n    data = json.load(json_file)\n    json_file.close()\n\n# Add to Json\nfor line in data:\n    for element in data[str(line)]:\n        addTextToJson(element, 'content', element['screenshotPath'])\n        addCategoriesToJson(element, \"categories\", element['linkedinScreenshot'])\n\n# Write to Json file\nwith open('./data/WebscrapeData.json', 'w') as out:\n    json.dump(data, out)\n", "362": "# Requirements\n# Create environment variable \n# NIT_USR : Your User Name in Nit mail server\n# NIT_PW : Your mail server password\n# MAIL_USR : Your gmail server username\n# MAIL_PW : Your gmail server password\n# MAIL_SND : To which mail the notification to send to\n\n\nimport os,re\nimport requests\nfrom bs4 import BeautifulSoup\n\n\n_ERROR = -1\nER_WEBPAGE_UNAVAILABLE = \"couldn't connect to url\";\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36'\n}\nlogin_data ={\n    'loginOp': 'login',\n    'client': 'standard',\n}\nlogin_data['username'] = os.environ.get(\"NIT_USR\")\nlogin_data['password'] = os.environ.get(\"NIT_PW\")\n\n\ndef check4Urgent(msg):\n    try:\n        return msg.find(\"img\").attrs['alt'] == \"High Priority\"\n    except:\n        return False\n    \ndef check4Movie(msg):\n    text = \"\"\n    try:\n        text = str(msg.find(\"span\",attrs={'class':\"Fragment\"}))\n    except:\n        return False\n    matches = re.search(r\"[M|m]ovie[s]?\", text, re.MULTILINE)\n    if matches != None: return True\n    return False\n        \ndef webscrape():\n    with requests.Session() as s:\n        url = \"https://mail.nitrkl.ac.in/\"\n        try:\n            r= s.get(url,headers=headers)\n        except:\n            return _ERROR\n        soup = BeautifulSoup(r.content,features=\"html.parser\")\n        login_data[\"login_csrf\"] = soup.find('input',attrs={'name':'login_csrf'})['value']\n        \n        r= s.post(url,data=login_data,headers=headers)\n        soup = BeautifulSoup(r.content)\n        inbox = soup.find(\"tbody\", {\"id\": \"mess_list_tbody\"})\n        message_list =list(inbox.find_all(\"tr\",{\"class\":\"Unread\"}))\n        return message_list\ndef mailDetails():\n    detail = dict()\n\n    mgs = webscrape()\n\n    if mgs == _ERROR:\n        detail[\"error\"] = ER_WEBPAGE_UNAVAILABLE\n        return detail\n\n    urgent = len(list(filter(check4Urgent,mgs)))\n    movie  = len(list(filter(check4Movie,mgs)))\n    \n    detail[\"mails\"]  = len(mgs)\n    detail[\"urgent\"] = urgent\n    detail[\"movie\"]  = movie\n\n    return detail\n\nif __name__ == \"__main__\":\n    # numberOfMails()\n    print(mailDetails())\n    pass\n\n\n\n\n", "363": "# Standard\nimport sqlite3\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nfrom datetime import date\n\n# PyPI\n\n# LOCAL\nfrom interface_db.reference import table_classes as table\n\n# Variables to make pylint happy\ntest_database=None\n# if os.environ['WEBSCRAPE_DB']:\n#     prod_database=Path(os.environ['WEBSCRAPE_DB'])\n\n\n# class url_error:\n#     \"\"\"Intended use is with an insert function into the url_error_list table.\"\"\"\n#     today = date.today()\n#     last_date = datetime.now()\n#     def __init__(self, url, url_type, parent_id, resolved=False,\n#                  id=None, \n#                  last_date=last_date, last_user='default', \n#                  create_date=today, create_user='default'):\n#         self.id          = id if id != None else None\n#         self.url         = url\n#         self.url_type    = url_type\n#         self.parent_id   = parent_id\n#         self.resolved    = resolved\n#         self.last_date   = last_date\n#         self.last_user   = last_user\n#         self.create_date = create_date\n#         self.create_user = create_user\n\n\n# class website:\n#     \"\"\"Intended use is with an insert function into the websites table.\"\"\"\n#     today = date.today()\n#     last_date = datetime.now()\n#     def __init__(self, name, url, \n#                  id=None, \n#                  last_date=last_date, last_user='default', \n#                  create_date=today, create_user='default'):\n#         self.id          = id if id != None else None\n#         self.name        = name\n#         self.url         = url\n#         self.last_date   = last_date\n#         self.last_user   = last_user\n#         self.create_date = create_date\n#         self.create_user = create_user\n\n\n# class category:\n#     \"\"\"Intended use is with an insert function into the categories table.\"\"\"\n#     today = date.today()\n#     last_date = datetime.now()\n#     def __init__(self, name=None, url=None, website_id=None,\n#                  id=None, context=None,\n#                  last_date=last_date, last_user='default',\n#                  create_date=today, create_user='default'):\n#         self.id          = id if id != None else None\n#         self.name        = name\n#         self.context     = context\n#         self.url         = url\n#         self.website_id  = website_id\n#         self.last_date   = last_date\n#         self.last_user   = last_user\n#         self.create_date = create_date\n#         self.create_user = create_user\n    \n    \n# class blog:\n#     \"\"\"Intended use is with an insert function into the blogs table.\"\"\"\n#     today = date.today()\n#     last_date = datetime.now()\n#     def __init__(self, author=None, name=None, url=None, \n#                  id=None, category_id=None, \n#                  last_date=last_date, last_user='default', \n#                  create_date=today, create_user='default'):\n#         self.id          = id\n#         self.author      = author\n#         self.name        = name\n#         self.url         = url\n#         self.category_id = category_id\n#         self.last_date   = last_date\n#         self.last_user   = last_user\n#         self.create_date = create_date\n#         self.create_user = create_user\n\n\n# class post:\n#     \"\"\"Intended use is with an insert function into the pposts table.\"\"\"\n#     today = date.today()\n#     last_date = datetime.now()\n#     def __init__(self, url, blog_id, title=None, author=None, date=None, tags=None, content=None, content_html=None,\n#                  id=None, \n#                  last_date=last_date, last_user='default',\n#                  create_date=today, create_user='default'):\n#         self.id          = id\n#         self.title       = title\n#         self.author      = author\n#         self.date        = date\n#         self.tags        = tags\n#         self.content     = content\n#         self.content_html= content_html\n#         self.url         = url\n#         self.blog_id     = blog_id\n#         self.last_date   = last_date\n#         self.last_user   = last_user\n#         self.create_date = create_date\n#         self.create_user = create_user\n\n\nclass database(object):\n    \"\"\"Handles all database connectsion, inputs, and outputs\n    \n    Class constructor initiates sqlite3 database connection. If used in WITH statement\n    the connection will cleanly close after the statement is finished. If there are\n    uncommitted transactions they will be rolled back prior to connection closure.\n    \n    \"\"\"\n    def __init__(self, test_database=None):\n        if test_database:\n            self.__db_connection = test_database\n            self.cur = self.__db_connection.cursor()\n        # elif os.path.exists(Path(os.environ['WEBSCRAPE_DB']) / 'webscraper.db'): #os.environ['WEBSCRAPE_DB']:\n        #     print(os.path.exists(Path(os.environ['WEBSCRAPE_DB']) / 'webscraper.db'))\n        #     print(os.path.exists(prod_database / 'webscraper.db'))\n        #     __DB_LOCATION = prod_database / 'webscraper.db'\n        #     print(__DB_LOCATION)\n\n        else:\n            __DB_LOCATION = (\n                Path.home() \n                / \"py_apps\" \n                / \"_appdata\" \n                / \"webscraper\"\n                / \"webscraper.db\"\n            )\n            if os.path.exists(__DB_LOCATION):\n                self.__db_connection = sqlite3.connect(str(__DB_LOCATION))\n                self.cur = self.__db_connection.cursor()\n            else:\n                Path(\n                    Path.home() / \"py_apps\" / \"_appdata\" / \"webscraper\"\n                ).mkdir(parents=True, exist_ok=True)\n                self.__db_connection = sqlite3.connect(str(__DB_LOCATION))\n                self.cur = self.__db_connection.cursor()\n\n    def __del__(self):\n        self.__db_connection.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, ext_type, exc_value, traceback):\n        self.cur.close()\n        if isinstance(exc_value, Exception):\n            self.__db_connection.rollback()\n        else:\n            self.__db_connection.commit()\n        self.__db_connection.close()\n    \n\n\n    def query_table_ids_all(self, table: str, where_column=None, table_id=None, last_date_ascending=False) -> list:\n        id_list = []\n        if table_id == None:\n            result = self.cur.execute(f\"\"\"SELECT id FROM {table} \n                                          ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\").fetchall()\n        else:\n            result = self.cur.execute(f\"\"\"SELECT id FROM {table} \n                                          WHERE {where_column} = {table_id} \n                                          ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\").fetchall()\n        # print(f\"\"\"SELECT id FROM {table} \n        #                                   WHERE {where_column} = {table_id} \n        #                                   ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\")\n        \n        for i in result:\n            id_list.append(i[0])\n        return id_list\n\n\n    def query_websites(self, name=None, url=None, website_id=None):\n        if name:\n            result = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE name = '{name}'\"\"\").fetchone()\n        elif url:\n            result = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE url = '{url}'\"\"\").fetchone()\n        elif website_id:\n            result = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE id = '{website_id}'\"\"\").fetchone()\n        else:\n            return 'query_websites() requires name or url'\n        if result:\n            return table.website(id          = result[0], # id\n                           name        = result[1], # name\n                           url         = result[2], # url\n                           last_date   = result[3], # last_date\n                           last_user   = result[4], # last_user\n                           create_date = result[5], # create_date\n                           create_user = result[6]) # create_user\n        else:\n            return None\n\n\n    def query_categories(self, name=None, url=None, category_id=None):\n        if category_id:\n            result = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE id = '{category_id}'\"\"\").fetchone()\n        elif name:\n            result = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE name = '{name}'\"\"\").fetchone()\n        elif url:\n            result = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE url = '{url}'\"\"\").fetchone()\n        if result:\n            return table.category(id          = result[0], # id\n                            name        = result[1], # name\n                            context     = result[2], # context\n                            url         = result[3], # url\n                            website_id  = result[4], # website_id\n                            last_date   = result[5], # last_date\n                            last_user   = result[6], # last_user\n                            create_date = result[7], # create_date\n                            create_user = result[8]) # create_user)\n            #return result\n        else:\n            return None\n\n    \n    def query_blogs(self, name=None, url=None, blog_id=None) -> object:   # blog object\n        if blog_id:\n            result = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE id = '{blog_id}'\"\"\").fetchone()\n        elif name:\n            result = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE name = '{name}'\"\"\").fetchone()\n        elif name:\n            result = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE url = '{url}'\"\"\").fetchone()\n        if result:\n            return table.blog(id          = result[0], # id\n                        author      = result[1], # name\n                        name        = result[2], # context\n                        url         = result[3], # url\n                        category_id = result[4], # website_id\n                        last_date   = result[5], # last_date\n                        last_user   = result[6], # last_user\n                        create_date = result[7], # create_date\n                        create_user = result[8]) # create_user)\n        else:\n            return None\n\n    \n    def execute(self, new_data: str) -> tuple:\n        \"\"\"Executes an valid SQL statement passed through as a string.\n\n        Arugments:\n            new_data (string): Valid SQL statement\n\n        \"\"\"\n        return self.cur.execute(new_data).fetchall()\n    \n\n    def insert_update_site_pages(self, page_number: int, site_id: int):\n        \"\"\"Inserts a website record. Designed for use with the website class.\n\n        Arguments:\n            website (website class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        # existing_url = self.insert_update_website(url=website.url)\n        # new = True if existing_url == None else False\n        today = date.today()\n        try:\n            result = self.cur.execute(f\"SELECT number FROM site_pages WHERE site_id = {site_id}\").fetchall()\n            #print(result)\n            if result[0]:\n                self.cur.execute(\n                    f\"\"\"UPDATE site_pages\n                        SET number = '{page_number}',\n                            last_date = '{today}',\n                            last_user = 'user'\n                        WHERE site_id = '{site_id}'\n                    \"\"\"\n                )\n            #print('updated or same')\n        except IndexError:\n            next_id = self.cur.execute(\"\"\"SELECT MAX(id) FROM site_pages\"\"\").fetchone()[0]\n            next_id = next_id + 1 if next_id else 1\n            self.cur.execute(\n                f\"\"\"INSERT INTO site_pages\n                                VALUES (\n                                        NULL,\n                                        \"{page_number}\",\n                                        \"{site_id}\",\n                                        \"{today}\",\n                                        \"user\",\n                                        \"{today}\",\n                                        \"user\"\n                            )\"\"\"\n            ).fetchall()\n            #print('new')\n        # else:\n        #     self.cur.execute(\n        #         f\"\"\"UPDATE websites\n        #             SET number = '{page_number}',\n        #                 last_date = '{today}',\n        #                 last_user = 'user'\n        #             WHERE website_id = '{website_id}'\n        #         \"\"\"\n        #     ).fetchall()\n        #     return self.query_websites(url=website.url)\n        # else:\n        #     return f'Not inserted. If exists, then {existing_url.url} is already present, named {existing_url.name}.'\n\n\n    def insert_website(self, website: object) -> object:\n        \"\"\"Inserts a website record. Designed for use with the website class.\n\n        Arguments:\n            website (website class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        existing_url = self.query_websites(url=website.url)\n        new = True if existing_url == None else False\n        if new == True:\n            next_id = self.cur.execute(\"\"\"SELECT MAX(id) FROM websites\"\"\").fetchone()[0]\n            if next_id:\n                next_id = next_id + 1 if next_id else 1\n            today = date.today()\n            self.cur.execute(\n                f\"\"\"INSERT INTO websites\n                                VALUES (\n                                        NULL,\n                                        \"{website.name}\",\n                                        \"{website.url}\",\n                                        \"{today}\",\n                                        \"user\",\n                                        \"{today}\",\n                                        \"user\"\n                            )\"\"\"\n            ).fetchall()\n            return self.query_websites(url=website.url)\n        else:\n            return f'Not inserted. If exists, then {existing_url.url} is already present, named {existing_url.name}.'\n\n    \n    def insert_category(self, category):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        category.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM categories\"\"\").fetchone()[0]\n        category.id = category.id + 1 if category.id else 1\n        return self.cur.execute(\n            f\"\"\"INSERT INTO categories\n                             VALUES (\n                                        \"{category.id}\",\n                                        \"{category.name}\",\n                                        \"{category.context}\",\n                                        \"{category.url}\",\n                                        \"{category.website_id}\",\n                                        \"{category.last_date}\",\n                                        \"{category.last_user}\",\n                                        \"{category.create_date}\",\n                                        \"{category.create_user}\"\n                        )\"\"\"\n        )\n    \n    \n    def insert_blog(self, blog):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        blog.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM blogs\"\"\").fetchone()[0]\n        blog.id = blog.id + 1 if blog.id else 1\n        blog.name = blog.name.replace('\"', '')\n        return self.cur.execute(\n            f\"\"\"INSERT INTO blogs VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n                                       (blog.id,\n                                        blog.author,\n                                        blog.name,\n                                        blog.url,\n                                        blog.category_id,\n                                        blog.last_date,\n                                        blog.last_user,\n                                        blog.create_date,\n                                        blog.create_user)\n        )\n\n\n    def update_date_blog(self, blog):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        last_date = datetime.now()\n        return self.cur.execute(\n            f\"\"\"UPDATE blogs\n                SET last_date = '{last_date}'\n                WHERE id = {blog.id}\"\"\"\n        )\n\n    \n    def update_date_category(self, category):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        last_date = datetime.now()\n        return self.cur.execute(\n            f\"\"\"UPDATE categories\n                SET last_date = '{last_date}'\n                WHERE id = {category.id}\"\"\"\n        )\n\n        \n    def insert_post(self, post):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        post.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM posts\"\"\").fetchone()[0]\n        post.id = post.id + 1 if post.id else 1\n        post.content = post.content.replace('\"', '\\\"')\n        post.content = post.content.replace(\"'\", \"\\'\")\n        post.tags = ','.join(post.tags)\n        self.cur.execute(\n             \"\"\"INSERT INTO posts\n                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n                                       (post.id,\n                                        post.title,\n                                        post.author,\n                                        post.date,\n                                        post.tags,\n                                        post.content,\n                                        post.content_html,\n                                        post.url,\n                                        post.blog_id,\n                                        post.last_date,\n                                        post.last_user,\n                                        post.create_date,\n                                        post.create_user)\n        )\n\n\n    def insert_error_url(self, url_error: object):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        url_error.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM url_error_list\"\"\").fetchone()[0]\n        url_error.id = url_error.id + 1 if url_error.id else 1\n        self.cur.execute(\n            \"\"\"INSERT INTO url_error_list\n                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n                                        (url_error.id,\n                                        url_error.url,\n                                        url_error.url_type,\n                                        url_error.parent_id,\n                                        url_error.resolved,\n                                        url_error.last_date,\n                                        url_error.last_user,\n                                        url_error.create_date,\n                                        url_error.create_user)\n        )\n\n    \n    def create_tables(self):\n        \"\"\"This function confirms the existence of or creates the path, database, and tables.\n        \n        Can be used by calling the function directly, but is designed to by used by install.py, which is called by the install.bat file.\n        \n        \"\"\"\n        if (\n            Path.home() / \"py_apps\" / \"_appdata\" / \"webscrape_patheos\" / \"patheos.db\"\n        ):\n            pass\n        else:\n            Path(Path.home() / \"py_apps\" / \"_appdata\" / \"webscrape_patheos\" / \"patheos.db\").mkdir(\n                parents=True, exist_ok=True\n            )\n\n        \"\"\"create a database table if it does not exist already\"\"\"\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS \n                            site_pages (\n                                id           INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n                                number       INTEGER NOT NULL,\n                                site_id      INTEGER NOT NULL UNIQUE,\n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                        )\"\"\"\n        )\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS \n                            url_error_list (\n                                id           INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n                                url          TEXT NOT NULL,\n                                url_type     TEXT NOT NULL,\n                                parent_id    INTEGER NOT NULL,\n                                resolved     TEXT NOT NULL,\n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                        )\"\"\"\n        )\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS \n                            websites (\n                                id           INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n                                name         VARCHAR(100) NOT NULL, \n                                url          VARCHAR(2000) NOT NULL,\n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                        )\"\"\"\n        )\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS \n                            categories (\n                                id           INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n                                name         VARCHAR(100) NOT NULL, \n                                context      VARCHAR(100), \n                                url          VARCHAR(2000) NOT NULL,\n                                website_id   INTEGER NOT NULL,\n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                        )\"\"\"\n        )\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS\n                            blogs (\n                                id           INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n                                auther       VARCHAR(255),\n                                name         VARCHAR(255), \n                                url          TEXT NOT NULL,\n                                category_id  INTEGER NOT NULL, \n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                    )\"\"\"\n        )\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS\n                            posts (\n                                id           INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, \n                                title        VARCHAR(255) NOT NULL,\n                                author       VARCHAR(255),\n                                date         TIMESTAMP, \n                                tags         VARCHAR(255), \n                                content      TEXT, \n                                content_html TEXT,\n                                url          TEXT NOT NULL,\n                                blog_id      INTEGER NOT NULL,\n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                    )\"\"\"\n        )\n\n\n    def check_url_new(self, table: str, url: str) -> bool:\n        \"\"\"Check a URL against a database table to see if it already exists.\n        \n        Arguments:\n            url (str): webpage page address to check against database table\n            table (str): database table to check against\n\n        Return\n            bool: True if url exists in given table; False if it doesn't exist.\n        \n        \"\"\"\n        existing_url = self.cur.execute(f'SELECT url FROM {table} WHERE url = \\'{url}\\'')\n        if len(existing_url) == 0:\n            return True\n        else: \n            return False\n\n    def commit(self):\n        \"\"\"Use after any other database class function to commit changes.\n        This function is separated from initial transactions to enable the __exit__ function to rollback changes in the case that errors are encountered.\n        \"\"\"\n        self.__db_connection.commit()\n", "364": "# Standard\nimport sqlite3\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nfrom datetime import date\n\n# PyPI\nimport psycopg2\n\n# LOCAL\nfrom interface_db.reference import table_classes as table\n\n# Variables to make pylint happy\ntest_database=None\n# if os.environ['WEBSCRAPE_DB']:\n#     prod_database=Path(os.environ['WEBSCRAPE_DB'])\n\n\n# class url_error:\n#     \"\"\"Intended use is with an insert function into the url_error_list table.\"\"\"\n#     today = date.today()\n#     last_date = datetime.now()\n#     def __init__(self, url, url_type, parent_id, resolved=False,\n#                  id=None, \n#                  last_date=last_date, last_user='default', \n#                  create_date=today, create_user='default'):\n#         self.id          = id if id != None else None\n#         self.url         = url\n#         self.url_type    = url_type\n#         self.parent_id   = parent_id\n#         self.resolved    = resolved\n#         self.last_date   = last_date\n#         self.last_user   = last_user\n#         self.create_date = create_date\n#         self.create_user = create_user\n\n\n# class website:\n#     \"\"\"Intended use is with an insert function into the websites table.\"\"\"\n#     today = date.today()\n#     last_date = datetime.now()\n#     def __init__(self, name, url, \n#                  id=None, \n#                  last_date=last_date, last_user='default', \n#                  create_date=today, create_user='default'):\n#         self.id          = id if id != None else None\n#         self.name        = name\n#         self.url         = url\n#         self.last_date   = last_date\n#         self.last_user   = last_user\n#         self.create_date = create_date\n#         self.create_user = create_user\n\n\n# class category:\n#     \"\"\"Intended use is with an insert function into the categories table.\"\"\"\n#     today = date.today()\n#     last_date = datetime.now()\n#     def __init__(self, name=None, url=None, website_id=None,\n#                  id=None, context=None,\n#                  last_date=last_date, last_user='default',\n#                  create_date=today, create_user='default'):\n#         self.id          = id if id != None else None\n#         self.name        = name\n#         self.context     = context\n#         self.url         = url\n#         self.website_id  = website_id\n#         self.last_date   = last_date\n#         self.last_user   = last_user\n#         self.create_date = create_date\n#         self.create_user = create_user\n    \n    \n# class blog:\n#     \"\"\"Intended use is with an insert function into the blogs table.\"\"\"\n#     today = date.today()\n#     last_date = datetime.now()\n#     def __init__(self, author=None, name=None, url=None, \n#                  id=None, category_id=None, \n#                  last_date=last_date, last_user='default', \n#                  create_date=today, create_user='default'):\n#         self.id          = id\n#         self.author      = author\n#         self.name        = name\n#         self.url         = url\n#         self.category_id = category_id\n#         self.last_date   = last_date\n#         self.last_user   = last_user\n#         self.create_date = create_date\n#         self.create_user = create_user\n\n\n# class post:\n#     \"\"\"Intended use is with an insert function into the pposts table.\"\"\"\n#     today = date.today()\n#     last_date = datetime.now()\n#     def __init__(self, url, blog_id, title=None, author=None, date=None, tags=None, content=None, content_html=None,\n#                  id=None, \n#                  last_date=last_date, last_user='default',\n#                  create_date=today, create_user='default'):\n#         self.id          = id\n#         self.title       = title\n#         self.author      = author\n#         self.date        = date\n#         self.tags        = tags\n#         self.content     = content\n#         self.content_html= content_html\n#         self.url         = url\n#         self.blog_id     = blog_id\n#         self.last_date   = last_date\n#         self.last_user   = last_user\n#         self.create_date = create_date\n#         self.create_user = create_user\n\n\nclass db_connect_postgres:\n    def __init__(self, user: str, password: str, host: str, port: int, environment=None):\n        self.user        = user\n        self.password    = password\n        self.host        = host\n        self.port        = port\n        self.environment = environment\n\n\nclass database(object):\n    \"\"\"Handles all database connectsion, inputs, and outputs\n    \n    Class constructor initiates sqlite3 database connection. If used in WITH statement\n    the connection will cleanly close after the statement is finished. If there are\n    uncommitted transactions they will be rolled back prior to connection closure.\n    \n    \"\"\"\n    def __init__(self, test_database=None):\n        if test_database:\n            self.__db_connection = test_database\n            self.cur = self.__db_connection.cursor()\n        # elif os.path.exists(Path(os.environ['WEBSCRAPE_DB']) / 'webscraper.db'): #os.environ['WEBSCRAPE_DB']:\n        #     print(os.path.exists(Path(os.environ['WEBSCRAPE_DB']) / 'webscraper.db'))\n        #     print(os.path.exists(prod_database / 'webscraper.db'))\n        #     __DB_LOCATION = prod_database / 'webscraper.db'\n        #     print(__DB_LOCATION)\n        else:\n            self.__db_connection = psycopg2.connect(\n                user='postgres',\n                password='postgrest',\n                host='192.168.86.108',\n                port='32834',\n                #database='postgres_db'\n            )\n            self.cur = self.__db_connection.cursor()\n    def __del__(self):\n        self.__db_connection.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, ext_type, exc_value, traceback):\n        self.cur.close()\n        if isinstance(exc_value, Exception):\n            self.__db_connection.rollback()\n        else:\n            self.__db_connection.commit()\n        self.__db_connection.close()\n    \n\n\n    def query_table_ids_all(self, table: str, where_column=None, table_id=None, last_date_ascending=False) -> list:\n        id_list = []\n        if table_id == None:\n            result = self.cur.execute(f\"\"\"SELECT id FROM {table} \n                                          ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\").fetchall()\n        else:\n            result = self.cur.execute(f\"\"\"SELECT id FROM {table} \n                                          WHERE {where_column} = {table_id} \n                                          ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\").fetchall()\n        # print(f\"\"\"SELECT id FROM {table} \n        #                                   WHERE {where_column} = {table_id} \n        #                                   ORDER BY last_date {'ASC' if last_date_ascending==False else 'DESC'}\"\"\")\n        \n        for i in result:\n            id_list.append(i[0])\n        return id_list\n\n\n    def query_websites(self, name=None, url=None, website_id=None):\n        if name:\n            result = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE name = '{name}'\"\"\").fetchone()\n        elif url:\n            result = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE url = '{url}'\"\"\")\n            print(result)\n        elif website_id:\n            result = self.cur.execute(f\"\"\"SELECT * FROM websites WHERE id = '{website_id}'\"\"\").fetchone()\n        else:\n            return 'query_websites() requires name or url'\n        if result:\n            return table.website(id          = result[0], # id\n                           name        = result[1], # name\n                           url         = result[2], # url\n                           last_date   = result[3], # last_date\n                           last_user   = result[4], # last_user\n                           create_date = result[5], # create_date\n                           create_user = result[6]) # create_user\n        else:\n            return None\n\n\n    def query_categories(self, name=None, url=None, category_id=None):\n        if category_id:\n            result = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE id = '{category_id}'\"\"\").fetchone()\n        elif name:\n            result = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE name = '{name}'\"\"\").fetchone()\n        elif url:\n            result = self.cur.execute(f\"\"\"SELECT * FROM categories WHERE url = '{url}'\"\"\").fetchone()\n        if result:\n            return table.category(id          = result[0], # id\n                            name        = result[1], # name\n                            context     = result[2], # context\n                            url         = result[3], # url\n                            website_id  = result[4], # website_id\n                            last_date   = result[5], # last_date\n                            last_user   = result[6], # last_user\n                            create_date = result[7], # create_date\n                            create_user = result[8]) # create_user)\n            #return result\n        else:\n            return None\n\n    \n    def query_blogs(self, name=None, url=None, blog_id=None) -> object:   # blog object\n        if blog_id:\n            result = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE id = '{blog_id}'\"\"\").fetchone()\n        elif name:\n            result = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE name = '{name}'\"\"\").fetchone()\n        elif name:\n            result = self.cur.execute(f\"\"\"SELECT * FROM blogs WHERE url = '{url}'\"\"\").fetchone()\n        if result:\n            return table.blog(id          = result[0], # id\n                        author      = result[1], # name\n                        name        = result[2], # context\n                        url         = result[3], # url\n                        category_id = result[4], # website_id\n                        last_date   = result[5], # last_date\n                        last_user   = result[6], # last_user\n                        create_date = result[7], # create_date\n                        create_user = result[8]) # create_user)\n        else:\n            return None\n\n    \n    def execute(self, new_data: str) -> tuple:\n        \"\"\"Executes an valid SQL statement passed through as a string.\n\n        Arugments:\n            new_data (string): Valid SQL statement\n\n        \"\"\"\n        return self.cur.execute(new_data).fetchall()\n    \n\n    def insert_update_site_pages(self, page_number: int, site_id: int):\n        \"\"\"Inserts a website record. Designed for use with the website class.\n\n        Arguments:\n            website (website class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        # existing_url = self.insert_update_website(url=website.url)\n        # new = True if existing_url == None else False\n        today = date.today()\n        try:\n            result = self.cur.execute(f\"SELECT number FROM site_pages WHERE site_id = {site_id}\").fetchall()\n            #print(result)\n            if result[0]:\n                self.cur.execute(\n                    f\"\"\"UPDATE site_pages\n                        SET number = '{page_number}',\n                            last_date = '{today}',\n                            last_user = 'user'\n                        WHERE site_id = '{site_id}'\n                    \"\"\"\n                )\n            #print('updated or same')\n        except IndexError:\n            next_id = self.cur.execute(\"\"\"SELECT MAX(id) FROM site_pages\"\"\").fetchone()[0]\n            next_id = next_id + 1 if next_id else 1\n            self.cur.execute(\n                f\"\"\"INSERT INTO site_pages\n                                VALUES (\n                                        NULL,\n                                        \"{page_number}\",\n                                        \"{site_id}\",\n                                        \"{today}\",\n                                        \"user\",\n                                        \"{today}\",\n                                        \"user\"\n                            )\"\"\"\n            ).fetchall()\n            #print('new')\n        # else:\n        #     self.cur.execute(\n        #         f\"\"\"UPDATE websites\n        #             SET number = '{page_number}',\n        #                 last_date = '{today}',\n        #                 last_user = 'user'\n        #             WHERE website_id = '{website_id}'\n        #         \"\"\"\n        #     ).fetchall()\n        #     return self.query_websites(url=website.url)\n        # else:\n        #     return f'Not inserted. If exists, then {existing_url.url} is already present, named {existing_url.name}.'\n\n\n    def insert_website(self, website: object) -> object:\n        \"\"\"Inserts a website record. Designed for use with the website class.\n\n        Arguments:\n            website (website class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        existing_url = self.query_websites(url=website.url)\n        new = True if existing_url == None else False\n        if new == True:\n            next_id = self.cur.execute(\"\"\"SELECT MAX(id) FROM websites\"\"\").fetchone()[0]\n            if next_id:\n                next_id = next_id + 1 if next_id else 1\n            today = date.today()\n            self.cur.execute(\n                f\"\"\"INSERT INTO websites\n                                VALUES (\n                                        NULL,\n                                        \"{website.name}\",\n                                        \"{website.url}\",\n                                        \"{today}\",\n                                        \"user\",\n                                        \"{today}\",\n                                        \"user\"\n                            )\"\"\"\n            ).fetchall()\n            return self.query_websites(url=website.url)\n        else:\n            return f'Not inserted. If exists, then {existing_url.url} is already present, named {existing_url.name}.'\n\n    \n    def insert_category(self, category):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        category.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM categories\"\"\").fetchone()[0]\n        category.id = category.id + 1 if category.id else 1\n        return self.cur.execute(\n            f\"\"\"INSERT INTO categories\n                             VALUES (\n                                        \"{category.id}\",\n                                        \"{category.name}\",\n                                        \"{category.context}\",\n                                        \"{category.url}\",\n                                        \"{category.website_id}\",\n                                        \"{category.last_date}\",\n                                        \"{category.last_user}\",\n                                        \"{category.create_date}\",\n                                        \"{category.create_user}\"\n                        )\"\"\"\n        )\n    \n    \n    def insert_blog(self, blog):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        blog.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM blogs\"\"\").fetchone()[0]\n        blog.id = blog.id + 1 if blog.id else 1\n        blog.name = blog.name.replace('\"', '')\n        return self.cur.execute(\n            f\"\"\"INSERT INTO blogs VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n                                       (blog.id,\n                                        blog.author,\n                                        blog.name,\n                                        blog.url,\n                                        blog.category_id,\n                                        blog.last_date,\n                                        blog.last_user,\n                                        blog.create_date,\n                                        blog.create_user)\n        )\n\n\n    def update_date_blog(self, blog):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        last_date = datetime.now()\n        return self.cur.execute(\n            f\"\"\"UPDATE blogs\n                SET last_date = '{last_date}'\n                WHERE id = {blog.id}\"\"\"\n        )\n\n    \n    def update_date_category(self, category):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        last_date = datetime.now()\n        return self.cur.execute(\n            f\"\"\"UPDATE categories\n                SET last_date = '{last_date}'\n                WHERE id = {category.id}\"\"\"\n        )\n\n        \n    def insert_post(self, post):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        post.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM posts\"\"\").fetchone()[0]\n        post.id = post.id + 1 if post.id else 1\n        post.content = post.content.replace('\"', '\\\"')\n        post.content = post.content.replace(\"'\", \"\\'\")\n        post.tags = ','.join(post.tags)\n        self.cur.execute(\n             \"\"\"INSERT INTO posts\n                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n                                       (post.id,\n                                        post.title,\n                                        post.author,\n                                        post.date,\n                                        post.tags,\n                                        post.content,\n                                        post.content_html,\n                                        post.url,\n                                        post.blog_id,\n                                        post.last_date,\n                                        post.last_user,\n                                        post.create_date,\n                                        post.create_user)\n        )\n\n\n    def insert_error_url(self, url_error: object):\n        \"\"\"Inserts a category record. Designed for use with the category class.\n\n        Arguments:\n            category (category class): class or dictionary containing the following values:\n                -\n\n        \"\"\"\n        url_error.id = self.cur.execute(\"\"\"SELECT MAX(id) FROM url_error_list\"\"\").fetchone()[0]\n        url_error.id = url_error.id + 1 if url_error.id else 1\n        self.cur.execute(\n            \"\"\"INSERT INTO url_error_list\n                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n                                        (url_error.id,\n                                        url_error.url,\n                                        url_error.url_type,\n                                        url_error.parent_id,\n                                        url_error.resolved,\n                                        url_error.last_date,\n                                        url_error.last_user,\n                                        url_error.create_date,\n                                        url_error.create_user)\n        )\n\n    \n    def create_tables(self):\n        \"\"\"This function confirms the existence of or creates the path, database, and tables.\n        \n        Can be used by calling the function directly, but is designed to by used by install.py, which is called by the install.bat file.\n        \n        \"\"\"\n        if (\n            Path.home() / \"py_apps\" / \"_appdata\" / \"webscrape_patheos\" / \"patheos.db\"\n        ):\n            pass\n        else:\n            Path(Path.home() / \"py_apps\" / \"_appdata\" / \"webscrape_patheos\" / \"patheos.db\").mkdir(\n                parents=True, exist_ok=True\n            )\n\n        \"\"\"create a database table if it does not exist already\"\"\"\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS \n                            site_pages (\n                                id           BIGSERIAL PRIMARY KEY, \n                                number       INTEGER NOT NULL,\n                                site_id      INTEGER NOT NULL UNIQUE,\n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                        )\"\"\"\n        )\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS \n                            url_error_list (\n                                id           BIGSERIAL PRIMARY KEY,\n                                url          TEXT NOT NULL,\n                                url_type     TEXT NOT NULL,\n                                parent_id    INTEGER NOT NULL,\n                                resolved     TEXT NOT NULL,\n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                        )\"\"\"\n        )\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS \n                            websites (\n                                id           BIGSERIAL PRIMARY KEY, \n                                name         VARCHAR(100) NOT NULL, \n                                url          VARCHAR(2000) NOT NULL,\n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                        )\"\"\"\n        )\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS \n                            categories (\n                                id           BIGSERIAL PRIMARY KEY,\n                                name         VARCHAR(100) NOT NULL, \n                                context      VARCHAR(100), \n                                url          VARCHAR(2000) NOT NULL,\n                                website_id   INTEGER NOT NULL,\n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                        )\"\"\"\n        )\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS\n                            blogs (\n                                id           BIGSERIAL PRIMARY KEY,\n                                auther       VARCHAR(255),\n                                name         VARCHAR(255), \n                                url          TEXT NOT NULL,\n                                category_id  INTEGER NOT NULL, \n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                    )\"\"\"\n        )\n        self.cur.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS\n                            posts (\n                                id           BIGSERIAL PRIMARY KEY,\n                                title        VARCHAR(255) NOT NULL,\n                                author       VARCHAR(255),\n                                date         TIMESTAMP, \n                                tags         VARCHAR(255), \n                                content      TEXT, \n                                content_html TEXT,\n                                url          TEXT NOT NULL,\n                                blog_id      INTEGER NOT NULL,\n                                last_date    TIMESTAMP,\n                                last_user    VARCHAR(100),\n                                create_date  TIMESTAMP,\n                                create_user  VARCHAR(100)\n                    )\"\"\"\n        )\n\n\n    def check_url_new(self, table: str, url: str) -> bool:\n        \"\"\"Check a URL against a database table to see if it already exists.\n        \n        Arguments:\n            url (str): webpage page address to check against database table\n            table (str): database table to check against\n\n        Return\n            bool: True if url exists in given table; False if it doesn't exist.\n        \n        \"\"\"\n        existing_url = self.cur.execute(f'SELECT url FROM {table} WHERE url = \\'{url}\\'')\n        if len(existing_url) == 0:\n            return True\n        else: \n            return False\n\n    def commit(self):\n        \"\"\"Use after any other database class function to commit changes.\n        This function is separated from initial transactions to enable the __exit__ function to rollback changes in the case that errors are encountered.\n        \"\"\"\n        self.__db_connection.commit()\n", "365": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# ==============================================================================\n#\n#       webscrape.py - Search and scrape internet for song lyrics\n#\n# ==============================================================================\n\n\"\"\"\nStart with google search:   https://stackoverflow.com/a/43531098/6929343\nFiltering:                  Skip youtube\nStore results in:           /run/user/1000/mserve.scrape_list.txt\n\nNext get lyrics:            https://stackoverflow.com/a/63436536/6929343\nFiltering:                  Include genius.com\nStore results in:           /run/user/1000/mserve.scrape_lyrics.txt\n\n    TODO:\n\n        Provide tkinter windows to display input, output and control selections\n\n        A valid song should be less than 100 lines and each line should be less\n        than 95 characters. Otherwise the result is just random.\n\n\n\"\"\"\n\nfrom __future__ import print_function  # Must be first import\nfrom __future__ import with_statement  # Error handling for file opens\n\n# from __future__ import unicode_literals     # Unicode errors fix\n# Above causes error on save icon utf-8\n# _tkinter.TclError: character U+1f4be is above the range (U+0000-U+FFFF) allowed by Tcl\n\ntry:\n    import tkinter as tk\n    import tkinter.ttk as ttk\n    import tkinter.font as font\n    import tkinter.filedialog as filedialog\n    import tkinter.messagebox as messagebox\n    import tkinter.scrolledtext as scrolledtext\n\n    PYTHON_VER = \"3\"\nexcept ImportError:  # Python 2\n    import Tkinter as tk\n    import ttk\n    import tkFont as font\n    import tkFileDialog as filedialog\n    import tkMessageBox as messagebox\n    import ScrolledText as scrolledtext\n\n    PYTHON_VER = \"2\"\n# print (\"Python version: \", PYTHON_VER)\n\nimport sys\nimport os\nimport os.path\nimport json\nimport time\n\nreload(sys)\nsys.setdefaultencoding('utf8')\n\nimport requests\nfrom six.moves import urllib            # Python 2/3 compatibility library\nfrom bs4 import BeautifulSoup\nimport re\nfrom collections import namedtuple\n\nimport global_variables as g\nimport external as ext  # Time formatting routines\nimport image as img\nimport monitor\nimport sql\nimport toolkit\nimport message\n\n# Web scraping song lyrics IPC file names\nSCRAPE_CTL_FNAME = '/run/user/1000/mserve.scrape_ctl.json'\nSCRAPE_LIST_FNAME = '/run/user/1000/mserve.scrape_list.txt'\nSCRAPE_LYRICS_FNAME = '/run/user/1000/mserve.scrape_lyrics.txt'\n\n# Names list is used in our code for human readable formatting\nNAMES_LIST = ['Metro Lyrics', 'AZ Lyrics', 'Lyrics',\n              'Lyrics Mode', 'Lets Sing It', 'Genius',\n              'Musix Match', 'Lyrics Planet']\n\n# Website list is used in webscrape.py for internet formatting\nWEBSITE_LIST = ['www.metrolyrics.com', 'www.azlyrics.com', 'www.lyrics.com',\n                'www.lyricsmode.com', 'www.letssingit.com', '//genius.com',\n                'www.musixmatch.com', 'www.lyricsplanet.com']\n\n# Empty control list (template)\nCTL_LIST = [{} for _ in range(len(WEBSITE_LIST))]\n# CTL_LIST = [ {}, {}, {}, {}, {}, {}, {}, {} ]\n\n# If we try to print normally an error occurs when launched in background\n# print(\"CTL_LIST:\", CTL_LIST, file=sys.stderr)\n\n# Empty control list dictionary element (template)\nWS_DICT = {\"rank\": 0, \"name\": \"\", \"website\": \"\",\n           \"link\": \"\", \"score\": 0, \"flag\": \"\", \"lyrics\": \"\"}\n''' flag values: preference passed to webscrape.py. result passed to mserve\n    preference:  1-8 try to get lyrics in this order, 'skip' = skip site\n    result:      'found' lyrics returned. 'available' lyrics can be returned\n                 'not found' no link or link is empty (eg artist but no lyrics)\n'''\n\nDEFAULT_LIST = [\n    {\"rank\": 1, \"name\": \"Megalobiz\", \"website\": \"www.megalobiz.com\",\n     \"link\": \"\", \"score\": 0, \"flag\": \"\", \"lyrics\": \"\"},\n    {\"rank\": 2, \"name\": \"Metro Lyrics\", \"website\": \"www.metrolyrics.com\",\n     \"link\": \"\", \"score\": 0, \"flag\": \"\", \"lyrics\": \"\"},\n    {\"rank\": 3, \"name\": \"AZ Lyrics\", \"website\": \"www.azlyrics.com\",\n     \"link\": \"\", \"score\": 0, \"flag\": \"\", \"lyrics\": \"\"},\n    {\"rank\": 4, \"name\": \"Lyrics\", \"website\": \"www.lyrics.com\",\n     \"link\": \"\", \"score\": 0, \"flag\": \"\", \"lyrics\": \"\"},\n    {\"rank\": 5, \"name\": \"Lyrics Mode\", \"website\": \"www.lyricsmode.com\",\n     \"link\": \"\", \"score\": 0, \"flag\": \"\", \"lyrics\": \"\"},\n    {\"rank\": 6, \"name\": \"Lets Sing It\", \"website\": \"www.letssingit.com\",\n     \"link\": \"\", \"score\": 0, \"flag\": \"\", \"lyrics\": \"\"},\n    {\"rank\": 7, \"name\": \"Genius\", \"website\": \"'//genius.com\",\n     \"link\": \"\", \"score\": 0, \"flag\": \"\", \"lyrics\": \"\"},\n    {\"rank\": 8, \"name\": \"Musix Match\", \"website\": \"www.musixmatch.com\",\n     \"link\": \"\", \"score\": 0, \"flag\": \"\", \"lyrics\": \"\"},\n    {\"rank\": 9, \"name\": \"Lyrics Planet\", \"website\": \"www.lyricsplanet.com\",\n     \"link\": \"\", \"score\": 0, \"flag\": \"\", \"lyrics\": \"\"}\n]\n\nBLACK_LIST = ['youtube', 'wikipedia', 'facebook', 'pinterest']\n\nBLACK_LIST_COUNT = 0\nWHITE_LIST_COUNT = 0\nlist_output = []\nlyrics_output = []              # Was not defined global in scrape()\n\n# Global search string\nSEARCH = \"\"\nMUSIC_ID = \"\"\n\n\n\nclass Results:\n    def __init__(self):\n        self.blacklist_count = 0\n        self.whitelist_count = 0\n        self.list = []\n        self.result = namedtuple('Result', 'order, type, link')\n\n    def add_blacklist(self):\n        self.blacklist_count += 1\n\n    def add_whitelist(self):\n        self.blacklist_count += 1\n\n\nresults = Results()             # global class\n\n\ndef save_ctl():\n    \"\"\"\n        CANCEL: webscrape has full access to sql to see what has been\n                downloaded in the past.\n\n        Save Control file containing list of dictionaries\n\n        USED by mserve and webscrape.py\n            mserve passes previous list of names with flags to scrape.\n            webscrape.py passes back name of website that was scraped.\n            webscrape.py passes names of websites that CAN BE scraped.\n    \"\"\"\n    with open(SCRAPE_CTL_FNAME, \"w\") as ctl:\n        ctl.write(json.dumps(CTL_LIST))\n\n\ndef load_ctl():\n    \"\"\"\n        Return contents of CTL file or empty list of dictionaries\n    \"\"\"\n    data = CTL_LIST\n    if os.path.isfile(SCRAPE_CTL_FNAME):\n        with open(SCRAPE_CTL_FNAME, \"r\") as ctl:\n            data = json.loads(ctl.read())\n\n    return data\n\n\ndef check_files(select='all'):\n    \"\"\" NOT USED \"\"\"\n    if select == 'all' or select == 'list':\n        if not os.path.isfile(SCRAPE_LIST_FNAME):\n            return False\n\n    if select == 'all' or select == 'lyrics':\n        if not os.path.isfile(SCRAPE_LYRICS_FNAME):\n            return False\n\n    return True\n\n\ndef delete_files(select='all'):\n    \"\"\" USED by mserve and webscrape.py \"\"\"\n    if select == 'all' or select == 'list':\n        try:\n            os.remove(SCRAPE_LIST_FNAME)\n        except OSError:\n            pass\n\n    if select == 'all' or select == 'lyrics':\n        try:\n            os.remove(SCRAPE_LYRICS_FNAME)\n        except OSError:\n            pass\n\n\n# TODO: Create dictionary with website and last time scraped. This way we can\n#       cycle through names and not bombard a single site with quick requests.\n#       At two seconds per site it will be ~15 seconds between requests which\n#       should satisfy most robot detectors. Dictionary is saved as pickle in\n#       ~/.config/mserve/webscrape.pkl\n\n\nMEGALOBIZ = METROLYRICS = AZLYRICS = LYRICS = LYRICSMODE = None\nLETSSINGIT = GENIUS = MUSIXMATCH = LYRICSPLANET = None\n\n\ndef google_search(search):\n    \"\"\" Get google search results for \"song lyrics\" + Artist + Song\n        TODO: Accept parameter for which of 8 websites to focus on getting:\n            1. Metrolyrics\n            2. AZ Lyrics\n            3. Lyrics.com\n            4. LyricsMode\n            5. Letsingit\n            6. Genius\n            7. Musixmatch\n            8. LyricsPlanet\n    \"\"\"\n    global WS_DICT, CTL_LIST\n    global MEGALOBIZ, AZLYRICS, LYRICS, LYRICSMODE, LETSSINGIT\n    global GENIUS, MUSIXMATCH, LYRICSPLANET, list_output\n\n    # If we try to print normally an error occurs when launched in background\n    # print(\"CTL_LIST start search:\", CTL_LIST, file=sys.stderr)\n\n    # print('requests header:', requests.utils.default_headers())\n    # Avoid google robot detection\n    # noinspection SpellCheckingInspection\n    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1)' + \\\n                 'AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14'\n    headers = {'User-Agent': user_agent,\n               'Accept': 'text/html,application/xhtml+xml,application/' +\n                         'xml;q=0.9,image/webp,*/*;q=0.8'}\n    # inspection SpellCheckingInspection\n\n    results = 100  # valid options 10, 20, 30, 40, 50, and 100\n    #    page = requests.get(f\"https://www.google.com/search?q={search}&num={results}\")\n    query = \"https://www.google.com/search?q={search}&num={results}\". \\\n        format(search=search, results=results, headers=headers)\n    # print('query:', query)\n\n    # noinspection PyBroadException\n    try:\n        page = requests.get(query)\n    except:\n        list_output.append('Internet not available.')\n        # print('Internet not available.')\n        return\n\n    soup = BeautifulSoup(page.content, \"html5lib\")\n    # print('soup:', soup)\n    links = soup.findAll(\"a\")\n    for link in links:\n        link_href = link.get('href')\n        if \"url?q=\" in link_href and \"webcache\" not in link_href:\n            t = (link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0])\n            # if 'youtube' in t or 'wikipedia' in t or 'facebook' in t \\\n            # or 'pinterest' in t:\n            if any(s in t for s in BLACK_LIST):\n                # print('skipping youtube, wikipedia, facebook & pinterest')\n                # TODO: keep these but put in blacklist\n                add_blacklist(t)\n                continue\n            else:\n                list_output.append(t)\n                add_whitelist(t)\n\n            # TODO: look up in list and get ranking\n            if 'www.metalobiz.com' in t:\n                MEGALOBIZ = t\n            if 'www.metrolyrics.com' in t:\n                METROLYRICS = t\n            if 'www.azlyrics.com' in t:\n                AZLYRICS = t\n            if 'www.lyrics.com' in t:\n                LYRICS = t\n            if 'www.lyricsmode.com' in t:\n                LYRICSMODE = t\n            if 'www.letssingit.com' in t:\n                LETSSINGIT = t\n            if '//genius.com' in t:  # Trap out //dekgenius.com\n                GENIUS = t\n            if 'www.musixmatch.com' in t:\n                MUSIXMATCH = t\n            if 'www.lyricsplanet.com' in t:  # Not sure if '//' or 'www.' prefix\n                LYRICSPLANET = t\n\n            # New method February 21/2021\n            for i, website in enumerate(WEBSITE_LIST):\n                if website in t:\n                    # If we try to print normally an error occurs when launched in background\n                    # print(\"\\ni, website in t:\", i, website, NAMES_LIST[i], t, file=sys.stderr, sep=\" | \")\n                    # noinspection PyDictCreation\n                    WS_DICT = {}  # Reset dynamic link to earlier elements\n                    WS_DICT['name'] = NAMES_LIST[i]\n                    WS_DICT['website'] = website\n                    WS_DICT['link'] = t\n                    WS_DICT['flag'] = \"available\"\n                    CTL_LIST[i] = WS_DICT\n                    break\n\n    # Save files\n    with open(SCRAPE_LIST_FNAME, \"w\") as outfile:\n        for line in list_output:\n            outfile.write(line + \"\\n\")\n\n    # If we try to print normally an error occurs when launched in background\n    # print(\"\\nCTL_LIST after search:\", CTL_LIST, file=sys.stderr)\n\n    save_ctl()\n\n\n\ndef add_blacklist(t):\n    global BLACK_LIST_COUNT\n    BLACK_LIST_COUNT += 1\n\n\n\ndef add_whitelist(t):\n    global WHITE_LIST_COUNT\n    WHITE_LIST_COUNT += 1\n\n\n\n\ndef scrape(search):\n    global lyrics_output\n    if GENIUS:\n        get_from_genius()\n\n    # We didn't find anything in genius.com\n    if len(lyrics_output) == 0:\n        if MEGALOBIZ:\n            get_from_megalobiz()\n\n    # We didn't find anything in megalobiz.com\n    if len(lyrics_output) == 0:\n        if AZLYRICS:\n            get_from_azlyrics()\n\n    # We didn't find anything in azlyrics.com\n    if len(lyrics_output) == 0:\n        if METROLYRICS:\n            get_from_metrolyrics()\n\n    if len(lyrics_output) == 0:\n        # We didn't find anything in genius.com or azlyrics.com\n        lyrics_output.append('No lyrics found for: ' + search)\n        lyrics_output.append('Popular sites search results:')\n\n        if METROLYRICS:\n            lyrics_output.append(METROLYRICS)\n        if AZLYRICS:\n            lyrics_output.append(AZLYRICS)\n        if LYRICS:\n            lyrics_output.append(LYRICS)\n        if LYRICSMODE:\n            lyrics_output.append(LYRICSMODE)\n        if LETSSINGIT:\n            lyrics_output.append(LETSSINGIT)\n        if GENIUS:\n            lyrics_output.append(GENIUS)\n        if MUSIXMATCH:\n            lyrics_output.append(MUSIXMATCH)\n        if LYRICSPLANET:\n            lyrics_output.append(LYRICSPLANET)\n\n        lyrics_output.append('Or consider scraping following sites for lyrics:')\n        for line in list_output:\n            # Write possible lyrics websites as song lyrics to display\n            lyrics_output.append(line)\n\n    # Save file\n    with open(SCRAPE_LYRICS_FNAME, \"w\") as outfile:\n        for line in lyrics_output:\n            outfile.write(line + \"\\n\")\n\n\ndef get_from_genius():\n    global lyrics_output\n\n    url = GENIUS\n    # noinspection PyBroadException\n    try:\n        soup = BeautifulSoup(requests.get(url).content, 'lxml')\n        for tag in soup.select('div[class^=\"Lyrics__Container\"], \\\n                               .song_body-lyrics p'):\n            t = tag.get_text(strip=True, separator='\\n')\n            if t:\n                # print(t)\n                lyrics_output.append(t)\n    except:\n        lyrics_output.append('Error occurred retrieving genius.com lyrics')\n        lyrics_output.append(url)\n        lyrics_output.append('Search String: ' + search)\n\n\ndef get_from_azlyrics():\n    global lyrics_output\n\n    url = AZLYRICS\n\n    # noinspection PyBroadException\n    try:\n        html_page = urllib.request.urlopen(url)\n        soup = BeautifulSoup(html_page, 'html.parser')\n        html_pointer = soup.find('div', attrs={'class': 'ringtone'})\n        # song_name = html_pointer.find_next('b').contents[0].strip()\n        lyrics = html_pointer.find_next('div').text.strip()\n        lyrics_output.append(lyrics)\n    except:\n        lyrics_output.append('Error occurred retrieving azlyrics.com lyrics')\n        lyrics_output.append(url)\n        lyrics_output.append('Search String: ' + SEARCH)\n\n\ndef get_from_metrolyrics():\n    from lxml import html\n    global lyrics_output\n\n    url = METROLYRICS\n    \"\"\"Load the lyrics from MetroLyrics.\"\"\"\n    page = requests.get(url)\n\n    if page.status_code > 200:\n        # raise TswiftError(\"No lyrics available for requested song\")\n        return\n\n    # Forces utf-8 to prevent character mangling\n    page.encoding = 'utf-8'\n\n    tree = html.fromstring(page.text)\n    try:\n        lyric_div = tree.get_element_by_id('lyrics-body-text')\n        verses = [c.text_content() for c in lyric_div.find_class('verse')]\n    except KeyError:\n        # raise \"No lyrics available for requested song\"\n        return\n    else:\n        # Not sure what do do with following line just yet (Feb 21 2021)\n        # lyrics = '\\n\\n'.join(verses)\n        pass\n\n    return verses\n\n\n\"\"\"\n\n\n\n            5059 - Creed - My Sacrifice [HQ]\n                         [04:47.14]\n3 years ago\n\n\nby\n            Guest\n    \n\n\n\nKeep untagged brackets and blank lines?\n\n\nShow Lyrics Only (without LRC tag)\n\n\n\nCopied\n\nCopy\n\nEdit Time [xx:yy.zz]\nx 111\nViews x 839\nDownload\nx 239\n\n\n\nLRC TIME [04:47.14] may not match your music. Click \n                Edit Time \n            above and in the LRC Maker & \\\n            Generator page simply apply an offset (+0.8 sec, -2.4 sec, etc.)\n\n\n\n\n\n\n\n\n[length:04:47.14]\n        [re:www.megalobiz.com/lrc/maker]\n        [ve:v1.2.3]\n        [00:33.09]Hello my friend\n        [00:34.59] we meet again\n        ...\n        [04:20.53]hello again\n        [04:26.02]My sacrifice\n\n\n\n\n\n        \n    \n\n                        \n                \n\n\"\"\"\n\n\ndef get_from_megalobiz():\n    \"\"\" Song # 878 Creed - My Sacrifice\n\n        Lyrics are split up into two lines versus one in Genius.com\n        .LRC times are to 100th of a second\n    \"\"\"\n\n    from lxml import html\n    global lyrics_output\n\n    url = MEGALOBIZ\n    \"\"\"Load the lyrics from MetroLyrics.\"\"\"\n    page = requests.get(url)\n\n    if page.status_code > 200:\n        # raise TswiftError(\"No lyrics available for requested song\")\n        return\n\n    # Forces utf-8 to prevent character mangling\n    page.encoding = 'utf-8'\n\n    tree = html.fromstring(page.text)\n\n    try:\n        lyric_div = tree.get_element_by_id('lyrics_details entity_more_info')\n        verses = [c.text_content() for c in lyric_div.find_class('verse')]\n    except KeyError:\n        # raise \"No lyrics available for requested song\"\n        return\n    else:\n        # Not sure what do do with following line just yet (Feb 21 2021)\n        # lyrics = '\\n\\n'.join(verses)\n        pass\n\n    return verses\n\n\nroot = None\n\n\ndef no_parameters():\n    \"\"\" Called with no parameters \"\"\"\n    global root\n    sql.open_db()\n    root = tk.Tk()  # Create \"very top\" toplevel for all top levels\n    root.withdraw()  # Remove default window because we have treeview\n    # background From: https://stackoverflow.com/a/11342481/6929343\n    default_bg = root.cget('bg')\n    print('default_bg:', default_bg)  # d9d9d9 It's a little bright\n\n    ''' Set font style for all fonts including tkSimpleDialog.py '''\n    img.set_font_style()  # Make messagebox text larger for HDPI monitors\n    ''' Set program icon in taskbar '''\n    img.taskbar_icon(root, 64, 'black', 'green', 'red', char='W')\n\n    ''' console splash message '''\n    print(r'  ######################################################')\n    print(r' //////////////                            \\\\\\\\\\\\\\\\\\\\\\\\\\\\')\n    print(r'<<<<<<<<<<<<<<   webscrape.py - Get Lyrics  >>>>>>>>>>>>>>')\n    print(r' \\\\\\\\\\\\\\\\\\\\\\\\\\\\                            //////////////')\n    print(r'  ######################################################')\n\n    HistoryTree()  # Build treeview of sql history\n\n    root.mainloop()  # When splash screen calls us there is mainloop\n\n\n# ==============================================================================\n#\n#       HistoryTree class - Define lib (library of Message Headers)\n#\n# ==============================================================================\n\n\nclass HistoryTree:\n    \"\"\" Create self.his_tree = tk.Treeview() via CheckboxTreeview()\n\n        Resizeable, Scroll Bars, select songs, play songs.\n\n        If toplevel is not None then it is the splash screen to destroy.\n\n    \"\"\"\n\n    def __init__(self, sbar_width=12):\n\n        # Define self. variables in backups() where play_top frame is used.\n        self.bup_top = None  # Toplevel for Backups\n        self.bup_top_is_active = None  # Is backups top level open?\n        self.bup_view = None  # Treeview using data dictionary\n        self.bup_view_btn1 = None\n        self.bup_view_btn2 = None\n        self.bup_view_btn3 = None\n        self.bup_view_btn4 = None\n        self.bup_view_btn5 = None\n        self.bup_view_btn6 = None\n        self.bup_view_btn7 = None\n        self.bup_search = None  # Searching for trash, etc?\n\n        self.hdr_top = None  # Toplevel for gmail message header\n        self.hdr_top_is_active = False  # Displaying gmail message header?\n        self.scrollbox = None  # Holds pretty print dictionary\n\n        ''' HistoryTree Instances '''\n        self.view = None  # = lib_view or bup_view\n        self.region = None  # 'heading', 'separator' or 'cell'\n        self.bup_gen = None  # Backup generations\n        self.subject_list = []  # GMAIL_SUBJECT.split('!')\n        self.iid = None\n\n        ''' get parameters in SQL setup by mserve '''\n        now = time.time()\n        last_time = sql.hist_last_time('scrape', 'parm')\n        if last_time is None:\n            last_time = now\n        hist_row = sql.hist_get_row(sql.HISTORY_ID)\n        lag = now - last_time\n        if lag > 1.0:\n            print('It took more than 1 second for webscrape to start:', lag)\n        else:\n            print('webscrape start up time:', lag)\n            pass\n        print(hist_row)\n\n        ''' TODO: Relocate dtb '''\n        dtb = message.DelayedTextBox(title=\"Building history table view\",\n                                     toplevel=None, width=1000)\n\n        # If we are started by splash screen get object, else it will be None\n        # self.splash_toplevel = toplevel\n        self.splash_toplevel = tk.Toplevel()\n\n        self.splash_removed = False  # Did we remove splash screen?\n\n        # Create our tooltips pool (hover balloons)\n        self.tt = toolkit.ToolTips()\n\n        ''' Create toplevel and set program icon in taskbar '''\n        self.his_top = tk.Toplevel()\n        self.his_top.title(\"SQL History Table - webscrape\")\n        img.taskbar_icon(self.his_top, 64, 'black', 'green', 'red', char='W')\n\n        ''' Initial size of Window 75% of HD monitor size '''\n        _w = int(1920 * .75)\n        _h = int(1080 * .75)\n        _root_xy = (3800, 200)  # Temporary hard-coded coordinates\n\n        ''' Mount window at popup location '''\n        self.his_top.minsize(width=g.WIN_MIN_WIDTH, height=g.WIN_MIN_HEIGHT)\n        # self.his_top.geometry('%dx%d+%d+%d' % (_w, _h, _root_xy[0], _root_xy[1]))\n        geom = monitor.get_window_geom('history')\n        self.his_top.geometry(geom)\n\n        self.his_top.configure(background=\"Gray\")\n        self.his_top.columnconfigure(0, weight=1)\n        self.his_top.rowconfigure(0, weight=1)\n\n        ''' Create frames '''\n        master_frame = tk.Frame(self.his_top, bg=\"olive\", relief=tk.RIDGE)\n        master_frame.grid(sticky=tk.NSEW)\n        master_frame.columnconfigure(0, weight=1)\n        master_frame.rowconfigure(0, weight=1)\n\n        ''' Create treeview '''\n        history_dict = sql.history_treeview()\n        columns = [\"time\", \"type\", \"action\", \"master\", \"detail\", \"target\",\n                   \"size\", \"count\", \"seconds\", \"music_id\", \"comments\"]\n        \"\"\" FIELDS NOT SHOWN:\n            (\"column\", \"row_id\"), (\"heading\", \"Row ID\"), (\"sql_table\", \"History\"),\n            (\"column\", \"user\"), (\"heading\", \"User\"), (\"sql_table\", \"History\"),\n            (\"column\", \"delete_on\"), (\"heading\", \"Delete On\"), (\"sql_table\", \"calc\"),\n            (\"column\", \"reason\"), (\"heading\", \"Reason\"), (\"sql_table\", \"calc\"),\n\n        \"\"\"\n        toolkit.select_dict_columns(columns, history_dict)\n\n        self.his_view = toolkit.DictTreeview(\n            history_dict, self.his_top, master_frame, columns=columns,\n            sbar_width=sbar_width)\n\n        # toolkit.print_dict_columns(history_dict)\n\n        '''\n                    B I G   T I C K E T   E V E N T\n\n        Create Treeview item list with NO songs selected YET. '''\n        self.manually_checked = False  # Used for self.reverse/self.toggle\n        self.populate_his_tree(dtb)\n\n        ''' Treeview Buttons '''\n        frame3 = tk.Frame(master_frame, bg=\"Blue\", bd=2, relief=tk.GROOVE,\n                          borderwidth=g.BTN_BRD_WID)\n        frame3.grid_rowconfigure(0, weight=1)\n        frame3.grid_columnconfigure(0, weight=0)\n        frame3.grid(row=1, column=0, sticky=tk.NW)\n\n        ''' Global variables of active children '''\n        self.play_top = None  # Backup server selected headers\n        self.play_top_is_active = False  # Playing songs window open?\n\n        ''' \u00e2\u0153\u02dc Close Button \u00e2\u0153\u02dc \u00e2\u0153\u201d '''\n        self.his_top.bind(\"", "366": "import yfinance as yf\nimport pandas as pd\nfrom selenium import webdriver\nimport collections\nfrom bs4 import BeautifulSoup as soup\nimport re\nimport requests\n\n#Function\ndef stock_info_to_csv (tickerStrings, file_name):\n    df_list = list()\n    for ticker in tickerStrings:\n        data = yf.download(ticker, group_by=\"Ticker\", period='5y')\n        data['Ticker'] = ticker  #add column with ticker\n        df_list.append(data)\n        \n    #combine all dataframes into a single dataframe\n    df = pd.concat(df_list)\n    \n    #save to csv\n    df.to_csv(file_name)\n    \n#Webscrape of U.S. News-Stocks Under $10 with Selenium and Regex\n#download chromedriver\nd = webdriver.Chrome('/Users/owner/chromedriver')\nd.get('https://money.usnews.com/investing/stocks/stocks-under-10')\ns = soup(d.page_source, 'lxml')\nwhile True:\n  try:\n    d.find_element_by_link_text(\"Load More\").click() #get all data\n  except:\n    break\ncompany = collections.namedtuple('company', ['name', 'abbreviation', 'description', 'stats'])\nheaders = [['a', {'class':'search-result-link'}], \n           ['a', {'class':'text-muted'}], \n           ['p', {'class':'text-small show-for-medium-up ellipsis'}], \n           ['dl', {'class':'inline-dl'}], \n           ['span', {'class':'stock-trend'}], \n           ['div', {'class':'flex-row'}]]\n\n#formatting in reference to page source code\nfinal_data = [[getattr(i.find(a, b), 'text', None) for a, b in headers] \n              for i in soup(d.page_source, 'html.parser').find_all('div', {'class':'search-result flex-row'})]\n\nnew_data = [[i[0], i[1], re.sub('\\n+\\s{2,}', '', i[2]), \n             [re.findall('[\\$\\w\\.%/]+', d) for d in i[3:]]] for i in final_data]\n\nfinal_results = [i[:3]+[dict(zip(['Price', 'Daily Change', 'Percent Change'], \n                                 filter(lambda x:re.findall('\\d', x), i[-1][0])))] for i in new_data]\n\n#all info iterated by company into list\nnew_results = [company(*i) for i in final_results]\n\n#grab tickers into a list\nabbrevs = [i.abbreviation for i in new_results]\n\n\n#Webscrape Yahoo! Finance-Most Active Stocks with BeautifulSoup\nurl = 'https://finance.yahoo.com/most-active'\nheader = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'\n         }\nresponse = requests.get(url, headers=header)\nsoup = BeautifulSoup(response.content, 'lxml')\n\n#iterate over dataframe and append tickers to list\nsymbols = list()\nfor item in soup.select('.simpTblRow'):\n    symbols.append(item.select('[aria-label=Symbol]')[0].get_text())\n\n    \n#Websrape Wikipedia for DOW 30 Symbols\ndf = pd.read_html('https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average')[1]\n#grab tickers into a list\ntickers = df.Symbol.to_list()\n\n\n#Output stock info to csv\n#US New's stocks under $10\nstock_info_to_csv(abbrevs, 'ticker_under10.csv')\n#Yahoo's most active stocks\nstock_info_to_csv(symbols, 'ticker_active.csv')\n#DOW 30\nstock_info_to_csv(tickers, 'ticker_dow.csv')\n", "367": "\"\"\"Combines the entire source code to analyse a conference based on authorship information\"\"\"\nimport os\nimport attr\nimport pprint\nimport pandas as pd\nfrom typing import Sequence\n\nfrom analyse_conf import sigir_extract\nfrom analyse_conf import author_info\n\nconference_to_webscraper = {\n    \"SIGIR2022\": sigir_extract\n}\n\n\ndef make_output_dir(conf: str) -> str:\n    \"\"\"Create the output_dir for a conference, and return the path to it\"\"\"\n    output_dir = f\"outputs/{conf}\"\n    if not os.path.exists(\"outputs\"):\n        os.mkdir(\"outputs\")\n    if not os.path.exists(output_dir):\n        os.mkdir(output_dir)\n    return output_dir\n\n\ndef write_class_list(objects: Sequence[object], file_name: str) -> None:\n    \"\"\"Write a list of attrs objects to a csv file, overwriting old results\"\"\"\n    # Delete the file if it exists\n    if os.path.exists(file_name):\n        os.remove(file_name)\n\n    # Write data to file\n    rows = [attr.asdict(obj) for obj in objects]\n    df = pd.DataFrame(rows)\n    df.to_csv(file_name, index=False)\n\n\ndef analyse_conf(conf: str) -> None:\n    \"\"\"\n    Webscrape the conference site to get author data\n    Use SemanticScholar API to get citation and institution data\n    TODO: Perform analysis of this data\n        TODO: Create visualisations and tables of the results\n    \"\"\"\n    output_dir = make_output_dir(conf)\n\n    # Webscrape conference data\n    papers = conference_to_webscraper[conf].extract_data()\n    print(\"Papers:\")\n    pprint.pprint(papers[:10])\n\n    # Get author data from SemanticScholar\n    authors = author_info.get_author_data(papers)\n    print(\"\\n\\nAuthors:\")\n    pprint.pprint(authors[:10])\n\n    # Extract authorships after the author_id information has been added to papers list\n    authorships = [ats for paper in papers for ats in paper.authorships]\n    print(\"\\n\\nAuthorships:\")\n    pprint.pprint(authorships[:10])\n\n    # Write data to file\n    write_class_list(authors, f\"{output_dir}/authors.csv\")\n    write_class_list(papers, f\"{output_dir}/papers.csv\")\n    write_class_list(authorships, f\"{output_dir}/authorships.csv\")\n\nif __name__ == \"__main__\":\n    analyse_conf(\"SIGIR2022\")\n", "368": "from flask import Flask, render_template, url_for, redirect, session, request, g\napp = Flask(__name__)\napp.static_folder = 'static'\n\n\n\n\n@app.route(\"/login\", methods=['GET', 'POST'])\ndef login_main():\n    return render_template('login.html')\n        \n@app.route(\"/\")\n@app.route(\"/home\")\ndef home_main():\n    return render_template('home.html', title=\"Home\")\n\n@app.route(\"/webscrape\")\ndef survey_main():\n    return render_template('webscrape.html')\n\n@app.route(\"/configgen\")\ndef prep_main():\n    return render_template('configgen.html')\n\n@app.route(\"/deployment\")\ndef deployment_main():\n    return render_template('deployment.html')\n\n@app.route(\"/validation\")\ndef validation_main():\n    return render_template('validation.html')\n\n@app.route(\"/integrationengineer\")\ndef integrationengineer_main():\n    return render_template('integrationengineer.html')\n\nif __name__ == \"__main__\":\n    app.run(debug=True, host='192.168.0.135')\n\n", "369": "from bs4 import BeautifulSoup as bs4\nfrom dash import html\nfrom datetime import date, timedelta, datetime\nfrom dotenv import load_dotenv, find_dotenv\nfrom geopy.geocoders import GoogleV3\nimport glob\nfrom imagekitio import ImageKit\nfrom numpy import NaN\nimport logging\nimport os\nimport pandas as pd\nimport requests\n\n## SETUP AND VARIABLES\nload_dotenv(find_dotenv())\ng = GoogleV3(api_key=os.getenv('GOOGLE_API_KEY')) # https://github.com/geopy/geopy/issues/171\n\nlogging.getLogger().setLevel(logging.INFO)\n\n# ImageKit.IO\n# https://docs.imagekit.io/api-reference/upload-file-api/server-side-file-upload#uploading-file-via-url\n# Create keys at https://imagekit.io/dashboard/developer/api-keys\nimagekit = ImageKit(\n    public_key=os.getenv('IMAGEKIT_PUBLIC_KEY'),\n    private_key=os.getenv('IMAGEKIT_PRIVATE_KEY'),\n    url_endpoint = os.getenv('IMAGEKIT_URL_ENDPOINT')\n)\n\n# Make the dataframe a global variable\nglobal df\n\n### PANDAS DATAFRAME OPERATIONS\n# Load all CSVs and concat into one dataframe\n# https://stackoverflow.com/a/21232849\npath = \".\"\nall_files = glob.glob(os.path.join(path, \"*.csv\"))\ndf = pd.concat((pd.read_csv(f, float_precision=\"round_trip\", skipinitialspace=True) for f in all_files), ignore_index=True)\n\npd.set_option(\"display.precision\", 10)\n\n# Strip leading and trailing whitespaces from the column names\n# https://stackoverflow.com/a/36082588\ndf.columns = df.columns.str.strip()\n\n# Standardize the column names by renaminmg them\n# https://stackoverflow.com/a/65332240\ndf = df.rename(columns=lambda c: 'mls_number' if c.startswith('Listing') else c)\ndf = df.rename(columns=lambda c: 'subtype' if c.startswith('Sub Type') else c)\ndf = df.rename(columns=lambda c: 'street_number' if c.startswith('St#') else c)\ndf = df.rename(columns=lambda c: 'street_name' if c.startswith('St Name') else c)\ndf = df.rename(columns=lambda c: 'list_price' if c.startswith('List Price') else c)\ndf = df.rename(columns=lambda c: 'garage_spaces' if c.startswith('Garage Spaces') else c)\ndf = df.rename(columns=lambda c: 'phone_number' if c.startswith('List Office Phone') else c)\ndf = df.rename(columns=lambda c: 'ppsqft' if c.startswith('Price Per') else c)\n\n# Drop all rows that don't have a MLS mls_number (aka misc data we don't care about)\n# https://stackoverflow.com/a/13413845\ndf = df[df['mls_number'].notna()]\n\n# Drop all duplicate rows based on MLS number\n# Keep the last duplicate in case of updated listing details\ndf = df.drop_duplicates(subset='mls_number', keep=\"last\")\n\n# Remove all $ and , symbols from specific columns\n# https://stackoverflow.com/a/46430853\nif 'ppsqft' in df.columns:\n  cols = ['DepositKey', 'DepositOther', 'DepositPets', 'DepositSecurity', 'list_price', 'ppsqft', 'Sqft']\nelif 'ppsqft' not in df.columns:\n  cols = ['DepositKey', 'DepositOther', 'DepositPets', 'DepositSecurity', 'list_price', 'Sqft']\n# pass them to df.replace(), specifying each char and it's replacement:\ndf[cols] = df[cols].replace({'\\$': '', ',': ''}, regex=True)\n\n# Remove the square footage & YrBuilt abbreviations and cast as nullable Integer type\ndf['Sqft'] = df['Sqft'].str.split('/').str[0].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\ndf['YrBuilt'] = df['YrBuilt'].str.split('/').str[0].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\n\n# Cast the list price column as integers\ndf['list_price'] = df['list_price'].apply(pd.to_numeric, errors='coerce', downcast='integer')\n\n# Calculate the price per square foot if the column doesn't already exist\n# Round the decimal to two places\nif 'ppsqft' or 'Price Per Square Foot' not in df.columns:\n  for row in df.itertuples():\n    try:\n      df.at[row.Index, 'ppsqft'] = (row.list_price / row.Sqft).round(2)\n    except Exception:\n      df.at[row.Index, 'ppsqft'] = pd.NA\n\n# Create a function to get coordinates from the full street address\ndef return_coordinates(address):\n    try:\n        geocode_info = g.geocode(address)\n        lat = float(geocode_info.latitude)\n        lon = float(geocode_info.longitude)\n    except Exception as e:\n        lat = NaN\n        lon = NaN\n        logging.warning(f\"Couldn't fetch geocode information for {address} because of {e}.\")\n    logging.info(f\"Fetched coordinates {lat}, {lon} for {address}.\")\n    return lat, lon\n\n# Create a function to get a missing city\ndef fetch_missing_city(address):\n    try:\n        geocode_info = g.geocode(address)\n        # Get the city by using a ??? whatever method this is\n        # https://gis.stackexchange.com/a/326076\n        # First get the raw geocode information\n        raw = geocode_info.raw['address_components']\n        # Then dig down to find the 'locality' aka city\n        city = [addr['long_name'] for addr in raw if 'locality' in addr['types']][0]\n    except Exception as e:\n        city = NaN\n        logging.warning(f\"Couldn't fetch city for {address} because of {e}.\")\n    logging.info(f\"Fetched city ({city}) for {address}.\")\n    return  city\n\n# Fetch missing city names\nfor row in df.loc[(df['City'].isnull()) & (df['PostalCode'].notnull())].itertuples():\n  df.at[row.Index, 'City'] = fetch_missing_city(f\"{row.street_number} {row.street_name} {str(row.PostalCode)}\")\n\n# Create a new column with the Street Number & Street Name\ndf[\"short_address\"] = df[\"street_number\"] + ' ' + df[\"street_name\"].str.strip() + ',' + ' ' + df['City']\n\n# Create a function to find missing postal codes based on short address\ndef return_postalcode(address):\n    try:\n        # Forward geocoding the short address so we can get coordinates\n        geocode_info = g.geocode(address)\n        # Reverse geocoding the coordinates so we can get the address object components\n        components = g.geocode(f\"{geocode_info.latitude}, {geocode_info.longitude}\").raw['address_components']\n        # Create a dataframe from this list of dictionaries\n        components_df = pd.DataFrame(components)\n        for row in components_df.itertuples():\n            # Select the row that has the postal_code list\n            if row.types == ['postal_code']:\n                postalcode = row.long_name\n    except Exception as e:\n        logging.warning(f\"Couldn't fetch postal code for {address} because {e}.\")\n        return pd.NA\n    logging.info(f\"Fetched postal code {postalcode} for {address}.\")\n    return int(postalcode)\n\n# Filter the dataframe and return only rows with a NaN postal code\n# For some reason some Postal Codes are \"Assessor\" :| so we need to include that string in an OR operation\n# Then iterate through this filtered dataframe and input the right info we get using geocoding\nfor row in df.loc[((df['PostalCode'].isnull()) | (df['PostalCode'] == 'Assessor'))].itertuples():\n    missing_postalcode = return_postalcode(df.loc[(df['PostalCode'].isnull()) | (df['PostalCode'] == 'Assessor')].at[row.Index, 'short_address'])\n    df.at[row.Index, 'PostalCode'] = missing_postalcode\n\n## Webscraping Time\n# Create a function to scrape the listing's Berkshire Hathaway Home Services (BHHS) page using BeautifulSoup 4 and extract some info\ndef webscrape_bhhs(url, row_index):\n    try:\n        response = requests.get(url)\n        soup = bs4(response.text, 'html.parser')\n        # Split the p class into strings and get the last element in the list\n        # https://stackoverflow.com/a/64976919\n        listed_date = soup.find('p', attrs={'class' : 'summary-mlsnumber'}).text.split()[-1]\n        # Now find the URL for the \"feature\" photo of the listing\n        photo = soup.find('a', attrs={'class' : 'show-listing-details'}).contents[1]['src']\n        # Now find the URL to the actual listing instead of just the search result page\n        link = 'https://www.bhhscalifornia.com' + soup.find('a', attrs={'class' : 'btn cab waves-effect waves-light btn-details show-listing-details'})['href']\n    except AttributeError as e:\n        listed_date = pd.NaT\n        photo = NaN\n        link = NaN\n        logging.warn(f\"Couldn't fetch some BHHS webscraping info because of {e}.\")        \n    logging.info(f\"Fetched Listed Date, MLS Photo, and BHHS link for row {row_index}...\")\n    return listed_date, photo, link\n\n# Create a function to upload the file to ImageKit and then transform it\n# https://github.com/imagekit-developer/imagekit-python#file-upload\ndef imagekit_transform(bhhs_url, mls):\n    # if the MLS photo URL from BHHS isn't null (a photo IS available), then upload it to ImageKit\n    if pd.isnull(bhhs_url) == False:\n        try:\n            uploaded_image = imagekit.upload_file(\n                file= f\"{bhhs_url}\", # required\n                file_name= f\"{mls}.jpg\", # required\n                options= {\n                    \"is_private_file\": False,\n                    \"use_unique_file_name\": False,\n                }\n            )\n        except Exception as e:\n            logging.warning(f\"Couldn't upload image to ImageKit because {e}. Passing on...\")\n            uploaded_image = 'ERROR'\n    elif pd.isnull(bhhs_url) == True:\n        uploaded_image = 'ERROR'\n        logging.info(f\"No image URL found. Not uploading anything to ImageKit.\")\n    # Now transform the uploaded image\n    # https://github.com/imagekit-developer/imagekit-python#url-generation\n    if 'ERROR' not in uploaded_image:\n        try:\n            global transformed_image\n            transformed_image = imagekit.url({\n                \"src\": f\"{uploaded_image['response']['url']}\",\n                \"transformation\" : [{\n                    \"height\": \"300\",\n                    \"width\": \"400\"\n                }]\n            })\n        except Exception as e:\n            logging.warning(f\"Couldn't transform image because {e}. Passing on...\")\n            transformed_image = None\n    elif 'ERROR' in uploaded_image:\n        logging.info(f\"No image URL found. Not transforming anything.\")\n        transformed_image = None\n    return transformed_image\n\n# Create a new column with the full street address\n# Also strip whitespace from the St Name column\n# Convert the postal code into a string so we can combine string and int\n# https://stackoverflow.com/a/11858532\ndf[\"full_street_address\"] = df[\"street_number\"] + ' ' + df[\"street_name\"].str.strip() + ',' + ' ' + df['City'] + ' ' + df[\"PostalCode\"].map(str)\n\n# Iterate through the dataframe and get the listed date and photo for rows that don't have them\n# If the Listed Date column is already present, iterate through the null cells\n# We can use the presence of a Listed Date as a proxy for MLS Photo; generally, either both or neither exist/don't exist together\n# This assumption will reduce the number of HTTP requests we send to BHHS\nif 'listed_date' in df.columns:\n    for row in df.loc[(df['listed_date'].isnull()) & df['date_generated'].isnull()].itertuples():\n        mls_number = row[1]\n        webscrape = webscrape_bhhs(f\"https://www.bhhscalifornia.com/for-lease/{mls_number}-t_q;/\", {row.Index})\n        df.at[row.Index, 'listed_date'] = webscrape[0]\n        df.at[row.Index, 'MLS Photo'] = imagekit_transform(webscrape[1], row[1])\n        df.at[row.Index, 'bhhs_url'] = webscrape[2]\n# if the Listed Date column doesn't exist (i.e this is a first run), create it using df.at\nelif 'listed_date' not in df.columns:\n    for row in df.itertuples():\n        mls_number = row[1]\n        webscrape = webscrape_bhhs(f\"https://www.bhhscalifornia.com/for-lease/{mls_number}-t_q;/\", {row.Index})\n        df.at[row.Index, 'listed_date'] = webscrape[0]\n        df.at[row.Index, 'MLS Photo'] = imagekit_transform(webscrape[1], row[1])\n        df.at[row.Index, 'bhhs_url'] = webscrape[2]\n\n# Iterate through the dataframe and fetch coordinates for rows that don't have them\n# If the Latitude column is already present, iterate through the null cells\n# This assumption will reduce the number of API calls to Google Maps\nif 'Latitude' in df.columns:\n    for row in df['Latitude'].isnull().itertuples():\n        coordinates = return_coordinates(df.at[row.Index, 'full_street_address'])\n        df.at[row.Index, 'Latitude'] = coordinates[0]\n        df.at[row.Index, 'Longitude'] = coordinates[1]\n# If the Coordinates column doesn't exist (i.e this is a first run), create it using df.at\nelif 'Latitude' not in df.columns:\n    for row in df.itertuples():\n        coordinates = return_coordinates(df.at[row.Index, 'full_street_address'])\n        df.at[row.Index, 'Latitude'] = coordinates[0]\n        df.at[row.Index, 'Longitude'] = coordinates[1]\n\n# Split the Bedroom/Bathrooms column into separate columns based on delimiters\n# Based on the example given in the spreadsheet: 2 (beds) / 1 (total baths),1 (full baths) ,0 (half bath), 0 (three quarter bath)\n# Realtor logic based on https://www.realtor.com/advice/sell/if-i-take-out-the-tub-does-a-bathroom-still-count-as-a-full-bath/\n# TIL: A full bathroom is made up of four parts: a sink, a shower, a bathtub, and a toilet. Anything less than thpdat, and you can\u00e2\u20ac\u2122t officially consider it a full bath.\ndf['Bedrooms'] = df['Br/Ba'].str.split('/', expand=True)[0]\ndf['Total Bathrooms'] = (df['Br/Ba'].str.split('/', expand=True)[1]).str.split(',', expand=True)[0]\ndf['Full Bathrooms'] = (df['Br/Ba'].str.split('/', expand=True)[1]).str.split(',', expand=True)[1]\ndf['Half Bathrooms'] = (df['Br/Ba'].str.split('/', expand=True)[1]).str.split(',', expand=True)[2]\ndf['Three Quarter Bathrooms'] = (df['Br/Ba'].str.split('/', expand=True)[1]).str.split(',', expand=True)[3]\n\n# Convert a few columns into int64\n# pd.to_numeric will convert into int64 or float64 automatically, which is cool\n# These columns are assumed to have NO MISSING DATA, so we can cast them as int64 instead of floats (ints can't handle NaNs)\ndf['Bedrooms'] = df['Bedrooms'].apply(pd.to_numeric, errors='coerce')\ndf['Total Bathrooms'] = df['Total Bathrooms'].apply(pd.to_numeric)\n# These columns should stay floats\ndf['ppsqft'] = df['ppsqft'].apply(pd.to_numeric, errors='coerce')\ndf['Latitude'] = df['Latitude'].apply(pd.to_numeric, errors='coerce')\ndf['Longitude'] = df['Longitude'].apply(pd.to_numeric, errors='coerce')\n# Convert the rest into nullable integer data types\n# We should do this because these fields will often have missing data, forcing a conversion to float64 \n# https://pandas.pydata.org/docs/user_guide/integer_na.html\n# https://medium.com/when-i-work-data/nullable-integers-4060089f92ec\n# We don't really have a need for floats here, just ints\n# And this will prevent weird TypeError shit like TypeError: '>=' not supported between instances of 'str' and 'int'\n# And this will also convert non-integers into NaNs\ndf['PostalCode'] = df['PostalCode'].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\ndf['Sqft'] = df['Sqft'].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\ndf['YrBuilt'] = df['YrBuilt'].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\ndf['garage_spaces'] = df['garage_spaces'].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\ndf['DepositKey'] = df['DepositKey'].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\ndf['DepositOther'] = df['DepositOther'].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\ndf['DepositPets'] = df['DepositPets'].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\ndf['DepositSecurity'] = df['DepositSecurity'].apply(pd.to_numeric, errors='coerce').astype(pd.Int64Dtype())\n\n# Replace all empty values in the following columns with NaN and cast the column as dtype string\n# https://stackoverflow.com/a/47810911\ndf.Terms = df.Terms.astype(\"string\").replace(r'^\\s*$', pd.NA, regex=True)\ndf.Furnished = df.Furnished.astype(\"string\").replace(r'^\\s*$', pd.NA, regex=True)\n\n# Convert the listed date into DateTime and set missing values to be NaT\n# Infer datetime format for faster parsing\n# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html\ndf['listed_date'] = pd.to_datetime(df['listed_date'], errors='coerce', infer_datetime_format=True)\n\n# Per CA law, ANY type of deposit is capped at rent * 3 months\n# It doesn't matter the type of deposit, they all have the same cap\n# Despite that, some landlords/realtors will list the property with an absurd deposit (100k? wtf) so let's rewrite those\n# Use numpy .values to rewrite anything greater than $18000 ($6000 rent * 3 months) into $18000\n# https://stackoverflow.com/a/54426197\ndf['DepositSecurity'].values[df['DepositSecurity'] > 18000] = 18000\ndf['DepositPets'].values[df['DepositPets'] > 18000] = 18000\ndf['DepositOther'].values[df['DepositOther'] > 18000] = 18000\ndf['DepositKey'].values[df['DepositKey'] > 18000] = 18000\n\n# Rewrite anything greater than 5000 square feet as NaN\n# Because there's no fucking way there's a RENTAL PROPERTY that is 5000+ sqft in this city\n# It clearly must be some kind of clerical error so a NaN (unknown) is more appropriate\n# All that being said, I should peruse new spreadsheets to make sure there isn't actually a valid property exceeds 5000 sqft\ndf['Sqft'].values[df['Sqft'] > 5000] = pd.NA\n\n# Tag each row with the date it was generated\nfor row in df.itertuples():\n  if 'date_generated' in df.columns:\n    pass\n  elif 'date_generated' not in df.columns:\n    df.at[row.Index, 'date_generated'] = datetime.now().date()\n\n# The rental marker is hot and properties go off market fast\n# Keep all rows less than a \"month\" old (31 days)\ndf['date_generated'] = df['date_generated'] >= date.today() - timedelta(31)\n\n# Keep rows with less than 6 bedrooms\n# 6 bedrooms and above are probably multi family investments and not actual rentals\n# They also skew the outliers, causing the sliders to go way up\ndf = df[df.Bedrooms < 6]\n\n# Reindex the dataframe\ndf.reset_index(drop=True, inplace=True)\n\n# Define HTML code for the popup so it looks pretty and nice\ndef popup_html(row):\n    i = row.Index\n    short_address = df['short_address'].at[i]\n    postalcode = df['PostalCode'].at[i]\n    full_address = f\"{short_address} {postalcode}\"\n    mls_number=df['mls_number'].at[i]\n    mls_number_hyperlink=df['bhhs_url'].at[i]\n    mls_photo = df['MLS Photo'].at[i]\n    lc_price = df['list_price'].at[i] \n    price_per_sqft=df['ppsqft'].at[i]                  \n    brba = df['Br/Ba'].at[i]\n    square_ft = df['Sqft'].at[i]\n    year = df['YrBuilt'].at[i]\n    garage = df['garage_spaces'].at[i]\n    pets = df['PetsAllowed'].at[i]\n    phone = df['phone_number'].at[i]\n    terms = df['Terms'].at[i]\n    sub_type = df['subtype'].at[i]\n    listed_date = pd.to_datetime(df['listed_date'].at[i]).date() # Convert the full datetime into date only. See https://stackoverflow.com/a/47388569\n    furnished = df['Furnished'].at[i]\n    key_deposit = df['DepositKey'].at[i]\n    other_deposit = df['DepositOther'].at[i]\n    pet_deposit = df['DepositPets'].at[i]\n    security_deposit = df['DepositSecurity'].at[i]\n    # If there's no square footage, set it to \"Unknown\" to display for the user\n    # https://towardsdatascience.com/5-methods-to-check-for-nan-values-in-in-python-3f21ddd17eed\n    if pd.isna(square_ft) == True:\n        square_ft = 'Unknown'\n    # If there IS a square footage, convert it into an integer (round number)\n    elif pd.isna(square_ft) == False:\n        square_ft = f\"{int(square_ft)} sq. ft\"\n    # Repeat above for Year Built\n    if pd.isna(year) == True:\n        year = 'Unknown'\n    # If there IS a square footage, convert it into an integer (round number)\n    elif pd.isna(year) == False:\n        year = f\"{int(year)}\"\n    # Repeat above for garage spaces\n    if pd.isna(garage) == True:\n        garage = 'Unknown'\n    elif pd.isna(garage) == False:\n        garage = f\"{int(garage)}\"\n    # Repeat for ppsqft\n    if pd.isna(price_per_sqft) == True:\n        price_per_sqft = 'Unknown'\n    elif pd.isna(price_per_sqft) == False:\n        price_per_sqft = f\"${float(price_per_sqft)}\"\n    # Repeat for listed date\n    if pd.isna(listed_date) == True:\n        listed_date = 'Unknown'\n    elif pd.isna(listed_date) == False:\n        listed_date = f\"{listed_date}\"\n    # Repeat for furnished\n    if pd.isna(furnished) == True:\n        furnished = 'Unknown'\n    elif pd.isna(furnished) == False:\n        furnished = f\"{furnished}\"\n    # Repeat for the deposits\n    if pd.isna(key_deposit) == True:\n        key_deposit = 'Unknown'\n    elif pd.isna(key_deposit) == False:\n        key_deposit = f\"${int(key_deposit)}\"\n    if pd.isna(pet_deposit) == True:\n        pet_deposit = 'Unknown'\n    elif pd.isna(pet_deposit) == False:\n        pet_deposit = f\"${int(pet_deposit)}\"\n    if pd.isna(security_deposit) == True:\n        security_deposit = 'Unknown'\n    elif pd.isna(security_deposit) == False:\n        security_deposit = f\"${int(security_deposit)}\"\n    if pd.isna(other_deposit) == True:\n        other_deposit = 'Unknown'\n    elif pd.isna(other_deposit) == False:\n        other_deposit = f\"${int(other_deposit)}\"\n   # If there's no MLS photo, set it to an empty string so it doesn't display on the tooltip\n   # Basically, the HTML block should just be an empty Img tag\n    if pd.isna(mls_photo) == True:\n        mls_photo_html_block = html.Img(\n          src='',\n          referrerPolicy='noreferrer',\n          style={\n            'display':'block',\n            'width':'100%',\n            'margin-left':'auto',\n            'margin-right':'auto'\n          },\n          id='mls_photo_div'\n        )\n    # If there IS an MLS photo, just set it to itself\n    # The HTML block should be an Img tag wrapped inside a parent  tag so the image will be clickable\n    elif pd.isna(mls_photo) == False:\n        mls_photo_html_block = html.A( # wrap the Img inside a parent  tag \n            html.Img(\n              src=f'{mls_photo}',\n              referrerPolicy='noreferrer',\n              style={\n                'display':'block',\n                'width':'100%',\n                'margin-left':'auto',\n                'margin-right':'auto'\n              },\n              id='mls_photo_div'\n            ),\n          href=f\"{mls_number_hyperlink}\",\n          referrerPolicy='noreferrer',\n          target='_blank'\n        )\n    # Return the HTML snippet but NOT as a string. See https://github.com/thedirtyfew/dash-leaflet/issues/142#issuecomment-1157890463 \n    return [\n      html.Div([ # This is where the MLS photo will go (at the top and centered of the tooltip)\n          mls_photo_html_block\n      ]),\n      html.Table([ # Create the table\n        html.Tbody([ # Create the table body\n          html.Tr([ # Start row #1\n            html.Td(\"Listed Date\"), html.Td(f\"{listed_date}\")\n          ]), # end row #1\n          html.Tr([ \n            html.Td(\"Street Address\"), html.Td(f\"{full_address}\")\n          ]),\n          html.Tr([ \n            # Use a hyperlink to link to BHHS, don't use a referrer, and open the link in a new tab\n            # https://www.freecodecamp.org/news/how-to-use-html-to-open-link-in-new-tab/\n            html.Td(html.A(\"Listing ID (MLS#)\", href=\"https://github.com/perfectly-preserved-pie/larentals/wiki#listing-id\", target='_blank')), html.Td(html.A(f\"{mls_number}\", href=f\"{mls_number_hyperlink}\", referrerPolicy='noreferrer', target='_blank'))\n          ]),\n          html.Tr([ # https://www.elegantthemes.com/blog/wordpress/call-link-html-phone-number\n            html.Td(\"List Office Phone\"), html.Td(html.A(f\"{phone}\", href=f\"tel:{phone}\")),\n          ]),          \n          html.Tr([ \n            html.Td(\"Rental Price\"), html.Td(f\"${lc_price}\")\n          ]),\n          html.Tr([\n            html.Td(\"Security Deposit\"), html.Td(f\"{security_deposit}\"),\n          ]),\n          html.Tr([\n            html.Td(\"Pet Deposit\"), html.Td(f\"{pet_deposit}\"),\n          ]),\n          html.Tr([\n            html.Td(\"Key Deposit\"), html.Td(f\"{key_deposit}\"),\n          ]),\n          html.Tr([\n            html.Td(\"Other Deposit\"), html.Td(f\"{other_deposit}\"),\n          ]),          \n          html.Tr([\n            html.Td(\"Square Feet\"), html.Td(f\"{square_ft}\")\n          ]),\n          html.Tr([\n            html.Td(\"Price Per Square Foot\"), html.Td(f\"{price_per_sqft}\")\n          ]),\n          html.Tr([\n            html.Td(html.A(\"Bedrooms/Bathrooms\", href=\"https://github.com/perfectly-preserved-pie/larentals/wiki#bedroomsbathrooms\", target='_blank')), html.Td(f\"{brba}\")\n          ]),\n          html.Tr([\n            html.Td(\"Garage Spaces\"), html.Td(f\"{garage}\"),\n          ]),\n          html.Tr([\n            html.Td(\"Pets Allowed?\"), html.Td(f\"{pets}\"),\n          ]),\n          html.Tr([\n            html.Td(\"Furnished?\"), html.Td(f\"{furnished}\"),\n          ]),\n          html.Tr([\n            html.Td(\"Year Built\"), html.Td(f\"{year}\")\n          ]),\n          html.Tr([\n            html.Td(html.A(\"Rental Terms\", href=\"https://github.com/perfectly-preserved-pie/larentals/wiki#rental-terms\", target='_blank')), html.Td(f\"{terms}\"),\n          ]),             \n          html.Tr([                                                                                            \n            html.Td(html.A(\"Physical Sub Type\", href=\"https://github.com/perfectly-preserved-pie/larentals/wiki#physical-sub-type\", target='_blank')), html.Td(f\"{sub_type}\")                                                                                    \n          ]), # end rows\n        ]), # end body\n      ]), # end table\n    ]\n\n# Iterate through and generate the HTML code\nif 'popup_html' in df.columns:\n    for row in df['popup_html'].isnull().itertuples():\n        df.at[row.Index, 'popup_html'] = popup_html(row)\n# If the popup_html column doesn't exist (i.e this is a first run), create it using df.at\nelif 'popup_html' not in df.columns:\n    df['popup_html'] = ''\n    for row in df.itertuples():\n        df.at[row.Index, 'popup_html'] = popup_html(row)\n\n# Pickle the dataframe for later ingestion by app.py\n# https://www.youtube.com/watch?v=yYey8ntlK_E\n# If there's no pickle file on GitHub, then make one\npickle_url = 'https://github.com/perfectly-preserved-pie/larentals/raw/master/dataframe.pickle'\nif requests.head(pickle_url).status_code == 404:\n  df.to_pickle(\"dataframe.pickle\")\n# Otherwise load in the old pickle file and concat it with the new dataframe\\\nelif requests.head(pickle_url).status_code == 200:\n  # Read the old dataframe in\n  df_old = pd.read_pickle(filepath_or_buffer=pickle_url)\n  # Combine both old and new dataframes\n  df_combined = pd.concat([df, df_old], ignore_index=True)\n  # Pickle the new combined dataframe\n  df_combined.to_pickle(\"dataframe.pickle\")", "370": "import requests\nfrom bs4 import BeautifulSoup\nfrom pyltp import SentenceSplitter\n\nclass WebScrape(object):\n    def __init__(self, word, url):\n        self.url = url\n        self.word = word\n\n    # \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n    def web_parse(self):\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n        req = requests.get(url=self.url, headers=headers)\n\n        # \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n        if req.status_code == 200:\n            soup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n            return soup\n        return None\n\n    # \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n    def get_gloss(self):\n        soup = self.web_parse()\n        if soup:\n            lis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n            if lis:\n                for li in lis('li'):\n                    if '", "371": "import argparse\nimport sys\n\nfrom overdrive_reconcile.reconcile import reconcile\nfrom overdrive_reconcile.utils import date_subdirectory, count_rows\nfrom overdrive_reconcile.webscraper import scrape, check_status\n\n\ndef main(args: list) -> None:\n\n    # process: str: library: str, src_fh: str)\n\n    parser = argparse.ArgumentParser(\n        prog=\"Overdrive-Reconcile\",\n        description=\"Sets of scripts reconciling records between Sierra and OverDrive platform.\",\n    )\n\n    parser.add_argument(\n        \"action\",\n        help=(\n            \"'reconcile' runs entire set of scripts | \"\n            \"'webscrape' runs only webscraping of OverDrive catalog | \"\n            \"'check-overdrive' runs scraping for a single resource\"\n        ),\n        type=str,\n        choices=[\"reconcile\", \"webscrape\", \"check-overdrive\"],\n    )\n    parser.add_argument(\n        \"library\", help=\"'BPL' or 'NYPL'\", type=str, choices=[\"BPL\", \"NYPL\"]\n    )\n    parser.add_argument(\n        \"source\",\n        help=\"Sierra export or csv for webscraping source file handle\",\n        type=str,\n    )\n\n    parser.add_argument(\n        \"start\",\n        help=\"Starting row of data to be verified.\",\n        type=int,\n        nargs=\"?\",\n        default=0,\n    )\n\n    pargs = parser.parse_args(args)\n\n    if pargs.action == \"reconcile\":\n        reconcile(pargs.library, pargs.source)\n    if pargs.action == \"webscrape\":\n        if pargs.source == \"default\":\n            # assume today's file\n            work_dir = date_subdirectory(pargs.library)\n            src_fh = (\n                f\"{work_dir}/{pargs.library}-for-deletion-verification-required.csv\"\n            )\n            total = count_rows(src_fh) - 1\n            scrape(pargs.library, src_fh, total, pargs.start)\n        else:\n            total = count_rows(pargs.source) - 1\n            scrape(pargs.library, pargs.source, total, pargs.start)\n\n    if pargs.action == \"check-overdrive\":\n        print(f\"Checking status of {pargs.source} on OverDrive platform...\")\n        check_status(pargs.source, pargs.library)\n\n\nif __name__ == \"__main__\":\n    main(sys.argv[1:])\n", "372": "from tkinter import *\nfrom WebScraper import webscrape\n\n\nwindow = Tk()\n\n\nwindow.title(\"WIKI-DICTIONARY\")\nwindow.resizable(height=False, width=False)\nwindow.configure(bg=\"white\")\n\n\ndef onbtn2press():\n    e1.delete(0, END)\n\n\ndef onbtn1press():\n    t1.delete('1.0', END)\n    entryinput = e1.get()\n    try:\n        text = webscrape(entryinput)\n    except:\n        text = \"We could not find a proper definition. Please check your entry.\"\n    t1.insert('1.0', text)\n\n\nf1 = Frame(window, height=15, width=700, bg=\"yellow\", pady=6).grid(column=0, row=0)\n\nl1 = Label(master=window, text=\"Wikipedia-Dictionary\", bg=\"#ffe0d5\").grid(column=0, row=0)\n\nf2 = Frame(window, width=900, height=100, bg=\"#7fffd4\", pady=10, padx=10)\n\ne1 = Entry(master=f2, bg=\"#ffede3\")\ne1.grid(column=0, row=0)\nt1 = Text(master=f2, bg=\"#fffbe9\")\nt1.grid(column=0, row=2)\n\nf3 = Frame(master=f2, bg=\"yellow\")\n\nbtn1 = Button(master=f3, text=\"Enter\", bg=\"#ffede3\", command=onbtn1press).grid(column=1, row=0)\nbtn2 = Button(master=f3, text=\"Clear\", bg=\"#ffede3\", command=onbtn2press).grid(column=0, row=0)\n\nf3.grid(column=0, row=1)\n\nf2.grid(column=0, row=2)\n\n\nwindow.mainloop()\n\n", "373": "import socket \nimport webScrape \nimport chordServer            \n\nsoc = socket.socket(socket.AF_INET, socket.SOCK_STREAM)         \nprint (\"Socket successfully created\")\n\nport = 12345               \nsoc.bind((\"\", port))        \nprint (\"socket binded to %s\" %(port))\n \nsoc.listen(5)     \nprint (\"socket is listening\")  \n\nconn, addr = soc.accept()     \nprint ('Got connection from', addr)\n \nwhile True:\n\ttry:\n\t\tprint ('Receiving')\n\t\tdata = conn.recv(1028)\n\t\tif data:\n\t\t\tdata = data.decode().rstrip(\"\\n\\r\")\n\t\t\tprint(\"to send \"+data, \", length: \", len(data))\n\t\t\tchordServer\n\n\t\t\tif len(data)!=0:\n\t\t\t\tnewData = data.split(',')\n\t\t\t\tsong_data = newData[0]\n\t\t\t\tartist_data = newData[1]\n\t\t\t\tResult = webScrape.main(song_data, artist_data)\n\t\t\t\tprint(type(Result))\n\t\t\t\tconn.close()\n\t\t\t\tprint('1st closed')\n\t\t\t\tbreak\n\texcept:\n\t\tprint(\"error\")\n\t\tbreak\n\nchordServer.receive(Result)", "374": "def run():\n    from urllib.request import urlopen\n    from bs4 import BeautifulSoup\n\n    ##### Link collector to send crawler to iteratively\n\n    import requests\n\n    baseurl = 'https://nationalpost.com'\n\n    headers = {\n        'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.128 Safari/537.36'\n    }\n\n    columnlinks = []\n\n    # Pulling the urls from the last 5 pages worth of columns from here: https://nationalpost.com/category/opinion/?from=1\n        \n    for x in range(1,25,25):\n        r = requests.get(f'https://nationalpost.com/category/opinion/?from={x}')\n        soup2 = BeautifulSoup(r.content, 'lxml')\n        columnlist = soup2.find_all('div', class_='article-card__details')\n        for item in columnlist:\n                for link in item.find_all('a', href=True):\n                    columnlinks.append(baseurl + link['href'])\n\n\n    #remove this string  from array columnlinks['http://nationalpost.com//category/opinion/']\n\n    fPostLink = 'https://financialpost.com/'\n    gPostLink = 'https://nationalpost.com/news/'\n    hPostLink = 'https://nationalpost.com/category/politics'\n    iPostLink = 'https://nationalpost.com/category/election-2021'\n\n    # NEED TO REMOVE https://nationalpost.com/news/\n\n    for elem in list(columnlinks):\n        if elem == \"https://nationalpost.com/category/opinion/\":\n            columnlinks.remove(elem)\n        elif fPostLink in elem:        \n            columnlinks.remove(elem)\n        elif gPostLink in elem:\n            columnlinks.remove(elem)\n        elif hPostLink in elem:\n            columnlinks.remove(elem)\n        elif iPostLink in elem:\n            columnlinks.remove(elem)\n\n            \n\n    linkarray = columnlinks\n\n    count = 0\n\n    for i in range(0, len(linkarray)):\n\n        count += 1\n\n        url = linkarray[i] # this variable imports into Database under newsColumns\n        html = urlopen(url)\n        soup = BeautifulSoup(html.read(), 'html.parser')\n    ##### CLEAN WEBSCRAPE OF ALL PERTINENT INFORMATION ######\n        ps = soup.find_all('p')\n        bodyarray = []\n        bodytext = \"\"\n\n        #   collect all p tags then append all text items to array - then add all items into bodytext string var \n        for p in ps:\n            ptext = p.get_text()\n            bodyarray.append(ptext)\n\n        for item in bodyarray:\n            bodytext += item\n            \n        #   headline webscrape\n        headline = soup.find('h1').get_text()\n\n        #   publish date webscrape\n        try:\n            pubdatescrape = soup.find('span', \"published-date__since\").get_text()\n        except AttributeError:\n            pubdate = \"Jan 1, 1991\"\n        else:\n            pubdate = pubdatescrape\n\n        #   author webscrape - if null do not add to the thing (this doesn't work because the thing is already pulled and transferred)\n        try:\n            author = soup.find('span', \"published-by__author\").get_text()\n        except AttributeError:\n            print('Error with following link' + headline + '\\nskipping link...')\n            pass\n        \n        #split the author name into fname and lname\n        try:\n            splitauthor = author.split(' ', 1)\n            authorfname = splitauthor[0]\n            authorlnamewhole = splitauthor[1].split(',', 1)\n            authorlname = authorlnamewhole[0]\n        except IndexError:\n            authorfname = author\n            authorlname = \" \"\n                                    \n        print( count,\". =====UPLOADING COLUMN PUBLISHED:====== \", pubdate,'\\n', authorfname,'\\n', authorlname,'\\n', headline,)\n\n        ######      BEGIN INSERT DATA INTO DATABASE     #####\n\n        import mysql.connector\n        import re\n\n        #DB information to connect to localhost\n\n        mydb = mysql.connector.connect(\n        host=\"localhost\",\n        port=\"3306\",\n        user=\"root\",\n        password=\"root\",\n        database =\"columnwatcher\"\n        )\n\n        #get everything from the columnist table\n\n        mycursor = mydb.cursor()\n\n        mycursor.execute(\"SELECT * FROM columnists\")\n        result = mycursor.fetchall()\n\n        #check if the first and last name of the columnist matches any of the existing columnists, columnist id = \n\n        #give columnist id to existing columnist, attribute id to new columnist\n\n        for x in range(0,len(result)):\n            if authorfname in result[x] and authorlname in result[x]:\n                entryColumnistID = x+1\n                newColumnist = False\n                break\n                \n            else: \n                entryColumnistID = len(result)+1\n                newColumnist = True\n\n        #if the name of the new columnist does not exist in the columnist table, add it to the table\n        if newColumnist == True:\n            print('name does not exist in record. Creating new Columnist ID:')\n            sqlColumnists = \"INSERT INTO columnists (first_name, last_name) VALUES (%s, %s)\"\n            valColumnists = (authorfname, authorlname)\n\n            mycursor.execute(sqlColumnists, valColumnists)\n            print(\"Adding to database: \", authorfname, authorlname)\n            mydb.commit()\n        else:\n            print(authorfname, authorlname,\"exists in record under id#: \", entryColumnistID)\n\n        ##-- insert article --##\n        \n        '''\n        columnist_id - auto incremented\n        paper_id - National Post is 1 \n        headline \n        publishdate - PUBDATE\n        body_text\n        url\n        \n        '''\n\n\n        mycursor2 = mydb.cursor()\n        mycursor2.execute(\"SELECT headline FROM newscolumns\")\n        colresult = mycursor2.fetchall()\n\n        #placeholder and specific vars\n        #entryColumnistID\n        paper_id = 1 #NationalPost\n        newHeadline = headline\n\n        import datetime\n        #switch PUBLISH DATE to YYYY-mm-dd\n        f = '%b %d, %Y'\n        entryDateTime = datetime.datetime.strptime(pubdate, f)\n\n\n        #try 2 on newscolumn insert\n\n        for x in range(0,len(colresult)):\n            if newHeadline in colresult[x]:        \n                newColumn = False\n                break\n                \n            else: \n                newColumn = True\n\n\n\n        #if this headline does not already exist, add it to my database please\n        if newColumn == True:\n            print('headline does not exist in record. Creating new Column entry')\n            sqlNewsColumns = \"INSERT INTO newscolumns (columnist_id, paper_id, headline, body_text, url, publishdate) VALUES (%s, %s, %s, %s, %s, %s)\"\n            valNewsColumns = (entryColumnistID, 1, newHeadline, bodytext, url, entryDateTime)\n            mycursor2.execute(sqlNewsColumns, valNewsColumns)\n            mydb.commit()\n            print('new entry', newHeadline, 'added to the database')\n            \n        else:\n            print(\"headline already in database, will not add duplicate:\", newHeadline)\n\n\n    print('===NATPO SCRAPE COMPLETE===')", "375": "#Configure common settings for all tasks\nimport datetime as dt\n\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\n\nimport webscrapeData\nimport dataCleaning\nimport uploadMySQL\nimport executeMySQLScripts\nimport createLukesPicksTxt\nimport sendEmail\n\n#Create Task\n\ndefault_args = {\n    'owner': 'me',\n    'start_date': dt.datetime(2021, 1, 2),\n    'retries': 1,\n    #allowed to retry workflow once if it fails with delay of 5 minutes\n    'retry_delay': dt.timedelta(minutes=5),\n}\n\nwith DAG('steam_data_pipeline',\n         default_args=default_args,\n        #The DAG will run every day at 00:00 can also use '@daily' or '@hourly'\n         schedule_interval='@daily',\n         ) as dag:\n\n    # Task 1: print hello\n    t1 = PythonOperator(task_id='extract_data_from_steam',\n                               python_callable= webscrapeData.get_data)\n    # Task 2: print world\n    t2 = PythonOperator(task_id='clean_data',\n                                 python_callable=dataCleaning.clean_data)\n\n    t3 = PythonOperator(task_id='upload_data_MySQL',\n                        python_callable=uploadMySQL.upload_MySQL)\n\n    t4 = PythonOperator(task_id='create_new_table_with_SQL_script',\n                        python_callable=executeMySQLScripts.executeScripts)\n\n    t5 = PythonOperator(task_id='create_txt_file_with_game_suggestions',\n                        python_callable=createLukesPicksTxt.create_Lukes_Picks)\n\n    t6 = PythonOperator(task_id='send_email_with_txt_file',\n                        python_callable=sendEmail.send_email)\n\n#Make chain of operations so dependencies do not get confused\nt1 >> t2 >> t3 >> t4 >> t5 >> t6", "376": "from ast import expr_context\nfrom email import header\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport os \nimport json\n\n#{\"Payhip_Link\", \"Product_Title\", \"Product_Price\", \"Product_Image\"]\n\n#dir_path = os.path.dirname(os.path.realpath(__file__))\n#csv_file_path = dir_path + \"/webscrape_dump.csv\"\n\n#URL Input\nURL = input(\"URL: \")\n\n#assign URL\ntry:\n    page = requests.get(URL)\nexcept:\n    print(\"Invalid URL\")\n    quit()\n\n#define html parser\nsoup = BeautifulSoup(page.content, \"html.parser\")\n#scope into page-collection\nresults = soup.find(id=\"page-collection\")\nproducts = results.find_all(\"div\", class_=\"product-list-block\")\n\n\nlinks = []\nimages = []\ntitles = []\nprices = []\n#iterate through product-list-block (even though its just one)\nfor i in range(len(products)):\n    #get all attributes\n    links = products[i].find_all(\"a\", class_=\"grid-item-link\")\n    prices = products[i].find_all(\"div\", class_=\"price\")\n    titles = products[i].find_all(\"span\", class_=\"text\")\n    images = products[i].find_all(\"img\", class_=\"section-image-fallback\")\n\n    #make sure reading of attributes are correct\n    if (len(prices) == len(titles) and len(titles) == len(images)):\n        num_product = len(prices)\n    else:\n        print(\"Something went wrong\")\n    \n    for product in range(num_product):\n        links[product] = links[product]['href']\n        images[product] = images[product]['src']\n        titles[product] = titles[product].text\n        prices[product] = prices[product].text.split()[0]\n\n#write to json\njson_data = [{\"Payhip_Link\": l, \"Product_Title\": t, \"Product_Price\" : p, \"Product_Image\": i} for l, t, p, i in zip(links, images, titles, prices)]\nwith open('webscrape_dump.json', 'w+') as f:\n    json.dump(json_data, f, indent=2)", "377": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom pandas import ExcelWriter\nimport datetime\n\n\ndef main():\n    date = datetime.date(2018, 5, 19)\n    print(date)\n    webscrape(date)\n\n\n\ndef webscrape(date):\n\n    date = date\n    # get url data\n    url = \"https://www.billboard.com/charts/billboard-korea-k-pop-100/\" + date.isoformat()\n    r = requests.get(url)\n\n    # parse url information\n    soup = BeautifulSoup(r.content, \"html.parser\")\n\n    # lists for Panda DataFrame\n    peakPosition = []\n    top100 = []\n    currRank = []\n    pastRank = []\n    artist = []\n    title = []\n\n    # further narrow data down\n    g_data = soup.find_all(\"div\", {\"class\": \"chart-row__secondary\"})\n\n    for item in g_data:\n        # print out peak position\n        # print \"Peak Position\"\n        peakPosition.append(item.contents[1].find_all(\"div\", {\"class\": \"chart-row__top-spot\"})[0].text[15:])\n\n        # print out number of weeks on chart\n        # print \"Number of Weeks in Top 100\"\n        top100.append(item.contents[1].find_all(\"div\", {\"class\": \"chart-row__weeks-on-chart\"})[0].text[14:])\n\n    # new search in data\n    g_data = soup.find_all(\"div\", {\"class\": \"chart-row__main-display\"})\n\n    for item in g_data:\n        # prints out rank and previous week's rank\n        # print \"This week's ranking\"\n        currRank.append(item.contents[1].find_all(\"span\", {\"class\": \"chart-row__current-week\"})[0].text)\n        # print \"Last week's ranking\"\n        pastRank.append(item.contents[1].find_all(\"span\", {\"class\": \"chart-row__last-week\"})[0].text[11:])\n\n        # prints out the artist\n        # print \"Artist\"\n        # different format for artist names in code\n        # some of the artists' names are hyperlinked\n        # try unlinked case\n        try:\n            artist.append(item.contents[5].find_all(\"span\", {\"class\": \"chart-row__artist\"})[0].text[1:-1])\n        # artist name is hyperlinked\n        except:\n            artist.append(item.contents[5].find_all(\"a\", {\"class\": \"chart-row__artist\"})[0].text[1:-1])\n\n        # prints out the song title\n        # print \"Song Title\"\n        title.append(item.contents[5].find_all(\"h2\", {\"class\": \"chart-row__song\"})[0].text)\n\n    # Create a Pandas dataframe from the data.\n    df = pd.DataFrame(\n        {\"Current Ranking\": currRank, \"Last Week's Ranking\": pastRank, \"Song Title\": title, \"Artist\": artist,\n         \"Peak Position\": peakPosition, \"Num Weeks in TOP 100\": top100})\n\n    df.to_csv(date.isoformat() + ' Billboard Kpop Rankings.csv', sep=',')\n\n\n\n\nmain()\n", "378": "#import scrappy\nimport scrapy\nfrom ..items import WebscrapeItem\nfrom scrapy.loader import ItemLoader\n# Import the CrawlerProcess: for running the spider\nfrom scrapy.crawler import CrawlerProcess\n\nclass OpenDeltaCrawler(scrapy.Spider):\n    name = \"openDeltaCrawler\"\n\n    def start_requests(self):\n        url = \"https://www.moneyweb.co.za/moneyweb-crypto/\"\n        yield scrapy.Request(url, callback = self.parse_blog)\n\n    def parse_blog(self,response):\n        # Go to the blocks that contain blog posts\n        blog_posts = response.xpath('//h3[contains(@class,\"title list-title m0005\")]')\n        # Go to the blog links\n        blog_links = blog_posts.xpath('./a/@href')\n        print(blog_links)\n        # Extract the links (as a list of strings)\n        links_to_follow = blog_links.extract()\n        print(links_to_follow)\n        # Follow the links in the next parser\n\n\n        for url in links_to_follow:\n            #headers = {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n            yield response.follow(url=url, callback=self.parse_pages)\n\n    def parse_pages(self, response):\n        i = ItemLoader(item=WebscrapeItem(), selector=response)\n        i.add_css('blog_title','h1.article-headline')\n        i.add_css('blog_excerpt','div.article-excerpt')\n        i.add_xpath('blog_author','//div[@class=\"article-meta grey-text\"]/span[1]/span')\n        i.add_xpath('blog_publish_date','//div[@class=\"article-meta grey-text\"]/span[2]')\n        i.add_xpath('blog_text','//p')\n        i.add_value('blog_url',response.url)\n        yield i.load_item()\n\n\n", "379": "from asyncore import write\nimport read_sheet\nimport webscrape\nimport write_sheet\nfrom openpyxl.workbook import Workbook\n\nif __name__ == '__main__':\n    physicians = read_sheet.get_physicians()\n    webscrape.webscrape_list(physicians)\n\n    row = 2\n\n    headers       = ['ID','First Name', 'Last Name', 'Source', 'Overall Rating', 'Residency', \n        'Research Tier', 'Comment Date', 'Comment Rating', 'Comment', 'Comment Helpful Count']\n\n    wb = Workbook()\n    page = wb.active\n    page.title = 'Tier_Data'\n    page.append(headers) # write the headers to the first line\n\n    for physician in physicians:\n        row = write_sheet.write_physician(physician, row, wb, page)\n\n    wb.save(filename = 'Tier_Data.xlsx')\n", "380": "import urllib2\nfrom bs4 import BeautifulSoup\nimport sys\nfrom operator import itemgetter\nimport random\n\n#------------------------For webscrape 1\npage_count_acm=1\n#------------------------For webscrape 1\n\ndictionary = {}\ndkeys={}\ndkeys1={}\nsearchvalue = []\nconcat_string =''\n\n#------------------------For webscrape 2\npage_count_indeed = 0\nc_indeed = 1\ntest_indeed = 0\ncount_total_indeed = 0\ncounter_indeed = 1\ncal_page_indeed = 0\nval_indeed = 0\n#------------------------For webscrape 2\n\n#------------------------For webscrape 3\npage_count_ieee = 1\n#------------------------For webscrape 3\n\n#------------------------Dictionary count\ndic_count = 1\n#------------------------Dictionary count\n\n\n\ndef webscrape(stream,area,field,location):\n\tglobal page_count_acm\n\tglobal dictionary\n\tglobal dic_count\n\tif area == \"\":\n\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'?page='+str(page_count_acm)\n\telif stream == \"\": \n\t\tquote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+area+'?page='+str(page_count_acm)\n\telse: quote_page_acm = 'http://jobs.acm.org/jobs/results/keyword/'+stream+'/'+area+'?page='+str(page_count_acm)\n\tprint quote_page_acm,'\\n'\n\tpage_acm = urllib2.urlopen(quote_page_acm)\n\tsoup_acm = BeautifulSoup(page_acm, 'html.parser')\n\tbox_acm = soup_acm.find_all('div',attrs={'class':'aiResultsMainDiv'})\n\ttemp_acm = soup_acm.find('span',attrs={'class':'aiPageTotalTop'}) \n\tif  temp_acm == None:\n\t\tprint \"No results found\"\n\t\tsys.exit(1)\n\tcount_total_acm = int(temp_acm.get_text())\n\tfor bx_acm in box_acm:\n\t\ttitle_acm = str(bx_acm.find('div',attrs={'class':'aiResultTitle'}).get_text().strip().encode('utf-8'))\n\t\t#print 'Title:',bx.find('div',attrs={'class':'aiResultTitle'}).get_text().strip()\n\t\turl_acm = 'http://jobs.acm.org'+str(bx_acm.find('div',attrs={'class':'aiResultTitle'}).find('h3').find('a').get('href').encode('utf-8'))\n\t\t#print 'URL:',bx.find('div',attrs={'class':'aiResultTitle'}).find('h3').find('a').get('href')\n\t\tdetails_acm = bx_acm.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find_all('li')\n\t\tcompany_acm = str(details_acm[0].get_text().strip().encode('utf-8'))\n\t\t#print 'Company:',details[0].get_text().strip()\n\t\tlocation_acm = str(details_acm[1].get_text().strip().encode('utf-8'))\n\t\t#print 'Location:',details[1].get_text().strip()\n\t\tdate_acm = str(details_acm[2].get_text().strip().encode('utf-8'))\n\t\t#print 'Date:',details[2].get_text().strip()+\"\\n\"\n\t\tif bx_acm.find('li',attrs={'id':'searchResultsCategoryDisplay'}) != None:\n\t\t\tcategory_acm = str(details_acm[3].get_text().strip().encode('utf-8'))\n\t\telse :\n\t\t\tcategory_acm = 'None'\n\t\tif bx_acm.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}) == None:\n\t\t\tdescription_acm = str(bx_acm.find('div',attrs={'class':'aiResultsDescription'}).get_text().strip().encode('utf-8'))\n\t\telse :\n\t\t\tdescription_acm = str(bx_acm.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip().encode('utf-8'))\n\t\t#print 'Description:', bx.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip()\n\t\t\n\t\t\n\n\t\tdictionary[dic_count] = [title_acm,company_acm,location_acm,date_acm,category_acm,description_acm,url_acm,0,'x']\n\t\tjacard(field,location,dic_count)\n\t\tdic_count = dic_count+1\n\n\t\t#if (title_acm,location_acm) in dictionary:\n\t\t#\tdictionary[title_acm,location_acm] = (dictionary[title_acm,location_acm],[title_acm,company_acm,location_acm,date_acm,category_acm,description_acm,0])\n\t\n\t\t#else:\n\t\t#\tdictionary[title_acm,location_acm] = ([title_acm,company_acm,location_acm,date_acm,category_acm,description_acm,0])\n  \n\n\n\n    \t#dont use as of now\n    \t#print 'Company:',bx.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find('li',attrs={'class':'aiResultsCompanyName'}).get_text().strip()\n\t\t#print 'Location:',bx.find('div',attrs={'class':'aiDescriptionPod'}).find('ul').find_all('li')[1].get_text() + \"\\n\"\n\t\n\t\t#print bx.find('div',attrs={'class':'aiResultsDescriptionNoAdvert'}).get_text().strip()\n\t\t#print box.find('div',attrs={'class':'aiResultTitle'})\n\t\t#find('h3').get_text()\n\n\tif (page_count_acm < count_total_acm):\n\t\tpage_count_acm = page_count_acm + 1\n\t\twebscrape(stream,area,field,location)\n\t#for element in dictionary:\n\t#\tprint x\n\t#\tprint y\n\t#\tprint dictionary[x,y][0]\n\t#\tprint dictionary[x,y][1]\n\t#\tprint dictionary[x,y][2]\n\t#\tprint dictionary[x,y][3]\n\t#\tprint \"\\n\"\n\t#print dictionary  \n\n\t\n\n\n\ndef webscrape1(stream,area,field,location):\n\tglobal page_count_indeed\n\tglobal c_indeed\n\tglobal test_indeed\n\tglobal count_total_indeed \n\tglobal counter_indeed\n\tglobal cal_page_indeed\n\tglobal val_indeed\n\tglobal dictionary\n\tglobal dic_count\n\t\n\tquote_page_indeed = 'https://www.indeed.com/jobs?q='+stream+'&l='+area+'&start='+str(page_count_indeed)\n\tprint quote_page_indeed,'\\n'\n\tpage_indeed = urllib2.urlopen(quote_page_indeed)\n\tsoup_indeed = BeautifulSoup(page_indeed, 'html.parser')\n\t\n\t#a = str(soup_indeed)\n\t#f = open('1.html','w')\n\t#f.write(a)\n\t#f.close()\n\t\n\t#print soup_indeed\n\n\t\n\tbox_indeed = soup_indeed.find_all('div',attrs={'class':'row'})\n\ttemmp_indeed = soup_indeed.find('div',attrs={'id':'searchCount'})\n\tif temmp_indeed == None:\n\t\tprint 'No results found'\n\t\tsys.exit(1)\n\tif test_indeed == 0:\n\t\tcount_total_indeed = int(temmp_indeed.get_text().split()[5].replace(',',''))\n\t\twhile val_indeed <= count_total_indeed:\n\t\t\tcal_page_indeed = cal_page_indeed + 1\n\t\t\tval_indeed = 25 * cal_page_indeed\n\t\ttest_indeed = 1\n\t\tcal_page_indeed = cal_page_indeed + 1\n\t\tif cal_page_indeed > 50:\n\t\t\tcal_page_indeed = 50\n\t\t#print page_indeed\n\t\t#print val_indeed\n\t#print count_total_indeed\n\t\n\tfor bx_indeed in box_indeed:\n\t\t#print c_indeed\n\t\t#print 'Title:',bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get_text().strip()\n\t\ttitle_indeed = str(bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get_text().strip().encode('utf-8'))\n\t\t#print 'Company:', bx_indeed.find('span',attrs={'class':'company'}).get_text().strip()\n\t\tcompany_indeed = str(bx_indeed.find('span',attrs={'class':'company'}).get_text().strip().encode('utf-8'))\n\t\t#print 'Location:', bx_indeed.find('span',attrs={'class':'location'}).get_text().strip()\n\t\tlocation_indeed = str(bx_indeed.find('span',attrs={'class':'location'}).get_text().strip().encode('utf-8'))\n\t\t#print 'Description:', bx_indeed.find('span',attrs={'class':'summary'}).get_text().strip(),'\\n'\n\t\tdescription_indeed = str(bx_indeed.find('span',attrs={'class':'summary'}).get_text().strip().encode('utf-8'))\n\t\t#c_indeed = c_indeed+1\n\t\tdate_indeed = 'None'\n\t\tcategory_indeed = 'None'\n\t\turl_indeed = str(bx_indeed.find('a',attrs={'data-tn-element':'jobTitle'}).get('href').encode('utf-8'))\n\t\n\t\tdictionary[dic_count] = [title_indeed,company_indeed,location_indeed,date_indeed,category_indeed,description_indeed,url_indeed,0,'x']\n\t\tjacard(field,location,dic_count)\n\t\tdic_count = dic_count +1\n\n\t#if (title_indeed,location_indeed) in dictionary:\n\t#\tdictionary[title_indeed,location_indeed] = (dictionary[title_indeed,location_indeed],[title_indeed,company_indeed,location_indeed,date_indeed,category_indeed,description_indeed,0])\n\t#else:\n\t#\tdictionary[title_indeed,location_indeed] = ([title_indeed,company_indeed,location_indeed,date_indeed,category_indeed,description_indeed,0])\t\n\n\t\n\tif (counter_indeed", "381": "\"\"\"buyzilla URL Configuration\n\nThe `urlpatterns` list routes URLs to views. For more information please see:\n    https://docs.djangoproject.com/en/3.0/topics/http/urls/\nExamples:\nFunction views\n    1. Add an import:  from my_app import views\n    2. Add a URL to urlpatterns:  path('', views.home, name='home')\nClass-based views\n    1. Add an import:  from other_app.views import Home\n    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')\nIncluding another URLconf\n    1. Import the include() function: from django.urls import include, path\n    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))\n\"\"\"\nfrom django.contrib import admin\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('', views.index, name='index'),\n    path('home/', views.index, name='index'),\n    path('webscrape/', views.webscrape, name='webscrape'),\n    path('home/dashboard/',views.dashboard,name='dashboard'),\n    \n]\n", "382": "from __future__ import absolute_import, unicode_literals\n\nfrom celery import shared_task\n# from webscrape_the_one.webscrape_the_one.celery import app\nfrom .main import main\nfrom .Notifications import send_notifications\n\nfrom celery import task\n\nfrom .models import ProductDetails\nfrom .parallel_fetch import concurrent_map, call_main\nfrom .serializers import ProductDetailsSerializer\n\n\n@task()\ndef scrape_periodically(name='scrape_periodically'):\n    print('here')\n    products = list(ProductDetails.objects.all())\n    product_urls = []\n    names, prices, rating, image_urls = [], [], [], []\n    for product in products:\n        product_urls.append(product.product_url)\n\n    return_values = concurrent_map(call_main, product_urls)\n    # send_notifications(title='Title', message='Message')\n\n    for i in range(len(return_values)):\n        prices.append(return_values[i][1][0])\n\n    for i in range(len(products)):\n        temp = float(products[i].product_price)\n        print(temp, prices[i])\n        if float(prices[i]) != float(products[i].product_price):\n            products[i].product_price = prices[i]\n        if float(prices[i]) <= float(products[i].all_time_low):\n            products[i].all_time_low = prices[i]\n            message = products[i].product_name + ' is at an all low of ' + prices[i]\n            send_notifications(title='Product is at an all time low!', message=message)\n            products[i].save()\n            continue\n        if float(prices[i]) < temp:\n            print(type(products[i].product_name), type(prices[i]))\n            message = products[i].product_name + ' has reduced to ' + str(prices[i]) + ' from ' + str(temp)\n            send_notifications(title='Product price decreased!', message=message)\n        products[i].save()\n", "383": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri May 29 15:07:07 2020\n\n@author: Hayden Rampadarath (haydenrampadarath@gmail.com)\n\nScript to webscrape My Anime List\n\"\"\"\n\n\nimport urllib\nimport requests\nimport bs4\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re\nfrom tqdm import tqdm\nfrom time import sleep\nfrom bs4.element import NavigableString, Tag\n\n\n\ndef extractNavigableStrings(context):\n    \"\"\" from https://stackoverflow.com/questions/29110820/how-to-scrape-between-span-tags-using-beautifulsoup\"\"\"\n    strings = []\n    for e in context.children:\n        if isinstance(e, NavigableString):\n            strings.append(e)\n        if isinstance(e, Tag):\n            strings.extend(extractNavigableStrings(e))\n    return strings\n\n\n\ndef parse_MAl(url):\n    \"\"\"\n    Parameters\n    ----------\n    url : string\n        myanimelist.net url string \n\n    Returns\n    -------\n    df : DataFrame\n        returns a dataframe with columns \"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"\n\n    \"\"\"\n    html = requests.get(url)\n    soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n    results = soup.find_all(class_= \"ranking-list\")\n    \n    df = pd.DataFrame(columns=[\"name\",\"english_name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\", \"url\"])\n    i = 0\n    for result in results:\n        #print(i)\n        url_= result.find(class_=\"hoverinfo_trigger fl-l fs14 fw-b\")[\"href\"]\n        html_ = requests.get(url_)\n        soup_ = BeautifulSoup(html_.content, 'html.parser', from_encoding=\"utf-8\")\n        \n        t1name = extractNavigableStrings(soup_.find(class_=\"h1-title\"))\n        if len(t1name) == 1:\n            name = t1name[0]\n            english_name = None\n        elif len(t1name) >= 2:\n            name=t1name[0]\n            english_name=t1name[1]\n        else:\n            name = None\n            english_name = None\n            \n        Type, Dates, members = result.find(class_=\"information di-ib mt4\").text.strip().splitlines()\n        try:\n            members = float(\"\".join(members.split()[0].split(\",\")))\n        except:\n            members = None\n            \n        [Type_, eps, n] = [\", \".join(x.split()) for x in re.split(r'[()]',Type)]\n        \n        try:\n            eps = float(eps.split(\",\")[0])\n        except:\n            eps = None\n        \n        try:\n            genres = [genre.text.strip() for genre in soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"genre\")]\n            \n        except:\n            genres = None\n        \n        try:\n            score = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingValue\")[0].text.strip())\n        except:\n            score = None\n        #try:\n        #    score_members = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingCount\")[0].text.strip())\n        #except:\n         #   score_members = None\n        \n        df = df.append({\n            \"name\": name,\n            \"english_name\":english_name,\n            \"type\": Type_,\n            \"episodes\": eps,\n            \"members\": members,\n            #\"score_members\": score_members,\n            \"rating\": score,\n            \"genre\": genres,\n            \"dates\": Dates,\n            \"url\": url_\n        },ignore_index=True)\n        \n        i+=1\n    return df\n\n\ndef webscrape_MAl(anime_limit=16750, start=0):\n    url_template = \"https://myanimelist.net/topanime.php?limit={}\"\n    df = pd.DataFrame(columns=[\"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"])\n    for limit in tqdm(range(start,anime_limit, 50)): # iterate in steps of 50\n        url = url_template.format(limit)\n        df_temp = parse_MAl(url)\n        if df_temp[\"name\"].isnull().sum() >= 40:\n            print(\"Number of missing names, for limit {} = {}\".format(limit, df_temp[\"name\"].isnull().sum()))\n            print(\"--------Halting---------\")\n            raise SystemExit()\n        save_mal_temp(df_temp, limit)\n        \n        # I think MAL has a limit on the number of conenctions per minute/second/hour\n        # and after 200-400, the site blocks access. Adding the pause for 1 minute below soleves the issue\n        sleep(60) # pause the loop for 1 minute. \n        \n\n\n\n\n\ndef save_mal_temp(df, limit):\n    csvTemp = \"temp/MAL_start_{}.csv\".format(limit)\n    df.to_csv(csvTemp)\n        \n    #print(\"Number of missing names, for limit {} = {}\".format(limit, df[\"name\"].isnull().sum()))\n    \n    \n\nwebscrape_MAl(anime_limit = 16750, start = 16550)", "384": "# Import necessary packages\nimport requests, time, re, os\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport functions as fns\n\n# Set the path for exporting files\nusername = os.environ['username']\npath = r'C:\\Users' + '\\\\' + username + r'\\Desktop\\Test'\nif not os.path.exists(path):\n    os.makedirs(path)\n\n# Set url and access the site with requests lib\nurl_old = 'https://www.niu.edu/publicsafety/emergency/safetybulletin/archive.shtml'\nresponse_old = urlopen(url_old)\nurl_new = 'https://www.niu.edu/publicsafety/emergency/safetybulletin/index.shtml'\nresponse_new = urlopen(url_new)\n\n# Parse the html with BeautifulSoup and find all the  tag\nsoup_old = BeautifulSoup(response_old, 'lxml')\nrows_old = soup_old.find_all('tr')\nsoup_new = BeautifulSoup(response_new, 'lxml')\nrows_new = soup_new.find_all('tr')\n\n# Calulate elapsed time and cpu time (start)\nstart_elapsed = time.perf_counter()\nstart_cpu = time.process_time()\n\n# Put the data that was scraped from web into a dataframe using the function\n# \"process_records(x)\" that stores in the functions.py file\nlst_old = fns.process_records(rows_old)\nlst_new = fns.process_records(rows_new)\n\n# Calulate time (end)\nend_elapsed = time.perf_counter()\nend_cpu = time.process_time()\n\nprint()\nprint('Elapsed time is: ', end_elapsed - start_elapsed)\nprint('CPU time is:     ', end_cpu - start_cpu)\nprint()\n\n#**************** Export data scrapped from websites **************************\n# Set the maximum number of columns so that all the df columns can be shown\npd.set_option('display.max_columns', 100)\n\n# Convert the list into df\ndf_old = pd.DataFrame(lst_old)\ndf_new = pd.DataFrame(lst_new)\n\n# Combine two dfs\nframes = [df_new, df_old]\ndf = pd.concat(frames, ignore_index = True)\n\n# Export the two df to csv files. (will overwrite existing files)\ndf.to_csv(path + r'\\webscrape_bulletin.csv', index = False, encoding = 'utf-8-sig')\n\n#***************************** Data Cleaning **********************************\n# Clean up the DataFrame using \"clean_df(df)\" function\ndf_clean = fns.clean_df(df)\n\n# Export the two df to csv files. (will overwrite existing files)\ndf_clean.to_csv(path + r'\\clean_bulletin.csv', index = False, encoding = 'utf-8-sig')\n\nprint('Both \"webscrape_bulletin.csv\" and \"clean_bulletin.csv\" exported successfully!\\n')\n", "385": "from api.googleAPI import GoogleAPI\nfrom api.websiteScrape import WebsiteScrape\n\nfrom utils.data import *\nfrom utils.const import __CUR_DIR__\nimport openpyxl\n\na = openpyxl\ndef places_search(area_table: TableManger, keyword: str, table: TableManger, call_api):\n\n    if call_api is True:\n        searches = list_search_strings(keyword, area_table)\n\n        for search in searches:\n            GoogleAPI.places_search(search)\n\n        df = places_search_to_business_directory()\n    else:\n        table.load_df(\".json\")\n        df = table.get_df()\n    df = extract_add_postcode(df)\n    #df = postcode_to_authcode(df)\n    #df = authcode_to_income(df)\n    #df = create_scores(df)\n    table.save_df(df, \".json\")\n\n\ndef detailed_search(table: TableManger, call_api):\n    df = table.get_df()\n\n    if call_api is True:\n        for index, row in df.iterrows():\n           GoogleAPI.detailed_search(row['place_id'])\n\n    df = detailed_search_to_business_directory(df)\n    #df = is_chain(df)\n    table.save_df(df, \".json\")\n\n\ndef webscrape(table: TableManger, call_api):\n    df = table.get_df()\n\n    if call_api is True:\n        website_list = df[df['website'] != \"\"]['website'].tolist()\n\n        tot = len(website_list)\n        i = 0\n        for website in website_list:\n          WebsiteScrape.scrape(website)\n          print(str(i*100/tot) + \"% Complete\")\n          i = i + 1\n\n    df = webscrape_to_business_directory(df)\n\n    df = cleanup_emails(df)\n    df = create_score(df)\n    table.save_df(df, \".json\")\n\n\ndef export(table: TableManger):\n    print(\"Export\")\n    df = table.get_df()\n    file_path = table.file_path\n    #\n    # print(file_path)\n    # i = input(\"aa\")\n    #\n    # writer = pd.ExcelWriter(os.path.join(__CUR_DIR__,\"test.xlsx\"), engine='openpyxl')\n    # df.to_excel(writer, index=False)\n    # writer.save()\n\n    table.save_df(df, \".xlsx\")\n    table.save_df(df[df[\"advanced_keyword_count\"] > 0], \".xlsx\", file_path+\"_filtered\")\n\n# class Pipeline:\n#     pipeline = []\n#     pipeline_inp = {}\n#     input_data = None\n#\n#     def __init__(self, pipeline, pipeline_inp, datalocation):\n#         self.pipeline = pipeline\n#         self.pipeline_inp = pipeline_inp\n\n\n\n", "386": "# coding=utf-8\n\nimport logging\nlogging.basicConfig( format='%(levelname)-8s %(asctime)s %(filename)s %(lineno)d %(message)s', level=logging.DEBUG )\n\nclass WebScrape :\n\n    def __init__(self) :\n        pass\n    pass\n\n    def doScrape(self) : \n        import xlsxwriter\n        workbook = xlsxwriter.Workbook( \"\uc624\ub298\uc758 \uc99d\uad8c\uc2dc\uc138.xlsx\" )\n        worksheet = workbook.add_worksheet()\n        row = 0\n        col = 0 \n\n        line = \"\\n\" + \"#\"*80 + \"\\n\"\n\n        # import libraries\n        from bs4 import BeautifulSoup\n\n        # specify the url\n        html_url = \"http://vip.mk.co.kr/newSt/rate/item_all.php\"\n\n        # query the website and return the html to the variable \u2018page\u2019\n\n        logging.info( \"Getting a html data from %s\" % html_url )\n\n        from urllib.request import urlopen\n        html_page = urlopen( html_url )\n        html_src = html_page.read()\n\n        logging.info( \"Done. Getting a html data from %s\" % html_url )\n\n        # parse the html using beautiful soup and store in variable `soup`\n        soup = BeautifulSoup( html_src, \"html.parser\" , from_encoding=\"euc-kr\" )\n\n        print( \"title = %s\" % soup.title ) \n        print( \"title.string = %s\" % soup.title.string )\n        table = soup.table        \n        table_subs = table.find_all( \"table\" )\n        print( line )         \n        jisu = table_subs[ 1 ]\n        trs = jisu.find_all( \"tr\" )\n        for tr in trs :\n            tds = tr.find_all( \"td\" )\n            col = 0 \n            for td in tds : \n                print( \"td = %s\" % td.string )\n                worksheet.write(row, col, td.string )\n                col += 1\n            pass\n            row += 1\n        pass \n\n        jisu = table_subs[ 4 ]\n        trs = jisu.find_all( \"tr\" )\n        for tr in trs :\n            tds = tr.find_all( \"td\" )\n            col = 0 \n            for td in tds : \n                print( \"td = %s\" % td.string )\n                worksheet.write(row, col, td.string )\n                col += 1\n            pass\n            row += 1\n        pass \n        print( line )\n        workbook.close() \n    pass\n\npass\n\nif __name__ == '__main__' :   \n    web_scrape = WebScrape()\n    web_scrape.doScrape()\npass", "387": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nDescription: Webscrape Futures Price of Relevant MOPS Commodities for Rest of 2022\nAuthor: @sihir_sains\n\"\"\"\n\n#############################################################################\n#   0. Load Necessary Library and Set Up Necessary Variables\n#############################################################################\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait as WDW\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom multiprocessing import get_context\n\nimport pandas as pd\nimport numpy as np\nimport time \nimport re\n\nexec_path = \"\"\nout_path =  \"\"\n\nurl_mo95 = 'https://www.tradingview.com/chart/?symbol=NYMEX%3AAV0'\nurl_mo92 = 'https://www.tradingview.com/chart/?symbol=NYMEX%3AN1B'\n\nurl_gs10 = 'https://www.tradingview.com/chart/?symbol=NYMEX%3ASGB'\nurl_gs500 = 'https://www.tradingview.com/chart/?symbol=NYMEX%3AGHS'\n\noptions = Options()\noptions.add_argument('--disable-popup-blocking') # differ on driver version. can ignore. \noptions.add_argument('--disable-notifications') # differ on driver version. can ignore. \n\n# Get List of Months for Futures Price in NYMEX\nnymex_mo = ['F','G','H','J','K','M','N','Q','U','V','X','Z']\nyrs = ['2022']\n\n#############################################################################\n#   1. Define Sub-Functions\n#############################################################################\n\n## To wait when initializing the page until elements are located\ndef wait_at_start(driver):\n    while True:\n        try:\n            WDW(driver,10).until(EC.presence_of_element_located(\n                (By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]')))\n            break\n        except:\n            driver.refresh()\n            continue\n\n## To fetch closing price at a given date\ndef fetch_price(driver):\n    date  = WDW(driver,5).until(EC.presence_of_element_located(\n            (By.XPATH , '''/html/body/div[2]/div[1]/div[2]/div[1]/div/table/\n             tr[1]/td[2]/div/div[2]/div[1]/div[1]/div[1]/div[1]/div[1]'''))).text\n    close = WDW(driver,5).until(EC.presence_of_element_located(\n            (By.XPATH , '''/html/body/div[2]/div[6]/div/div[1]/div[1]/div[5]/\n             div/div[2]/div[1]/div[2]/div[2]/div[4]/div[2]/span'''))).text\n    return date, close\n\ndef extract_date_string(txt):\n    return re.findall(r'\\((.*?)\\)',txt)[1].title()\n\n#############################################################################\n#   2. Create Wrapper Function for Webscraping\n#############################################################################\n\ndef webscrape_fut_prc(comms):\n    \n    ### Variables and Dataframe Setup, Initialize Webdriver ###\n        \n    url_comms = mp_dict[comms]\n    cols = ['date',comms]\n    fut_df = pd.DataFrame(columns=cols)\n    \n  \n    driver = webdriver.Chrome(executable_path=exec_path,options=options)\n    \n    ### Loop to Obtain YTD Price Dataset ###\n    for yr in yrs:\n        for mo in nymex_mo:       \n            driver.get(url_comms+mo+yr)\n            \n            wait_at_start(driver)\n            \n            if (yr == yrs[0]) & (mo == nymex_mo[0]):\n                WDW(driver,10).until(EC.presence_of_element_located(\n                    (By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]'))).click()\n            else:\n                WDW(driver,10).until(EC.presence_of_element_located(\n                    (By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]'))).click()\n                WDW(driver,10).until(EC.presence_of_element_located(\n                    (By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]'))).click()\n            \n            time.sleep(2)\n            temp_ls = [fetch_price(driver)]\n            temp_df = pd.DataFrame(temp_ls,columns=cols)\n            \n            fut_df = pd.concat([fut_df,temp_df],axis=0)\n            time.sleep(2)\n            \n            \n    \n    ### Ensure No Duplicate Date (Same Date, Different Closing Price) ###\n    fut_df['date'] = np.vectorize(extract_date_string)(fut_df['date'])\n\n    fut_df = fut_df.drop_duplicates().reset_index(drop=True)\n    fut_df['dup'] = fut_df.duplicated(subset='date')\n    assert fut_df['dup'].unique().shape[0], \"Duplicate obs identified!\"\n    \n    ### Clean Price Dataset ###\n    \n    fut_df['month'] = pd.to_datetime(fut_df['date']).dt.to_period('M')\n    fut_df = fut_df[['month',comms]].set_index(['month'])\n    \n    ### Close Webdriver ###\n    try:\n        driver.close()\n    except:\n        alert = WDW(driver, 3).until(EC.alert_is_present())\n        alert.accept()\n        driver.close()\n\n    \n    return fut_df\n\n#############################################################################\n#   3. Fetch Closing Price Time Series by Commodities\n#############################################################################\n\n### Multiprocess the webscraping\n\ncolname = ['mogas_92','mogas_95','gasoil_10','gasoil_500']\nto_mp = [url_mo92, url_mo95, url_gs10, url_gs500]\nmp_dict = dict(zip(colname,to_mp))\n\nif __name__ == '__main__':\n    with get_context(\"spawn\").Pool(processes=4) as p:\n        res = p.map(webscrape_fut_prc,mp_dict)\n        p.close()\n        p.join()\n        \n        consol_prc = pd.concat(res,axis=1)\n                \nelse:\n    pass\n\n#############################################################################\n#   4. Clean and Process the Platts Price Dataset\n#############################################################################\n    \ntry:\n    print(consol_prc)\n    consol_prc.to_csv(out_path+'data/platts_price_futures.csv') \nexcept:\n    pass\n", "388": "#! /usr/bin/env python3\n\"\"\"VirtualHerbarium scrapes the web for information about plants and outputs them in a nice pdf page\nmade by dr. Matteo Jucker Riva, Geography and PhD and data scientist\n\"\"\"\n# -*- coding: utf-8 -*-\n\"\"\"main module calling all other operations to build a VirtualHerbarium\nOperations are: \n1. create or check output folder\n2. import plant list name\n3. scrape data about plant name from the following websites:\n    3.1 Wikipedia\n    3.2 World Flore Online\n    3.3 CABI repository of invasive plants\n    3.4 JSTOR (maybe)\n\nNOTES: This is the simplified version, no OOP no spyders, no shenanigans\n\"\"\"\n\n# ----------Settings\nimport csv\nfrom utils import *\nfrom collections import OrderedDict\nfrom wikiScraping import *\n# import sys\n# sys.path.append('/usr/local/lib/python3.6/dist-packages')\n# print(\" \\n %s\" % (sys.path))\n\n# ---------Parameters\ninputPath = '/home/matt/Dropbox/github/VirtualHerbarium/Tests/TestInputFile.csv'\n# get input from csv file\ninputDicList=getInputAsDictionary(inputPath, colName=colName)\nplantObjList=[ Plant(i) for i in inputDicList]\n# [i.__str__() for i in plantObjList]\n\n#     lineCount = 0\n#     # get list of plant names\n\n# just for testing\ncurrentPlant = plantObjList[1]\n\n# webscrapeWikipedia\ncurrentPlantWiki=wikiScraper(\"wikipedia\", currentPlant.name, wikiSectionTitles )\ncurrentPlantWiki.webscrapeWikipedia()\n\n\n# get wikipedia page\ndef checkBeforeScraping(wikiText):\n    for i in ['journalName', 'plantName', 'sectionTitles']:\n        assert(getattr(wikiText, i )), f\"{i} is still missing, cannot scrape wikipedia\"\n\ncheckBeforeScraping(wikiText)\ni=wikiText.plantName\n\nwiki_wiki = wiki.Wikipedia('en')\npage_py = wiki_wiki.page(wikiText.plantName)\n# select sections\n\nsectionsTitle = [i.title for i in page_py.section]\na= page_py.sections[-3]\nc=(a.text)\ndir(page_py)\n\nchosenTitle = \"Uses\"\nchosenTitleIndex = sectionsTitle.index(chosenTitle)\nsectionText = page_py.sections[chosenTitleIndex]\n\n# extract relevant paragraph with scrappy\n\n# get cabi page and save it\n# extract relevant paragraph with scrappy\n\n# get world flora and save it\n\n# extract relevant paragraph with scrappy\n", "389": "# -*- mode: python -*-\n\nblock_cipher = None\n\n\na = Analysis(['webscrape_cryptocurrency.py'],\n             pathex=['C:\\\\Users\\\\nelso_8bi0ds3\\\\Desktop\\\\webscraping\\\\cryptocurrency-updater'],\n             binaries=[],\n             datas=[],\n             hiddenimports=[],\n             hookspath=[],\n             runtime_hooks=[],\n             excludes=[],\n             win_no_prefer_redirects=False,\n             win_private_assemblies=False,\n             cipher=block_cipher)\npyz = PYZ(a.pure, a.zipped_data,\n             cipher=block_cipher)\nexe = EXE(pyz,\n          a.scripts,\n          a.binaries,\n          a.zipfiles,\n          a.datas,\n          name='webscrape_cryptocurrency',\n          debug=False,\n          strip=False,\n          upx=True,\n          runtime_tmpdir=None,\n          console=False , icon='C:\\\\Users\\\\nelso_8bi0ds3\\\\Desktop\\\\webscraping\\\\cryptocurrency-updater\\\\crypto.ico.ico')\n", "390": "from django.apps import AppConfig\n\n\nclass WebscrapeConfig(AppConfig):\n    name = 'webscrape'\n", "391": "'''\nCreated on 2021-06-23\n\n@author: wf\n'''\nfrom lodstorage.jsonable import JSONAble, JSONAbleList\nfrom lodstorage.lod import LOD\nfrom datetime import datetime\nfrom mwdocker.webscrape import WebScrape\nimport os\nimport urllib\n\nclass ExtensionList(JSONAbleList):\n    '''\n    represents a list of MediaWiki extensions\n    '''\n    def __init__(self):\n        '''\n        constructor\n        '''\n        super(ExtensionList, self).__init__('extensions', Extension)\n    \n    @staticmethod\n    def storeFilePrefix():\n        '''\n        get my storeFilePrefix\n        \n        Returns:\n            str: the path to where my stored files (e.g. JSON) should be kept\n        '''\n        scriptdir=os.path.dirname(os.path.realpath(__file__))\n        resourcePath=os.path.realpath(f\"{scriptdir}/resources\")\n        storeFilePrefix=f\"{resourcePath}/extensions\"\n        return storeFilePrefix\n    \n    @classmethod\n    def fromSpecialVersion(cls,url:str,excludes=[\"skin\",\"editor\"],showHtml=False,debug=False):\n        '''\n        get an extension List from the given url\n        \n        Args:\n            url(str): the Special:Version MediaWiki page to read the information from\n            exclude (list): a list of types of extensions to exclude\n            showHtml(bool): True if the html code should be printed for debugging\n            debug(bool): True if debugging should be active\n            \n        Returns:\n            ExtensionList: an extension list derived from the url\n        '''\n        webscrape=WebScrape()\n        soup=webscrape.getSoup(url, showHtml=showHtml)\n        \n        # search for\n        # \n        exttrs=soup.findAll(attrs={\"class\" : \"mw-version-ext\"})\n        extList=ExtensionList()\n        for exttr in exttrs:\n            if showHtml:\n                print (exttr)\n            doExclude=False\n            for exclude in excludes:\n                if f\"-{exclude}-\" in exttr.get(\"id\"):\n                    doExclude=True\n            if not doExclude:\n                ext=Extension.fromSpecialVersionTR(exttr,debug=debug)\n                if ext:\n                    extList.extensions.append(ext)\n        return extList\n        \n        \n    @classmethod \n    def restore(cls):\n        '''\n        restore \n        '''\n        extList=ExtensionList()\n        extList.restoreFromJsonFile(ExtensionList.storeFilePrefix())\n        return extList\n        \n    def save(self):\n        '''\n        save the extension list\n        '''\n        super().storeToJsonFile(ExtensionList.storeFilePrefix())\n    \nclass Extension(JSONAble):\n    '''\n    represents a MediaWiki extension\n    '''\n    @classmethod\n    def getSamples(cls):\n        samplesLOD = [{\n            \"name\": \"Admin Links\",\n            \"extension\": \"AdminLinks\",\n            \"url\": \"https://www.mediawiki.org/wiki/Extension:Admin_Links\",\n            \"purpose\": \"\"\"Admin Links is an extension to MediaWiki that defines a special page, \"Special:AdminLinks\",\nthat holds links meant to be helpful for wiki administrators;\nit is meant to serve as a \"control panel\" for the functions an administrator would typically perform in a wiki.\nAll users can view this page; however, for those with the 'adminlinks' permission (sysops/administrators, by default),\na link to the page also shows up in their \"Personal URLs\", between \"Talk\" and \"Preferences\".\"\"\",\n            \"since\": datetime.fromisoformat(\"2009-05-13\"),\n            \"giturl\":\"https://gerrit.wikimedia.org/r/mediawiki/extensions/AdminLinks.git\",\n            \"localSettings\": \"\"\n        }]\n        return samplesLOD\n    \n    @classmethod\n    def fromSpecialVersionTR(cls,exttr,debug=False):\n        '''\n        Construct an extension from a beautifl soup TR tag\n        derived from Special:Version\n        \n        Args:\n            exttr: the beautiful soup TR tag\n            debug(bool): if True show debugging information\n        '''\n        ext=None\n        extNameTag=exttr.find(attrs={\"class\" : \"mw-version-ext-name\"})\n        extPurposeTag=exttr.find(attrs={\"class\" : \"mw-version-ext-description\"})\n        if extNameTag:\n            ext=Extension()\n            ext.name=extNameTag.string\n            ext.extension=ext.name.replace(\" \",\"\")\n            ext.url=extNameTag.get(\"href\")\n            if extPurposeTag and extPurposeTag.string:\n                ext.purpose=extPurposeTag.string\n            ext.getDetailsFromUrl(debug=debug)\n        return ext\n\n    def __init__(self):\n        '''\n        Constructor\n        '''\n        \n    def __str__(self):\n        text=\"\"\n        delim=\"\"\n        samples=self.getJsonTypeSamples()\n        for attr in LOD.getFields(samples):\n            if hasattr(self, attr):\n                text+=f\"{delim}{attr}={self.__dict__[attr]}\"\n                delim=\"\\n\"\n        return text\n    \n    def getDetailsFromUrl(self,showHtml=False,debug=False):\n        '''\n        get more details from my url\n        '''\n        webscrape=WebScrape()\n        try:\n            soup=webscrape.getSoup(self.url, showHtml=showHtml)\n            for link in soup.findAll('a',attrs={\"class\" : \"external text\"}):\n                if (\"GitHub\" == link.string) or (\"git repository URL\") == link.string:\n                    self.giturl=link.get('href')\n        except urllib.error.HTTPError as herr:\n            if debug:\n                print(f\"HTTPError {str(herr)} for {self.url}\")\n        \n    \n    def asWikiMarkup(self):\n        '''\n        return me as wiki Markup\n        '''\n        samples=self.getJsonTypeSamples()\n        nameValues=\"\"\n        for attr in LOD.getFields(samples):        \n            if hasattr(self, attr):\n                nameValues+=f\"|{attr}={self.__dict__[attr]}\\n\"\n        wikison=f\"\"\"{{{{Extension\n{nameValues}\n}}}}\"\"\"\n        return wikison\n    \n    \n    def getLocalSettingsLine(self,mwShortVersion:str):\n        '''\n        get my local settings line\n        \n        Args:\n            mwShortVersion(str): the MediaWiki short version e.g. 127\n        \n        Returns:\n            entry for LocalSettings\n        '''\n        localSettingsLine=f\"wfLoadExtension( '{self.extension}' );\"\n        if hasattr(self,\"require_once_until\"):\n            if self.require_once_until>=mwShortVersion:\n                localSettingsLine=f'require_once \"$IP/extensions/{self.extension}/{self.extension}.php\";'\n\n        if hasattr(self,\"localSettings\"):\n            localSettingsLine+=f\"\\n  {self.localSettings}\"\n        return localSettingsLine\n    \n    def asScript(self,branch=\"master\"):\n        '''\n        return me as a shell Script command line list\n        \n        Args:\n            branch(str): the branch to clone \n        '''\n        if hasattr(self, \"giturl\"):\n            if \"//github.com/wikimedia/\" in self.giturl:\n                # glone from the branch\n                return (f\"git clone {self.giturl} --single-branch --branch {branch} {self.extension}\")\n            else:    \n                return (f\"git clone {self.giturl} {self.extension}\")\n        else:\n            text = \"# no installation script command specified\"\n            if hasattr(self,\"composer\"):\n                text+=f\"\\n# installed with composer require {self.composer}\"\n            return text\n        ", "392": "# author: Bartlomiej \"furas\" Burek (https://blog.furas.pl)\n# date: 2022.05.29\n\n# [web scraping - How to webscrape an interactive webpage with python - Stack Overflow](https://stackoverflow.com/questions/72423199/how-to-webscrape-an-interactive-webpage-with-python/72425518#72425518)\n    \nimport requests\n\nurl = 'http://chonos.ifop.cl/flow/mapclick'\n\nparams = {\n    'REQUEST': 'GetFeatureInfo',\n    'SERVICE': 'WMS',\n    'SRS': 'EPSG:4326',\n    'STYLES': '',\n    'TRANSPARENT': 'true',\n    'VERSION': '1.1.1',\n    'FORMAT': 'image.png',\n    'BBOX': '-84.48486328125,-50.16282433381728,-59.54589843750001,-45.75219336063107',\n    'HEIGHT': '300',\n    'WIDTH': '1135',\n    'LAYERS': 'aguadulce:outlet_points',\n    'QUERY_LAYERS': 'aguadulce:outlet_points',\n    'INFO_FORMAT': 'text.html',\n    'LAT': '-46.528634695271684',\n    'LON': '-71.41113281250001',\n    'X': '595',\n    'Y': '51',\n}\n\nresponse = requests.get(url, params=params)\n\ndata = response.json()\n\n# show first 5 values\nfor item in data['series']['sim'][:5]:\n    print(item)\nprint('...')    \n\n\n\n\n", "393": "import folium\nimport pandas\nfrom selenium import webdriver\nimport os\nfrom selenium.common.exceptions import NoSuchElementException\n\nreader = pandas.read_csv(\"city_of_new_york.csv\")\nmap = None\n\ndef createMap():\n    map = folium.Map(location=[40.7128, -74.0060],\n                    zoom_start=12,\n                    tiles='Stamen Terrain')\n    absPath, filename = os.path.split(os.path.abspath(__file__))\n\n    for i in range(100):\n        x = reader[\"NUMBER\"][i] + \" \" + reader[\"STREET\"][i]\n        color1 = webScrape(x, reader[\"POSTCODE\"][i])\n        folium.CircleMarker([reader[\"LAT\"][i],reader[\"LON\"][i]], radius = 5, popup=reader['STREET'][i], color = color1).add_to(map)\n\n    map.save(absPath + '/map.html')\n\ndef webScrape(addr, zip):\n    absPath, filename = os.path.split(os.path.abspath(__file__))\n    browser = webdriver.Chrome(executable_path=absPath + '/chromedriver')\n    url = 'https://datawarehouse.hrsa.gov/tools/analyzers/geo/ShortageArea.aspx'\n    browser.get(url)\n    address = browser.find_element_by_name(\"ctl00$ctl00$MainContent$ContentPlaceHolder1$tbxAddress\")\n    city = browser.find_element_by_name(\"ctl00$ctl00$MainContent$ContentPlaceHolder1$tbxCity\")\n    postalCode = browser.find_element_by_name(\"ctl00$ctl00$MainContent$ContentPlaceHolder1$tbxZIP\")\n\n    address.send_keys(str(addr))\n    city.send_keys(\"New York\")\n    postalCode.send_keys(str(zip))\n    browser.find_element_by_name(\"ctl00$ctl00$MainContent$ContentPlaceHolder1$lstState\").send_keys(\"New York\")\n    browser.find_element_by_name(\"ctl00$ctl00$MainContent$ContentPlaceHolder1$btnDrill\").click()\n    try:\n        a = browser.find_element_by_id(\"redesignDiv\")\n        a = str(a)\n        if a.count(\"No\") == 0:\n            browser.close()\n            return \"#18ff01\"\n        elif a.count(\"No\") == 1:\n            browser.close()\n            return \"#ffff00\"\n        elif a.count(\"No\") == 2:\n            browser.close()\n            return \"#ffa200\"\n        elif a.count(\"No\") == 3:\n            browser.close()\n            return \"#ffa1b2\"\n        elif a.count(\"No\") == 4:\n            browser.close()\n            return \"#ff2600\"\n    except NoSuchElementException:\n        browser.close()\n        pass\n\n\ncreateMap()", "394": "from django.urls import path, include\n\nfrom core import views\n\nurlpatterns = [\n    path('get-location/', views.getLocation),\n    path('create-diagnosis/', views.createDiagnosis),\n    path('web-scrape/', views.webScrape),\n    path('get-data/', views.getUserData),\n    path('diagnoses/', views.DiagnosesList.as_view()),\n]", "395": "import web_scraper\nimport tokenize_content\nimport content_extractor\nimport sentence_segmentation\nimport pos_tagger\nimport stemmer\nimport config\nimport os\nimport sys\nimport argparse\n\ndef webscrape(domain):\n    web_scraper.scrape_web(domain)\n\ndef tokenize(domain):\n    tokenize_content.tokenize(domain,config.get_all_data(domain),\"Before Stemming\")\n\ndef stem(domain):\n    tokens_data, len_dist = tokenize_content.tokenize(domain,config.get_all_data(domain),\"After Stemming\",False)\n    stemmer.stem(domain,tokens_data,len_dist)\n\ndef sentence_segment(domain):\n    sentence_segmentation.segment_sentence(domain,config.get_all_data(domain))\n\ndef pos_tag(domain):\n    pos_tagger.pos_tag(domain,sentence_segmentation.segment_sentence(domain,config.get_all_data(domain),True))\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-d\", required = True, help=\"Domain - [domain-1, domain-2, domain-3]\")\n    parser.add_argument(\"-t\", \"-task\" , required = True, help=\"Task to Run - [webscrape,tokenize,stem,sentence_segment,pos_tag]\")\n\n    args = vars(parser.parse_args())\n    globals()[args['t']](args[\"d\"])\n\nif __name__ == \"__main__\":\n    main()\n", "396": "import web_scraper\nimport tokenize_content\nimport content_extractor\nimport sentence_segmentation\nimport pos_tagger\nimport stemmer\nimport config\nimport os\nimport sys\nimport argparse\n\ndef webscrape(domain):\n    web_scraper.scrape_web(domain)\n\ndef tokenize(domain):\n    tokenize_content.tokenize(domain,config.get_all_data(domain),\"Before Stemming\")\n\ndef stem(domain):\n    tokens_data, len_dist = tokenize_content.tokenize(domain,config.get_all_data(domain),\"After Stemming\",False)\n    stemmer.stem(domain,tokens_data,len_dist)\n\ndef sentence_segment(domain):\n    sentence_segmentation.segment_sentence(domain,config.get_all_data(domain))\n\ndef pos_tag(domain):\n    pos_tagger.pos_tag(domain,sentence_segmentation.segment_sentence(domain,config.get_all_data(domain),True))\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-d\", required = True, help=\"Domain - [domain-1, domain-2, domain-3]\")\n    parser.add_argument(\"-t\", \"-task\" , required = True, help=\"Task to Run - [webscrape,tokenize,stem,sentence_segment,pos_tag]\")\n\n    args = vars(parser.parse_args())\n    globals()[args['t']](args[\"d\"])\n\nif __name__ == \"__main__\":\n    main()\n", "397": "from django.conf.urls import patterns, include, url\r\nfrom django.contrib import admin\r\n\r\nurlpatterns = patterns('',\r\n    url(r'^$', 'wishableapp.views.home', name='home'),\r\n    url(r'^login$', 'django.contrib.auth.views.login', \r\n    \t{'template_name':'login.html'}, name='login'),\r\n    url(r'^register$', 'wishableapp.views.register', name='register'),\r\n    url(r'^confirm-registration/(?P[a-zA-Z0-9_@\\+\\-]+)/(?P[a-z0-9\\-]+)$', \r\n        'wishableapp.views.confirm_registration', name='confirm'),\r\n    url(r'^logout$', 'django.contrib.auth.views.logout_then_login', name='logout'),\r\n    url(r'^wishing-well$', 'wishableapp.views.wishing_well', name='wishing-well'),\r\n    # url(r'^profile/(?P.+)$', 'wishableapp.views.profile', name='profile'),\r\n    # url(r'^profile$', 'wishableapp.views.profile', name='profile'),\r\n    url(r'^profile/(?P\\w+)$', 'wishableapp.views.profile', name='profile'),\r\n    # url(r'^webscrape-demo$', 'wishableapp.views.webscrape_demo', name='demo'),\r\n    url(r'^catalog/(?P\\w+)$', 'wishableapp.views.catalog', name='catalog'),\r\n    url(r'^photo/(?P\\d+)$', 'wishableapp.views.get_photo', name='photo'),\r\n    # url(r'^add-to-wishlist/(?P\\d+)/(?P\\d+)$', 'wishableapp.views.add_to_wishlist', name='add-to-wishlist'),\r\n    url(r'^add-to-wishlist/$', 'wishableapp.views.add_to_wishlist', name='add-to-wishlist'),\r\n    url(r'^scrape-product/$', 'wishableapp.views.scrape_product', name='scrape-product'),\r\n    url(r'^add-new-product/$', 'wishableapp.views.add_new_product', name='add-new-product'),\r\n    url(r'^product-photo/(?P\\d+)$', 'wishableapp.views.get_product_photo', name='product-photo'),\r\n    url(r'^wishlist/(?P\\d+)$', 'wishableapp.views.wishlist', name='wishlist'),\r\n    url(r'^delete-wishlist-item/$', 'wishableapp.views.delete_wishlist_item', name='delete-wishlist-item'),\r\n    url(r'^follow/(?P\\w+)$', 'wishableapp.views.follow', name='follow'),\r\n    url(r'^unfollow/(?P\\w+)$', 'wishableapp.views.unfollow', name='unfollow'),\r\n    url(r'^delete-wishlist/$', 'wishableapp.views.delete_wishlist', name='delete-wishlist'),\r\n    url(r'^wishlist-item/(?P\\d+)$', 'wishableapp.views.wishlist_item', name=\"wishlist-item\"),\r\n    url(r'^product/(?P\\d+)$', 'wishableapp.views.product', name=\"product\"),\r\n    url(r'^edit-profile/(?P\\w+)$', 'wishableapp.views.editProfile', name=\"editProfile\"),\r\n    url(r'^add-pre-companies/(?P\\d+)$', 'wishableapp.views.addPrefCompanies', name=\"addPrefCompanies\"),\r\n    url(r'^remove-pre-companies/(?P\\d+)$', 'wishableapp.views.removePrefCompanies', name=\"removePrefCompanies\"),\r\n    url(r'^fairyGodMother/(?P\\d+)$', 'wishableapp.views.fairyGodMother', name=\"fairyGodMother\"),\r\n    url(r'^editWishList/(?P\\d+)$', 'wishableapp.views.editWishList', name=\"editWishList\"),\r\n)\r\n", "398": "def run():\n    from urllib.request import urlopen\n    from bs4 import BeautifulSoup\n\n    ##### Link collector to send crawler to iteratively\n\n    import requests\n\n    baseurl = 'https://torontosun.com'\n\n    headers = {\n        'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.128 Safari/537.36'\n    }\n\n    columnlinks = []\n\n    # Pulling the urls from the last 5 pages worth of columns from here: https://torontosun.com/category/opinion/columnists/?from=1\n        \n    for x in range(900,925,25):\n        r = requests.get(f'https://torontosun.com/category/opinion/columnists/?from={x}')\n        soup2 = BeautifulSoup(r.content, 'lxml')\n        columnlist = soup2.find_all('div', class_='article-card__details')\n        for item in columnlist:\n                for link in item.find_all('a', href=True):\n                    columnlinks.append(baseurl + link['href'])\n\n    print('Collecting newscolumn links. Beep Boop.')\n\n    fPostLink = 'https://torontosun.com/category/national/'\n    gPostLink = 'https://torontosun.com/category/news/'\n    hPostLink = 'https://torontosun.com/category/local-news/'\n    iPostLink = 'https://torontosun.com/category/news/provincial/'\n    jPostLink = 'https://torontosun.com/category/provincial/'\n    kPostLink = 'https://torontosun.com/category/world/'\n    lPostLink = 'https://torontosun.com/category/sports/'\n    mPostLink = 'https://torontosun.com/category/toronto-maple-leafs/'\n\n    for elem in list(columnlinks):\n        if elem == \"https://torontosun.com/category/columnists/\":\n            columnlinks.remove(elem)\n        elif elem == \"https://torontosun.com/category/opinion/\":\n            columnlinks.remove(elem)\n        elif fPostLink in elem:        \n            columnlinks.remove(elem)\n        elif gPostLink in elem:        \n            columnlinks.remove(elem)\n        elif hPostLink in elem:        \n            columnlinks.remove(elem)\n        elif iPostLink in elem:        \n            columnlinks.remove(elem)\n        elif jPostLink in elem:        \n            columnlinks.remove(elem)\n        elif kPostLink in elem:        \n            columnlinks.remove(elem)\n        elif lPostLink in elem:        \n            columnlinks.remove(elem)\n        elif mPostLink in elem:        \n            columnlinks.remove(elem)\n\n    print('links cleaned, boop boop')        \n\n    linkarray = columnlinks\n\n    count = 0\n\n    for i in range(0, len(linkarray)):\n        url = linkarray[i] # this variable imports into Database under newsColumns\n        html = urlopen(url)\n        soup = BeautifulSoup(html.read(), 'html.parser')\n    ##### CLEAN WEBSCRAPE OF ALL PERTINENT INFORMATION ######\n        ps = soup.find_all('p')\n        bodyarray = []\n        bodytext = \"\"\n\n        #   collect all p tags then append all text items to array - then add all items into bodytext string var \n        for p in ps:\n            ptext = p.get_text()\n            bodyarray.append(ptext)\n\n        for item in bodyarray:\n            bodytext += item\n            \n        #   headline webscrape\n        headline = soup.find('h1').get_text()\n\n        #   publish date webscrape\n        try:\n            pubdatescrape = soup.find('span',class_='published-date__since').get_text()\n        except AttributeError:\n            pubdate = 'Jan 1, 1991'\n        else: \n            pubdate = pubdatescrape\n\n\n        #   author webscrape - if null do not add to the thing (this doesn't work because the thing is already pulled and transferred)\n        try:\n            author=soup.find('span',class_='published-by__author').get_text()\n        except AttributeError:\n            print('Error with following article' + headline + '\\nskipping link...')\n            pass\n            '''if headline == 'ELECTION 2021':\n                pass\n            else:\n                print(headline)\n                author= str(input('Enter author name for' + headline + '\\n' + url +': '))\n            '''\n        try:\n            splitauthor = author.split(' ', 1)\n            authorfname = splitauthor[0]\n            authorlnamewhole = splitauthor[1].split(' ', 1)\n            authorlname = authorlnamewhole[0]\n        except:\n            authorfname = author\n            authorlname = author\n\n        #number the entry\n        count += 1\n        print(\"==== Column #\",count)\n                                    \n        print(\"=====UPLOADING COLUMN PUBLISHED:====== \", pubdate,'\\n', authorfname,'\\n', authorlname,'\\n', headline,)\n\n        ######      BEGIN INSERT DATA INTO DATABASE     #####\n\n        import mysql.connector\n        import re\n\n        #DB information to connect to localhost\n\n        mydb = mysql.connector.connect(\n        host=\"localhost\",\n        port=\"3306\",\n        user=\"root\",\n        password=\"root\",\n        database =\"columnwatcher\"\n        )\n\n        #get everything from the columnist table\n\n        mycursor = mydb.cursor()\n\n        mycursor.execute(\"SELECT * FROM columnists\")\n        result = mycursor.fetchall()\n\n        #check if the first and last name of the columnist matches any of the existing columnists, columnist id = \n\n        #give columnist id to existing columnist, attribute id to new columnist\n\n        for x in range(0,len(result)):\n            if authorfname in result[x] and authorlname in result[x]:\n                entryColumnistID = x+1\n                newColumnist = False\n                break\n                \n            else: \n                entryColumnistID = len(result)+1\n                newColumnist = True\n\n        #if the name of the new columnist does not exist in the columnist table, add it to the table\n        if newColumnist == True:\n            print('name does not exist in record. Creating new Columnist ID:')\n            sqlColumnists = \"INSERT INTO columnists (first_name, last_name) VALUES (%s, %s)\"\n            valColumnists = (authorfname, authorlname)\n\n            mycursor.execute(sqlColumnists, valColumnists)\n            print(\"Adding to database: \", authorfname, authorlname)\n            mydb.commit()\n        else:\n            print(authorfname, authorlname,\"exists in record under id#: \", entryColumnistID)\n\n        ##-- insert article --##\n\n        #columnist_id - auto incremented\n        #paper_id - Toronto Sun = 5\n        #headline \n        #publishdate - PUBDATE\n        #body_text\n        #url\n\n        mycursor2 = mydb.cursor()\n        mycursor2.execute(\"SELECT headline FROM newscolumns\")\n        colresult = mycursor2.fetchall()\n\n        #placeholder and specific vars\n        #entryColumnistID\n        paper_id = 5 #TorontoSun\n        newHeadline = headline\n\n        import datetime\n        #switch PUBLISH DATE to YYYY-mm-dd\n        f = '%b %d, %Y'\n        entryDateTime = datetime.datetime.strptime(pubdate, f)\n\n\n        #try 2 on newscolumn insert\n\n        for x in range(0,len(colresult)):\n            if newHeadline in colresult[x]:        \n                newColumn = False\n                break\n                \n            else: \n                newColumn = True\n\n\n\n        \n\n\n        #if this headline does not already exist, add it to my database please\n        if newColumn == True:\n            print('headline does not exist in record. Creating new Column entry')\n            sqlNewsColumns = \"INSERT INTO newscolumns (columnist_id, paper_id, headline, body_text, url, publishdate) VALUES (%s, %s, %s, %s, %s, %s)\"\n            valNewsColumns = (entryColumnistID, paper_id, newHeadline, bodytext, url, entryDateTime)\n            mycursor2.execute(sqlNewsColumns, valNewsColumns)\n            mydb.commit()\n            print('new entry', newHeadline, 'added to the database')\n            \n        else:\n            print(\"headline already in database, will not add duplicate:\", newHeadline)\n\n\n    print('===TORONTO SUN SCRAPE COMPLETE===')    \n\n\n", "399": "\"\"\"\nDjango settings for webscrape project.\n\nGenerated by 'django-admin startproject' using Django 3.2.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.2/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/3.2/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\nTEMPLATES_DIR = BASE_DIR/'templates'\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/3.2/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-@)fmyp=cfzi+9e*4%i1l-^y(qf8rf^ihxh%io2ch^8qufk681y'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'webscraper_app.apps.WebscraperAppConfig',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'webscrape.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [TEMPLATES_DIR, ],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'webscrape.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/3.2/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/3.2/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/3.2/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/3.2/howto/static-files/\n\nSTATIC_URL = '/static/'\nSTATICFILES_DIR = [\n    BASE_DIR/'static',\n]\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/3.2/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n", "400": "\"\"\"\nASGI config for PumpWebscrape project.\n\nIt exposes the ASGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.1/howto/deployment/asgi/\n\"\"\"\n\nimport os\n\nfrom django.core.asgi import get_asgi_application\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"PumpWebscrape.settings\")\n\napplication = get_asgi_application()\n", "401": "\"\"\"\nWSGI config for PumpWebscrape project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.1/howto/deployment/wsgi/\n\"\"\"\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"PumpWebscrape.settings\")\n\napplication = get_wsgi_application()\n", "402": "# Copyright 2021 VMware, Inc.\n# SPDX-License-Identifier: Apache-2.0\nimport pandas as pd\nimport logging\nimport datefinder\nfrom datetime import datetime\nimport time\nimport webscrape\nfrom vdk.api.job_input import IJobInput\n\nlog = logging.getLogger(__name__)\n\n\ndef run(job_input: IJobInput):\n    \"\"\"\n    Scrape bad Amazon Reviews for one of the most popular Yankee candles on Amazon\n    and ingest them into a cloud Trino database.\n    \"\"\"\n\n    log.info(f\"Starting job step {__name__}\")\n\n    # Create/retrieve the data job property storing latest ingested date for yankee_candle_reviews table.\n    # If the property does not exist, set it to \"2020-01-01\" (around the start of the pandemic).\n    props = job_input.get_all_properties()\n    if \"last_date_amazon\" in props:\n        pass\n    else:\n        # <- !!! INITIALIZE THE \"last_date_amazon\" PROPERTY TO '2020-01-01' !!!\n\n    # Initialize variables\n    i = 1\n    rev_result = []\n    date_result = []\n    # Date to start iterating from = current date (in the format \"2020-01-01\")\n    date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Go through the review pages and scrape reviews\n    while date > props[\"last_date_amazon\"]:\n        log.info(f'Rendering page {i}...')\n        # Parameterize the URL to iterate over the pages\n        url = f\"https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/product-reviews/B000JDGC78/ref=cm_cr_arp_d_\\\n            viewopt_srt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber={i}&sortBy=recent\"\n\n        # Get HTML code into a BeautifulSoup object\n        soup = webscrape.html_code(url)\n        # Get the reviews and dates for the current page\n        rev_page = webscrape.cus_rev(soup)\n        date_page = webscrape.rev_date(soup)[2:]\n\n        # Append reviews text into a list removing the empty reviews\n        for j in rev_page:\n            if j.strip() == \"\" or j.strip() == \"The media could not be loaded.\":\n                pass\n            else:\n                rev_result.append(j.strip())\n        log.info(len(rev_result))\n\n        # Append review dates into a list by extracting the date from text\n        for d in date_page:\n            if d.strip() == \"\":\n                pass\n            else:\n                # Initially, dates are in the format \"Reviewed in the United States on February 14, 2022\"\n                # datefinder package extracts the date from the text and converts it to datetime object\n                date_match = datefinder.find_dates(d)\n                for date in date_match:\n                    # Convert to string\n                    date = date.strftime(\"%Y-%m-%d\")\n                    date_result.append(date)\n        log.info(len(date_result))\n\n        # In each page, check whether there are more dates than reviews (empty reviews with photo only) and remove them\n        while len(rev_result) < len(date_result):\n            date_result.pop(-1)\n\n        # Go to the next page\n        i += 1\n\n    # Create a pandas dataframe with the review text and dates\n    df = pd.DataFrame(zip(date_result, rev_result), columns=['Date', 'Review'])\n    # Since the while loop above always executes at least once (current timestamp > last ingested review), the first review\n    # page will always be scraped, so delete the already ingested records manually from the df using the DJ property\n    df = df[df['Date'] > props[\"last_date_amazon\"]]\n    # Remove emojis from the Review column since they are not utf-8 compliant and break ingestion\n    for i in range(0, len(df)):\n        # Go through each review and clean it if needed\n        df.loc[i, 'Review'] = webscrape.remove_emoji(df.loc[i, 'Review'])\n    log.info(f\"Shape of the review dataset: {df.shape}\")\n\n    # Ingest the dataframe into a SQLite database using VDK's job_input method (if any results are fetched)\n    if len(df) > 0:\n        job_input.send_tabular_data_for_ingestion(\n            rows=, # <- !!! ENTER HERE THE VALUES THAT WILL BE INSERTED INTO THE ROWS OF THE TABLE !!!\n            column_names=, # <- !!! ENTER HERE THE COLUMNS NAMES USING THE SAME COLUMN NAMES AS IN THE REVIEWS DATA FRAME !!!\n            destination_table=\"\" # <- !!! ENTER BETWEEN THE QUOTES THE NAME OF THE TABLE WE CREATED IN SCRIPT \"02_create_yankee_candle_reviews.sql\" !!!\n        )\n        # Reset the last_date property value to the latest date in the amazon source db table\n        props[\"last_date_amazon\"] = max(df['Date'])\n        job_input.set_all_properties(props)\n\n    log.info(f\"Success! {len(df)} rows were inserted in raw yankee candle reviews table.\")\n    # Delay execution for 10 seconds so that records are ingested into the DB before going to the next script\n    time.sleep(10)\n", "403": "'''\nCreated on Aug 7, 2021\n\n@author: 4king\n'''\n\nfrom urllib.request import urlopen, Request\nimport textwrap\nfrom webscrape.searchresults.categorydetails.tvdetail.tvparse.tv_parser import tv_parser\nfrom webscrape.searchresults.categorydetails.tvdetail.tvdisplay import tv_user_reviews\nfrom webscrape import clear\n\ndef view_tv_details(tv_link, season_number):\n    #clears the command line output\n    clear()\n    #if the link is the season of a show\n    if (season_number != 0):\n        req = Request(tv_link + \"/season-\" + str(season_number), headers={'User-Agent': 'Mozilla/5.0'})\n    \n    #if the link is the series overview\n    else:\n        req = Request(tv_link, headers={'User-Agent': 'Mozilla/5.0'})\n    \n    page = urlopen(req).read()\n    html_tv = page.decode(\"utf-8\")\n    \n    \n    #create the parser and parse each element\n    parser = tv_parser(html_tv)\n    name = parser.get_name()\n    cast = parser.get_cast()\n    summary = parser.get_summary()\n    summary = summary.replace('', '\\n')\n    credit = parser.get_credit()\n    seasons = parser.get_seasons()\n    num_seasons = parser.get_number_seasons()\n    #print the results to the command line\n    print(name)\n    print(cast + \"\\n\")\n    \n    #create the formatting for the summary\n    body = '\\n\\n'.join(['\\n'.join(textwrap.wrap(line, 100,\n                 break_long_words=False, replace_whitespace=False))\n                 for line in summary.splitlines() if line.strip() != ''])\n    \n    print(body)\n    print(\"\\n\" + credit + \"\\n\\n\")\n    print(seasons + \"\\n\") \n    \n    \n    #prompt the user to view the reviews of the show\n    answer = input(\"Type r to view user reviews. Type s to change to a different season. Type anything else to go back to results page\\n\")\n    \n    if (answer == \"s\"):\n        season = input(\"Enter the number that corresponds with the season you want to view. Invalid numbers default to series overview\\n\")\n        if (season.isdigit() == False):\n            season = \"0\"\n        \n        season = int(season)\n        \n        if (season > num_seasons):\n            season = 0\n        view_tv_details(tv_link, season)\n        \n        return\n    \n    #while the answer s still user reviews\n    while (answer == \"r\"):\n        \n        \n        tv_user_reviews.view_tv_user_reviews(tv_link + \"/user-reviews\")\n        \n        #clears the command line output\n        clear()\n        \n        #print the results to the command line\n        print(name)\n        print(cast + \"\\n\")\n        print(body)\n        print(\"\\n\" + credit + \"\\n\\n\")\n        \n        \n        #prompt the user to view the reviews of the show\n        answer = input(\"Type r to view user reviews. Type s to change to a different season. Type anything else to go back to results page\\n\")\n    \n        if (answer == \"s\"):\n            season = input(\"Enter the number that corresponds with the season you want to view. Invalid numbers default to series overview\\n\")\n            \n            if (season.isdigit() == False):\n                season = \"0\"\n        \n            season = int(season)\n        \n            if (season > num_seasons):\n                season = 0\n            \n            view_tv_details(tv_link, season)\n        \n            return\n    ", "404": "from webScrape.scraping_jobs.parsers.job_parser import JobParser\nfrom bs4 import BeautifulSoup \nfrom webScrape.scraping_jobs.locators.all_jobs_locator import ALLJobLocators\n\nclass AllJobsPage:\n    \n    def __init__(self, page_content):\n        self.soup = BeautifulSoup(page_content, 'html.parser')\n\n    @property\n    def jobs(self):\n        \"\"\"returns all the jobs in a page\"\"\"\n        return [JobParser(e) for e in self.soup.select(ALLJobLocators.JOBS)]\n\n    @property\n    def page_count(self):\n        \"\"\"RETURNS THE NUMBER OF MAXIMUM PAGES\"\"\"\n        content = self.soup.select(ALLJobLocators.PAGINATION)\n        page_num = int(content[-2].string)\n        return page_num\n", "405": "import os\nimport time\nimport pandas as pd\nfrom selenium import webdriver\nimport selenium.common.exceptions\nfrom selenium.webdriver.common.by import By\n\n\nPYCHARM_DIR = \"C:\\\\Users\\\\baseb\\\\PycharmProjects\\\\DS3500Project\\\\DraftResolutions\"\nSERIUM_CHROME_PATH = \"C:/Users/baseb/Downloads/chromedriver.exe\"\n\n\ndef open_file(file):\n    \"\"\"\n    :param file: File to read into a dataframe\n    :return: Dataframe\n    \"\"\"\n    df = pd.read_csv(file, low_memory=False)\n    return df\n\n\ndef selenium_setup():\n    \"\"\"\n    :return: Options setup to be used for selenium\n    \"\"\"\n    # Configures selenium\n    options = webdriver.ChromeOptions()\n    profile = {\"plugins.plugins_list\": [{\"enabled\": False, \"name\": \"Chrome PDF Viewer\"}],  # Disable Chrome's PDF Viewer\n               \"download.default_directory\": PYCHARM_DIR, \"download.extensions_to_open\": \"applications/pdf\"}\n    options.add_experimental_option(\"prefs\", profile)\n    return options\n\n\ndef un_webscrape(df, options):\n    \"\"\"\n    :param df: dataframe with links\n    :param options: options from selenium setup func\n    :return: none\n    Downloads all the pdfs from the links in the dataframe\n    \"\"\"\n    counter = 0\n    pdf_list = []\n    error_list = []\n    for i in range(len(df.index)):\n        driver = webdriver.Chrome(executable_path= SERIUM_CHROME_PATH, chrome_options=options)\n        link = df.iloc[i][9]\n        driver.get(link)\n        try:\n            # Clicks the buttons from the UN webpage to download the files\n            l = driver.find_element(By.XPATH, '//*[@id=\"details-collapse\"]/div[5]/span[2]/a')\n            l.click()\n            l = driver.find_element(By.XPATH, '//*[@id=\"record-files-list\"]/tbody/tr[2]/td[1]/a')\n            l.click()\n            num_files = len([f for f in os.listdir('.\\\\DraftResolutions') if f.endswith('.pdf')])\n            pdf_list.append((driver.find_element(By.XPATH, '//*[@id=\"record-files-list\"]/tbody/tr[2]/td[2]')).text)\n            time.sleep(.5)\n            # Checks if the pdf has been added to the target location, if not, waits and tries again\n            while num_files == len([f for f in os.listdir('.\\\\DraftResolutions') if f.endswith('.pdf')]):\n                time.sleep(.1)\n        except selenium.common.exceptions.NoSuchElementException:\n            pdf_list.append('Error')\n            error_list.append(counter)\n        print(counter)\n        counter += 1\n        driver.quit()\n    return pdf_list, error_list\n\n\ndef write_to_file(pdf_list, error_list):\n    \"\"\"\n    :param pdf_list: list of pdfs\n    :param error_list: list of positions of errors\n    :return: None\n    Writes pdf_lists and error_lists to files for later use\n    \"\"\"\n    with open('PDFList.txt', 'w') as infile:\n        for item in pdf_list:\n            infile.write(str(item) + '\\n')\n    with open('ErrorList.txt', 'w') as infile:\n        for item in error_list:\n            infile.write(str(item) + '\\n')\n\n\ndef main():\n    df = open_file('UN DATA.csv')\n    options = selenium_setup()\n    pdf_list, error_list = un_webscrape(df, options)\n    write_to_file(pdf_list, error_list)\n    print('Complete!')\n\n\nif __name__ == \"__main__\":\n    main()\n", "406": "import numpy as np\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport time\nimport json\nimport requests\nfrom pymongo import MongoClient\n\nfrom webscrape_app.celery_controller import app\nfrom webscrape_app.webscrape_util.scrape_util import add_new_line, load_last_line, setup_mongo_client\n\n\ndef insert_edge(response, collection):\n    \"\"\"Insert records into Mongo database.\n\n    Args:\n        response (request response object): Response from request.get('url')\n        collection (pymongo.Collection): Collection object for record insertion.\n    \"\"\"\n    edges = response.json()['data']['hashtag']['edge_hashtag_to_media']['edges']\n    for edge in edges:\n        collection.insert_one(edge)\n\ndef get_page_info(response):\n    \"\"\"Find and save page_info from response json.\n\n    Args:\n        response (request response object): Response from request.get('url')\n\n    Returns:\n        page_info (dict): Dictionary from response json.\n            Keys: 'has_next_page' (bool) and 'end_cursor' (unicode)\n    \"\"\"\n    page_info = response.json()['data']['hashtag']['edge_hashtag_to_media']['page_info']\n    return page_info\n\n\n@app.task\ndef instascrape(page_info_filepath, num_requests, collection_name):\n    \"\"\"\n    Scrape instagram hashtag search\n\n    Args:\n        page_info_filepath (str): Filepath to text file with page_info dicts\n        num_requests (int): Number of pages to be scraped\n\n    Action: saves influencer node information to pymongo database\n\n    Output: None\n    \"\"\"\n\n    client, collection = setup_mongo_client('instascrape2', collection_name)\n\n\n    page_info = \"\"\n    response = requests.get(\"https://www.instagram.com/graphql/query/?query_id=17875800862117404&variables={{%22tag_name%22:%22tennis%22,%22first%22:{}}}\"\n                            .format('12'))\n\n    if response.status_code == 200:\n        print(response)\n        insert_edge(response, collection)\n        page_info = get_page_info(response)\n        add_new_line(page_info, page_info_filepath)\n\n    page_info = load_last_line(page_info_filepath)\n    base_url_search = \"https://www.instagram.com/graphql/query/?query_id=17875800862117404&variables={{%22tag_name%22:%22tennis%22,%22first%22:{},%22after%22:%22{}%22}}\"\n\n    for i in range(num_requests):\n        print(page_info['end_cursor'])\n\n        if page_info['has_next_page']:\n            response = requests.get(base_url_search.format('11', str(page_info['end_cursor'])))\n\n            if response.status_code == 200:\n                insert_edge(response, collection)\n                page_info = get_page_info(response)\n                add_new_line(page_info, page_info_filepath)\n\n            else:\n                print(\"Status Code = \" + str(response.status_code))\n                return None\n\n        time.sleep(np.random.uniform(15, 45))\n        print(\"Finished scraping {} pages of {}\".format(i + 1, num_requests))\n\n    client.close()\n\n    print(\"\\n Finished scraping {} pages of 12 influencers each\".format(num_requests))\n    return None\n#\n# ex = \"https://www.instagram.com/graphql/query/?query_id=17875800862117404&variables={{%22tag_name%22:%22womenwhoclimb%22,%22first%22:{},%22after%22:%22{}%22}}\".format('12', \" \")\n# print(ex)\n# response = requests.get(ex)\n# print(response.url)\n\n# instascrape(\"../date/influenc_d.json\", 10)", "407": "import json\nimport os\n\n\ndef parse_basic_parcel_data(line):\n    sanitized_line = line.replace(\"'}\", '\"}').replace(\"{'\", '{\"').replace(\"': '\", '\": \"').replace(\"', '\", '\", \"').\\\n        replace(\"\\\", '\", '\", \"').replace(\"\\\": '\", '\": \"').replace(\"': \\\"\", '\": \"').replace(\"\\\": \\\"\\\"\", \"\\\": \\\"\").\\\n        replace(\"\\\"\\\", \\\"\", \"\\\", \\\"\").replace(\"\\\": \\\"}\", \"\\\": \\\"\\\"}\")\n    #print(line)\n    #print(sanitized_line)\n    #basic_parcel_data = json.loads(line)\n    basic_parcel_data = json.loads(sanitized_line)\n    return basic_parcel_data\n\n\ndef create_dictionary_of_basic_parcel_data(parcel_data_path=\"/home/dave/PycharmProjects/FultonCountyBallotScanner/\"\n                                                            \"canvass/address_projects/scrap/\"\n                                                            \"property_webscrape_parcel_basics_all_except_8.txt\"):\n    parcel_data_file = open(parcel_data_path, 'r')\n    dictionary_of_basic_parcel_data = {}\n    for line_of_parcel_data in parcel_data_file.readlines():\n        parcel_data = parse_basic_parcel_data(line_of_parcel_data)\n        parcel_id = parcel_data[\"Parcel ID\"]\n        address = parcel_data[\"Parcel Address\"]\n        dictionary_of_basic_parcel_data[parcel_id] = address\n\n    parcel_data_file.close()\n\n    return dictionary_of_basic_parcel_data\n\n\ndef update_addresses_on_detailed_parcel(old_datafile=\"/home/dave/Documents/Election Fraud/canvass/property_data/\"\n                                                        \"Fulton_property_webscrape_v1.0.txt\",\n                                           new_datafile=\"/home/dave/Documents/Election Fraud/canvass/property_data/\"\n                                                        \"Fulton_property_webscrape_v1.1.txt\"):\n    old_file_with_detailed_parcel_data = open(old_datafile, 'r')\n    if os.path.exists(new_datafile):\n        os.remove(new_datafile)\n    new_file_with_detailed_parcel_data = open(new_datafile, 'a')\n    dictionary_of_basic_parcel_data = create_dictionary_of_basic_parcel_data()\n    number_of_adjusted_addresses = 0\n    number_of_unadjusted_addresses = 0\n    for old_datafile_line in old_file_with_detailed_parcel_data.readlines():\n        parcel_data = json.loads(old_datafile_line)\n        #print(parcel_data)\n        parcel_id = parcel_data[\"Parcel ID\"]\n        parcel_id = parcel_id.replace(\"  LL\", \" LL\")\n        try:\n            new_parcel_address = dictionary_of_basic_parcel_data[parcel_id]\n            number_of_adjusted_addresses += 1\n        except KeyError:\n            #print(parcel_id)\n            try:\n                new_parcel_address = parcel_data[\"Property Location\"]\n            except KeyError:\n                new_parcel_address = \"\"\n            number_of_unadjusted_addresses += 1\n        parcel_data[\"Property Location\"] = new_parcel_address\n        new_file_with_detailed_parcel_data.write(json.dumps(parcel_data) + \"\\n\")\n    print(f\"Number of adjusted addresses: {number_of_adjusted_addresses}\")\n    print(f\"Number of unadjusted addresses: {number_of_unadjusted_addresses}\")\n    print(f\"\\nNew file: {new_datafile}\")\n\n    return True\n\n\n\n\n\nif __name__ == \"__main__\":\n    path_to_parcel_data = \"property_webscrape_parcel_basics.txt\"\n    parcel_data_file = open(path_to_parcel_data)\n    dictionary_with_parcel_IDs_as_keys = {} #Dictionaries are efficient at sorting through keys, and keep the keys unique\n    for line in parcel_data_file.readlines():\n        #For the record:\n        #Turning a string into a python dictionary is a difficult task.\n        #\n        #JSON makes it easy, but we screwed up the way we wrote the dictionary into the file, so we need to convert\n        #the single quotes into double quotes\n        #\n        #What makes this more complicated is that some parcels have owners that contain a single quote (i.e. apostrophe)\n        #in their name, which caused our software to automatically turn all the single quotes surrounding the name\n        #into double quotes, which led to some weird, cascading failures.\n        #\n        #I could go on, but to really understand all the craziness that went on behind the scenes,\n        #just modify the following line:\n        sanitized_line = line.replace(\"{'\", '{\"').replace(\"': '\", '\": \"').replace(\"', '\", '\", \"').replace(\"'}\", '\"}').replace(\n            \"\\\", '\", '\", \"').replace(\"\\\": '\", '\": \"').replace(\"': \\\"\", '\": \"')\n        try:\n            parcel_id = json.loads(sanitized_line)[\"Parcel ID\"]\n        except:\n            print(\"Error in parsing the JSON data. Let's play a game. Find the differences between the following 2 lines:\")\n            print(line)\n            print(sanitized_line)\n            print(\"\\n\\n\")\n        dictionary_with_parcel_IDs_as_keys.update({parcel_id: None})\n    print(f\"The number of unique parcel IDs: {len(dictionary_with_parcel_IDs_as_keys.keys())}\")\n\n\n", "408": "# running this file should output terms_synonyms.csv which is a csv file\n# of synonyms from the terms file, in order to start a database full of words.\nfrom Webscrape import scrape\n", "409": "__author__ = 'null'\n\nimport urllib2\nimport re\n\nclass WebScrape:\n    def __init__(self, input_url):\n        \"\"\"\n\n        :type self: url input\n        \"\"\"\n        self.url = input_url\n        self.html_text = ''\n        self.text = ''\n        \n    def get_text(self):\n        return self.text\n\n    def process_html(self):\n\n        multiple_space_regx = re.compile(r'\\s\\s+')\n        url_text_afterspace = multiple_space_regx.sub(' ', self.html_text)\n\n        remove_comment_regx = re.compile(r'')\n        url_text_aftercomment = remove_comment_regx.sub('', url_text_afterspace)\n\n        remove_htmltag_regx = re.compile(r'<[^>]*>')\n        url_text_afterhtml = remove_htmltag_regx.sub('',url_text_aftercomment)\n\n        remove_punctuation_regx = re.compile(r'[,\\'\\\"!=\\$\\.\\-\\|:;~\\*\\\\@\\/%\\#\\?\\_\\{\\}\\+\\&>]+')\n        url_text_afterpunctuation = remove_punctuation_regx.sub(' ',url_text_afterhtml)\n\n        remove_digits = re.compile(r'[0-9]')\n        url_text_digitsremove = remove_digits.sub('', url_text_afterpunctuation)\n\n        remove_parenthesis = re.compile(r'[\\(\\)]+')\n        url_text_parenthesis_reomove = remove_parenthesis.sub('',url_text_digitsremove)\n\n        remove_nonascii = re.compile(r'[^\\x00-\\x7F]+')\n        url_text_after_nonascii = remove_nonascii.sub(' ',url_text_parenthesis_reomove)\n\n        remove_extra1 = re.compile(r'&gt')\n        url_text_afterextra1 = remove_extra1.sub('', url_text_after_nonascii)\n\n        remove_extra2 = re.compile(r'&nbsp')\n        url_text_afterextra2 = remove_extra2.sub('', url_text_afterextra1)\n\n        remove_extra3 = re.compile(r'&amp')\n        url_text_afterextra3 = remove_extra3.sub('', url_text_afterextra2)\n\n        remove_opening_closing_bracket = re.compile(r'[\\[\\]]+')\n        url_text_removing_brackets = remove_opening_closing_bracket.sub('', url_text_afterextra3)\n\n        url_text_space_removal = multiple_space_regx.sub(' ', url_text_removing_brackets)\n\n        remove_leading_trailing_whitespace = re.compile(r'^\\s+|\\s+$')\n        url_text_leading_trailing_space = remove_leading_trailing_whitespace.sub('', url_text_space_removal)\n\n        remove_newline_tab = re.compile(r'\\n\\r\\t')\n        url_text_final = remove_newline_tab.sub(' ', url_text_leading_trailing_space)\n        self.text = url_text_final\n        #print url_text_final\n\n    def fetch_html(self):\n        headers = {}\n        headers['User-Agent'] = 'Googlebot'\n        request = urllib2.Request(self.url, headers=headers)\n        try:\n            response = urllib2.urlopen(request)\n        except urllib2.HTTPError as e:\n            if e.code == 404:\n                print '404 Error', e.msg\n            else:\n                print 'Id onot know', e.msg\n        except urllib2.URLError as e:\n            print 'Url Error:', e.msg\n        else:\n            print response.getcode()\n            url_text = response.read()\n            print response.info()\n            print response.geturl()\n            #print url_text\n            self.html_text = url_text\n            #self.process_html(url_text)\n            response.close()\n\n#def main():\n    #url = 'https://www.microsoft.com'\n    #url = 'http://www.cs.ucsb.edu/~vigna/research.html'\n    #url = 'https://www.memphis.edu/psychology/'\n    #get_html(url)\n    #webScraper = WebScrape(url)\n    #webScraper.fetch_html()\n    #webScraper.process_html()\n    #print webScraper.get_text()\n\n#if __name__=='__main__':\n    #main()", "410": "from bs4 import BeautifulSoup\r\nimport requests\r\nimport json\r\nfrom datetime import datetime\r\n\r\n\r\ndate = datetime.today().strftime('%Y-%m-%d')\r\n\r\n# choose cities to webscrape\r\ncity_1 = 'Hamburg'\r\ncity_2 = 'Berlin'\r\ncountry = 'Germany'\r\ncountry2 = 'Germany'\r\n\r\n# Webscrape page data (table)\r\nurl = f'https://www.numbeo.com/cost-of-living/compare_cities.jsp?country1={country}&country2={country2}&city1={city_1}&city2={city_2}&tracking=getDispatchComparison'\r\npage = requests.get(url)\r\nsoup = BeautifulSoup(page.content, 'html.parser')\r\ntable = soup.find('table', attrs={'class': 'data_wide_table new_bar_table cost_comparison_table'})\r\nrows = table.find_all('tr')\r\n\r\n# create dictionary date in dictionary 'dictionary'\r\ndictionary = {date: {}}\r\n\r\n# filter for important information\r\nfor x in rows[1:9]:\r\n    no_text = x.text.split()\r\n    dictionary[date][' '.join(no_text[0:-6])] = (float(no_text[-6]), float(no_text[-4]))\r\n\r\n# open file and save data\r\nwith open('prices.json') as file:\r\n    data = json.load(file)\r\n\r\nwith open('prices.json', 'w') as file:\r\n    res = data | dictionary\r\n    json.dump(res, file)\r\n", "411": "from lxml import etree\nimport aiohttp\n\nfrom m42pl.commands import GeneratingCommand\nfrom m42pl.fields import Field, FieldsMap\nfrom m42pl.event import derive\n\n\nclass WebScrape(GeneratingCommand):\n    \"\"\"Scrapes a website according to simple scraping rules.\n    \"\"\"\n\n    _aliases_   = ['webscrape', 'web_scrape', 'webscrapper', 'web_scrapper']\n    _syntax_    = '[url=] [articles=] [iterator=]'\n\n    def __init__(self, url: str, articles: list, iterator: str):\n        \"\"\"\n        :param url:         Base URL\n        :param articles:    XPath expression(s) to extract articles\n        :param iterator:    XPath expression(s) to get next index page\n        \"\"\"\n        super().__init__(url, articles, iterator)\n        self.fields = FieldsMap(\n            url=Field(url, type=str),\n            articles=Field(articles, type=str),\n            iterator=Field(iterator, type=str)\n        )\n        self.xml_parser = etree.HTMLParser()\n\n    async def setup(self, event, pipeline):\n        self.fields = await self.fields.read(event, pipeline)\n        # Build articles' XPath expressions\n        self.articles_xpath = etree.XPath(self.fields.articles)\n        # Build iterator's XPath expressions\n        self.iterator_xpath = etree.XPath(self.fields.iterator)\n\n    def scrape_articles(self, tree):\n        _articles = []\n        for article in self.articles_xpath(tree):\n            try:\n                _articles.append({\n                    'attrs': dict(getattr(article, 'attrib', {})),\n                    'text': getattr(article, 'text', '')\n                })\n            except Exception:\n                pass\n        return _articles\n\n    def scrape_iterator(self, tree):\n        try:\n            return self.iterator_xpath(tree)[0]\n        except Exception:\n            return None\n\n    async def target(self, event, pipeline):\n        nextpage = self.fields.url\n        async with aiohttp.ClientSession() as session:\n            # Loop until there is no more \n            while nextpage is not None:\n                async with session.get(nextpage) as response:\n                    # Get response content\n                    page_src = (await response.read()).decode('UTF-8')\n                    # Parse response content as XML\n                    page_tree = etree.fromstring(\n                        page_src,\n                        parser=self.xml_parser\n                    )\n                    # Backup current URL\n                    currpage = nextpage\n                    # Get articles and next page url\n                    articles = self.scrape_articles(page_tree)\n                    nextpage = self.scrape_iterator(page_tree)\n                    # Yield one event per article\n                    for article in articles:\n                        yield derive(event, article)\n", "412": "from django.shortcuts import render\nfrom django.http import HttpResponse\nimport os\nimport json\nimport pandas as pd\nimport subprocess\nfrom json import dumps \nfrom django.views.decorators.csrf import csrf_exempt\nimport time\nimport datetime\nfrom django.utils import timezone\nfrom django.contrib.sessions.models import Session\n\ndef index(request):\n    data = os.listdir('autolibrary/documents')\n    data = dumps(data) \n    \n    os.system('mkdir -p ../data/raw')\n    os.system('mkdir -p ../data/out')\n    os.system('mkdir -p static/autolibrary/documents')\n    os.system('mkdir -p static/autolibrary/web_scrap')\n\n    shared_obj = request.session.get('myobj',{}) \n    shared_obj['selected_doc'] = ''\n    shared_obj['selected_pdf'] = ''\n    shared_obj['if_customized'] = \"true\"\n    shared_obj['selected_domain'] = ''\n    shared_obj['selected_subdomain'] = ''\n    shared_obj['selected_keywords'] = ''\n    shared_obj['phrases'] = []\n    shared_obj['in_queue'] = \"false\"\n    shared_obj['timestamp'] = ''\n    shared_obj['first_run'] = \"true\"\n    request.session['myobj'] = shared_obj\n\n    return render(request, 'autolibrary/index.html', {\"data\": data})\n\ndef result(request):\n    data = os.listdir('autolibrary/documents')\n    domains = json.load(open('../config/domains_full.json'))\n\n    shared_obj = request.session['myobj']\n    selected_doc = shared_obj['selected_doc']\n    selected_pdf = shared_obj['selected_pdf']\n\n    content = {\n        \"data\": dumps(data), \n        \"selected_doc\": dumps([selected_doc]), \n        \"selected_pdf\": dumps([selected_pdf]), \n        \"domains\": dumps(domains)\n    }\n    \n    shared_obj['in_queue'] = \"false\"\n    request.session['myobj'] = shared_obj\n    return render(request, 'autolibrary/result.html', content)\n\ndef customization(request):\n    data = os.listdir('autolibrary/documents')\n    domains = json.load(open('../config/domains_full.json'))\n\n    shared_obj = request.session['myobj']\n    if_customized = shared_obj['if_customized']\n    selected_pdf = shared_obj['selected_pdf']\n    selected_doc = shared_obj['selected_doc']\n    selected_keywords = shared_obj['selected_keywords']\n    if shared_obj['first_run'] == \"true\":\n        shared_obj['selected_domain'] = ''\n        shared_obj['selected_subdomain'] = ''\n        shared_obj['phrases'] = []\n    selected_domain = shared_obj['selected_domain']\n    selected_subdomain = shared_obj['selected_subdomain']\n    phrases = shared_obj['phrases']\n\n    content = {\n        \"customized\": dumps([if_customized]),\n        \"data\": dumps(data), \n        \"selected_doc\": dumps([selected_doc]), \n        \"selected_pdf\": dumps([selected_pdf]), \n        \"domains\": dumps(domains),\n        \"domain\": dumps([selected_domain]),\n        \"subdomain\": dumps([selected_subdomain]),\n        \"phrases\": dumps(phrases),\n        \"keywords\":dumps([selected_keywords]),\n    }\n    if if_customized == \"false\":\n        if_customized = \"true\"\n        shared_obj['if_customized'] = if_customized\n\n    shared_obj['in_queue'] = \"false\"\n    request.session['myobj'] = shared_obj\n    return render(request, 'autolibrary/customization.html', content)\n\n@csrf_exempt\ndef get_file(request):\n    if request.method == 'POST':\n        if \"file_name\" in request.POST:\n            shared_obj = request.session['myobj']\n            if_customized = \"false\"\n            shared_obj['if_customized'] = if_customized\n\n            # rename document\n            file_name = request.POST['file_name']\n            pdfname = file_name.replace(\"'\", \"\")\n            pdfname = pdfname.replace(\" \", \"_\")\n            os.system('bash autolibrary/rename.sh')\n            # save doc name and move to static\n            selected_doc = file_name\n            selected_pdf = pdfname\n            shared_obj['selected_pdf'] = selected_pdf\n            shared_obj['selected_doc'] = selected_doc\n\n            command = 'cp autolibrary/documents_copy/' + pdfname + ' static/autolibrary/documents'\n            os.system(command)\n\n            shared_obj['in_queue'] = \"false\"\n            shared_obj['first_run'] = \"true\"\n            request.session['myobj'] = shared_obj\n            return HttpResponse('get file')\n    return HttpResponse('fail to get file')\n\n@csrf_exempt\ndef get_domain(request): \n    if request.method == 'POST':\n        if \"domain\" in request.POST:\n            shared_obj = request.session['myobj'] \n            unique_key = request.session.session_key\n            \n            # save selected domain to data/out\n            selected_domain = request.POST['domain']\n            selected_subdomain = request.POST['subdomain']\n            selected_pdf = shared_obj['selected_pdf']\n            if selected_domain == '':\n                selected_domain = 'ALL'\n            if selected_subdomain == '':\n                selected_subdomain = 'ALL'\n\n            shared_obj['selected_domain'] = selected_domain \n            shared_obj['selected_subdomain'] = selected_subdomain\n\n            # with open('../data/out/selected_domain_' + unique_key + '.txt', 'w') as fp:\n            with open('../data/out/selected_domain.txt', 'w') as fp:\n                fp.write(selected_subdomain)\n            config = {'fos': [selected_domain]}\n            # with open('../data/out/fos_' + unique_key + '.json', 'w') as fp:\n            with open('../data/out/fos.json', 'w') as fp:\n                json.dump(config, fp)\n            # rewrite data-params.json\n            config = json.load(open('../config/data-params.json'))\n            config['pdfname'] = selected_pdf\n            with open('autolibrary/data-params.json', 'w') as fp:\n                json.dump(config, fp)\n            with open('autolibrary/run.sh', 'w') as rsh:\n                # move selected document to data/raw\n                rsh.write('''cp autolibrary/documents_copy/''')\n                rsh.write(selected_pdf)\n                rsh.write(''' ../data/raw \\n''')\n                # move new data-params.json to config\n                rsh.write('''cp autolibrary/data-params.json  ../config \\n''')\n                # run all targets\n                rsh.write('''cd .. \\n''')\n                rsh.write('''python run.py data \\n''')\n                rsh.write('''python run.py autophrase \\n''')\n                rsh.write('''python run.py weight ''' + unique_key +  '''\\n''')\n                rsh.write('''python run.py webscrape ''' + unique_key +  '''\\n''')\n                rsh.write('''cp data/out/scraped_AutoPhrase.json website/static/autolibrary/web_scrap/scraped_AutoPhrase.json \\n''')\n            process = subprocess.Popen(['bash', 'autolibrary/run.sh'])\n            process.wait()\n\n            # display phrases with a weighted quality score > 0.5\n            data = pd.read_csv('../data/out/weighted_AutoPhrase.csv', index_col = \"Unnamed: 0\")\n            phrases = data[data['score'] > 0.5]['phrase'].to_list()\n            if len(phrases) < 5:\n                phrases = data['phrase'][:5].to_list()\n            shared_obj['phrases'] = phrases \n\n            new_keywords = phrases[0] + ', ' + phrases[1] + ', ' + phrases[2] + ', '\n            shared_obj['selected_keywords'] = new_keywords\n\n            shared_obj['in_queue'] = \"false\"\n            shared_obj['first_run'] = \"false\"\n            request.session['myobj'] = shared_obj\n            return HttpResponse('get domain')\n    return HttpResponse('fail to get domain')\n\n@csrf_exempt\ndef get_keywords(request):  \n    if request.method == 'POST':\n        if \"keywords\" in request.POST:\n            shared_obj = request.session['myobj'] \n            # save selected keywords to data/out\n            selected_keywords = request.POST['keywords']\n            shared_obj['selected_keywords'] = selected_keywords\n            config = {'keywords': selected_keywords}\n            with open('../data/out/selected_keywords.json', 'w') as fp:\n                json.dump(config, fp)\n            with open('autolibrary/run.sh', 'w') as rsh:\n                # display new webscrape result\n                rsh.write('''cd .. \\n''')\n                rsh.write('''python run.py webscrape \\n''')\n                rsh.write('''cp data/out/scraped_AutoPhrase.json website/static/autolibrary/web_scrap/scraped_AutoPhrase.json''')\n            process = subprocess.Popen(['bash', 'autolibrary/run.sh'])\n            process.wait()\n\n            shared_obj['in_queue'] = \"false\"\n            request.session['myobj'] = shared_obj\n            return HttpResponse('get keywords')\n    return HttpResponse('fail to get keywords')", "413": "#!/usr/bin/python3\n\nfrom bs4 import BeautifulSoup\nimport argparse\nimport requests\nimport re\nfrom setup_logging import logger\nimport json\n\nclass Webscrape():\n    '''classes are cool, no other real reason to use this - probably going to only have one function'''\n    def __init__(self):\n        self.webpath = \"https://rocketleague.tracker.network/rocket-league/profile\"\n        self.latestseason = '16' #need a better way to update this, perhaps dynamically?\n        self.rltrackermissing = \"We could not find your stats,\"\n        self.psyonixdisabled = \"Psyonix has disabled the Rocket League API\"\n\n    def retrieveDataRLTracker(self,gamertag=\"memlo\",platform=\"steam\"):\n        ''' Python BeautifulSoup4 Webscraper to https://rocketleague.tracker.network/ to retrieve gamer data\n        '''\n        latestseason = self.latestseason\n        webpath = self.webpath\n        rltrackermissing = self.rltrackermissing\n        psyonixdisabled = self.psyonixdisabled\n        playerdata = {} # define the playerdata dict\n        playerdata[gamertag] = {} # define the gamertag dict\n        playerdata[gamertag][latestseason] = {} # define the latestseason dict\n        page = requests.get(\"%(webpath)s/%(platform)s/%(gamertag)s\" % locals())\n        # correct platform names\n        if 'ps' or 'ps4' in platform:\n            platform = \"psn\"\n        if 'xbox' in platform:\n            platform = \"xbl\"\n        if page.status_code == 200:\n            soup = BeautifulSoup(page.content, features=\"lxml\")\n            if soup(text=re.compile(rltrackermissing)): # find \"we could not find your stats\" on webpage\n                logger.critical(\"Player Missing - URL:%(webpath)s/%(platform)s/%(gamertag)s\" % locals())\n            elif soup(text=re.compile(psyonixdisabled)): # find \"Psyonix has disabled the Rocket League API\" on webpage\n                logger.critical(\"Psyonix Disabled API - URL:%(webpath)s/%(platform)s/%(gamertag)s\" % locals())\n            else:\n                script_data = [l for l in [str(l.parent) for l in soup.find_all('script')] if 'INITIAL_STATE' in l][0]\n                json_data = script_data.split('INITIAL_STATE__=')[1].split(\";(function()\")[0]\n                data = json.loads(json_data)['stats-v2']['standardProfiles']\n                try:\n                    trn_gamertag = list(data.keys())[0]\n                    gamer_data = data[trn_gamertag]['segments']\n                    for segment in gamer_data:\n                        if \"playlist\" in segment['type']:\n                            playerdata[gamertag][latestseason].update(self._parsePlaylist(data=segment))\n                except Exception as e:\n                    logger.critical(\"Player Data not found - URL:%(webpath)s/%(platform)s/%(gamertag)s\" % locals())\n        return playerdata\n\n    def _parsePlaylist(self,data=None):\n        ''' Using the json data to assign values to the proper fields\n        '''\n        a = {}\n        playlist = data['metadata']['name']\n        a[playlist] = {\"Tier Rank\": None,\n                       \"Tier Number\": None,\n                       \"Tier Division\": None,\n                       \"Games Played\": None,\n                       \"MMR\": None}\n        try:\n            a[playlist][\"Tier Rank\"] = data['stats']['tier']['metadata']['name']\n            a[playlist][\"Tier Number\"] = data['stats']['tier']['value']\n            a[playlist][\"Tier Divisio\"] = data['stats']['division']['metadata']['name']\n            a[playlist][\"Games Played\"] = data['stats']['matchesPlayed']['value']\n            a[playlist][\"MMR\"] = data['stats']['rating']['value']\n        except Exception as e:\n            logger.info(\"Could not find %(playlist)s data with error: \" % locals(),e)\n\n        return a\n                    \ndef singleRun(gamertag,platform):\n    '''Single run of Webscrape.retrieveDataRLTracker'''\n    logger.info(\"Start for gamertag:%(gamertag)s\"% locals())\n    scrape = Webscrape()\n    data = scrape.retrieveDataRLTracker(gamertag=gamertag,platform=platform)\n    if data is not None:\n        pprint(data)\n        logger.info(\"Finish for gamertag:%(gamertag)s\"% locals())\n\nif __name__ == \"__main__\":\n    '''Run locally to this script'''\n\n    from pprint import pprint # pprint is cool\n    #Pass arguments for name and platform\n    parser = argparse.ArgumentParser(description='Scrape Commandline Options', add_help=True)\n    parser.add_argument('-p', action='store', dest='platform', help='platform options. Example: steam', choices=('steam','psn','xbl'), default='steam')\n    parser.add_argument('-g', action='store', dest='gamertag', help='your gamertag', default='memlo')\n    ###\n    # no longer can search for multiple seasons - this may be revisited at some point\n    # parser.add_argument('-s', action='store', dest='seasons', help='retrieve for season(s) defined. Example: 8 9 11', nargs='+', default=['14']) #need a better way to update this, perhaps dynamically?\n    ###\n\n    results = parser.parse_args()\n    platform = results.platform\n    gamertag = results.gamertag\n    \n    singleRun(gamertag,platform)\n", "414": "from webscrape import scraper\nfrom prediction import form_prediction\nimport torch\n\nif __name__==\"__main__\":\n    \n    #retrieve data\n    scrape=True\n    while scrape:\n        try:\n            open_data,close_data,high_data,low_data,td_days=scraper('1H')\n            scrape=False\n        except:\n            print('Error obtaining data')\n            pass\n    \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    #form prediction and obtain trade size\n    trade_size=form_prediction(open_data,close_data,high_data,low_data,td_days,device) #still need to add prediction value/direction\n    print(trade_size)", "415": "from urllib import urlopen\nfrom BeautifulSoup import BeautifulSoup\nimport re\n\ndef webScrape():\n    float counter = 4170\n    float limit = 4172\n    \n    while counter < limit:\n        houseNum = str(counter)\n        webpage = urlopen(\"http://www.cpshomes.co.uk/lettings/properties/property_details.aspx?reference=P\" + houseNum).read()\n        \n    regexFindURL = re.compile('Postcode: (.*)')\n    \n    deface = re.findall(regexFindURL, webpage)\n    \n    \n    \n    postCode = regexFindURLfrom urllib import urlopen\nfrom BeautifulSoup import BeautifulSoup\nimport re\n\ndef webScrape():\n    float counter = 4170\n    float limit = 4172\n    \n    while counter < limit:\n        houseNum = str(counter)\n        webpage = urlopen(\"http://www.cpshomes.co.uk/lettings/properties/property_details.aspx?reference=P\" + houseNum).read()\n        \n    regexFindURL = re.compile('Postcode: (.*)')\n    \n    deface = re.findall(regexFindURL, webpage)\n    \n    \n    \n    postCode = regexFindURL", "416": "import sys\nimport requests\nfrom bs4 import BeautifulSoup\nimport os\nimport time\nfrom selenium.webdriver.support.ui import WebDriverWait as wait\nfrom selenium import webdriver\nfrom selenium.webdriver.support import expected_conditions\nfrom selenium.webdriver.common.by import By\n\nissueStates = ['open', 'closed']\n\nframeworkOrg = {\n    'tfjs' : 'tensorflow',\n    'tensorflow' : 'tensorflow',\n}\n\nlabel_type = 'performance'\n\nframework = sys.argv[1]\n\nif framework not in frameworkOrg.keys():\n    raise Exception(\"Please enter a valid framework in the command line\")\n\n\nif not os.path.exists(framework):\n    os.makedirs(framework)\n\nfor issueState in issueStates:\n    if not os.path.exists('{}/{}'.format(framework, issueState)):\n        os.makedirs('{}/{}'.format(framework, issueState))\n\n# pages = []\n\ndef webscrape(issueState):\n    url = 'https://github.com/{}/{}/issues?q=is%3Aissue+is%3A{}+label%3Atype%3A{}'.format(frameworkOrg[framework], framework, issueState, label_type)\n    page = requests.get(url)\n    soup = BeautifulSoup(page.text, 'html5lib')\n\n    #getting the number of issues to be investigated so we know when to terminate our loop as we can trust not being able to find a\n    #new page anymore because of the potential html dicrepencies\n    n_issues = soup.find('a', {'class' : 'btn-link selected'})\n    n = int(n_issues.text.replace(',', '').replace(' ', '').replace('\\n', '').replace(issueState[0].upper() + issueState[1:].lower(), ''))\n\n    print('Number of {} issues to be explored : {}'.format(issueState, n))\n\n    #getting all the issues in the page\n    resultSet = soup.find_all('a', {'class' : \"Link--primary v-align-middle no-underline h4 js-navigation-open markdown-title\"})\n    print(resultSet)\n    issueCount = 0\n    pageCount = 1\n    #we encapsulate the code with a while True and try statement so if the connection is lost, we don't lose our progress\n    while True:\n\n        try:\n\n            #if we've investigated all the issues closed, we're done\n            while issueCount < n:\n                \n                print('\\nPage {}:\\n'.format(pageCount))\n\n                for res in resultSet:\n\n                    issueCount = issueCount + 1\n                    issueId = ''\n\n                    #as we progress through the pages, there will exist small html discrepensies. However, at this stage of the code\n                    #we know that there's still issues to be explored so we use a while loop to keep retrying untill we get our wanted\n                    #results\n                    allTags = None\n                    issueDesc = None\n                    issueSoup = None\n                    while allTags == None or issueDesc == None:\n\n                        try:\n                            issueUrl = 'https://github.com' + res['href']\n                            issuePage = requests.get(issueUrl)\n                            issueSoup = BeautifulSoup(issuePage.text, 'html5lib')\n\n                            #in case the issue has a \"Load more\" section, we use selenium to open it up to make sure to scrape the complete issue page\n                            loadMoreButton = issueSoup.find('button', {'class' : 'text-gray pt-2 pb-0 px-4 bg-white border-0'})\n                            if loadMoreButton != None:\n                                driver = webdriver.Chrome('./selenium-webdrivers/chromedriver')\n                                driver.get(issueUrl)\n                                while loadMoreButton != None:\n                                    loadMoreButtonSelen = wait(driver, 20).until(expected_conditions.presence_of_element_located((By.CSS_SELECTOR, 'button.bg-white.border-0')))\n                                    loadMoreButtonSelen.click()\n                                    wait(driver, 5).until(expected_conditions.invisibility_of_element_located(loadMoreButtonSelen))\n                                    issueSoup = BeautifulSoup(driver.page_source, 'html5lib')\n                                    loadMoreButton = issueSoup.find('button', {'class' : 'text-gray pt-2 pb-0 px-4 bg-white border-0'})\n\n                            #getting the first post which will correspond to the issue's description\n                            issueDesc = issueSoup.find('td', class_=\"d-block comment-body markdown-body js-comment-body\")\n                            if issueDesc == None:\n                                print('descrepency (no issue description found). Retrying...')\n                                continue\n\n                            #collecting all the tags from the issue\n                            allTags = issueDesc.findAll()\n                            if allTags == None:\n                                print('descrepency (issue description tags not found). Retrying...')\n                                continue\n\n                            issueId = issueUrl[issueUrl.rfind('/') + 1:]\n                            \n                            try:\n                                f = open('{}/{}/issue{}.html'.format(framework, issueState, issueId), 'w+', encoding='utf-8')\n                                f.write(issueSoup.prettify())\n                            except:\n                                print('Something went wrong')\n                            finally:\n                                f.close()\n                            \n                        except KeyboardInterrupt:\n                            print('Keyboard Interrupt')\n                            return\n\n                    print('Number of {} issues written: {}/{}'.format(issueState, issueCount, n))\n\n                #if the past issue was the last issue, we're done (we put '>=' instead or '==' because the number of closed issue\n                #increase while our program is running)\n                if issueCount >= n:\n                    print('Finished')\n                    return\n                \n                pageCount = pageCount + 1\n\n                #if there is a next page, we get its link and we keep going\n                url = 'https://github.com/{}/{}/issues?page={}&q=is%3Aissue+is%3A{}+label%3Atype%3A{}'.format(frameworkOrg[framework], framework, pageCount, issueState, label_type)\n\n                #as we progress through the pages, there will exist small html discrepensies. However, at this stage of the code\n                #we know that there's still issues to be explored so we use a while loop to keep retrying untill we get our wanted\n                #results\n                resultSet = None\n                while resultSet == None:\n                    try:\n                        page = requests.get(url)\n                        soup = BeautifulSoup(page.text, 'html5lib')\n\n                        pageResult = soup.find('h3')\n                        if pageResult != None:\n                            if pageResult.text == 'No results matched your search.':\n                                print('Error: no more pages (finished).')\n                                return\n\n                        #getting all the issues in the page\n                        resultSet = soup.find_all('a', {'class' : \"Link--primary v-align-middle no-underline h4 js-navigation-open markdown-title\"})\n                        if resultSet == None or len(resultSet) == 0:\n                            print('descrepency (resultset). Retrying...')\n                            continue\n                    except KeyboardInterrupt:\n                        print('Keyboard Interrupt')\n                        return\n        except KeyboardInterrupt:\n            print('Keyboard Interrupt')\n            return\n\n        break\n\nstart = time.time()\n\n#calling the webscrape method\nfor issueState in issueStates:\n    webscrape(issueState)\n\ntimeTaken = time.time() - start\n\nprint('Time taken: {}'.format(timeTaken))", "417": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nDescription: Webscrape Continuous Futures Price of Relevant MOPS Commodities (up to July '22)\nAuthor: @sihir_sains\n\"\"\"\n\n\n#############################################################################\n#   0. Load Necessary Library and Set Up Necessary Variables\n#############################################################################\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait as WDW\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom multiprocessing import get_context\n\nimport pandas as pd\nimport time\n\nexec_path = \"\"\nout_path =  \"\"\n\nurl_mo95 = 'https://www.tradingview.com/chart/?symbol=NYMEX%3AAV01!'\nurl_mo92 = 'https://www.tradingview.com/chart/?symbol=NYMEX%3AN1B1!'\n\nurl_gs10 = 'https://www.tradingview.com/chart/?symbol=NYMEX%3ASGB1!'\nurl_gs500 = 'https://www.tradingview.com/chart/?symbol=NYMEX%3AGHS1!'\n\noptions = Options()\noptions.add_argument('--disable-popup-blocking') # differ on driver version. can ignore. \noptions.add_argument('--disable-notifications') # differ on driver version. can ignore. \n\n\n#############################################################################\n#   1. Define Sub-Functions\n#############################################################################\n\n## To wait when initializing the page until elements are located\ndef wait_at_start(driver):\n    while True:\n        try:\n            WDW(driver,15).until(EC.presence_of_element_located(\n                (By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]')))\n            break\n        except:\n            driver.refresh()\n            continue\n\n## To fetch closing price at a given date\ndef fetch_price(driver):\n    date  = WDW(driver,5).until(EC.presence_of_element_located(\n            (By.XPATH , '''/html/body/div[2]/div[6]/div/div[1]/div[1]/div[5]/\n             div/div[2]/div[1]/div[1]/div[2]/div/div[2]/span'''))).text\n    close = WDW(driver,5).until(EC.presence_of_element_located(\n            (By.XPATH , '''/html/body/div[2]/div[6]/div/div[1]/div[1]/div[5]/\n             div/div[2]/div[1]/div[2]/div[2]/div[4]/div[2]/span'''))).text\n    return date, close\n\n## Sub-function to slightly move mouse until the next date\ndef move_mouse(driver):\n    ActionChains(driver)\\\n        .move_by_offset(1, 0)\\\n        .perform()\n    \n#############################################################################\n#   2. Create Wrapper Function for Webscraping\n#############################################################################\n\ndef webscrape_price(comms):\n    \n    ### Variables and Dataframe Setup ###\n    \n    url_comms = mp_dict[comms]\n    dup_threshold = 80\n    \n    cols = ['date',comms]\n    price_df = pd.DataFrame(columns=cols)\n    ls_prev = pd.DataFrame(columns=cols)\n    state_var = 0 # Initializing state var to count # of duplicate observation    \n    \n    \n    ### Loop to Obtain 1-Year Period Price Dataset ###\n    \n    driver = webdriver.Chrome(executable_path=exec_path,options=options)\n    driver.get(url_comms)\n    \n    wait_at_start(driver)\n    WDW(driver,15).until(EC.presence_of_element_located(\n        (By.XPATH, '/html/body/div[2]/div[6]/div/div[2]/div/div/div/div/div[4]'))).click()\n    WDW(driver,15).until(EC.presence_of_element_located(\n        (By.XPATH, '/html/body/div[2]/div[1]/div[1]/div/div[2]/div/div[1]/div[7]'))).click()\n    \n    start_point = driver.find_element(By.XPATH, '//*[@id=\"drawing-toolbar\"]/div/div/div/div/div[4]/div/div/div[1]/div')\n    ActionChains(driver)\\\n        .move_to_element(start_point)\\\n        .perform()\n    \n    ### Loop to Obtain YTD Price Dataset ###\n    while state_var <= dup_threshold:\n        \n        temp_ls = [fetch_price(driver)]\n        temp_df = pd.DataFrame(temp_ls,columns=cols)\n        \n        if temp_df.equals(ls_prev):\n            state_var = state_var + 1\n            ls_prev = temp_df\n        else:\n            state_var = 0\n            ls_prev = temp_df\n        \n        price_df = pd.concat([price_df,temp_df],axis=0)\n        move_mouse(driver)\n        time.sleep(0.15)\n    \n    \n    ### Ensure No Duplicate Date (Same Date, Different Closing Price) ###\n    price_df = price_df.drop_duplicates().reset_index(drop=True)\n    \n    price_df = price_df.loc[price_df['date']!='\u00e2\u02c6\u2026']\n    price_df['dup'] = price_df.duplicated(subset='date')\n    assert price_df['dup'].unique().shape[0], \"Duplicate obs identified!\"\n    \n    ### Clean Price Dataset ###\n    \n    price_df['data_dt'] = pd.to_datetime(price_df['date']).dt.date\n    price_df = price_df[['data_dt',comms]].set_index(['data_dt'])\n    \n    ### Close Webdriver ###\n    try:\n        driver.close()\n    except:\n        alert = WDW(driver, 3).until(EC.alert_is_present())\n        alert.accept()\n        driver.close()\n\n    \n    return price_df\n\n#############################################################################\n#   3. Fetch Closing Price Time Series by Commodities\n#############################################################################\n\n### Multiprocess the webscraping\n\ncolname = ['mogas_92','mogas_95','gasoil_10','gasoil_500']\nto_mp = [url_mo92, url_mo95, url_gs10, url_gs500]\nmp_dict = dict(zip(colname,to_mp))\n\nif __name__ == '__main__':\n    with get_context(\"spawn\").Pool(processes=4) as p:\n        res = p.map(webscrape_price,mp_dict)\n        p.close()\n        p.join()\n        \n        consol_prc = pd.concat(res,axis=1)\n                \nelse:\n    pass\n\n#############################################################################\n#   4. Clean and Process the Platts Price Dataset\n#############################################################################\n\ntry:\n    print(consol_prc)\n    consol_prc.to_csv(out_path+'data/platts_price.csv') \nexcept:\n    pass", "418": "from selenium import webdriver \nfrom selenium.webdriver.chrome.service import Service\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport csv\nimport urllib.request\nfrom lxml import html\nfrom selenium.webdriver.common.by import By\nimport time\nfrom pprint import pprint as pp\n\nscrape_file = 'Classic-Imports-and-Design/python webscrape/url-lists/johnrichard-sku.csv'\nlogin_url = 'https://www.johnrichard.com/'\ncredentials = ['REGFI1','21030']\n\nwith open(scrape_file) as f:\n    reader = csv.reader(f, delimiter=',')\n    scrape_file = list(reader)\n\nwritefile = open('outputfile.csv', 'w+', encoding='UTF8')\noutput_file = csv.writer(writefile, delimiter=\",\")\n\ndriver = webdriver.Chrome('Classic-Imports-and-Design\\python webscrape\\chromedriver.exe')\n\n#login\ndriver.get(login_url)\nprint(\"Please log into the wholesale account.\")\nprint(credentials[0])\nprint(credentials[1])\ninput()\n\ndata = ['sku', 'Name', 'Brand', 'Category', 'Wholesale', 'Retail', 'Description', 'Images', 'Length', 'Width', 'Height']\noutput_file.writerow(data)\n\nfails = []\n\nitr = 1\nfor product in scrape_file:\n    product[0] = product[0].strip()\n    product[1] = product[1].strip()\n    pp('Scarping product ' + product[0] + ' | Item ' +  str(itr) + ' / ' + str(len(scrape_file)))\n    itr = itr + 1\n    data = []\n    \n    try: \n        driver.get('https://www.johnrichard.com/shop/' + product[0])\n    except:\n        fails.append(str(itr) + product[0])\n        continue\n    time.sleep(2.5)\n    content = driver.page_source\n    soup = BeautifulSoup(content, \"html.parser\")\n    tree = html.fromstring(driver.page_source) \n\n    #SKU\n    data.append(product[0])\n    #Name\n    try:\n        data.append(driver.find_elements(By.XPATH, '/html/body/div[1]/section[3]/div/div/div/div/div/ui-view/shopping-container/div/ui-view/shopping-one-up/div[1]/div[1]/h2')[0].text) \n    except:\n        data.append('ERROR')\n        fails.append(product[0] + ' has no name')\n    #Brand\n    data.append('John Richard')\n    #Category\n    data.append(product[1])\n    #Wholesale\n    try:\n        data.append( driver.find_elements(By.XPATH, '/html/body/div[1]/section[3]/div/div/div/div/div/ui-view/shopping-container/div/ui-view/shopping-one-up/div[2]/div/div[2]/div/div/div[7]/p/span/span')[0].text.lstrip('$').replace(',',''))\n    #Retail\n        data.append( round(float(data[4])*2.5) )\n    except:\n        data.append('ERROR')\n        data.append('ERROR')\n        fails.append(product[0] + ' has no wholesale')\n    #Description\n    try:\n        data.append( driver.find_elements(By.XPATH, '/html/body/div[1]/section[3]/div/div/div/div/div/ui-view/shopping-container/div/ui-view/shopping-one-up/div[2]/div/div[2]/div/div/div[8]/div/div/p')[0].text ) \n    except:\n        data.append('  ')\n        fails.append(product[0] + ' has no description')\n    #Images\n    img_filenames = []\n    c = 1\n    while c < 20:\n        img_url = \"https://s3.amazonaws.com/emuncloud-staticassets/productImages/jr045/large/\" + product[0] + (\"\" if c==1 else \"_\") + (\"\" if c==1 else str(c))\n        img_url = img_url + \".jpg\"\n        try:\n            urllib.request.urlretrieve(img_url, \"Classic-Imports-and-Design/python webscrape/product-images/\" + product[0] + \"-\" + str(c)+\".jpg\")\n            img_filenames.append(product[0] + \"-\" + str(c))\n            c = c+1\n        except:\n            #continue to nextimg\n            c = c+1\n    data.append(\",\".join(img_filenames))\n    #Dimensions\n    try:\n        dimension_string = driver.find_elements(By.XPATH, '/html/body/div[1]/section[3]/div/div/div/div/div/ui-view/shopping-container/div/ui-view/shopping-one-up/div[2]/div/div[2]/shopping-one-up-heading/div[2]/div[1]/p[2]')[0].text\n        dimension_list = dimension_string.split('X')\n        data.append(dimension_list[2].strip())\n        data.append(dimension_list[1].strip())\n        data.append(dimension_list[0].strip())\n    except:\n        fails.append(product[0] + ' has no dimensions')\n    pp(data)\n\n    output_file.writerow(data)\n\nwritefile.close()\npp(fails)\n\n#driver.get(url)\n#time.sleep()\n#content = driver.page_source\n#soup = BeautifulSoup(content, \"html.parser\")\n#tree = html.fromstring(driver.page_source) \n\n#tree.xpath()[0].text (.text needed if end of xpath NOT /text)\n#soup.find(\"TAG-TYPE\", class_=\"CLASS-NAME\").text\n\n#urllib.request.urlretrieve(IMG-URL, FILENAME) \n\n#output_file.writerow(data)", "419": "import smtplib\nfrom email.message import EmailMessage\nfrom urllib.request import urlopen as uReq\nfrom bs4 import BeautifulSoup as soup\nimport time\n\n\n\n# Scrapes Robinhood for price (just ETH for now)\ndef webScrape():\n\tmy_url = 'https://finance.yahoo.com/quote/BTC-USD?p=BTC-USD&.tsrc=fin-srch'\n\n\t#opening up connection and grab page\n\tuClient = uReq(my_url)\n\tpage_html = uClient.read()\n\tuClient.close()\n\n\t#html parsing\n\tpage_soup = soup(page_html, \"html.parser\")\n\n\t#grab price diff %\n\tdata = str(page_soup.findAll(\"span\", {\"data-reactid\":\"34\"})[1].text)\n\n\tdata = data[:-2]\n\tnewData = \"\"\n\tfor element in reversed(data):\n\t\tif element == '(':\n\t\t\tbreak\n\t\telse:\n\t\t\tnewData += element\n\tnewData = newData[::-1]\n\treturn float(newData)\n\n\n\n# Handles sending the sms mesage\ndef sendMessage(body, subject, to):\n\t#use EmailMessage library and set variables based on function arguments\n\tmsg.set_content(body)\n\tmsg['subject'] = subject\n\tmsg['to'] = to\n\n\tserver.send_message(msg)\n\tprint('sent message')\n\n\tdel msg['to']\n\n\n\n\n\n# Driver\nif __name__ == '__main__':\n\t#email and login setup\n\tmsg = EmailMessage()\n\tuser = \"cryptoalertcoms@gmail.com\"\n\tmsg['from'] = user\n\tpassword = \"gsvcsffnwykpzjwx\"\n\n\tserver = smtplib.SMTP(\"smtp.gmail.com\", 587)\n\tserver.starttls()\n\tserver.login(user, password)\n\n\n\t#intialize variables\n\tcurrentChange = 1.0\n\tpriorChange = 1000.0\n\tsubject = 'This is a subject'\n\tbody = 'This is a body'\n\tto = \"5869071926@vtext.com\" #the number or email that we're sending the message to. Make this an option later\n\tcount = 0\n\n\n\n\t#get initial % change in price \n\tpriorChange = webScrape()\n\t#TODO: (send message stating that this is what we'll go off of)\n\tsubject = 'INITIAL'\n\tbody = 'This is the first message. Sending alerts when a % change in daily price differs by 1 or more, starting at {}%'.format(priorChange)\n\tsendMessage(body, subject, to)\n\n\n\n\t#enter while loop that will continue to run program\n\twhile count != 60:\n\t\tprint('------This is loop {}------'.format(count + 1))\n\t\tprint('Previous change: {}'.format(priorChange))\n\n\t\t#get current 24 hour % change\n\t\tcurrentChange = webScrape()\n\t\tprint('Current change: {}'.format(currentChange))\n\n\t\tif currentChange != priorChange:\n\t\t\tif priorChange > 0 and currentChange < 0:\n\t\t\t\t#output message saying it's now down today\n\t\t\t\tsubject = 'DOWN'\n\t\t\t\tbody = 'ETH is now down today at {}%'.format(currentChange)\n\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t#set prior to now be current\n\t\t\t\tpriorChange = currentChange\n\t\t\telif priorChange < 0 and currentChange > 0:\n\t\t\t\t#output message saying it's now up today\n\t\t\t\tsubject = 'UP'\n\t\t\t\tbody = 'ETH is now up today at {}%'.format(currentChange)\n\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t#set prior to now be current\n\t\t\t\tpriorChange = currentChange\n\t\t\telif currentChange > 0:\n\t\t\t\tif currentChange - priorChange >= 1:\n\t\t\t\t\t#output message saying that it's now up even more today at ___\n\t\t\t\t\tsubject = 'UP'\n\t\t\t\t\tbody = 'ETH is now up even more today at {}%'.format(currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\t\t\t\telif currentChange - priorChange <= -1:\n\t\t\t\t\t#output message saying that it's now up less today at ___\n\t\t\t\t\tsubject = 'UP'\n\t\t\t\t\tbody = 'ETH is now up less today at {}%'.format(currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\t\t\telif currentChange < 0:\n\t\t\t\tif currentChange - priorChange >= 1:\n\t\t\t\t\t#ouptut message saying that it's now down even more today at ___\n\t\t\t\t\tsubject = 'DOWN'\n\t\t\t\t\tbody = 'ETH is now down even more today at {}%'.format(currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\t\t\t\telif currentChange - priorChange <= -1:\n\t\t\t\t\t#output message saying that it's now down less today at ___\n\t\t\t\t\tsubject = 'DOWN'\n\t\t\t\t\tbody = 'ETH is now down less today at {}%'.format(currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\n\t\t#wait 2 minutes before entering loop again, making this program go on for 2 hours w/ how the while loop is set up\n\t\ttime.sleep(120)\n\n\t\tcount = count + 1\n\n\t#exits while loop, we are done\t\t\t\n\tserver.quit()\n\n\t\t\t\t\n\n\n\n\n\n", "420": "from bs4 import BeautifulSoup\nimport requests\n\n\ndef main2(player_search):\n    webscraped_list = webscrape(player_search)\n    if not webscraped_list:\n        return None\n    search_result = search(player_search, webscraped_list)\n\n    if search_result is not False:\n        cleanid_result_sorted = id_cleaner(search_result)\n        id_sorted_list = id_sorter(cleanid_result_sorted)\n\n        player_sorted_list = player_sort(id_sorted_list)\n        elo_sorted_list = sort_by_elo(player_sorted_list)\n        return elo_sorted_list\n    else:\n        return None\n\n\ndef webscrape(player_search):\n    value_list = []\n    new_list = []\n    source = requests.get(f'http://ratingupdate.info/?name={player_search}').text\n    soup = BeautifulSoup(source, 'lxml')\n\n    for x in soup.find('div', class_='table-container').find_all('td'):\n        player_data = x.text\n        if \"\u2192\" in player_data:\n            player_data = player_data.replace(\" \u2192\\n\", \"\")\n            test = x.find('a', href=True)\n            player_id = test[\"href\"]\n            value_list.append(player_id)\n        value_list.append(player_data)\n        new_list = [value_list[i:i + 5] for i in range(0, len(value_list), 5)]\n\n    return new_list\n\n\ndef search(user_search, webscraped_list):\n    newer_list = []\n    for i, z in enumerate(webscraped_list):  # i dont think enumerate is needed?\n        if user_search.lower() == z[1].lower():\n            newer_list.append(z)\n    return newer_list\n\n\ndef id_cleaner(search_result):\n    cleanid_result = []\n    for z in search_result:\n        temp_id = z[0]\n        temp_id = temp_id[:-3]\n        temp_id = temp_id.replace(\"/player/\", \"\")\n        cleanid_result.append(temp_id)\n        for i, elements in enumerate(z):\n            if elements == z[0]:\n                continue\n            cleanid_result.append(z[i])\n    cleanid_result_sorted = [cleanid_result[i:i + 5] for i in range(0, len(cleanid_result), 5)]\n    return cleanid_result_sorted\n\n\ndef id_sorter(clean_id):\n    temp_id_sort = []\n    perm_id_sort = []\n    for ix, elems in enumerate(clean_id):\n        if ix == 0:\n            temp_id_sort.append(elems)\n            perm_id_sort.append(elems)\n            continue\n        new_player_id = elems[0]\n\n        for zx, ex in enumerate(temp_id_sort):\n            old_player_id = ex[0]\n            if new_player_id == old_player_id:\n                temp_id_sort.insert(zx, elems)\n                perm_id_sort.insert(zx, elems)\n                break\n            elif new_player_id != old_player_id:\n                if new_player_id != old_player_id and ex == temp_id_sort[-1]:\n                    temp_id_sort.insert(zx + 1, elems)\n                    break\n                else:\n                    continue\n\n# I need to loop over this: cleanid_result_sorted = [cleanid_result[i:i + 5] for i in range(0, len(cleanid_result), 5)]\n    temp_id_sort.reverse()\n    return temp_id_sort\n\n\ndef player_sort(id_sorted_list):\n    temp_player_sort = []\n    new_player_list = []\n    for ix, elems in enumerate(id_sorted_list):\n        if ix == 0:\n            temp_player_sort.append(elems)\n            continue\n\n        if elems[0] == id_sorted_list[ix-1][0]:\n            temp_player_sort.append(elems)\n\n        elif elems[0] != id_sorted_list[ix-1][0]:\n            testo = temp_player_sort.copy()\n            new_player_list.append(testo)\n            temp_player_sort.clear()\n            temp_player_sort.append(elems)\n    temp_2 = temp_player_sort.copy()\n    new_player_list.append(temp_2)\n    return new_player_list\n\n\ndef sort_by_elo(player_sorted_list):\n    perm_elo_sort = []\n    temp_elo_sort = []\n    for lists in player_sorted_list:\n        if temp_elo_sort:\n            testo = temp_elo_sort.copy()\n            testo.reverse()\n            perm_elo_sort.append(testo)\n            temp_elo_sort.clear()\n        for ix, elems in enumerate(lists):\n            if ix == 0:\n                temp_elo_sort.append(elems)\n                continue\n            just_elo = elo_splitter(elems[3])\n\n            for zx, ex in enumerate(temp_elo_sort):\n                ex_elo = elo_splitter(ex[3])\n                if just_elo > ex_elo:\n                    temp_elo_sort.insert(zx, elems)\n                    break\n                elif just_elo < ex_elo:\n                    if just_elo < ex_elo and ex == temp_elo_sort[-1]:\n                        temp_elo_sort.insert(zx + 1, elems)\n                        break\n                    else:\n                        continue\n                elif just_elo == ex_elo:\n                    temp_elo_sort.insert(zx, elems)\n                    break\n                else:\n                    temp_elo_sort.insert(zx, elems)\n                    break\n    testo = temp_elo_sort.copy()\n    testo.reverse()\n    perm_elo_sort.append(testo)\n    temp_elo_sort.clear()\n    return perm_elo_sort\n\n\ndef elo_splitter(unsplit_elo):\n    just_elo = unsplit_elo\n    just_elo = just_elo.split(\" \u00b1\")  # splits the elo string into a list\n    just_elo = just_elo[0]  # gives me only the elo number in the list\n    just_elo = int(just_elo)\n    return just_elo\n\n\ndef paste(search_data):\n    perm_string = \"\"\n    temp_string = \"\"\n    max_chr_alert = False\n    length_check = 0\n    for elements in search_data:\n        if length_check > 1024:\n            max_chr_alert = True\n            break\n        if perm_string:\n            new_player = \"__***New player below!***__\\n\"\n            perm_string = new_player + perm_string\n        for ele in elements:\n            length_check = len(temp_string + perm_string)\n            if length_check > 1024:\n                max_chr_alert = True\n                break\n            temp_string = f\"**{ele[1]} **| *{ele[2]}* | {ele[3]} | {ele[4]}\\n\"\n            perm_string = temp_string + perm_string\n    return perm_string, max_chr_alert\n", "421": "import requests\nfrom bs4 import BeautifulSoup\nfrom pyltp import SentenceSplitter\n\nclass WebScrape(object):\n    def __init__(self, word, url):\n        self.url = url\n        self.word = word\n\n    # \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n    def web_parse(self):\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n        req = requests.get(url=self.url, headers=headers)\n\n        # \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n        if req.status_code == 200:\n            soup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n            return soup\n        return None\n\n    # \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n    def get_gloss(self):\n        soup = self.web_parse()\n        if soup:\n            lis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n            if lis:\n                for li in lis('li'):\n                    if '", "422": "# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n\n\n# useful for handling different item types with a single interface\nfrom itemadapter import ItemAdapter\n\n\nclass AmazonWebscrapePipeline:\n    def process_item(self, item, spider):\n        return item\n", "423": "import sqlite3 as sql\nimport webscrape\n\nimport warnings\n\nfrom tqdm import tqdm\n\n\nclass DataBase:\n    \"\"\"\n    Base object for database connection and cursor creation.\n    \n    Attributes:\n        conn: sqlite3.Connection \n        cursor: sqlite3.Cursor \n    \"\"\"\n    def __init__(self, database:str):\n        self.conn = sql.connect(database)\n        self.cursor = self.conn.cursor()\n\n        self.conn.execute('PRAGMA synchronous = 0;')\n\n\nclass Table(DataBase):\n    \"\"\"\n    Base object for handling tables within a database, child object of DataBase.\n\n    Attributes:\n        name(str): table name\n    \"\"\"\n    def __init__(self, database:str, name:str):\n        super().__init__(database)\n        self.name = name\n\n    def delete_table(self):\n        self.cursor.execute(f\"\"\"\n                DROP TABLE IF EXISTS {self.name};\n                \"\"\")\n\n    def create_table(self):\n        print(\"original function called 'create table'\")\n        self.cursor.execute(f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.name}\n                (\n                    column TEXT\n                );\n            \"\"\"\n            )\n        self.conn.commit()\n    \n    def re_init(self):\n        self.delete_table()\n        self.create_table()\n\n    def fetch_hrefs(self):\n        try:\n            self.cursor.execute(f\"\"\"\n                SELECT href FROM {self.name}\n                \"\"\")\n            hrefs = [href[0] for href in self.cursor.fetchall()]\n            if hrefs:\n                return hrefs\n            warnings.warn(f\"No hrefs found in {self.name}\")\n            return None\n        except sql.OperationalError:\n            self.re_init()\n\n\n\nclass href_table(Table):\n    \"\"\"\n    Manages href database, creation, loading, updating\n\n    Attributes:\n        name(str): Name of the table\n\n    \"\"\"\n    def __init__(self, database:str, name:str):\n        super().__init__(database, name)\n\n    def create_table(self):\n        self.cursor.execute(f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.name}\n                (\n                href TEXT\n                );\n            \"\"\")\n        self.conn.commit()\n\n    def load_hrefs(self, page_end):\n        print(\"\\nReloading hrefs from yiffy site\")\n        self.re_init()\n        \n        page_range = range(1, page_end + 1)\n        for page in tqdm(page_range):\n            hrefs = webscrape.MoviesHref(page)\n            for href in hrefs.movies_href:\n                self.insert_href(href)\n    \n    def insert_href(self, href):\n        self.cursor.execute(f\"\"\"\n                INSERT INTO {self.name} (href)\n                    VALUES (\"{href}\")\n        \"\"\")\n        self.conn.commit()       \n\n    def update(self, new_hrefs, current_hrefs):\n        concat_hrefs = new_hrefs + current_hrefs\n        if not concat_hrefs:\n            raise ValueError(\"No href values found\")\n            \n        self.re_init()\n        for href in concat_hrefs:\n            self.insert_href(href)\n\n    def remove_href(self, href):\n        self.cursor.execute(f\"\"\"\n                DELETE FROM {self.name} WHERE href = \"{href}\";\n        \"\"\")\n        self.conn.commit()\n\n\nclass MovieTable(Table):\n    def __init__(self, database:str, name:str):\n        super().__init__(database, name)\n        self.broken_hrefs = []\n\n    def create_table(self):\n        self.cursor.execute(f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.name}\n                (\n                title TEXT, \n                year INT, \n                genre TEXT, \n                rating FLOAT,\n                href TEXT\n                );\n            \"\"\")\n        self.conn.commit()\n\n    def load_movies(self, hrefs):\n        print(\"\\nLoading movies\")\n        missing_hrefs = self._find_missing_hrefs(hrefs)\n        for href in tqdm(missing_hrefs):\n            movie = webscrape.Movie(href)\n            self.insert_movie(movie)\n\n    def _find_missing_hrefs(self, hrefs):\n        movie_hrefs = self.fetch_hrefs()\n        if not movie_hrefs:\n            return hrefs\n        else:\n            return [href for href in hrefs if href not in movie_hrefs]\n        \n    def insert_movie(self, Movie):\n        if not Movie.info:\n            self.broken_hrefs.append(Movie.href)\n            return\n        title = Movie.info.get(\"title\")\n        year = Movie.info.get(\"year\")\n        genre = Movie.info.get(\"genre\")\n        rating = Movie.info.get(\"rating\")\n        href = Movie.info.get(\"href\")\n\n        self.cursor.execute(f\"\"\"\n                INSERT INTO movie (title, year, genre, rating, href)\n                    VALUES (\"{title}\", \"{year}\", \"{genre}\", \"{rating}\", \"{href}\")\n        \"\"\")\n        self.conn.commit()\n\n    def drop_broken_hrefs(self, href_table):\n        for broken in self.broken_hrefs:\n            href_table.remove_href(broken)\n\n    def update(self, new_hrefs):\n        \"\"\"\n        Inserts new movies from hrefs to database\n        \n        Parameters:\n            new_hrefs: list containing new hrefs to insert into database\n\n        Note:\n            This method does not check current movie db for copies, if inserting make sure movies\n            are not already in database.\n        \"\"\"\n        print(\"Updating movies\")\n        for href in tqdm(new_hrefs):\n            movie = webscrape.Movie(href)\n            self.insert_movie(movie)\n\n\n    \nclass DataBaseManager:\n    \"\"\"Convient object for managing href and movie tables\"\"\"\n    def __init__(self, href_table, movie_table):\n        self.movie_table = movie_table\n        self.href_table = href_table\n\n    def update_databases(self):\n        \"\"\"Append latest movies released from Yify site to databases\"\"\"\n        pre_loaded_hrefs = self.href_table.fetch_hrefs()\n        new_hrefs = self._get_new_hrefs(pre_loaded_hrefs)\n\n        if not new_hrefs:\n            warnings.warn(\"Nothing to update\")\n        else:\n            self.movie_table.update(new_hrefs)\n            self.href_table.update(new_hrefs, pre_loaded_hrefs)\n\n\n    @staticmethod\n    def _get_new_hrefs(table_hrefs):\n        \"\"\"Search yify latest until href matches first_href in list\"\"\"\n        first_href = table_hrefs[0]\n\n        new_hrefs = []\n        page = 1\n        while True:\n            hrefs = webscrape.MoviesHref(page).movies_href\n            for href in hrefs:\n                if href == first_href:\n                    return new_hrefs                 \n                else:\n                    new_hrefs.append(href)\n            page +=1", "424": "import requests\nfrom bs4 import BeautifulSoup\nfrom ltp import LTP\nltp = LTP()\n\nclass WebScrape(object):\n    def __init__(self, word, url):\n        self.url = url\n        self.word = word\n\n    # \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n    def web_parse(self):\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n        req = requests.get(url=self.url, headers=headers)\n\n        # \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n        if req.status_code == 200:\n            soup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n            return soup\n        return None\n\n    # \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n    def get_gloss(self):\n        soup = self.web_parse()\n        if soup:\n            lis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n            if lis:\n                for li in lis('li'):\n                    if '", "425": "import requests\nfrom bs4 import BeautifulSoup\nfrom ltp import LTP\nltp = LTP()\n\nclass WebScrape(object):\n    def __init__(self, word, url):\n        self.url = url\n        self.word = word\n\n    # \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n    def web_parse(self):\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n        req = requests.get(url=self.url, headers=headers)\n\n        # \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n        if req.status_code == 200:\n            soup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n            return soup\n        return None\n\n    # \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n    def get_gloss(self):\n        soup = self.web_parse()\n        if soup:\n            lis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n            if lis:\n                for li in lis('li'):\n                    if '", "426": "import requests\nfrom bs4 import BeautifulSoup\nfrom ltp import SentenceSplitter\n\nclass WebScrape(object):\n    def __init__(self, word, url):\n        self.url = url\n        self.word = word\n\n    # \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n    def web_parse(self):\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n        req = requests.get(url=self.url, headers=headers)\n\n        # \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n        if req.status_code == 200:\n            soup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n            return soup\n        return None\n\n    # \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n    def get_gloss(self):\n        soup = self.web_parse()\n        if soup:\n            lis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n            if lis:\n                for li in lis('li'):\n                    if '", "427": "import requests\nfrom bs4 import BeautifulSoup\nimport ltp\n\nclass WebScrape(object):\n    def __init__(self, word, url):\n        self.url = url\n        self.word = word\n\n    # \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n    def web_parse(self):\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n        req = requests.get(url=self.url, headers=headers)\n\n        # \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n        if req.status_code == 200:\n            soup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n            return soup\n        return None\n\n    # \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n    def get_gloss(self):\n        soup = self.web_parse()\n        if soup:\n            lis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n            if lis:\n                for li in lis('li'):\n                    if '", "428": "import requests\nfrom bs4 import BeautifulSoup\nfrom pyltp import SentenceSplitter\n\nclass WebScrape(object):\n    def __init__(self, word, url):\n        self.url = url\n        self.word = word\n\n    # \u722c\u53d6\u767e\u5ea6\u767e\u79d1\u9875\u9762\n    def web_parse(self):\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \\\n                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n        req = requests.get(url=self.url, headers=headers)\n\n        # \u89e3\u6790\u7f51\u9875\uff0c\u5b9a\u4f4d\u5230main-content\u90e8\u5206\n        if req.status_code == 200:\n            soup = BeautifulSoup(req.text.encode(req.encoding), 'lxml')\n            return soup\n        return None\n\n    # \u83b7\u53d6\u8be5\u8bcd\u8bed\u7684\u4e49\u9879\n    def get_gloss(self):\n        soup = self.web_parse()\n        if soup:\n            lis = soup.find('ul', class_=\"polysemantList-wrapper cmn-clearfix\")\n            if lis:\n                for li in lis('li'):\n                    if '", "429": "# 2021-03-16\n#webdriver from google chrome:    https://www.youtube.com/watch?v=b5jt2bhSeXs\nfrom typing import Set\nfrom selenium.webdriver.common.by import By\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import WebDriverWait\n# from selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\n# from selenium.webdriver.common.action_chains import ActionChains\nimport pandas as pd, numpy as np\nimport datetime, time, random, glob, os, shutil, re\nfrom docx import Document\n\ndef main():\n\t#inputs\n\tpath = r'C:\\Users\\yongj\\Downloads\\2021-03-16 SEL Scraper\\Links.xlsx'\n\tpath_storage = r'C:\\Users\\yongj\\Downloads\\3. Business News\\To Read'\n\theadless_window = True\n\t# headless_window = False\n\tidx_link = 0\n\n\n\t#Part One: Get links from excel\n\tPartOne = ObtainLinks(path)\n\tlist_links = PartOne.ReadExcel()\n\n\n\t#setting up the options for the chromedriver for Part Two\n\toptions = Options()\n\tif headless_window == True:\n\t\toptions.add_argument('--headless')\n\t\toptions.add_argument(\"window-size=1920,1080\") \n\t#defining the window size of the headless browser avoids errors with finding location and date for each job posting\n\tpath_chromedriver = r'C:\\Program Files (x86)\\chromedriver.exe'\n\tdriver = webdriver.Chrome(executable_path=path_chromedriver, options=options)\n\n\t#Part Two: Webrscrape\n\tPartTwo = Webscrape(path_storage)\n\twhile idx_link < len(list_links):\n\t\tlink = list_links[idx_link]\n\t\tprint(f'{idx_link+1}/{len(list_links)})\\t{link}')\n\t\tarticle_type = PartTwo.Article_type(link)\n\t\t# print(article_type)\n\t\tPartTwo.AccessWebpage(driver, link, article_type)\n\t\tprint('\\n')\t\n\t\tidx_link+=1\n\n\n\t#Part Three: Moving Word Docs to the \"To Read Folder\" & updating excel\n\t#Refer to python saved in Superseded to move all word docs to 'To Read' folder and update the excel\n\timport sys\n\tsys.path.insert(1, 'C:/Users/yongj/Documents/Coding/Python/Web Scraping/0. Superseded')\n\tfrom Update_Excel_Move_Word import UpdateExcel_MoveWord\n\tUpdateExcel_MoveWord()\n\n\n\nclass ObtainLinks():\n\tdef __init__(self, path):\n\t\tself.path = path\n\n\tdef ReadExcel(self):\n\t\t# print(self.path)\n\t\tdf = pd.read_excel(self.path)\n\t\t# print(df['No Seekingalpha Links'].to_list()[:10])\n\t\tself.list_links = df['No Seekingalpha Links'].to_list()\n\t\treturn self.list_links\n\nclass SEL_Scrapers():\n\tdef __init__(self, driver, link):\n\t\tself.driver = driver\n\t\tself.link = link\n\n\t# comment below is for reference for the pieces of info required\n\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\tdef Create_docx(self):    \n\t    url = self.link\n\t    title = self.title\n\t    date = self.date\n\t    author = self.author\n\t    summary = self.summary\n\t    content = self.content\n\t    table_data = self.table_data\n\t    img_tags = self.img_tags\n\n\t    document = Document()\n\t        \n\t    document.add_heading(url)\n\n\t    #Removing characters that cannot be used in the title\n\t    title = title.replace('/', \" \")\n\t    title = title.replace('\"', '')\n\t    title = title.replace(':', ' - ') \n\t    title = title.replace('?', ' - ')\n\t    title = title.replace('*', ' - ')\n\t    title = title.replace('|', ' - ')\n\n\t    #add in content to the word docx\n\t    document.add_paragraph(title)\n\t    document.add_paragraph(str(date))\n\t    document.add_paragraph(author)\n\t    for a in summary:\n\t        document.add_paragraph(a.text)\n\t    row=1\n\t    for p in content:\n\t    # \tdocument.add_paragraph(f'Line {row}: \\n{p.text}')\n\t    # \trow+=1\n\t    \tif len(p.text.split()) >3:\n\t        \tdocument.add_paragraph(p.text)\n\n\t    #Create a table if table data is passed in\n\t    if table_data:\n\t        # rowNum = len(table_data)\n\t        colNum = len(table_data[0])\n\t        #initialize table with a blank row\n\t        Table = document.add_table(rows=1,cols=colNum)\n\t        # # table styles link:  https://python-docx.readthedocs.io/en/latest/user/styles-understanding.html#table-styles-in-default-template\n\t        Table.style = \"Table Grid\"\n\t        for row in table_data:\n\t            row_Cells = Table.add_row().cells \n\t            for cell in range(0,colNum):\n\t                row_Cells[cell].text = row[cell]    \n\n\n\t    #Append Images to word doc\n\t    links =[]\n\n\t    for img in img_tags:\n\t        try:\n\t            img_width = img['data-width']\n\t            links.append(img['src'])\n\t        except KeyError:\n\t            continue\n\t    # print(links)\n\t    list_imgtypes = ['.jpg','.png']\n\t    num = 1\n\t    for link in links:\n\t        #Get image type\n\t        image_type = link[-4:]\n\t        if image_type in list_imgtypes:\n\t            image_type=image_type\n\t        else:\n\t            image_type='.jpeg'\n\t        \n\t        try:\n\t            filename = \"Image\"+str(num)\n\t            imagefile = open(filename+image_type, \"wb\")\n\t            imagefile.write(requests.get(link).content)\n\t            imagefile.close()\n\t            # print('Created image')\n\t            document.add_paragraph(filename+image_type)\n\t            document.add_picture(filename+image_type,width=docx.shared.Inches(6))\n\t            # document.add_picture(requests.get(link).content,width=Inches(4))\n\t            os.remove(filename+image_type)\n\t            # print(f'added {filename}')\n\n\t        except:\n\t            # print('Then Failed to add')\n\t            filename = \"Image\"+str(num)\n\t            document.add_paragraph(filename+image_type + \" COULD NOT BE ADDED DUE TO SOME ERROR.\\n\"+link)\n\t        num+=1\n\n\t    wordname = str(date)+\" \" + title+\".docx\"\n\t    document.save(wordname)\n\t    # path_toREAD = r\"C:\\Users\\yongj\\Downloads\\3. Business News\\To Read\"+'\\\\'+wordname\n\t    # document.save(path_toREAD)\n\tdef Reset_PgINFO(self):\n\t\tself.url = ''\n\t\tself.title =''\n\t\tself.date =''\n\t\tself.author=''\n\t\tself.summary= []\n\t\tself.content=[]\n\t\tself.table_data=[]\n\t\tself.img_tags=[]\n\n\n\tdef oilprice(self):\n\t\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\t\tprint('Webscraping link from: \"Oilprice.com\"')\n\t\tdriver = self.driver\n\n\t\tself.title = driver.find_element_by_xpath('//*[@id=\"pagecontent\"]/div[3]/div/div[1]/div[2]/div[2]/h1').text\n\t\tself.author =  driver.find_element_by_xpath('//*[@id=\"pagecontent\"]/div[3]/div/div[1]/div[2]/div[2]/span/a').text\n\t\tself.date =  driver.find_element_by_xpath('//*[@id=\"pagecontent\"]/div[3]/div/div[1]/div[2]/div[2]/span').text\n\t\tself.date = Webscrape.FormatDate(self, self.date)\n\t\tself.summary =  []\n\t\tself.content =  driver.find_elements_by_xpath('//*[@id=\"news-content\"]')\n\t\tself.table_data =  []\n\t\tself.img_tags =  []\n\n\t\tprint(self.title)\n\t\t# for p in self.content:\n\t\t# \tprint(p.text)\n\n\tdef boereport(self):\n\t\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\t\tprint('Webscraping link from: \"boereport.com\"')\n\t\tdriver = self.driver\n\n\t\tself.title = driver.find_element_by_xpath('/html/body/div[1]/div/div/main/article/header/h1').text\n\t\tself.author =  'boereport'\n\t\tself.date =  driver.find_element_by_xpath('/html/body/div[1]/div/div/main/article/header/p/time[1]').text\n\t\tself.date = Webscrape.FormatDate(self, self.date)\n\t\tself.summary =  []\n\t\tself.content =  driver.find_elements_by_xpath('/html/body/div[1]/div/div/main/article/div')\n\t\tself.table_data =  []\n\t\tself.img_tags =  []\n\n\tdef cnbc(self):\n\t\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\t\tprint('Webscraping link from: \"cnbc.com\"')\n\t\tdriver = self.driver\n\n\t\tself.title = driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[1]/div[1]/h1').text\n\t\tself.author =  driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[2]/div[1]/div/div/div/div/a[1]').text\n\t\tself.date =  driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[1]/div[2]/time[1]').text\n\t\tself.date = Webscrape.FormatDate(self, self.date)\n\t\tself.summary =  driver.find_elements_by_xpath('//*[@id=\"RegularArticle-KeyPoints-4\"]/div/div[2]/div/div/ul')\n\t\tself.content =  driver.find_elements_by_xpath('//*[@id=\"RegularArticle-ArticleBody-5\"]/div[2]')\n\t\tself.table_data =  []\n\t\tself.img_tags =  []\n\n\t\tprint(self.title,'\\n')\n\t\t# for p in self.content:\n\t\t# \tprint(p.text)\n\n\tdef nbc(self):\n\t\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\t\tprint('Webscraping link from: \"nbc.com\"')\n\t\tdriver = self.driver\n\n\t\tself.title = driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[1]/div[1]/h1').text\n\t\tself.author =  driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[2]/div[1]/div/div/div/div/a[1]').text\n\t\tself.date =  driver.find_element_by_xpath('//*[@id=\"main-article-header\"]/div/div[1]/div[2]/time[1]').text\n\t\tself.date = Webscrape.FormatDate(self, self.date)\n\t\tself.summary =  driver.find_elements_by_xpath('//*[@id=\"RegularArticle-KeyPoints-4\"]/div/div[2]/div/div/ul')\n\t\tself.content =  driver.find_elements_by_xpath('//*[@id=\"RegularArticle-ArticleBody-5\"]/div[2]')\n\t\tself.table_data =  []\n\t\tself.img_tags =  []\n\n\t\tprint(self.title,'\\n')\n\t\t# for p in self.content:\n\t\t# \tprint(p.text)\n\n\tdef AB(self):\n\t\t# def Create_docx(self, url, title, date, author, summary, content, table_data, img_tags):\n\t\tprint('Webscraping link from: \"ab.ca\"')\n\t\tdriver = self.driver\n\n\t\tself.title = driver.find_element_by_xpath('//*[@id=\"top\"]/header/div[4]/div[2]/div/h1').text\n\t\tself.author =  'AB Government'\n\t\tself.date =  driver.find_element_by_xpath('//*[@id=\"top\"]/header/div[4]/div[2]/div/div/time').text\n\t\tself.date = Webscrape.FormatDate(self, self.date)\n\t\tself.summary =  driver.find_elements_by_xpath('//*[@id=\"top\"]/header/div[4]/div[2]/div')\n\t\tself.content =  driver.find_elements_by_xpath('//*[@id=\"main\"]/div[2]/div')\n\t\tself.table_data =  []\n\t\tself.img_tags =  []\n\n\t\tprint(self.title,'\\n')\n\t\t# for p in self.content:\n\t\t# \tprint(p.text)\n\n\t\t\t\nclass Webscrape():\n\tdef __init__(self, path_storage):\n\t\tself.path_storage = path_storage\n\n\tdef FormatDate(self, date):\n\t\t# print(f'Original date from link:   \"{date}\"')\n\t\tlist_years =[]\n\t\tyear1 = 2010\n\t\twhile year1 < 2050:\n\t\t\tyear1+=1\n\t\t\tlist_years.append(year1)\n\t\tfor year in list_years:\n\t\t\tif str(year) in date:\n\t\t\t\t# print(f'year found was \"{year}\" in date: \"{date}\"')\n\t\t\t\tidx_year = date.index(str(year))+4\n\t\t\t\tdate = date[:idx_year]\n\t\t\t\t# print(f'updated string: \"{date}\"')\n\t\t\t\tbreak\n\n\t\tlist_fullmonths = 'January February March April May June July August September October November December'.split()\n\t\tlist_months = 'Jan Feb Mar Apr May Jun July Aug Sept Oct Nov Dec'.split(' ')\n\t\t# dict_months = {}\n\t\t# print(f'length of list_fullmonths:\"{len(list_fullmonths)}\",  length of list_months: \"{len(list_months)}\"')\n\t\tlowercaps = False\n\t\tlowercapsFULL = False\n\t\tuppercapsFULL = False\n\n\t\t# print('Testing for Case 1: Upper 1st letter Only in Full Month word')\n\t\tfor idx,month in enumerate(list_fullmonths):\n\t\t\t# dict_months[month] = list_months[idx]\n\t\t\tif month in date:\n\t\t\t\t# print(f'the month \"{month}\" was found in the string \"{date}\"')\n\t\t\t\tdate = date.replace(month, list_months[idx]).replace('.','')\n\t\t\t\tidx_month = date.index(list_months[idx])\n\t\t\t\t# print(f'idx_month found was \"{idx_month}\"')\n\t\t\t\tlowercaps = True\n\t\t\t\tlowercapsFULL = True\n\t\t\t\tuppercapsFULL = True\n\t\t\t\tbreak\t\n\n\t\tif lowercaps == False:\n\t\t\t# print('Testing for Case 2: Upper 1st letter Only')\t\n\t\t\tfor idx,month in enumerate(list_months):\n\t\t\t\t# print(month)\n\t\t\t\tif month in date:\n\t\t\t\t\t# print(f'the month \"{month}\" was found in the string \"{date}\"')\n\t\t\t\t\tdate = date.replace(month, list_months[idx]).replace('.','')\n\t\t\t\t\tidx_month = date.index(list_months[idx])\n\t\t\t\t\t# print(f'idx_month found was \"{idx_month}\"')\n\t\t\t\t\tlowercapsFULL = True\n\t\t\t\t\tuppercapsFULL = True\n\t\t\t\t\tbreak\t\n\n\t\tif lowercapsFULL == False:\n\t\t\t# print('Testing for Case 3: Upper case for entire Full Month word')\n\t\t\tfor idx,month in enumerate(list_fullmonths):\n\t\t\t\t# dict_months[month] = list_months[idx]\n\t\t\t\tmonth = month.upper()\n\t\t\t\t# print(month)\n\t\t\t\tif month in date:\n\t\t\t\t\t# print(f'the month \"{month}\" was found in the string \"{date}\"')\n\t\t\t\t\tdate = date.replace(month, list_months[idx]).replace('.','')\n\t\t\t\t\tidx_month = date.index(list_months[idx])\n\t\t\t\t\t# print(f'idx_month found was \"{idx_month}\"')\n\t\t\t\t\tuppercapsFULL = True\n\t\t\t\t\tbreak\t\n\n\t\tif uppercapsFULL == False:\n\t\t\t# print('Testing for Case 4: Upper case for SHORTENED Month')\n\t\t\tfor idx,month in enumerate(list_months):\n\t\t\t\tmonth = month.upper()\n\t\t\t\t# print(month)\n\t\t\t\tif month in date:\n\t\t\t\t\t# print(f'the month \"{month}\" was found in the string \"{date}\"')\n\t\t\t\t\tdate = date.replace(month, list_months[idx]).replace('.','')\n\t\t\t\t\tidx_month = date.index(list_months[idx])\n\t\t\t\t\t# print(f'idx_month found was \"{idx_month}\"')\n\t\t\t\t\tbreak\t\n\n\n\t\t# print(f'idx_month is \"{idx_month}\"')\t\t\t\t\n\t\tdate = date[idx_month:]\n\t\t# print(f'new_date is \"{date}\"')\n\n\t\t#date should be in the format of \"Month. day, year\": for example == 'Oct. 2, 2020 7:27 AM ET'\n\t\tdate = date.split()[:3]\n\t\tday_int = int(date[1].replace(',',''))\n\t\tif day_int<10:\n\t\t\tdate[1]='0'+str(day_int)\n\t\tdate = ' '.join(date).replace(',','')\n\t\t# print(f'date found for FormatDate(): \"{date}\"')\n\t\tf_article_day = datetime.datetime.strptime(date, '%b %d %Y').date()\n\t\t# print(f_article_day)\n\t\treturn f_article_day\n\tdef Article_type(self, url):  \n\t    list_articleTypes = ['oilprice', 'boereport', 'transcript', 'seekingalpha.com/article', 'cnbc.com', 'nbcnews', 'alberta.ca', 'seekingalpha.com/news']\n\t    idx=0\n\n\t    while True:\n\t        RegexCondition = re.search(list_articleTypes[idx],url)\n\t        if RegexCondition:\n\t            article_type=RegexCondition.group()\n\t            break\n\t        else:\n\t            idx+=1\n\t    return article_type\n\n\tdef AccessWebpage(self, driver, link, article_type):\n\t\t# ['oilprice', 'boereport', 'cnbc.com', 'nbcnews', 'alberta.ca', 'seekingalpha.com/news', 'transcript', 'seekingalpha.com/article']\n\t\tdriver.get(link)   \n\n\t\tSEL = SEL_Scrapers(driver, link)\n\t\tif article_type == 'oilprice':\n\t\t\tSEL.oilprice()\n\t\telif article_type == 'boereport':\n\t\t\tSEL.boereport()\n\t\telif article_type == 'cnbc.com':\n\t\t\tSEL.cnbc()\n\t\telif article_type == 'nbcnews':\n\t\t\tSEL.nbc()\n\t\telif article_type == 'alberta.ca':\n\t\t\tSEL.AB()\n\n\t\tSEL.Create_docx()\n\t\tSEL.Reset_PgINFO()\n\n\n\nmain()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "430": "from urllib.request import Request, urlopen\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass WebscrapeBlogs:\n    def __init__(self, keywords):\n        self.keywords = keywords\n\n\n    @staticmethod\n    def jornalnoticias(search):\n        url = \"https://jornalnoticias.co.mz/?s=\" + search\n        page_request = requests.get(url)\n        data = page_request.content\n        soup = BeautifulSoup(data, \"html.parser\")\n\n        date_title_link_all = list()\n\n        for divtag in soup.find_all('div', {'class': 'tdb_module_loop td_module_wrap td-animation-stack'}):\n\n            date_title_link = list()\n\n            for div_time_tag in divtag.find_all('span', {'class': 'td-post-date'}):\n                date_title_link.append(div_time_tag.find('time').text)\n\n            for h3tag in divtag.find_all('h3', {'class': 'entry-title td-module-title'}):\n                date_title_link.append(h3tag.find('a')['title'])\n                date_title_link.append(h3tag.find('a')['href'])\n\n            date_title_link_all.append(date_title_link)\n\n        return date_title_link_all\n\n    @staticmethod\n    def opais(search):\n        url = \"https://opais.co.mz/?s=\" + search\n        page_request = Request(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36'}) # TO deal with blocks\n        data = urlopen(page_request).read()\n        soup = BeautifulSoup(data, \"html.parser\")\n\n        date_title_link_all = list()\n\n        for divtag in soup.find_all('div', {'class': \"elementor-section-wrap\"}):\n\n            date_title_link = list()\n\n            for litag in divtag.find_all('li', {'class': 'elementor-icon-list-item elementor-repeater-item-7014203 elementor-inline-item'}):\n                try:\n                    date_title_link.append(litag.find('span').text.strip())\n                except:\n                    break\n\n            for h2tag in divtag.find_all('h2', {'class': 'elementor-heading-title elementor-size-default'}):\n                try:\n                    date_title_link.append(h2tag.find('a').text)\n                    date_title_link.append(h2tag.find('a')['href'])\n                except:\n                    break\n\n            if len(date_title_link) == 3:\n                date_title_link_all.append(date_title_link)\n\n        return date_title_link_all\n\n    def all_process_jornalnoticias(self):\n        dic = dict()\n        for keyword in self.keywords:\n            try:\n                hold = self.jornalnoticias(keyword)\n                if not hold:\n                    continue\n                dic[keyword] = hold\n            except:\n                continue\n\n        return dic\n\n    def all_process_opais(self):\n        dic = dict()\n        for keyword in self.keywords:\n            try:\n                hold = self.opais(keyword)\n                if not hold:\n                    continue\n                dic[keyword] = hold\n            except:\n                continue\n\n        return dic\n\n\ntest = WebscrapeBlogs([\"Movitel\", \"e-Mola\", \"M-Edu\", \"Transfere\", \"Promo\u00e7\u00e3o\", \"Promo\u00e7\u00f5es\", \"#megas\", \"Validade infinita\", \"MeuBeat\", \"Abrir conta e-Mola\", \"Sempre ligados\", \"Pacotes de Internet\", \"Pacote Tik Tok\", \"M-Karaoke\", \"Adivinhala\", \"Liga Maningue\", \"#Emtodolugar_atodomomento\", \"Movgym.co.mz\", \"MovGym\"])\n\nwith open(\"jornalnoticias1.txt\",\"w\") as file:\n    for keys, values in test.all_process_jornalnoticias().items():\n        for i in values:\n            file.write(keys)\n            file.write(\",\")\n            for value in i:\n                file.write(value)\n                file.write(\",\")\n            file.write(\"\\n\")\n\nwith open(\"opais1.txt\",\"w\") as file:\n    for keys, values in test.all_process_opais().items():\n        for i in values:\n            file.write(keys)\n            file.write(\",\")\n            for value in i:\n                file.write(value)\n                file.write(\",\")\n            file.write(\"\\n\")\n", "431": "# Generated by Django 2.1.7 on 2019-03-29 13:01\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('webscrape', '0005_auto_20190329_1257'),\n        ('dataprocess', '0006_prochousedata'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='prochousedata',\n            name='raw',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, to='webscrape.RawHouseData'),\n        ),\n    ]\n", "432": "#!/usr/bin/env python\n\nimport urllib2\nimport re\n# not all systems have readline...if not, just pass and continue.\ntry:\n    import readline  # nice when you need to use arrow keys and backspace\nexcept:\n    pass\nimport sys\n\n\ndef scrape():\n    site = raw_input(\"Enter page: \")\n\n    #open site. read so we can read in a string context\n    #test for valid and complete URL\n    try:\n        data = urllib2.urlopen(site).read()\n    except ValueError:\n        print \"INVALID URL: Be sure to include protocol (e.g. HTTP)\"\n        return\n    \n    #print data\n\n    #try an open the pattern file.\n    try:\n        patternFile = open('config/webscrape.dat', 'r').read().splitlines()\n    except:\n        print \"There was an error opening the webscrape.dat file\"\n        raise\n    #create counter for counting regex expressions from webscrape.dat\n    counter = 0\n    #for each loop so we can process each specified regex\n    for pattern in patternFile:\n        m = re.findall(pattern, data)\n        #m will return as true/false. Just need an if m:\n        if m:\n            for i in m:\n                #open output/results file...append because we are cool\n                outfile = open('scrape-RESULTS.txt', 'a')\n            #print m\n                outfile.write(str(i))\n                outfile.write(\"\\n\")  # may be needed. can always be removed.\n\n            #close the file..or else\n                outfile.close()\n                counter+=1\n                print \"Scrape item \" + str(counter) + \" successsful. Data output to scrape-RESULTS.txt.\"\n        else:  # only need an else because m is boolean\n            counter+=1\n            print \"No match for item \" + str(counter) + \". Continuing.\"\n            # Continue the loop if not a match so it can go on to the next\n            # sequence\n            # NOTE: you don't *really* need an else here...\n            continue\n", "433": "#!/usr/bin/env python\n\nimport urllib2\nimport re\n# not all systems have readline...if not, just pass and continue.\ntry:\n    import readline  # nice when you need to use arrow keys and backspace\nexcept:\n    pass\nimport sys\n\n\ndef scrape():\n    site = raw_input(\"Enter page: \")\n\n    #open site. read so we can read in a string context\n    #test for valid and complete URL\n    try:\n        data = urllib2.urlopen(site).read()\n    except ValueError:\n        print \"INVALID URL: Be sure to include protocol (e.g. HTTP)\"\n        return\n    \n    #print data\n\n    #try an open the pattern file.\n    try:\n        patternFile = open('config/webscrape.dat', 'r').read().splitlines()\n    except:\n        print \"There was an error opening the webscrape.dat file\"\n        raise\n    #create counter for counting regex expressions from webscrape.dat\n    counter = 0\n    #for each loop so we can process each specified regex\n    for pattern in patternFile:\n        m = re.findall(pattern, data)\n        #m will return as true/false. Just need an if m:\n        if m:\n            for i in m:\n                #open output/results file...append because we are cool\n                outfile = open('scrape-RESULTS.txt', 'a')\n            #print m\n                outfile.write(str(i))\n                outfile.write(\"\\n\")  # may be needed. can always be removed.\n\n            #close the file..or else\n                outfile.close()\n                counter+=1\n                print \"Scrape item \" + str(counter) + \" successsful. Data output to scrape-RESULTS.txt.\"\n        else:  # only need an else because m is boolean\n            counter+=1\n            print \"No match for item \" + str(counter) + \". Continuing.\"\n            # Continue the loop if not a match so it can go on to the next\n            # sequence\n            # NOTE: you don't *really* need an else here...\n            continue\n", "434": "from typing import Collection\nfrom pymongo import MongoClient\nfrom pymongo.errors import ConnectionFailure\nfrom pandas import DataFrame\nfrom time import sleep\n\n\ndef _page_hopping(page_number: int, soup, driver) -> int:\n    \"\"\"page changing mechanism\"\"\"\n    url_2 = soup.find('ul', {'class': 'a-pagination'})\n    url_2_a = url_2.find_all('a')\n    if 'page=' not in driver.current_url:\n        for link_a in url_2_a:\n            if link_a.text == '2':\n                url_2_link = 'https://www.amazon.com/' + \\\n                    link_a['href']\n                page_number += 1\n                driver.get(url_2_link)\n    if f'page={page_number}' in driver.current_url:\n        link_page = driver.current_url\n        page_number_2 = page_number + 1\n        link_page_2 = link_page.replace(\n            f'page={page_number}', f'page={page_number_2}')\n        link_page_3 = link_page_2.replace(\n            f'sr_pg_{page_number}', f'sr_pg_{page_number_2}')\n        page_number += 1\n        driver.get(link_page_3)\n    return page_number\n\n\ndef _comprehensive_search(item_url_list: list, master_list: list, i: int, driver, tqdm, BeautifulSoup) -> None:\n    \"\"\"iterating over item urls for more data.\n    i: index of master_list\"\"\"\n    print('collecting entries')\n    pbar_2 = tqdm(total=len(item_url_list))  # load bar 2\n    for url_3 in item_url_list:\n        driver.get(url_3)\n        soup_product = BeautifulSoup(driver.page_source, 'html.parser')\n        for product in soup_product:\n            try:\n                img_product = product.find(\n                    'img', {'id': 'landingImage'})['src']\n            except:\n                img_product = 'None'\n            if 'img_url' not in master_list[i].keys():\n                master_list[i].setdefault('img_url', img_product)\n            try:\n                brand_product = product.find(\n                    'a', {'id': 'bylineInfo'})['href']\n            except:\n                brand_product = 'None'\n                if 'brand_url' not in master_list[i].keys():\n                    master_list[i].setdefault(\n                        'brand_url', brand_product)\n            if 'brand_url' not in master_list[i].keys():\n                master_list[i].setdefault(\n                    'brand_url', 'https://www.amazon.com/' + brand_product)\n            try:\n                stock_product = product.find(\n                    'span', {'class': 'a-size-medium a-color-success'}).text\n            except:\n                try:\n                    stock_product = product.find(\n                        'span', {'class': 'a-color-price a-text-bold'}).text\n                except AttributeError:\n                    stock_product = 'None'\n            else:\n                try:\n                    multi_seller_link = product.find_all('a')\n                    for link in multi_seller_link:\n                        if link.text == 'these sellers':\n                            stock_product = 'https://www.amazon.com/' + \\\n                                link['href']\n                except TypeError:\n                    stock_product = 'TypeError'\n            if 'in_stock' not in master_list[i].keys():\n                stock_product_form_1 = stock_product.replace('\\n', '')\n                master_list[i].setdefault(\n                    'in_stock', stock_product_form_1)\n            product_details = product.find(\n                'div', {'id': 'detailBulletsWrapper_feature_div'})  # no details for now\n        i += 1\n        pbar_2.update(1)\n    pbar_2.close()\n\n\ndef _captcha_solver(driver, url: str) -> None:\n    \"\"\"amazon bot solution for blocked scraping, opening new tab\"\"\"\n    for i in range(10):\n        driver.get(url)\n        captcha_bot = \"Sorry, we just need to make sure you're not a robot. For best results, please make sure your browser is accepting cookies.\"\n        check = driver.page_source\n        if captcha_bot not in check:\n            break\n        sleep(2)\n\n\ndef _save_data(collection: any, data: list, keyword: any, datatype: str, time: str) -> None:\n    \"\"\"how to save data:\n    -csv\n    -json\n    -MongoDB, local db named 'amazon', collection 'webscrape'\"\"\"\n    if datatype == \"csv\":\n        amazon_df = DataFrame(data)\n        amazon_df.to_csv(\n            f'./dags/programs/ws_amazon/ws_amazon_{keyword}_{time}.csv', index=False)\n    elif datatype == \"json\":\n        amazon_df = DataFrame(data)\n        amazon_df.to_json(\n            f'./dags/programs/ws_amazon/ws_amazon_{keyword}_{time}.json', index=False)\n    elif datatype == \"db\":\n        amazon_df = DataFrame(data)\n        amazon_df.reset_index(inplace=True)\n        amazon_dict = amazon_df.to_dict(\"records\")\n        collection.insert_many(amazon_dict)\n\n\ndef _connect_mongo() -> any:\n    \"\"\"connect to mongodb, db:amazon, collection:webscrape\"\"\"\n    client = MongoClient(\"mongodb://root:root@mongodb:27017\")\n    sleep(5)\n    try:  # test the connection\n        client.admin.command('ping')\n    except ConnectionFailure:\n        print(\"Server not available\")\n        return None\n    db = client[\"amazon\"]\n    collection = db[\"webscrape\"]\n    return collection\n", "435": "from django.core.management.base import BaseCommand, CommandError\nfrom actions.models import URLContent\nfrom bs4 import BeautifulSoup\nimport requests\n\n\n#hardcoded for testing purposes only\nhtml_doc = \"ala ma kota a kot ma Al\u0119\"\n\nclass Command(BaseCommand):\n    def add_arguments(self, parser):   #parser is inbuilt\n        parser.add_argument('url', type=str, help='URL to download from')\n        parser.add_argument('--shift', default=\"0\", type=int, help=\"The encryption shift value\")\n\n\n    def handle(self, *args, **options):\n        help = 'The help for downloadpage command'\n        self.stdout.write(f'Name and Surname: ... ')\n        url_text = webscrape(options['url'])\n        encrypted_msg = caesar_cipher_encrypt(plain_text=url_text, shift=options['shift'])\n\n        #check if url already exist in DB, if yes - overwrite (same ID)\n        if URLContent.objects.filter(url_address= options['url']):\n            #try except preferable here //TODO\n            p = URLContent.objects.filter(url_address=options['url']).update(encrypted_content=encrypted_msg)\n            #self.stdout.write(f'PageID: {p.id}')\n            self.stdout.write('Downloaded and encrypted again.')\n        else:\n            #try except preferable here //TODO\n            p = URLContent(url_address=options['url'], encrypted_content=encrypted_msg)\n            p.save()\n            #self.stdout.write(f'PageID: {p.id}')\n            self.stdout.write('Downloaded and encrypted.')\n\n\n#does it make a difference whether a page is not secured? check on that --> yes, error code 404\ndef webscrape(url):\n    r = requests.get(str(url))\n    soup = BeautifulSoup(r.content, features=\"html.parser\")   #can use differenet parser, they have unique traits (e.g. some are faster)\n    #soup = BeautifulSoup(html_doc, features=\"html.parser\")\n    if r.status_code != 200:\n        raise CommandError(f'Something went wrong while trying to reach given URL. Code status {r.status_code}')\n    return soup.get_text()\n\n\ndef caesar_cipher_encrypt(plain_text, shift):\n    result = \"\"\n    # transverse the plain text\n    for i in range(len(plain_text)):\n        char = plain_text[i]\n        if char == ' ':\n            result += ' '\n        elif char.upper() in ['\u0104', '\u0106', '\u0118', '\u0143', '\u00d3', '\u015a', '\u0179', '\u017b']:\n            result += char\n        else:\n            # Encrypt uppercase characters in plain text\n            if (char.isupper()):\n                result += chr( (ord(char) + shift-65) % 26 + 65)\n            # Encrypt lowercase characters in plain text\n            else:\n                result += chr( (ord(char) + shift-97) % 26 + 97)\n    return result\n\n''' # poprawic '''\ndef caesar_cipher_decrypt(encrypted_text, shift):\n    result = \"\"\n    # transverse the plain text\n    for i in range(len(encrypted_text)):\n        char = encrypted_text[i]\n        if char == ' ':\n            result += ' '\n        elif char.upper() in ['\u0104', '\u0106', '\u0118', '\u0143', '\u00d3', '\u015a', '\u0179', '\u017b']:\n            result += char\n        else:\n            # Decrypt uppercase characters in plain text\n            if (char.isupper()):\n                result += chr( (ord(char) - shift-65) % 26 + 65)\n            # Decrypt lowercase characters in plain text\n            else:\n                result += chr( (ord(char) - shift-97) % 26 + 97)\n    return result", "436": "#some imports\r\nfrom flask import Flask\r\nfrom flask_restful import Resource, Api, reqparse\r\nimport pandas as pd\r\nimport ast\r\nimport random\r\nimport urllib, urllib.request\r\nfrom urllib.request import urlopen\r\n\r\n#establish app and api\r\napp = Flask(__name__)\r\napi = Api(app)\r\n\r\n#primary method\r\ndef getTopSongs():\r\n    #identify url, proxy as Mozilla, scrape html into a string and clean up escaped chars\r\n    url= \"https://www.billboard.com/charts/hot-100\"\r\n    hdr = {'User-Agent':'Mozilla/5.0'}\r\n    req = urllib.request.Request(url, headers=hdr)\r\n    page = urlopen(req)\r\n    scrapedBytes = page.read()\r\n    html = scrapedBytes.decode(\"utf-8\")\r\n    html = html.encode('utf-8').decode('ascii', 'ignore')\r\n    html = html.replace(''', \"\\'\")\r\n    html = html.replace('&', \"&\")\r\n\r\n    #traverse html string, find all song and artist names, and store them in arrays defined below\r\n    # NOTE: this section is a little obtuse because it is mostly dependent \r\n    # on what the HTML from the webscrape looks like\r\n    titles = []\r\n    artists = []\r\n    startIndex = html.find(\"primary\\\">\") + len(\"primary\\\">\")\r\n    endIndex = html.find(\"<\", startIndex)\r\n    while (startIndex > -1):\r\n        titles.append(html[startIndex:endIndex])\r\n        startIndex = html.find(\"secondary\\\">\", endIndex)\r\n        if(startIndex > -1):\r\n            startIndex += len(\"secondary\\\">\")\r\n        endIndex = html.find(\"<\", startIndex)\r\n        artists.append(html[startIndex:endIndex])\r\n        startIndex = html.find(\"primary\\\">\", endIndex)\r\n        if(startIndex > -1):\r\n            startIndex  += len(\"primary\\\">\")\r\n        endIndex = html.find(\"<\", startIndex)\r\n\r\n    #append a dictionary for each song to a list and return the songList\r\n    #could add more dictionary entries later with a larger webscrape\r\n    songs = []\r\n    for i in range(100):\r\n        thisSong = {\r\n            \"title\":titles[i],\r\n            \"artist\":artists[i]\r\n        }\r\n        songs.append(thisSong)\r\n    return songs\r\n\r\n#define class with GET method\r\nclass Songs(Resource):\r\n    def get(self):\r\n        topSongs = getTopSongs()\r\n        return topSongs, 200\r\n    pass\r\n#establish url for class\r\napi.add_resource(Songs, '/songs')\r\n\r\n#automatically starts the app on script execution\r\nif __name__ == \"__main__\":\r\n    app.run(debug = True)\r\n", "437": "import marge_sort\nfrom scrape import WebScrape\nimport pytest\n\n\n@pytest.fixture\ndef scrape_table_data():\n    return [[], ['MH', '5226710', '78007', '4600196'], ['KA', '2053191', '20368', '1440621'],\n            ['KL', '2010934', '6053', '1571738'], ['UP', '1563235', '16369', '1340251'],\n            ['TN', '1468864', '16471', '1279658'], ['DL', '1361986', '20310', '1258951'],\n            ['ANDHRA', '1344386', '8988', '1138028'], ['WB', '1053117', '12728', '911705'],\n            ['CG', '883210', '11094', '749318'], ['RJ', '805658', '6158', '590390'],\n            ['GJ', '714611', '8731', '578397'], ['MP', '700202', '6679', '583595'],\n            ['HR', '652742', '6075', '539609'], ['BR', '622433', '3503', '519306'],\n            ['OR', '565648', '2232', '473680'], ['TG', '511711', '2834', '449744'],\n            ['PB', '467539', '11111', '376465'], ['AS', '310086', '1909', '265860'],\n            ['JH', '301257', '4182', '246608'], ['UK', '264683', '4123', '183478'],\n            ['JK', '229407', '2912', '174953'], ['HP', '145736', '2068', '104714'],\n            ['GA', '127639', '1874', '92974'], ['PY', '77031', '1045', '60424'],\n            ['CH', '52633', '599', '43506'], ['TR', '39054', '424', '34946'],\n            ['MN', '37036', '526', '31238'], ['ML', '20985', '250', '17354'],\n            ['AR', '20854', '69', '18691'], ['NL', '16890', '165', '13428'],\n            ['LADAKH', '15807', '158', '14102'], ['SK', '10392', '183', '7343'],\n            ['DADRA AND NAGAR HAVELI AND DAMAN AND DIU', '9150', '4', '8086'],\n            ['MZ', '8176', '23', '6120'], ['AN', '6470', '81', '6175'], ['LD', '4202', '11', '3171']\n            ]\n\n\ndef test_merge(scrape_table_data):\n    assert marge_sort.mergesort(scrape_table_data[1:]) == sorted(scrape_table_data[1:], key=lambda x: x[1])\n\n\ndef test_table():\n    ws = WebScrape()\n    assert ws.covidIndia() == [[\"Confirmed Cases\", \"23,703,665\"], [\"Total Deaths\", \"258,317\"],\n                               [\"Total Recovered\", \"19,734,823\"], [\"Active Cases\", \"3,710,525\"]]\n\n    assert ws.tableScrape() == [[], ['MH', '5226710', '78007', '4600196'], ['KA', '2053191', '20368', '1440621'],\n                                ['KL', '2010934', '6053', '1571738'], ['UP', '1563235', '16369', '1340251'],\n                                ['TN', '1468864', '16471', '1279658'], ['DL', '1361986', '20310', '1258951'],\n                                ['ANDHRA', '1344386', '8988', '1138028'], ['WB', '1053117', '12728', '911705'],\n                                ['CG', '883210', '11094', '749318'], ['RJ', '805658', '6158', '590390'],\n                                ['GJ', '714611', '8731', '578397'], ['MP', '700202', '6679', '583595'],\n                                ['HR', '652742', '6075', '539609'], ['BR', '622433', '3503', '519306'],\n                                ['OR', '565648', '2232', '473680'], ['TG', '511711', '2834', '449744'],\n                                ['PB', '467539', '11111', '376465'], ['AS', '310086', '1909', '265860'],\n                                ['JH', '301257', '4182', '246608'], ['UK', '264683', '4123', '183478'],\n                                ['JK', '229407', '2912', '174953'], ['HP', '145736', '2068', '104714'],\n                                ['GA', '127639', '1874', '92974'], ['PY', '77031', '1045', '60424'],\n                                ['CH', '52633', '599', '43506'], ['TR', '39054', '424', '34946'],\n                                ['MN', '37036', '526', '31238'], ['ML', '20985', '250', '17354'],\n                                ['AR', '20854', '69', '18691'], ['NL', '16890', '165', '13428'],\n                                ['LADAKH', '15807', '158', '14102'], ['SK', '10392', '183', '7343'],\n                                ['DADRA AND NAGAR HAVELI AND DAMAN AND DIU', '9150', '4', '8086'],\n                                ['MZ', '8176', '23', '6120'], ['AN', '6470', '81', '6175'], ['LD', '4202', '11', '3171']\n                                ]\n    ws.close_webpage()\n", "438": "import sqlite3\nimport pandas as pd\nimport sys\nimport argparse\nsys.path.append('src/')\nfrom src import ws_nbc, ner_model\n\n'''\nThis script has been optimized to handle one or more news article from NBC News.\n\nUser Arguments:\n                --multi_article (Default = False)\n                --num_articles (if multi_article is set to True you can place the amount of articles you want to extract)\n\nUser Inputs:\n                Article URL if multi_article is False\n                SQL commands\n                        SELECT * FROM article\n                        SELECT NAME FROM article\n                        SELECT ORGANIZATION FROM article\n                        SELECT LOCATION FROM article\n                        ... \n\noutputs:\n                result query\n'''\n\ndef create_sql_server():\n    conn = sqlite3.connect('ner_database')\n    c = conn.cursor()\n    c.execute('CREATE TABLE IF NOT EXISTS article (NAME, ORGANIZATION, LOCATION)')\n    conn.commit()\n    return conn, c\n\ndef ner_to_df(article):\n    df = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in article.items()]), dtype=object)\n    return df\n\ndef dict_to_df(ner_unique):\n    return pd.DataFrame(dict([(k,pd.Series(v)) for k,v in ner_unique.items()]))\n\nif __name__ == '__main__':\n\n    parser= argparse.ArgumentParser()\n    parser.add_argument('--multi_article', type=bool, default=False)\n    parser.add_argument('--num_articles', type=int, default=5)\n    args = parser.parse_args()\n\n    spacy_ner = ner_model.Model()\n\n    if args.multi_article:\n        nbc_news = 'https://www.nbcnews.com/'\n        nbc_article = ws_nbc.WebScrape(nbc_news)\n        article = nbc_article.scrape_n_articles(num_articles=args.num_articles)\n        print(\"Scraping {} articles from NBC News Webpage\".format(args.num_articles))\n        model_out = spacy_ner.get_ner_for_all(article)\n        df = pd.DataFrame()\n        for i in range(0,len(model_out)):\n            article_df = ner_to_df(model_out['article content'][i])\n            df = pd.concat([df, article_df])\n    else:\n        article_url = input(\"Input NBC News article url: \")\n        nbc_article = ws_nbc.WebScrape(article_url)\n        article = nbc_article.scrape_news_article()\n        model_out = spacy_ner.ner(article.get('article content'))\n        ner_unique = ner_model.get_unique_results(model_out)\n        df = dict_to_df(ner_unique)\n\n    conn, c = create_sql_server()\n\n    df.to_sql('article', conn, if_exists='replace', index = False)\n\n    sql_input = ''\n    while sql_input != 'quit':\n        sql_input = input(\"Input search query: \")\n\n        c.execute(sql_input)\n\n        for row in c.fetchall():\n            print (row)", "439": "#!/usr/bin/python\r\n# -*- coding: utf-8 -*-\r\n\r\n'''\r\n=====================================================================================\r\n\r\nCopyright (c) 2016 Universit\u00e9 de Lorraine & Lule\u00e5 tekniska universitet\r\nAuthor: Luca Di Stasio \n\r\n\r\nThis program is free software: you can redistribute it and/or modify\r\nit under the terms of the GNU General Public License as published by\r\nthe Free Software Foundation, either version 3 of the License, or\r\n(at your option) any later version.\r\n\r\nThis program is distributed in the hope that it will be useful,\r\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\r\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\r\nGNU General Public License for more details.\r\n\r\nYou should have received a copy of the GNU General Public License\r\nalong with this program.  If not, see .\r\n\r\n=====================================================================================\r\n\r\nDESCRIPTION\r\n\r\n\r\n\r\nTested with Python 2.7 Anaconda 2.4.1 (64-bit) distribution in Windows 7.\r\n\r\n'''\r\n\r\nimport sys, getopt, re, mechanize, urllib, urllib2, cookielib, os, time, csv, codecs, webscrape\r\nfrom BeautifulSoup import BeautifulSoup\r\nfrom time import strftime\r\nfrom random import randint\r\n\r\ndef init_json(language,outfilename,source,query):\r\n   # Initialize output file\r\n   today = strftime(\"%Y-%m-%d\")\r\n   now = strftime(\"%H-%M-%S\")\r\n   if outfilename != \"\":\r\n      pathJSON = today + \"_\" + outfilename + \".json\"\r\n   else:\r\n      pathJSON = today + \"_\" + language + \"_dictionary.json\"\r\n   with codecs.open(pathJSON,'w','utf-8') as json:\r\n      print ''\r\n      print 'Creating json file: ' + str(json)\r\n      print ''\r\n      json.write('{\\n')\r\n      json.write('    \"dictionary\": {\\n')\r\n      json.write('        \"title\": \"' + 'DICTIONARY' + '\",\\n')\r\n      json.write('        \"source\": \"' + source + '\",\\n')\r\n      json.write('        \"language\": \"' + language + '\",\\n')\r\n      json.write('        \"date\": \"' + today + '\",\\n')\r\n      json.write('        \"time\": \"' + now + '\",\\n')\r\n      json.write('        \"entries\": {\\n')\r\n      json.write('        \"status\":\"end\"\\n')\r\n      json.write('                   }\\n')\r\n      json.write('              }\\n')\r\n      json.write('}\\n')\r\n\r\n# entry = [word,part-of-speech]\r\ndef newentry_json(day,language,outfilename,entry):\r\n   if outfilename != \"\":\r\n      pathJSON = today + \"_\" + outfilename + \".json\"\r\n   else:\r\n      pathJSON = today + \"_\" + language + \"_dictionary.json\"\r\n   # read json file and find point of insertion\r\n   with codecs.open(pathJSON,'r','utf-8') as json:\r\n      lines = json.readlines()\r\n   newlines = []\r\n   for line in lines:\r\n      if '\"status\":\"end\"' in line:\r\n         newlines.append('         \"entry\": {\\n')\r\n         newlines.append('          \"mainform\": \"' + pubs[0] + '\",\\n')\r\n         newlines.append('          \"part-of-speech\": \"' + pubs[1] + '\",\\n')\r\n         newlines.append('                  },\\n')\r\n         newlines.append(line)\r\n      else:\r\n         newlines.append(line)\r\n   # re-write json file with new data\r\n   with codecs.open(pathJSON,'w','utf-8') as json:\r\n      for line in newlines:\r\n         json.write(line)\r\n\r\ndef screen_header():\r\n   print \"----------------------------------------------------------------------\"\r\n   print \"----------------------------------------------------------------------\"\r\n   print \"----------------------------------------------------------------------\"\r\n   print \"                              WORDS\"\r\n   print \"----------------------------------------------------------------------\"\r\n   print \"----------------------------------------------------------------------\"\r\n\r\ndef update_screen(selectbrowser,count,maxpage,pubs):\r\n   # print status to screen\r\n   if selectbrowser:\r\n      browser = \"Chrome on Windows 7 64 bit\"\r\n   else:\r\n      browser = \"Firefox on Windows 7 64 bit\"\r\n   print \"----------------------------------------------------------------------\"\r\n   print \"WORD number \" + str(count) + \" retrieved using \" + browser + \" on \" + strftime(\"%d/%m/%Y\") + \" at \" + strftime(\"%H:%M:%S\")\r\n   print \"\"\r\n   print pubs[0] + ', ' + pubs[1]\r\n   print \"\"\r\n   print \"----------------------------------------------------------------------\"\r\n\r\ndef main(argv):\r\n   dictionary = ''\r\n   source = ''\r\n   outputfile = ''\r\n   try:\r\n      opts, args = getopt.getopt(argv,\"hd:s:o:\",[\"help\",\"Help\",\"dict\",\"dictionary\",\"source\",\"ofile\",\"outfile\",\"outputfile\"])\r\n   except getopt.GetoptError:\r\n      print 'create_dictionary.py -d  -s  -o '\r\n      sys.exit(2)\r\n   for opt, arg in opts:\r\n    if opt in (\"-h\", \"--help\",\"--Help\"):\r\n        print ''\r\n        print ''\r\n        print '*********************************************************************************'\r\n        print '*                                                                               *'\r\n        print '*               A TOOL FOR SCIENTIFIC LITERATURE ANALYSIS                       *'\r\n        print '*                                                                               *'\r\n        print '*                                                                               *'\r\n        print '*                          Dictionary creation                                  *'\r\n        print '*                                                                               *'\r\n        print '*                                 by                                            *'\r\n        print '*                                                                               *'\r\n        print '*                        Luca Di Stasio, 2016                                   *'\r\n        print '*                                                                               *'\r\n        print '*********************************************************************************'\r\n        print ''\r\n        print 'Program syntax:'\r\n        print 'create_dictionary.py -d  -s  -o '\r\n        print ''\r\n        print 'Mandatory arguments:'\r\n        print '-s  (without extension, it must be in json format)'\r\n        print ''\r\n        print 'Optional arguments:'\r\n        print '-d  -o  (without extension, it must be in json format)'\r\n        print ''\r\n        print 'Available dictionaries:'\r\n        print 'Dictionary                                           Command-line option'\r\n        print '--------------------------------------------------------------------------'\r\n        print 'Oxford Dictionary (English)                          oxford'\r\n        print ''\r\n        print ''\r\n        sys.exit()\r\n    elif opt in (\"-d\", \"--dict\",\"--dictionary\"):\r\n        dictionary = arg\r\n    elif opt in (\"-s\", \"--source\"):\r\n        source = arg\r\n    elif opt in (\"-o\", \"--ofile\",\"--outfile\",\"--outputfile\"):\r\n        outputfile = arg\r\n    \r\n    with codecs.open(source + '.json','r','utf-8') as jsonfile:\r\n        lines = jsonfile.readlines()\r\n    \r\n    if dictionary=='':\r\n        dictionary = 'oxford'\r\n    \r\n    if dictionary == 'oxford':\r\n        url = 'http://www.oxforddictionaries.com/'\r\n    elif dictionary == 'larousse':\r\n        url = ''\r\n    \r\n    words = []\r\n    for line in lines:\r\n        if 'abstract' in line:\r\n            chunks = line.split(':')\r\n            items = \" \".join(chunks[1:])[2:-3].split(' ')\r\n            for item in enumerate(items):\r\n                if len(item)>0 and item[-1] in [',' , ';' , '.' , ':' , '!' , '?' , ')' , ']' , '}' , '\"' , '\\'']:\r\n                    item = item[:-1]\r\n                if len(item)>0 and item[0] in [',' , ';' , '.' , ':' , '!' , '?' , '(' , '[' , '{' , '\"' , '\\'']:\r\n                    item = item[1:]\r\n                    \r\n                # Create url for the query\r\n                query = url\r\n                \r\n                firstmech = webscrape.init_browser('chrome')\r\n                secondmech = webscrape.init_browser('firefox')\r\n                \r\n                main = firstmech.open(url)\r\n                html = main.read()\r\n                initmainsoup = BeautifulSoup(html)\r\n                print initmainsoup\r\n                \r\n                print initmainsoup\r\n                for form in main.forms():\r\n                    print form\r\n                '''\r\n                firstmech.select_form(nr=0)\r\n                firstmech[\"s\"] = item\r\n                resp = firstmech.submit()\r\n                \r\n                html = resp.read()\r\n                initmainsoup = BeautifulSoup(html)\r\n                \r\n                print initmainsoup\r\n'''\r\n            \r\n    \r\nif __name__ == \"__main__\":\r\n   main(sys.argv[1:])", "440": "import asyncio\nimport os\nfrom database import DataBase\nfrom dotenv import load_dotenv\nfrom discord.ext import commands\nfrom webscrapper import WebScrape\n\nload_dotenv()\n\nclient = commands.Bot(command_prefix='*')\ndb = DataBase(os.getenv('DBURL'))\nguilds = {}\n\n\n@client.event\nasync def on_ready():\n    text_channel = None\n\n    for guild in client.guilds:\n        text_channel_id = db.Get_Text_Channel(guild.id)\n        if text_channel_id is not None:\n            text_channel = text_channel_id\n        else:\n            text_channel_list = []\n            for channel in guild.text_channels:\n                text_channel_list.append(channel.id)\n            text_channel = text_channel_list[0]\n        guilds[guild.id] = text_channel\n\n    print('Bot is now logged in as {0.user}'.format(client))\n    client.loop.create_task(Listen_For_New_Games())\n\n\n@client.command(aliases=['echo'])\nasync def Echo_Game(ctx, link):\n    await ctx.send(link + \"\\n\")\n\n\n@client.command(aliases=['here'])\nasync def set_text_channel(ctx):\n    for guild in guilds:\n        if guild == ctx.guild.id:\n            guilds[guild] = ctx.channel.id\n    db.Define_Text_Channel(ctx.guild.id, ctx.channel.id)\n\n\nasync def Listen_For_New_Games():\n    await client.wait_until_ready()\n    message_link = None\n\n    while not client.is_closed():\n\n        WebScrape()\n        list = WebScrape.games_list\n        for game in list:\n            db.Add_Game(game)\n            if db.Is_New_Game():\n                for guild in guilds:\n                    if guilds[guild] is not None:\n                        channel = client.get_channel(guilds[guild])\n                    else:\n                        text_channel_list = []\n                        for channel in guild.text_channels:\n                            text_channel_list.append(channel)\n                        channel = text_channel_list[0]\n                    await channel.send(game.Message())\n\n        await asyncio.sleep(3600)  # task runs every 1 hour\n\nclient.run(os.getenv('TOKEN'))\npass\n", "441": "import json\nimport URLHelper\nfrom os.path import join, dirname\nfrom watson_developer_cloud import AlchemyLanguageV1\nfrom watson_developer_cloud import ToneAnalyzerV3\nimport webScrape\n\napi_key_alc='1983387b2fb355dca6bc2cf73437d3e4bbfa6dbf'\ntoneUser = '7dd29476-b1e0-4c5a-a68c-a16e72969667'\ntonePass = 'ZsytNE1fO65f'\n\ndef analyze(url):\n    alchemy_language = AlchemyLanguageV1(api_key=api_key_alc)\n    combined_operations = [ 'entity', 'keyword', 'title', 'concept', 'doc-emotion']\n    global data\n    data = alchemy_language.combined(url=url, extract=combined_operations)\n    tone_analyzer = ToneAnalyzerV3(username=toneUser,password=tonePass,version='2016-05-19')\n    global tone\n    text=webScrape.getText(url)\n    text = text.encode('ascii','ignore')\n    if len(text)>120000:\n        text = text[0:120000]\n    if text.count('\\n')>750:\n        text = text[0:URLHelper.linSearch(text,'\\n', 750)]\n    tone = tone_analyzer.tone(text=text)['document_tone']['tone_categories']\n\ndef getTitle():\n    global data\n    title = data['title']\n    return title.encode('ascii','ignore')\n\ndef getKeywords():\n    global data\n    return data['keywords']    \n\ndef getKeyKeywords():\n    words = []\n    global data\n    for i in range(0,min(7, len(getKeywords()))):\n        words.append(data['keywords'][i]['text'])\n    return words\n    \ndef getEmotions():\n    global data\n    emotions= data['docEmotions']\n    return emotions\n\ndef getEmotionTone():\n    global tone\n    return tone[0]['tones']\n\ndef getLanguageTone():\n    global tone\n    return tone[1]['tones']\n\ndef getSocialTone():\n    global tone\n    return tone[2]['tones']\n", "442": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n#==============================================================================\n#\n#       sql.py - SQLite3 module\n#\n#       Tables:\n#           \n#           Music - Master table of songs in all locations\n#           History - History table of events and settings\n#\n#==============================================================================\n\nfrom __future__ import print_function       # Must be first import\nfrom __future__ import unicode_literals     # Unicode errors fix\n\nimport sqlite3\nimport os\nimport re\nimport json\nimport time\nimport datetime\nfrom collections import namedtuple, OrderedDict\n\n\n# local modules\nimport global_variables as g        # should be self-explanatory\nimport timefmt                      # Our custom time formatting functions\nimport external as ext\n\ntry:\n    from location import FNAME_LIBRARY  # SQL database name (SQLite3 format)\nexcept ImportError:\n    FNAME_LIBRARY = \"/home/rick/.config/mserve/library.db\"\n\nCFG_THOUSAND_SEP = \",\"              # English \",\" to for thousands separator\nCFG_DECIMAL_SEP = \".\"               # English \".\" for fractional amount\nCFG_DECIMAL_PLACES = 1              # 1 decimal place, eg \"38.5 MB\"\nCFG_DIVISOR_AMT = 1000000           # Divide by million\nCFG_DIVISOR_UOM = \"MB\"              # Unit of Measure becomes Megabyte\n\n\nNO_ARTIST_STR = \"(No Artist)\"         # global User defined labels\nNO_ALBUM_STR = \"(No Album)\"\n'''\nFrom: https://www.sqlite.org/pragma.html#pragma_user_version\n\nPRAGMA schema.user_version;\nPRAGMA schema.user_version = integer ;\n\nThe user_version pragma will to get or set the value of the user-version \ninteger at offset 60 in the database header. The user-version is an integer\nthat is available to applications to use however they want. SQLite makes \nno use of the user-version itself.\n\nSee also the application_id pragma and schema_version pragma.\n\nAlso from: https://stackoverflow.com/questions/2354696/\nalter-table-sqlite-how-to-check-if-a-column-exists-before-alter-the-table/2354829#2354829\n\nNeed to add:\ngenius link, genius download time\nazlyrics link, azlyrics download time\n'''\n\n# Global variables must be defined at module level\ncon = cursor = hist_cursor = None\nSTART_DIR_SEP  = None\nMUSIC_ID  = None\n_START_DIR = _USER = _LODICT = None\n\n\ndef create_tables(SortedList, start_dir, pruned_subdirs, user, lodict):\n    \"\"\" Create SQL tables out of OS sorted music top directory\n\n        if START_DIR = '/mnt/music/'\n        then PRUNED_SUBDIRS = 0\n\n        if START_DIR = '/mnt/music/Trooper/'\n        then PRUNED_SUBDIRS = 1\n\n        if START_DIR = '/mnt/music/Trooper/Hits From 10 Albums/'\n        then PRUNED_SUBDIRS = 2\n\n        if START_DIR = '/mnt/... 10 Albums/08 Raise A Little Hell.m4a'\n        then PRUNED_SUBDIRS = 3\n\n    \"\"\"\n\n    global con, cursor, hist_cursor\n    global START_DIR_SEP    # Count of / or \\ separators in toplevel directory\n    global MUSIC_ID         # primary key into Music table used by History table\n\n    global _START_DIR, _USER, _LODICT\n    _START_DIR = start_dir  # Toplevel directory, EG /mnt/music/\n    _USER = user            # User ID to be stored on history records.\n    _LODICT = lodict        # Location dictionary\n\n    open_db()\n\n    last_time = hist_last_time('file', 'init')\n    print('last_time:', last_time)\n    # Fill the table\n    LastArtist = \"\"\n    LastAlbum = \"\"\n\n    START_DIR_SEP = start_dir.count(os.sep) - 1  # Number of / separators\n    #print('PRUNED_SUBDIRS:', pruned_subdirs)\n    START_DIR_SEP = START_DIR_SEP - pruned_subdirs\n\n    for i, os_name in enumerate(SortedList):\n\n        # split /mnt/music/Artist/Album/Song.m4a into list\n        '''\n            Our sorted list may have removed subdirectory levels using:\n            \n            work_list = [w.replace(os.sep + NO_ALBUM_STR + os.sep, os.sep) \\\n                 for w in work_list]\n\n        '''\n        # TODO: Check of os_name in Music Table. If so continue loop\n        #       Move this into mserve.py main loop and update access time in SQL.\n        groups = os_name.split(os.sep)\n        Artist = groups[START_DIR_SEP+1]\n        Album = groups[START_DIR_SEP+2]\n        # Song = groups[START_DIR_SEP+3]  # Not used\n        key = make_key(os_name)\n\n        if Artist != LastArtist:\n            # In future we can add Artist table with totals\n            LastArtist = Artist\n            LastAlbum = \"\"          # Force sub-total break for Album\n\n        if Album != LastAlbum:\n            # In future we can add Album table with totals\n            LastAlbum = Album\n\n        ''' Build full song path from song_list[] '''\n        full_path = os_name\n        full_path = full_path.replace(os.sep + NO_ARTIST_STR, '')\n        full_path = full_path.replace(os.sep + NO_ALBUM_STR, '')\n\n        # os.stat gives us all of file's attributes\n        stat = os.stat(full_path)\n        size = stat.st_size\n        # converted = float(size) / float(CFG_DIVISOR_AMT)   # Not used\n        # fsize = str(round(converted, CFG_DECIMAL_PLACES))  # Not used\n\n        ''' Add the song only if it doesn't exist (ie generates error) '''\n        sql = \"INSERT OR IGNORE INTO Music (OsFileName, \\\n               OsAccessTime, OsModificationTime, OsCreationTime, OsFileSize) \\\n               VALUES (?, ?, ?, ?, ?)\" \n\n        cursor.execute(sql, (key, stat.st_atime, stat.st_mtime,\n                             stat.st_ctime, size))\n\n    con.commit()\n    # Temporary during development to record history for lyrics web scrape and\n    # time index synchronizing to lyrics.\n    hist_init_lost_and_found(start_dir, user, lodict)\n    hist_init_lyrics_and_time(start_dir, user, lodict)\n\n\ndef open_db():\n    \"\"\" Open SQL Tables \"\"\"\n    global con, cursor, hist_cursor\n    # con = sqlite3.connect(\":memory:\")\n    con = sqlite3.connect(FNAME_LIBRARY)\n    # print('FNAME_LIBRARY:',FNAME_LIBRARY)\n\n    # MUSIC TABLE\n    \n    # Create the table (key must be INTEGER not just INT !\n    # See https://stackoverflow.com/a/7337945/6929343 for explanation\n    con.execute(\"create table IF NOT EXISTS Music(Id INTEGER PRIMARY KEY, \\\n                OsFileName TEXT, OsAccessTime FLOAT, \\\n                OsModificationTime FLOAT, OsCreationTime FLOAT, \\\n                OsFileSize INT, MetaArtistName TEXT, MetaAlbumName TEXT, \\\n                MetaSongName TEXT, ReleaseDate FLOAT, OriginalDate FLOAT, \\\n                Genre TEXT, Seconds INT, Duration TEXT, PlayCount INT, \\\n                TrackNumber INT, Rating TEXT, UnsynchronizedLyrics BLOB, \\\n                LyricsTimeIndex TEXT)\")\n\n    con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS OsFileNameIndex ON \\\n                Music(OsFileName)\")\n\n\n    # HISTORY TABLE\n\n    # One time table drop to rebuild new history format\n    # con.execute(\"DROP TABLE IF EXISTS History\")\n\n    con.execute(\"create table IF NOT EXISTS History(Id INTEGER PRIMARY KEY, \\\n                Time FLOAT, MusicId INTEGER, User TEXT, Type TEXT, \\\n                Action TEXT, SourceMaster TEXT, SourceDetail TEXT, \\\n                Target TEXT, Size INT, Count INT, Seconds FLOAT, \\\n                Comments TEXT)\")\n\n    con.execute(\"CREATE INDEX IF NOT EXISTS MusicIdIndex ON \\\n                History(MusicId)\")\n    con.execute(\"CREATE INDEX IF NOT EXISTS TimeIndex ON \\\n                History(Time)\")\n\n    '''\n        INDEX on OsSongName and confirm original when OsArtistName and\n            OsAlbumName match up to SORTED_LIST (aka self.song_list) which is\n            format of:\n                # split song /mnt/music/Artist/Album/Song.m4a into names:\n                groups = os_name.split(os.sep)\n                Artist = str(groups [START_DIR_SEP+1])\n                Album = str(groups [START_DIR_SEP+2])\n                Song = str(groups [START_DIR_SEP+3])\n\n            (last_playlist and last_selections uses the same record format)\n\n        Saving/retrieving LyricsTimeIndex (seconds from start):\n\n        >>> import json\n        >>> json.dumps([1.2,2.4,3.6])\n        '[1.2, 2.4, 3.6]'\n        >>> json.loads('[1.2, 2.4, 3.6]')\n        [1.2, 2.4, 3.6]\n\n    '''\n    # Retrieve column names\n    #    cs = con.execute('pragma table_info(Music)').fetchall() # sqlite column metadata\n    #    print('cs:', cs)\n    #    cursor = con.execute('select * from Music')\n    #    names = [description[0] for description in cursor.description]\n    #    print('names:', names)\n    con.row_factory = sqlite3.Row\n    cursor = con.cursor()\n    hist_cursor = con.cursor()\n\n\ndef close_db():\n    con.commit()\n    cursor.close()          # Aug 08/21 Fix \"OperationalError:\"\n    hist_cursor.close()     # See: https://stackoverflow.com/a/53182224/6929343\n    con.close()\n\n\ndef make_key(fake_path):\n    \"\"\" Create key to read Music index by OsFileName which is\n        /path/to/topdir/album/artist/song.ext\n\n    TODO: What about PRUNED_SUBDIRS from mserve code?\n\n        # Temporarily create SQL music tables until search button created.\n        sql.CreateMusicTables(SORTED_LIST, START_DIR, PRUNED_SUBDIRS)\n\n        What about '(NO_ARTIST)' and '(NO_ALBUM)' strings?\n    \"\"\"\n\n    groups = fake_path.split(os.sep)\n    artist = groups[START_DIR_SEP+1]\n    album = groups[START_DIR_SEP+2]\n    song = groups[START_DIR_SEP+3]\n    return artist + os.sep + album + os.sep + song\n\n\ndef update_lyrics(key, lyrics, time_index):\n    \"\"\"\n        Apply Unsynchronized Lyrics and Lyrics Time Index.\n        Should only be called when lyrics or time_index has changed.\n    \"\"\"\n\n    sql = \"UPDATE Music SET UnsynchronizedLyrics=?, LyricsTimeIndex=? \\\n           WHERE OsFileName = ?\" \n\n    if time_index is not None:\n        # count = len(time_index)  # Not used\n        time_index = json.dumps(time_index)\n        # print('Saving', count, 'lines of time_index:', time_index)\n\n    cursor.execute(sql, (lyrics, time_index, key))\n    con.commit()\n\n\ndef get_lyrics(key):\n    \"\"\"\n        Get Unsynchronized Lyrics and Lyrics Time Index\n    \"\"\"\n    global MUSIC_ID\n\n    cursor.execute(\"SELECT * FROM Music WHERE OsFileName = ?\", [key])\n    ''' For LyricsTimeIndex Music Table Column we need to do:\n        >>> json.dumps([1.2,2.4,3.6])\n        '[1.2, 2.4, 3.6]'\n        >>> json.loads('[1.2, 2.4, 3.6]')\n        [1.2, 2.4, 3.6]\n    '''\n    # Test if parent fields available:\n    # print('self.Artist:',self.Artist)\n    # NameError: global name 'self' is not defined\n    d = dict(cursor.fetchone())\n\n    MUSIC_ID = d[\"Id\"]\n\n    if d[\"LyricsTimeIndex\"] is not None:\n        return d[\"UnsynchronizedLyrics\"], json.loads(d[\"LyricsTimeIndex\"])\n    else:\n        return d[\"UnsynchronizedLyrics\"], None\n\n\ndef update_metadata(key, artist, album, song, genre, tracknumber, date, \n                    seconds, duration):\n    \"\"\"\n        Update Music Table with metadata tags.\n        Called from mserve.py and encoding.py\n\n        TODO: Check if history has a 'file' record first. If not then add it.\n              Add webscrape history record\n              Add lyrics 'init' record\n              Add time 'init' record\n              Add lyrics 'edit' record\n              Add time 'edit' record\n              Add History view functions with filters for done or none\n\n\n        First check if metadata has changed. If not then exit.\n\n        Update metadata in library and insert history record:\n            'meta' 'init' for first time\n            'meta' 'edit' for 2nd and subsequent changes\n\n        Metadata tags passed from following mserve variables:\n\n        Id = self.saved_selections[self.ndx]\n        list_index = int(Id)\n        key = self.song_list[list_index]\n\n        self.Artist=self.metadata.get('ARTIST', \"None\")\n        self.Album=self.metadata.get('ALBUM', \"None\")\n        self.Title=self.metadata.get('TITLE', \"None\")\n        self.Genre=self.metadata.get('GENRE', \"None\")\n        self.Track=self.metadata.get('TRACK', \"None\")\n        self.Date=self.metadata.get('DATE', \"None\")\n        self.Duration=self.metadata.get('DURATION', \"0,0\").split(',')[0]\n        self.Duration=self.Duration.split('.')[0]\n        self.DurationSecs=self.getSec(self.Duration)\n\n        sql.update_metadata(self.play_make_sql_key(), self.Artist, self.Album, \\\n                            self.Title, self.Genre, self.Track, self.Date, \\\n                            self.DurationSecs, self.Duration)\n\n    \"\"\"\n\n    # noinspection SpellCheckingInspection\n\n    # Crazy all the time spent encoding has to be decoded for SQLite3 or error:\n    # sqlite3.ProgrammingError: You must not use 8-bit bytestrings unless you\n    # use a text_factory that can interpret 8-bit bytestrings (like\n    # text_factory = str). It is highly recommended that you instead just\n    # switch your application to Unicode strings.\n    # TODO: Check for Python 3 may be required because Unicode is default type\n    artist = artist.decode(\"utf8\")          # Queensr\u00c3\u00bfche\n    # inspection SpellCheckingInspection\n    album = album.decode(\"utf8\")\n    song = song.decode(\"utf8\")\n    if type(date) is str:\n        if date != \"None\":      # Strange but true... See \"She's No Angel\" by April Wine.\n            # Problem with date \"1993-01-26\"\n            try:\n                date = float(date)\n            except ValueError:\n                pass  # Leave date as string\n    if genre is not None:\n        genre = genre.decode(\"utf8\")\n\n    #print('artist type:', type(artist), type(album), type(song))\n\n    cursor.execute(\"SELECT * FROM Music WHERE OsFileName = ?\", [key])\n    d = dict(cursor.fetchone())\n    if d is None:\n        print('SQL update_metadata() error no music ID for:', key)\n        return\n\n    # Debugging information to comment out later (or perhaps logging?)\n    '''\n    print('\\nSQL updating metadata for:',key)\n    print('artist type :', type(artist), type(album), type(song), \\\n                           type(genre))\n    print('library type:', type(d['MetaArtistName']), \\\n                           type(d['MetaAlbumName']), \\\n                           type(d['MetaSongName']), type(d['Genre']))\n    print(artist       , d['MetaArtistName'])\n    print(album        , d['MetaAlbumName'])\n    print(song         , d['MetaSongName'])\n    print(genre        , d['Genre'])\n    print(tracknumber  , d['TrackNumber'])\n    print(date         , d['ReleaseDate'])\n    print(seconds      , d['Seconds'])\n    print(duration     , d['Duration'])\n\n    if artist      != d['MetaArtistName']:\n        print('artist:', artist, d['MetaArtistName'])\n    elif album       != d['MetaAlbumName']:\n        print('album:', album, d['MetaAlbumName'])\n    elif song        != d['MetaSongName']:\n        print('song:', song, d['MetaSongName'])\n    elif genre       != d['Genre']:\n        print('genre:', genre, d['Genre'])\n    elif tracknumber != d['TrackNumber']:\n        print('tracknumber:', tracknumber, d['TrackNumber'])\n    elif date        != d['ReleaseDate']:\n        print('date:', date, d['ReleaseDate'])\n    elif seconds     != d['Seconds']:\n        print('seconds:', seconds, d['Seconds'])\n    elif duration    != d['Duration']:\n        print('duration:', duration, d['Duration'])\n    else:\n        print('All things considered EQUAL')\n    '''\n\n    # Are we adding a new 'init' or 'edit' history record?\n    if d['MetaArtistName'] is None:\n        action = 'init'\n        # print('\\nSQL adding metadata for:',key)\n    elif \\\n        artist       != d['MetaArtistName'] or \\\n        album        != d['MetaAlbumName'] or \\\n        song         != d['MetaSongName'] or \\\n        genre        != d['Genre'] or \\\n        tracknumber  != d['TrackNumber'] or \\\n        date         != d['ReleaseDate'] or \\\n        seconds      != d['Seconds'] or \\\n            duration != d['Duration']:\n        # To test, use kid3 to temporarily change track number\n        # float(date) != d['ReleaseDate'] or \\ <- They both could be None\n        # Metadata hsa changed from last recorded version\n        action = 'edit'\n\n    else:\n        return                                  # Metadata same as library\n\n    # Update metadata for song into library Music Table\n    sql = \"UPDATE Music SET MetaArtistName=?, MetaAlbumName=?, MetaSongName=?, \\\n           Genre=?, TrackNumber=?, ReleaseDate=?, Seconds=?, Duration=? \\\n           WHERE OsFileName = ?\" \n\n    cursor.execute(sql, (artist, album, song, genre, tracknumber, date,\n                         seconds, duration, key))\n    con.commit()\n\n    # Add history record\n    # Time will be file's last modification time\n    ''' Build full song path '''\n    full_path = _START_DIR.encode(\"utf8\") + key\n    # Below not needed because (No Xxx) stubs not in Music Table filenames\n    full_path = full_path.replace(os.sep + NO_ARTIST_STR, '')\n    full_path = full_path.replace(os.sep + NO_ALBUM_STR, '')\n\n    # os.stat gives us all of file's attributes\n    stat = ext.stat_existing(full_path)\n    if stat is None:\n        print(\"sql.update_metadata(): File below doesn't exist:\\n\")\n        for i in d:\n            # Pad name with spaces for VALUE alignment\n            print('COLUMN:', \"{:<25}\".format(i), 'VALUE:', d[i])\n        return\n\n    Size = stat.st_size                     # File size in bytes\n    Time = stat.st_mtime                    # File's current mod time\n    SourceMaster = _LODICT['name']\n    SourceDetail = time.asctime(time.gmtime(Time))\n    Comments = \"Found: \" + time.asctime(time.gmtime(time.time()))\n    if seconds is not None:\n        FloatSeconds = float(str(seconds))  # Convert from integer\n    else:\n        FloatSeconds = 0.0\n\n    Count = 0\n\n    # If adding, the file history record may be missing too.\n    if action == 'init' and \\\n       not hist_check(d['Id'], 'file', action):\n        hist_add(Time, d['Id'], _USER, 'file', action, SourceMaster,\n                 SourceDetail, key, Size, Count, FloatSeconds,\n                 Comments)\n\n    # Add the meta Found or changed record\n    '''\n    print(time.time(), d['Id'], _USER, 'meta', action, SourceMaster, \\\n             SourceDetail, key, Size, Count, FloatSeconds, \n             Comments, sep=\" # \")\n    '''\n    hist_add(Time, d['Id'], _USER, 'meta', action, SourceMaster,\n             SourceDetail, key, Size, Count, FloatSeconds, \n             Comments)\n\n    con.commit()\n\n\n#==============================================================================\n#\n#       sql.py - History table processing\n#\n#==============================================================================\n\n\ndef hist_get_row(key):\n    # Get the MusicID matching song file's basename\n    cursor.execute(\"SELECT * FROM History WHERE Id = ?\", [key])\n    row = cursor.fetchone()\n    if row is None:\n        print('sql.py - row not found:', key)\n        return None\n\n    return OrderedDict(row)\n\n\ndef hist_get_music_id(key):\n    # Get the MusicID matching song file's basename\n    cursor.execute(\"SELECT Id FROM Music WHERE OsFileName = ?\", [key])\n    row = cursor.fetchone()\n    if row is None:\n        print('hist_get_music_id(key) error no music ID for:', key)\n        return 0\n    elif row[0] == 0:\n        print('hist_get_music_id(key) error music ID is 0:', key)\n        return 0\n    else:\n        return row[0]\n\n\ndef hist_add_time_index(key, time_list):\n    \"\"\"\n        Add time index if 'init' doesn't exist.\n        If time index does exist, add an 'edit' if it has changed.\n    \"\"\"\n    # Get the MusicID matching song file's basename\n    MusicId = hist_get_music_id(key)\n    if MusicId == 0:\n        print('SQL hist_add_time_index(key) error no music ID for:', key)\n        return False\n\n    if hist_check(MusicId, 'time', 'init'):\n        # We found a time initialization record to use as default\n        Action = 'edit'\n        print('sql.hist_add_time_index(key) edit time, init:', key, HISTORY_ID)\n        hist_cursor.execute(\"SELECT * FROM History WHERE Id = ?\", [HISTORY_ID])\n        d = dict(hist_cursor.fetchone())\n        if d is None:\n            print('sql.hist_add_time_index() error no History ID:', key,\n                  HISTORY_ID)\n            return False\n        # TODO: Read music row's time_list and to see if it's changed.\n        #       If it hasn't changed then return. This means we have to\n        #       add history before we update music table. Or parent only\n        #       updates music table when lyrics or time index changes. In this\n        #       case another history record is required for 'lyrics', 'edit'.\n    else:\n        # Add time initialization record\n        Action = 'init'\n        d = hist_default_dict(key, 'access')\n        if d is None:\n            print('sql.hist_add_time_index() error creating default dict.')\n            return False\n\n    d['Count'] = len(time_list)\n    Comments = Action + \" time: \" + time.asctime(time.gmtime(time.time()))\n    hist_add(time.time(), d['Id'], _USER, 'time', Action, d['SourceMaster'],\n             d['SourceDetail'], key, d['Size'], d['Count'], d['Seconds'], \n             Comments)\n\n    return True\n\n\ndef hist_add_shuffle(Action, SourceMaster, SourceDetail):\n    Type = \"playlist\"\n    # Action = 'shuffle'\n    if Type == Action == SourceMaster == SourceDetail:\n        return  # Above test for pycharm checking  \n  \n\n\ndef hist_default_dict(key, time_type='access'):\n    \"\"\" Construct a default dictionary used to add a new history record \"\"\"\n\n    cursor.execute(\"SELECT * FROM Music WHERE OsFileName = ?\", [key])\n    d = dict(cursor.fetchone())\n    if d is None:\n        print('SQL hist_default_dict() error no music row for:', key)\n        return None\n\n    hist = {}                               # History dictionary\n    SourceMaster = _LODICT['name']\n    hist['SourceMaster'] = SourceMaster\n\n    ''' Build full song path '''\n    full_path = START_DIR.encode(\"utf8\") + d['OsFileName']\n    # Below not needed because (No Artist / No Album) not in filenames\n    # full_path = full_path.replace(os.sep + NO_ARTIST_STR, '')\n    # full_path = full_path.replace(os.sep + NO_ALBUM_STR, '')\n\n    # os.stat gives us all of file's attributes\n    stat = os.stat(full_path)\n    size = stat.st_size                     # File size in bytes\n    hist['Size'] = size\n    hist['Count'] = 0                       # Temporary use len(time_list)\n    if time_type == 'access':\n        Time = stat.st_atime                # File's current access time\n    elif time_type == 'mod':\n        Time = stat.st_mtime                # File's current mod time\n    elif time_type == 'birth':\n        Time = stat.st_mtime                # File's birth/creation time\n    else:\n        print('SQL hist_default_dict(key, time_type) invalid type:', time_type)\n        return None\n\n    SourceDetail = time.asctime(time.gmtime(Time))\n    hist['SourceDetail'] = SourceDetail\n    # Aug 10/2021 - Seconds always appears to be None\n    if Seconds is not None:\n        FloatSeconds = float(str(Seconds))  # Convert from integer\n    else:\n        FloatSeconds = 0.0\n    hist['Seconds'] = FloatSeconds\n\n    return hist                             # Some dict fields aren't populated\n\n\ndef hist_delete_time_index(key):\n    \"\"\"\n        All time indexes have been deleted.\n        Check if history 'init' record exists. If so copy it and use 'delete'\n        to add new history record.\n    \"\"\"\n    # Get the MusicID matching song file's basename\n    MusicId = hist_get_music_id(key)\n    if MusicId == 0:\n        print('SQL hist_delete_time_index(key) error no music ID for:', key)\n        return False\n\n    if not hist_check(MusicId, 'time', 'init'):\n        # We found a time initialization record to use as default\n        print('sql.hist_delete_time_index(key) error no time, init:', key)\n        return False\n\n    print('sql.hist_delete_time_index(key) HISTORY_ID:', key, HISTORY_ID)\n\n    hist_cursor.execute(\"SELECT * FROM History WHERE Id = ?\", [HISTORY_ID])\n    d = dict(hist_cursor.fetchone())\n    if d is None:\n        print('sql.hist_delete_time_index(key) error no History ID:', key,\n              HISTORY_ID)\n        return False\n\n    Comments = \"Removed: \" + time.asctime(time.gmtime(time.time()))\n    hist_add(time.time(), d['Id'], _USER, 'time', 'remove', d['SourceMaster'],\n             d['SourceDetail'], key, d['Size'], d['Count'], d['Seconds'], \n             Comments)\n\n    return True\n\n\ndef hist_init_lost_and_found(START_DIR, USER, LODICT):\n    \"\"\" Tool to initialize history time for all songs in database.\n        This step just records 'file' and 'init' for OS song filename.\n        If metadata present then 'meta' and 'init' also recorded.\n\n        The time for 'init' is the files modification time which should be\n            lowest time across all devices if synced properly.\n\n        If a file is both lost and found in the same second and the metadata\n            matches that simply means the file was renamed / moved.\n\n        The same song can be found in multiple locations and smaller devices\n            might have fewer songs that the master location.\n\n    \"\"\"\n\n    song_count = 0\n    add_count = 0\n    add_meta_count = 0\n    # History Table columns\n    # Time = time.time()    # Aug 8/21 use time.time() instead of job start\n    User = USER             # From location.py\n    Type = 'file'           # This records OS filename into history\n    Action = 'init'         # Means we \"found\" the file or it was renamed\n    '''\n    As of April 13, 2021:\n    DICT={'iid': iid, 'name': name, 'topdir': topdir, 'host': host, 'wakecmd':\n      wakecmd, 'testcmd': testcmd, 'testrep': testrep, 'mountcmd': \\\n      mountcmd, 'activecmd': activecmd, 'activemin': activemin}\n    '''\n    SourceMaster = LODICT['name']\n    #  Aug 8/21 comment out fields below not used\n    #SourceDetail = 'Today'  # Formatted time string \"DDD MMM DD HH:MM:SS YYYY\"\n    #Target = '/path/file'   # Replaced with OS filename below\n    #Size = 0                # File size in bytes\n    #Count = 0               # Number of renaming of path/name/filename\n    #Seconds = 0.0           # Song duration\n    Comments = 'Automatically added by hist_init_lost_and_found()'\n\n    # Select songs that have lyrics (Python 'not None:' = SQL 'NOT NULL')\n    for row in cursor.execute('SELECT Id, OsFileName, OsModificationTime, ' +\n                              'MetaSongName, Seconds FROM Music'):\n        song_count += 1\n        # Check if history already exists for song\n        MusicId = row[0]\n        if hist_check(MusicId, Type, Action):\n            continue\n\n        # Name our Music Table columns needed for History Table\n        OsFileName = row[1] \n        # OsModificationTime = row[2]  # Us as default for found time  # Not used\n        MetaSongName = row[3]       # If name not blank, we have metadata\n        Seconds = row[4]            # Song Duration in seconds (INT)\n\n        ''' TODO: What about PRUNED_SUBDIRS from mserve code?\n\n        # Temporarily create SQL music tables until search button created.\n        sql.CreateMusicTables(SORTED_LIST, START_DIR, PRUNED_SUBDIRS)\n        '''\n\n        ''' Build full song path '''\n        full_path = START_DIR.encode(\"utf8\") + OsFileName\n\n        # os.stat gives us all of file's attributes\n        stat = ext.stat_existing(full_path)\n        if stat is None:\n            print(\"sql.hist_init_lost_and_found(): File below doesn't exist:\\n\")\n            names = cursor.description\n            for i, name in enumerate(names):\n                # Pad name with spaces for VALUE alignment\n                print('COLUMN:', \"{:<25}\".format(name[0]), 'VALUE:', row[i])\n            continue  # TODO: do \"lost\" phase, mark song as deleted somehow\n\n        Size = stat.st_size                     # File size in bytes\n        Time = stat.st_mtime                    # File's current mod time\n        SourceDetail = time.asctime(time.gmtime(Time))\n        if Seconds is not None:\n            FloatSeconds = float(str(Seconds))  # Convert from integer\n        else:\n            FloatSeconds = 0.0\n\n        Count = 0\n        Target = OsFileName\n\n        # Add the Song Found row\n        # Aug 8/21 use time.time() instead of job start time.\n        hist_add(time.time(), MusicId, User, Type, Action, SourceMaster, SourceDetail, \n                 Target, Size, Count, FloatSeconds, Comments)\n        add_count += 1\n\n        if MetaSongName is not None:\n            # Add the Metadata Found row\n            hist_add(time.time(), MusicId, User, 'meta', Action, SourceMaster,\n                     SourceDetail, OsFileName, Size, Count, FloatSeconds, \n                     Comments)\n            add_meta_count += 1\n\n    #print('Songs on disk:', song_count, 'Added count:', add_count, \\\n    #      'Added meta count:', add_meta_count)\n\n    con.commit()                                # Save database changes\n\n\nHISTORY_ID = None\n\n\ndef hist_check(MusicId, check_type, check_action):\n    \"\"\" History table usage for Music Lyrics:\n\n        VARIABLE        DESCRIPTION\n        --------------  -----------------------------------------------------\n        Id              Primary integer key auto-incremented\n        Time            In system format with nano-second precision\n                        filetime = (unix time * 10000000) + 116444736000000000\n                        Secondary key\n        MusicId         Link to primary key in Music Table usually rowid\n                        For setting (screen, monitor, window, etc) the\n                        MusicId is set to 0.\n        User            User name, User ID or GUID varies by platform.\n        Type            'file', 'catalog', 'link', 'index', 'checkout', 'song'\n                        'lyrics', 'time', 'fine-tune', 'meta', 'playlist'\n        Action          'copy', 'download', 'remove', 'burn', 'edit', 'play'\n                        'scrape', 'init', 'shuffle', 'save', 'load'\n        SourceMaster    'Genius', 'Metro Lyrics', etc.\n                        Device name, Playlist\n        SourceDetail    '//genius.com' or 'www.metrolyrics.com', etc.\n                        Location number, song names in order (soon music IDs)\n        Target          'https://www.azlyrics.com/lyrics/greenday/wake...html'\n        Size            Total bytes in downloaded text file\n                        Duration of lyrics synchronized (end - start time)\n        Count           Number of lines in downloaded text file\n                        Number of lyrics lines synchronized\n        Seconds         How many seconds the operation took Float\n        Comments        For most records formatted date time\n    \"\"\"\n    global HISTORY_ID\n\n    for row in hist_cursor.execute(\"SELECT Id, Type, Action FROM History \" +\n                                   \"WHERE MusicId = ?\", [MusicId]):\n        Id = row[0]\n        Type = row[1]\n        Action = row[2]\n        if Type == check_type and Action == check_action:\n            HISTORY_ID = Id\n            return True\n\n    HISTORY_ID = 0\n    return False                # Not Found\n\n\ndef hist_last_time(check_type, check_action):\n    \"\"\" Get the last time the type + action occurred\n\n        Primarily used to get the last time a song was added / updated in\n        history. If this time is greater than time top directory was\n        last changed then refresh not required.\n    \"\"\"\n    global HISTORY_ID\n\n    for row in hist_cursor.execute(\"SELECT * FROM History \" +\n                                   \"INDEXED BY TimeIndex \" +\n                                   \"ORDER BY Time DESC\"):\n        d = dict(row)\n        Id = d['Id']\n        Type = d['Type']\n        Action = d['Action']\n        if Type == check_type and Action == check_action:\n            HISTORY_ID = Id\n            return d['Time']\n\n    HISTORY_ID = 0\n    return None                # Not Found\n\n\ndef hist_add(Time, MusicId, User, Type, Action, SourceMaster, SourceDetail, \n             Target, Size, Count, Seconds, Comments):\n    \"\"\" Add History Row for Synchronizing Lyrics Time Indices.\n    \"\"\"\n    # DEBUG:\n    # InterfaceError: Error binding parameter 1 - probably unsupported type.\n    # print(\"Time, MusicId, User, Type, Action, SourceMaster, SourceDetail,\")\n    # print(\"Target, Size, Count, Seconds, Comments:\")\n    # print(Time, MusicId, User, Type, Action, SourceMaster, SourceDetail,\n    #      Target, Size, Count, Seconds, Comments)\n    sql = \"INSERT INTO History (Time, MusicId, User, Type, Action, \\\n           SourceMaster, SourceDetail, Target, Size, Count, Seconds, Comments) \\\n           VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n\n    hist_cursor.execute(sql, (Time, MusicId, User, Type, Action, SourceMaster,\n                              SourceDetail, Target, Size, Count, Seconds,\n                              Comments))\n\n\ndef hist_delete_type_action(Type, Action):\n    \"\"\" Delete History Rows for matching Type and Action.\n        Created to get rid of thousands of 'meta' 'edit' errors\n    \"\"\"\n    # DEBUG:\n    sql = \"DELETE FROM History WHERE Type=? AND Action=?\"\n\n    hist_cursor.execute(sql, (Type, Action))\n    deleted_row_count = hist_cursor.rowcount\n    print('hist_delete_type_action(Type, Action):', Type, Action,\n          'deleted_row_count:', deleted_row_count)\n    con.commit()\n\n\ndef hist_init_lyrics_and_time(START_DIR, USER, LODICT):\n    \"\"\" Tool to initialize history time for all songs that have lyrics.\n        The time will be the last file access time.\n\n        If lyric time indices are set the lyrics webscrape is 5 minutes earlier\n        and the index time is the file access time.\n\n        Assume lyrics source is Genius but could have been clipboard or user\n        direct entry.\n    \"\"\"\n    song_count = 0\n    add_count = 0\n    add_time_count = 0\n    # History Table columns\n    # Time = time.time()    # Aug 8/21 not used\n    User = USER             # From location.py\n    Type = 'lyrics'\n    Action = 'scrape'\n    SourceMaster = 'Genius'  # Website lyrics were scraped from\n    # Aug 8/21 comment out fields below not used\n    # SourceDetail = 'Today'  # Formatted time string \"DDD MMM DD HH:MM:SS YYYY\"\n    # Target = 'https://genius.com/artist/album/song.html'\n    # Size = 0\n    # Count = 0\n    # Seconds = 0.0\n    Comments = 'Automatically added by hist_init_lyrics_and_time()'\n\n    # Select songs that have lyrics (Python 'not None:' = SQL 'NOT NULL')\n    for row in cursor.execute(\"SELECT Id, OsFileName, UnsynchronizedLyrics, \" +\n                              \"LyricsTimeIndex, OsAccessTime, Seconds FROM \" +\n                              \"Music WHERE UnsynchronizedLyrics IS NOT NULL\"):\n        song_count += 1\n        # Check if history already exists for song\n        MusicId = row[0]\n        if hist_check(MusicId, Type, Action):\n            continue\n\n        # Name our Music Table columns needed for History Table\n        OsFileName = row[1] \n        UnsynchronizedLyrics = row[2]\n        LyricsTimeIndex = row[3]\n        # OsAccessTime = row[4]                   # At time of Music Row creation\n        Seconds = row[5]                        # Song Duration\n\n        ''' TODO: What about PRUNED_SUBDIRS from mserve code?\n\n        # Temporarily create SQL music tables until search button created.\n        sql.CreateMusicTables(SORTED_LIST, START_DIR, PRUNED_SUBDIRS)\n        '''\n\n        ''' Build full song path '''\n        full_path = START_DIR.encode(\"utf8\") + OsFileName\n        # Below not needed because (No Xxx) stubs not in Music Table filenames\n        full_path = full_path.replace(os.sep + NO_ARTIST_STR, '')\n        full_path = full_path.replace(os.sep + NO_ALBUM_STR, '')\n\n        # os.stat gives us all of file's attributes\n        stat = os.stat(full_path)\n        # size = stat.st_size                     # Not used\n        # converted = float(size) / float(CFG_DIVISOR_AMT)\n        # fsize = str(round(converted, CFG_DECIMAL_PLACES))\n\n        Time = stat.st_atime                    # File's current access time\n        SourceDetail = time.asctime(time.gmtime(Time))\n        Size = len(UnsynchronizedLyrics)        # Can change after user edits\n        Count = UnsynchronizedLyrics.count('\\n')\n        Target = 'https://genius.com/' + OsFileName\n\n        if LyricsTimeIndex is None:\n            time_count = 0\n            time_index_list = None\n        else:\n            time_index_list = json.loads(LyricsTimeIndex)\n            time_count = len(time_index_list)\n            if time_count < 5:\n                print('time count:', time_count, Target)\n\n        # Estimate 4 seconds to download lyrics (webscrape)\n        hist_add(Time, MusicId, User, Type, Action, SourceMaster, SourceDetail, \n                 Target, Size, Count, 4.0, Comments)\n        add_count += 1\n\n        if time_count > 0:\n            Time = stat.st_atime + 300          # 5 minutes to sync lyrics\n            duration = time_index_list[-1] - time_index_list[0]\n            Size = int(duration)                # Amount of time synchronized\n            Count = len(time_index_list)        # How many lines synchronized\n            '''\n            As of April 12, 2021:\n            DICT={'iid': iid, 'name': name, 'topdir': topdir, 'host': host, 'wakecmd':\n              wakecmd, 'testcmd': testcmd, 'testrep': testrep, 'mountcmd': \n              mountcmd, 'activecmd': activecmd, 'activemin': activemin}\n            '''\n            hist_add(Time, MusicId, User, 'time', 'edit', LODICT['name'],\n                     LODICT['iid'], OsFileName, Size, Count, float(Seconds), \n                     Comments)\n            add_time_count += 1\n\n    #print('Songs with lyrics:', song_count, 'Added count:', add_count, \\\n    #      'Added time count:', add_time_count)\n    con.commit()\n\n\n# =================================  WEBSCRAPE  ===============================\n\nclass Webscrape:\n\n    def __init__(self, music_id):\n        # All columns in history row, except primary ID auto assigned.\n        self.MusicId = music_id\n        self.User = None\n        self.Type = None\n        self.Action = None\n        self.SourceMaster = None\n        self.SourceDetail = None\n        self.Target = None\n        self.Size = None\n        self.Count = None\n        self.Comments = None\n\n    def set_ws_parm(self, music_id):\n        \"\"\" Save Webscrape parameters - Currently in mserve.py:\n\n            MusicId = sql.hist_get_music_id(self.work_sql_key)\n            sql.hist_add(time.time(), MusicId, USER,\n                         'scrape', 'parm', artist, song,\n                         \"\", 0, 0, 0.0,\n                         time.asctime(time.gmtime(time.time())))\n            ext_name = 'python webscrape.py'\n            self.lyrics_pid = ext.launch_command(ext_name,\n                                                 toplevel=self.play_top)\n\n        \"\"\"\n        pass\n\n    def get_ws_parm(self, music_id):\n        \"\"\" Get Webscrape parameters\n            now = time.time()\n            last_time = sql.hist_last_time('scrape', 'parm')\n            hist_row = sql.hist_get_row(sql.HISTORY_ID)\n            lag = now - last_time\n            if lag > 1.0:\n                print('It took more than 1 second for webscrape to start:', lag)\n            else:\n                print('webscrape start up time:', lag)\n                pass\n            print(hist_row)\n\n        \"\"\"\n        pass\n\n    def set_ws_result(self, music_id):\n        \"\"\" Save Webscrape results\n        \"\"\"\n        pass\n\n    def get_ws_result(self, music_id):\n        \"\"\" Get Webscrape results\n        \"\"\"\n        pass\n\n    def read_music_id(self, music_id):\n\n        \"\"\"\n        ==========================   COPY from webscrape.py   =========================\n            \n        # Web scraping song lyrics IPC file names\n        SCRAPE_CTL_FNAME    = '/run/user/\" + g.USER_ID + \"/mserve.scrape_ctl.json'\n        SCRAPE_LIST_FNAME   = '/run/user/\" + g.USER_ID + \"/mserve.scrape_list.txt'\n        SCRAPE_LYRICS_FNAME = '/run/user/\" + g.USER_ID + \"/mserve.scrape_lyrics.txt'\n        \n        # Names list is used in our code for human readable formatting\n        NAMES_LIST =   ['Metro Lyrics',     'AZ Lyrics',        'Lyrics',\n                        'Lyrics Mode',      'Lets Sing It',     'Genius',\n                        'Musix Match',      'Lyrics Planet']\n        \n        # Website list is used in webscrape.py for internet formatting\n        WEBSITE_LIST = ['www.metrolyrics.com', 'www.azlyrics.com',    'www.lyrics.com',\n                        'www.lyricsmode.com',  'www.letssingit.com',  '//genius.com', \n                        'www.musixmatch.com',  'www.lyricsplanet.com']\n        \n        # Empty control list (template)\n        CTL_LIST = [{} for _ in range(len(WEBSITE_LIST))]\n        #CTL_LIST = [ {}, {}, {}, {}, {}, {}, {}, {} ]\n        \n        # If we try to print normally an error occurs when launched in background\n        #print(\"CTL_LIST:\", CTL_LIST, file=sys.stderr)\n        \n        # Empty control list dictionary element (template)\n        WS_DICT = { \"name\":\"\", \"website\":\"\", \"link\":\"\", \"flag\":\"\" }\n        ''' flag values: preference passed to webscrape.py. result passed to mserve\n            preference:  1-8 try to get lyrics in this order, 'skip' = skip site\n            result:      'found' lyrics returned. 'available' lyrics can be returned\n                         'not found' no link or link is empty (eg artist but no lyrics)\n        '''\n\ndef save_ctl():\n    '''\n        Save Control file containing list of dictionaries\n\n        USED by mserve and webscrape.py\n            mserve passes previous list of names with flags to scrape.\n            webscrape.py passes back name of website that was scraped.\n            webscrape.py passes names of websites that CAN BE scraped.\n    '''\n    with open(SCRAPE_CTL_FNAME, \"w\") as ctl:\n        ctl.write(json.dumps(CTL_LIST))\n\n\ndef load_ctl():\n    '''\n        Return contents of CTL file or empty list of dictionaries\n    '''\n    data = CTL_LIST\n    if os.path.isfile(SCRAPE_CTL_FNAME):\n        with open(SCRAPE_CTL_FNAME, \"r\") as ctl:\n            data = json.loads(ctl.read())\n\n    return data\n\n\n\"\"\"\n        return\n\n\n# ==============================================================================\n#\n#       PrettyHistory class - DIFFERENT than ~/bserve/sql.py\n#\n# ==============================================================================\n\n\nclass PrettyHistory:\n\n    def __init__(self, history_dict, calc=None):\n        \"\"\" \n            Copied from bserve/gmail_api.py PrettyHistory\n\n                1) top level dictionary key/values like Id, size\n\n             After massaging, four sections are at single dictionary level\n             Dictionary keys can be walked and compared to count of keys at\n             each part for separating sections in display.\n\n            This class serves double duty (for now) to display treeview column\n            data dictionary for class view()) at column name.\n\n            calc is optional function to append calculated fields to the\n            pretty dictionary.\n\n        \"\"\"\n\n        self.calc = calc  # Calculated fields such as delete_on\n        self.dict = OrderedDict()  # Python 2.7 version not needed in 3.7\n        self.scrollbox = None  # custom scrollbox for display\n\n        # IF dictionary for treeview column, format is straight forward\n        self.part_start = [0]  # Only 1 part\n        self.part_names = ['Tkinter Treeview']\n        self.part_color = ['red']\n        for key, value in history_dict.iteritems():\n            self.dict[key] = self.format_value(value)\n\n        if calc is None:\n            # If no calc callback we are done.\n            return\n\n        # List of part section headings at part_start[] list above\n        self.part_names = ['Google search results',\n                           'Webscrape results']\n        # List of part colors - applied to key names in that part\n        self.part_color = ['red',\n                           'blue']\n\n        self.part_start.append(len(self.dict))\n\n        self.calc(self.dict)  # Call external function passing our dict\n        # print(\"self.calc(self.dict)  # Call external function passing our dict\")\n\n        # print('\\n======================  pretty  =====================\\n')\n        # print(json.dumps(self.dict, indent=2))\n\n    @staticmethod\n    def format_value(value):\n\n        try:\n            formatted = str(value)  # Convert from int\n        except UnicodeEncodeError:\n            formatted = value\n        # return formatted.encode('utf8')\n        return formatted\n\n    def tkinter_display(self, scrollbox):\n        \"\"\" Popup display all values in pretty print format\n            Uses new tkinter window with single text entry field\n\n            Requires ordered dict and optional lists specifying sections\n            (parts) the part names and part colors for key names.\n        \"\"\"\n\n        self.scrollbox = scrollbox  # Temporary until code craft\n\n        # Allow program changes to scrollable text widget\n        self.scrollbox.configure(state=\"normal\")\n        self.scrollbox.delete('1.0', 'end')  # Delete previous entries\n\n        curr_key = 0  # Current key index\n        curr_level = 0  # Current dictionary part\n        curr_color = 'black'\n        # for key, value in self.dict.iteritems():    # Don't use iteritems\n        for key in self.dict:  # Don't need iteritems on ordered dict\n            if curr_key == self.part_start[curr_level]:\n                curr_level_name = self.part_names[curr_level]\n                curr_color = self.part_color[curr_level]\n                self.scrollbox.insert(\"end\", curr_level_name + \"\\n\")\n                # self.scrollbox.highlight_pattern(curr_level_name, 'yellow')\n                curr_level += 1\n\n                if curr_level >= len(self.part_start):\n                    curr_level = len(self.part_start) - 1\n                    # We are in last part so no next part to check\n                    # print('resetting curr_level at:', key)\n\n            # Insert current key and value into text widget\n            # TclError: character U+1f913 is above the range (U+0000-U+FFFF) allowed by Tcl\n            # noinspection PyBroadException\n            try:\n                self.scrollbox.insert(\"end\", u\"\\t\" + key + u\":\\t\" +\n                                      self.dict[key] + u\"\\n\", u\"margin\")\n            #                                  value + u\"\\n\", \"margin\")\n            except:\n                normal = normalize_tcl(self.dict[key])\n                self.scrollbox.insert(\"end\", u\"\\t\" + key + u\":\\t\" +\n                                      normal + u\"\\n\", u\"margin\")\n\n            self.scrollbox.highlight_pattern(key + u':', curr_color)\n            curr_key += 1  # Current key index\n\n        # Override for auto trader that contains multiple keys within value\n        self.scrollbox.highlight_pattern(\n            \"From:To:Subject:Date:List-Unsubscribe:List-Unsubscribe-Post:\" +\n            \"MIME-Version: Reply-To:List-ID:X-CSA-Complaints:Message-ID:\" +\n            \"Content-Type:\", \"black\")\n\n        self.scrollbox.highlight_pattern(\n            \"Date:Message-ID:Content-Type:Subject:To:\", \"black\")\n\n        # Override for disqus that contains multiple keys within value\n        self.scrollbox.highlight_pattern(\n            \"Subject:From:To:\", \"black\")\n\n        # Don't allow changes to displayed selections (test copy clipboard)\n        self.scrollbox.configure(state=\"disabled\")\n\n\ndef normalize_tcl(s):\n    \"\"\"\n        Fixes error:\n\n          File \"/usr/lib/python2.7/lib-tk/ttk.py\", line 1339, in insert\n            res = self.tk.call(self._w, \"insert\", parent, index, *opts)\n        _tkinter.TclError: character U+1f3d2 is above the\n            range (U+0000-U+FF FF) allowed by Tcl\n\n        From: https://bugs.python.org/issue21084\n    \"\"\"\n\n    astral = re.compile(r'([^\\x00-\\uffff])')\n    new_s = \"\"\n    for i, ss in enumerate(re.split(astral, s)):\n        if not i % 2:\n            new_s += ss\n        else:\n            new_s += '?'\n\n    return new_s\n\n\ndef music_treeview():\n    \"\"\" Define Data Dictionary treeview columns for history table\n    \"\"\"\n\n    music_treeview_list = [\n\n      OrderedDict([\n        (\"column\", \"row_id\"), (\"heading\", \"Row ID\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"Id\"), (\"select_order\", 0), (\"unselect_order\", 1),\n        (\"key\", False), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", \"{,,}\"),\n        (\"display_width\", 150), (\"display_min_width\", 80),\n        (\"display_long\", None), (\"stretch\", 0)]),  # 0=NO, 1=YES\n\n      OrderedDict([\n        (\"column\", \"os_filename\"), (\"heading\", \"OS Filename\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"OsFileName\"), (\"select_order\", 0), (\"unselect_order\", 2),\n        (\"key\", True), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 200), (\"display_min_width\", 120),\n        (\"display_long\", None), (\"stretch\", 0)]),  # 0=NO, 1=YES\n\n      OrderedDict([\n        (\"column\", \"os_atime\"), (\"heading\", \"Access Time\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"OsAccessTime\"), (\"select_order\", 0), (\"unselect_order\", 3),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", float), (\"format\", \"{0,,f}\"),\n        (\"display_width\", 80), (\"display_min_width\", 80),\n        (\"display_long\", None), (\"stretch\", 0)]),  # 0=NO, 1=YES\n\n      OrderedDict([\n        (\"column\", \"os_mtime\"), (\"heading\", \"Mod Time\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"OsModificationTime\"), (\"select_order\", 0), (\"unselect_order\", 4),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", float), (\"format\", \"{0,,f}\"),\n        (\"display_width\", 80), (\"display_min_width\", 80),\n        (\"display_long\", None), (\"stretch\", 0)]),\n\n      OrderedDict([\n        (\"column\", \"os_ctime\"), (\"heading\", \"Create Time\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"OsCreationTime\"), (\"select_order\", 0), (\"unselect_order\", 5),\n        (\"key\", False), (\"anchor\", \"e\"), (\"instance\", float),\n        (\"format\", \"{0,,f}\"), (\"display_width\", 80),\n        (\"display_min_width\", 80), (\"display_long\", None), (\"stretch\", 0)]),\n\n      OrderedDict([\n        (\"column\", \"os_file_size\"), (\"heading\", \"File Size\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"OsFileSize\"), (\"select_order\", 0), (\"unselect_order\", 6),\n        (\"key\", False), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", \"{:,}\"),\n        (\"display_width\", 150), (\"display_min_width\", 120),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"artist\"), (\"heading\", \"Artist\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"MetaArtistName\"), (\"select_order\", 0), (\"unselect_order\", 7),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 400), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),  # 0=NO, 1=YES\n\n      OrderedDict([\n        (\"column\", \"album\"), (\"heading\", \"Album\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"Album\"), (\"select_order\", 0), (\"unselect_order\", 8),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 400), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"song_name\"), (\"heading\", \"Song Name\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"MetaSongName\"), (\"select_order\", 0), (\"unselect_order\", 9),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 400), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"release_date\"), (\"heading\", \"Release Date\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"ReleaseDate\"), (\"select_order\", 0), (\"unselect_order\", 10),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", float), (\"format\", \"{0,,f}\"),\n        (\"display_width\", 80), (\"display_min_width\", 80),\n        (\"display_long\", None), (\"stretch\", 0)]),\n\n      OrderedDict([\n        (\"column\", \"original_date\"), (\"heading\", \"Original Date\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"OriginalDate\"), (\"select_order\", 0), (\"unselect_order\", 11),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", float), (\"format\", \"{0,,f}\"),\n        (\"display_width\", 80), (\"display_min_width\", 80),\n        (\"display_long\", None), (\"stretch\", 0)]),\n\n      OrderedDict([\n        (\"column\", \"genre\"), (\"heading\", \"Genre\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"Genre\"), (\"select_order\", 0), (\"unselect_order\", 12),\n        (\"key\", False), (\"anchor\", \"center\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 160), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"seconds\"), (\"heading\", \"Seconds\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"Seconds\"), (\"select_order\", 0), (\"unselect_order\", 13),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 160), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"duration\"), (\"heading\", \"Duration\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"Duration\"), (\"select_order\", 0), (\"unselect_order\", 14),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 160), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"play_count\"), (\"heading\", \"Play Count\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"PlayCount\"), (\"select_order\", 0), (\"unselect_order\", 15),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 160), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"track_number\"), (\"heading\", \"Track Number\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"TrackNumber\"), (\"select_order\", 0), (\"unselect_order\", 16),\n        (\"key\", False), (\"anchor\", \"e\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 160), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"rating\"), (\"heading\", \"Rating\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"Rating\"), (\"select_order\", 0), (\"unselect_order\", 17),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", int), (\"format\", \"{:,}\"),\n        (\"display_width\", 160), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"lyrics\"), (\"heading\", \"Lyrics\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"UnsynchronizedLyrics\"), (\"select_order\", 0), (\"unselect_order\", 18),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 600), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"time_index\"), (\"heading\", \"Time Index\"), (\"sql_table\", \"Music\"),\n        (\"var_name\", \"LyricsTimeIndex\"), (\"select_order\", 0), (\"unselect_order\", 19),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 160), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)])\n    ]\n\n    return music_treeview_list\n\n\ndef history_treeview():\n    \"\"\" Define Data Dictionary treeview columns for history table.  Snippet:\n        (\"column\", \"row_id\"), (\"heading\", \"Row ID\"), (\"sql_table\", \"History\"),\n        (\"column\", \"music_id\"), (\"heading\", \"Music ID\"), (\"sql_table\", \"History\"),\n        (\"column\", \"user\"), (\"heading\", \"User\"), (\"sql_table\", \"History\"),\n        (\"column\", \"type\"), (\"heading\", \"Type\"), (\"sql_table\", \"History\"),\n        (\"column\", \"action\"), (\"heading\", \"Action\"), (\"sql_table\", \"History\"),\n        (\"column\", \"master\"), (\"heading\", \"Master\"), (\"sql_table\", \"History\"),\n        (\"column\", \"detail\"), (\"heading\", \"Detail\"), (\"sql_table\", \"History\"),\n        (\"column\", \"target\"), (\"heading\", \"Target\"), (\"sql_table\", \"History\"),\n        (\"column\", \"size\"), (\"heading\", \"Size\"), (\"sql_table\", \"History\"),\n        (\"column\", \"count\"), (\"heading\", \"Count\"), (\"sql_table\", \"History\"),\n        (\"column\", \"comments\"), (\"heading\", \"Comments\"), (\"sql_table\", \"History\"),\n        (\"column\", \"delete_on\"), (\"heading\", \"Delete On\"), (\"sql_table\", \"calc\"),\n        (\"column\", \"reason\"), (\"heading\", \"Reason\"), (\"sql_table\", \"calc\"),\n\n    \"\"\"\n\n    history_treeview_list = [\n\n      OrderedDict([\n        (\"column\", \"time\"), (\"heading\", \"Time\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"Time\"), (\"select_order\", 0), (\"unselect_order\", 1),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", float),\n        (\"format\", \"{0:,.4f}\"), (\"display_width\", 240),\n        (\"display_min_width\", 120), (\"display_long\", None), (\"stretch\", 0)]),\n\n      OrderedDict([\n        (\"column\", \"music_id\"), (\"heading\", \"Music ID\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"MusicId\"), (\"select_order\", 0), (\"unselect_order\", 2),\n        (\"key\", False), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", None),\n        (\"display_width\", 100), (\"display_min_width\", 80),\n        (\"display_long\", None), (\"stretch\", 0)]),  # 0=NO, 1=YES\n\n      OrderedDict([\n        (\"column\", \"user\"), (\"heading\", \"User\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"User\"), (\"select_order\", 0), (\"unselect_order\", 3),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 150), (\"display_min_width\", 120),\n        (\"display_long\", None), (\"stretch\", 1)]),  # 0=NO, 1=YES\n\n      OrderedDict([\n        (\"column\", \"type\"), (\"heading\", \"Type\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"Type\"), (\"select_order\", 0), (\"unselect_order\", 4),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 80), (\"display_min_width\", 60),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"action\"), (\"heading\", \"Action\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"Action\"), (\"select_order\", 0), (\"unselect_order\", 5),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str),\n        (\"format\", None), (\"display_width\", 80),\n        (\"display_min_width\", 60), (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"master\"), (\"heading\", \"Master\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"SourceMaster\"), (\"select_order\", 0), (\"unselect_order\", 6),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 150), (\"display_min_width\", 100),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"detail\"), (\"heading\", \"Detail\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"SourceDetail\"), (\"select_order\", 0), (\"unselect_order\", 7),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 150), (\"display_min_width\", 100),\n        (\"display_long\", None), (\"stretch\", 1)]),  # 0=NO, 1=YES\n\n      OrderedDict([\n        (\"column\", \"target\"), (\"heading\", \"Target\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"Target\"), (\"select_order\", 0), (\"unselect_order\", 8),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 300), (\"display_min_width\", 200),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"size\"), (\"heading\", \"Size\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"Size\"), (\"select_order\", 0), (\"unselect_order\", 9),\n        (\"key\", False), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", \"{:,}\"),\n        (\"display_width\", 100), (\"display_min_width\", 80),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"count\"), (\"heading\", \"Count\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"Count\"), (\"select_order\", 0), (\"unselect_order\", 10),\n        (\"key\", False), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", \"{:,}\"),\n        (\"display_width\", 80), (\"display_min_width\", 60),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"comments\"), (\"heading\", \"Comments\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"Comments\"), (\"select_order\", 0), (\"unselect_order\", 11),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 160), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"seconds\"), (\"heading\", \"Seconds\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"Seconds\"), (\"select_order\", 0), (\"unselect_order\", 12),\n        (\"key\", False), (\"anchor\", \"e\"), (\"instance\", float), (\"format\", \"{0:,.4f}\"),\n        (\"display_width\", 140), (\"display_min_width\", 80),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"row_id\"), (\"heading\", \"Row ID\"), (\"sql_table\", \"History\"),\n        (\"var_name\", \"Id\"), (\"select_order\", 0), (\"unselect_order\", 13),\n        (\"key\", True), (\"anchor\", \"e\"), (\"instance\", int), (\"format\", None),\n        (\"display_width\", 140), (\"display_min_width\", 100),\n        (\"display_long\", None), (\"stretch\", 1)]),\n\n      OrderedDict([\n        (\"column\", \"reason\"), (\"heading\", \"Reason\"), (\"sql_table\", \"calc\"),\n        (\"var_name\", \"reason\"), (\"select_order\", 0), (\"unselect_order\", 14),\n        (\"key\", False), (\"anchor\", \"w\"), (\"instance\", str), (\"format\", None),\n        (\"display_width\", 160), (\"display_min_width\", 140),\n        (\"display_long\", None), (\"stretch\", 1)])\n    ]\n\n    ''' Future retention is calculated in order of Monthly, Weekly, Daily. \n        Last year retention is same as Future but Yearly test inserted first.\n        The newest backup will always be classified as Monthly until tomorrow.\n    '''\n\n    return history_treeview_list\n\n\ndef update_history(scraped_dict):\n\n    for i, website in enumerate(scraped_dict):\n        if len(website['link']) > 2 and website['flag'] != 'skip':\n            pass\n            # Check for duplicates\n        else:\n            pass\n\n# if there are existing history records identical to scraped_dict\n# we want to skip adding. We might want to update time though.\n\n\ndef create_webscrape(music_id, website):\n\n    # Read all history records finding the last one for each website\n    # if the flag is 'downloaded' then set dict flag to 'skip'\n    print('remove website parameter:', website)\n    for website in webscape.WEBSITE_LIST:\n        if get_history(music_id, website=website):\n            # update dict\n            pass\n        else:\n            add_dict(xxx)\n\n# After loop go back and assign priority flags 1 to 8. Initial priority list \n#can be redefined by user.\n\n\ndef create_radio_buttons(music_id):\n\n    # if already downloaded set text to grey with date in parentheses.\n    # if no link add (no link) after website name.\n    # can you make a deactivated tkinter radio button? Or simply\n    # don't add unavailable to list.\n\n    # From:\n    # https://stackoverflow.com/questions/49061602/how-to-disable-multiple-radiobuttons-in-python\n\n    print('music_id parameter not used:', music_id)\n    sum_label['text'] += var.get()\n    if sum_label['text'] >= 30:\n        for key in radiobuttons:\n            radiobuttons[key]['state'] = 'disabled'\n\n\ndef get_last_history(music_id, website='all'):\n\n    # match is on website human formatted name not internet\n    #  formatted name. EG \"Genius' not '//genius.com'\n    print('music_id parameter not used:', music_id, website)\n\n    pass\n\n\n# =================================  MISCELLANEOUS  ===========================\n\n\ndef alter_table1(cur, table, *args):\n    \"\"\" Copied from simple_insert(), still needs to be changed. \"\"\"\n    query = 'INSERT INTO '+table+' VALUES (' + '?, ' * (len(args)-1) + '?)'\n    cur.execute(query, args)\n\n\ndef simple_insert(cur, table, *args):\n    query = 'INSERT INTO '+table+' VALUES (' + '?, ' * (len(args)-1) + '?)'\n    cur.execute(query, args)\n\n\ndefault_value = {'TEXT': '\"\"', 'INT': '0', 'FLOAT': '0.0', 'BLOB': '\"\"'}\n\n\ndef insert_blank_line(table_name):\n    \"\"\" Add underscores to insert_blank_line and table_name for pycharm syntax checking.\n        If pragma breaks then remove underscores.\n    \"\"\"\n    cs = con.execute('pragma table_info('+table_name+')').fetchall()  # sqlite column metadata\n    con.execute('insert into '+table_name+' values ('+','.join([default_value[c[2]] for c in cs])+')')\n \n    #insert into Music(id) values (null);         # Simpler method\n    ''' DELETED Q&A: https://stackoverflow.com/questions/66072570/\n    how-to-initialize-all-columns-of-sqlite3-table?no redirect=1#comment116823421_66072570\n  \nInserting zeros and empty strings (what you explicitly asked for)\n\ndefault_value={'TEXT':'\"\"','INT':'0','FLOAT':'0.0','BLOB':'\"\"'}\ndef insert_blank_line(table_name):\n  cs = con.execute('pragma table_info(Music)').fetchall() # sqlite column metadata\n  con.execute('insert into '+table_name+' values ('+','.join([default_value[c[2]] for c in cs])+')')\n\nThen you can call this by using: insert_blank_line('Music')\nInsert nulls and use null-aware selects (alternative solution)\n\nInstead of using this blank line function you could just insert a bunch of nulls\n\ninsert into Music(id) values (null);\n\nAnd then upon selection, you treat the null data, in the following example you'll get no artist when it is null:\n\nselect if null(OsArtistName,'no artist') from Music;\n\nor\n\nselect coalesce(OsArtistName,'no artist') from Music\n\nFinal note\n\nBe aware that sqlite types are not enforced in the same way other databases\nenforce. In fact they are not enforced at all. If you insert a string in a \nsqlite number column or a string to a sqlite number column, the data is \ninserted as requested not as the data column specifies - each unit of data \nhas its own datatype.\n\n    '''\n\n# End of sql.py\n", "443": "import random\nimport time\nimport json\nimport keyboard\n\nwith open(\"config.json\", \"r\") as cfg:\n    settings = json.load(cfg)\n\n\ndef initialize_words(wordlist_path: str, sorting_mode: str) -> [str]:\n    print(f\"{sorting_mode=}\")\n    import random\n    word_list = []\n\n    with open(wordlist_path, \"r\") as f:\n        lines = f.readlines()\n        for line in lines:\n            word_list.append(line.strip())\n\n    if settings[\"scramble_word_list\"]:\n        random.shuffle(word_list)\n\n    if sorting_mode != \"alphabetical\":\n        if sorting_mode == \"increasing\":\n            word_list.sort(key=lambda x: len(x))\n            return word_list\n\n        if sorting_mode == \"decreasing\":\n            word_list.sort(key=lambda x: len(x), reverse=True)\n            return word_list\n\n        if sorting_mode == \"uniqueness\":\n            temp = []\n            for word in word_list:\n                temp.append((word, len(set(word))))\n\n            temp.sort(key=lambda x: x[1], reverse=True)\n\n            word_list = []\n\n            for word_tuple in temp:\n                word_list.append(word_tuple[0])\n            return word_list\n\n    return word_list\n\n\ndef keyboard_write(string: str, wpm: float, accuracy: float) -> None:\n    for char in string:\n        if random.randint(0, 1000) / 1000 > accuracy:\n            keyboard.write(chr(random.randint(97, 122)))\n            time.sleep((random.randint(50, 200) / 50) / (wpm * 5 / 60))\n            keyboard.send(\"backspace\")\n            time.sleep((random.randint(50, 200) / 50) / (wpm * 5 / 60))\n\n        keyboard.write(char)\n        time.sleep((random.randint(50, 200) / 100) / (wpm * 5 / 60))\n    keyboard.send(\"enter\")\n\n\ndef main() -> None:\n    ver = 3.1\n    prog_name = __file__.split('\\\\')[-1]\n    print(f\"Initializing '{prog_name}' version {ver}\\n\")\n\n    words = initialize_words(settings['wordlist_path'], settings[\"wordlist_sorting_mode\"])\n    print(f\"Initiation complete! {len(words)} words loaded into memory from {settings['wordlist_path']}.\\n\\n{'=' * 64}\\n\")\n\n    token = input(\"Input game code or url: \")\n    if len(token) == 4:\n        token = \"https://jklm.fun/\" + token.upper()\n\n    import webscrape_3\n    webscrape_3.connect(token)\n    print(f\"{'=' * 31}\")\n\n    used = set()\n\n    min_word_length = settings[\"min_word_length\"]\n    max_word_length = settings[\"max_word_length\"]\n\n    syllable = \"\"\n    last_syllable = \"\"\n    syllable_out = f\"\\033[92m{syllable}\\033[0m\"\n    word = \"\"\n\n    def find_word(syllable: str, do_print=True) -> str:\n        for word in words:\n            if syllable in word and word not in used and min_word_length < len(word) < max_word_length:\n                to_print = word.split(syllable, maxsplit=1)\n                if do_print:\n                    print(f\"Unused word containing {syllable_out} = {syllable_out.join(to_print)}\")\n                return word\n\n        print(f\"no words containing \\033[92m{syllable}\\033[0m in {settings['wordlist_path']}\")\n        return \"none found\"\n\n    while True:\n        temp = webscrape_3.get_syllable().lower()\n        if temp != last_syllable:\n            syllable = temp\n            syllable_out = f\"\\033[92m{syllable}\\033[0m\"\n            print(f\"\\n\\nsyllable = {syllable_out}\")\n            last_syllable = syllable\n\n            word = find_word(syllable)\n\n        if settings[\"autotype\"] and keyboard.is_pressed(settings[\"autotype_activation_key\"]):\n            used.add(word)\n            keyboard_write(word, settings[\"autotype_wpm\"], settings[\"autotype_accuracy\"])\n\n            word = find_word(syllable)\n\n        time.sleep(1 / settings[\"syllable_poll_freq\"])\n\n\nif __name__ == \"__main__\":\n    main()\n", "444": "#!/usr/bin/env python\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport csv # to open/close/append CSV\nfrom datetime import datetime # to accuractley stamp the CSV file\nimport os # to check if file exists\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\n\n# Webscrape all job ads in Seek.com.au based on a search query.\n\ndef _webscrape_seek():\n\n    SEARCH_TERMS= \"cloud architect\".replace(\" \", \"-\")\n\n    SALARY_BUCKETS = [0, 30000, 40000, 50000, 60000, 70000, 80000, 100000, 120000, 150000, 200000, 999999]\n    SALARY_LOW = SALARY_BUCKETS[9]\n    SALARY_HIGH = SALARY_BUCKETS[11]\n\n    PAGE = 1\n\n    BASE_URL = f\"https://www.seek.com.au/{SEARCH_TERMS}-jobs/in-All-Sydney-NSW?page={PAGE}&salaryrange={SALARY_LOW}-{SALARY_HIGH}&salarytype=annual\"\n    html = requests.get(BASE_URL).text\n    text = BeautifulSoup(html,'html.parser')\n\n    JOB_ADS_TOTAL = str(text.findAll('span', id=\"SearchSummary\")[0].text).split(' ')[0].replace(',', '')\n    JOB_HREFs = text.find_all('a', class_='CbjkqYz')\n    PAGES_TOTAL = int(JOB_ADS_TOTAL) / len(JOB_HREFs)\n\n    print(PAGES_TOTAL, JOB_ADS_TOTAL, len(JOB_HREFs))\n\n\n    # missing a fraction of a page.\n    for PAGE in tqdm(range(0, int(PAGES_TOTAL))):\n\n        JOB_ADS_DF = pd.DataFrame()\n\n        for i in range(0, len(JOB_HREFs)):\n\n            JOB_TITLE = JOB_HREFs[i].text\n            URL = \"https://www.seek.com.au/\" + JOB_HREFs[i]['href']\n            JOB_PAGE = requests.get(URL).text\n            JOB_PAGE_TEXT = BeautifulSoup(JOB_PAGE,'html.parser')\n            JOB_TEXT = str(JOB_PAGE_TEXT.find_all('div', class_='yvsb870 _1v38w810'))\n\n            JOB_TEXT_CLEANER = re.sub('\\W+',' ', JOB_TEXT )\n\n            STOPWORDS = [\"div\", \"class\", \"p\", \"yvsb870\", \"_1v38w810\", \"li\", \"strong\", \"br\", \"ul\"]\n            JOB_TEXT_CLEANER_SPLIT = JOB_TEXT_CLEANER.split()\n            JOB_TEXT_CLEANER_RESULT = [word for word in JOB_TEXT_CLEANER_SPLIT if word.lower() not in STOPWORDS]\n            JOB_TEXT_CLEANED =  ' '.join(JOB_TEXT_CLEANER_RESULT)\n\n            DETAILS = {\n                'JOB_NAME' : [JOB_TITLE],\n                'JOB_TEXT' : [JOB_TEXT_CLEANED],\n                'URL' : [URL],\n                'SALARY_LOW': [SALARY_LOW],\n                'SALARY_HIGH': [SALARY_HIGH]\n            }\n            \n            df = pd.DataFrame(DETAILS)\n            JOB_ADS_DF = pd.concat([JOB_ADS_DF, df])\n\n    print(JOB_ADS_DF.head())\n\n    CSV_NAME = \"SEEK_JOB_ADS\"\n    DATA_DIR = \"/\".join(os.getcwd().split('/')[1:6]) + '/data'\n    JOB_ADS_DF.to_csv(f\"/{DATA_DIR}/{CSV_NAME}_{SEARCH_TERMS}.csv\")\n\n_webscrape_seek()\n\n\n", "445": "# To add a new cell, type '# %%'\n# To add a new markdown cell, type '# %% [markdown]'\n# %%\nimport requests\nimport os\nimport json\nimport sys\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse\n\nimport mysql.connector\nimport re\nimport math\n\n# %%\nsys.path.append('D:\\Development\\Repositories\\pyLightspeed')\nprint(sys.path)\n\nfrom lsretail import api as lsretail\n#from lsecom import api as lsecom\n\n# %%\n\n#establishing the connection\nconn = mysql.connector.connect(user='jamie', password='W!neL0ver', host='127.0.0.1', database='vintagewine')\n#Creating a cursor object using the cursor() method\ncursor = conn.cursor()\n\n\n# %%\nKEY_FILE = \"D:\\Development\\.keys\\lightspeed_keys.json\"\nCODES_FILE = \"vintage_codes.json\"\n\nwith open(KEY_FILE) as f:\n    keys = json.load(f)\n\nstore_data = {\n            'account_id': keys[\"account_id\"],\n            'save_path': 'D:\\\\Development\\\\.keys\\\\'\n            }\n\ncredentials = {\n            'client_id': keys[\"client_id\"],\n            'client_secret': keys[\"client_secret\"],\n            'api_key': keys[\"api_key\"],\n            'api_secret': keys[\"api_secret\"]\n            }\n\n\n# %%\nlsr = lsretail.Connection(store_data, credentials, codes_file = CODES_FILE)\n\n# %%\ndef process_image(url, image_path, filename, item_id, description = \"Image\", ordering = 0):\n    lsr._manage_rate()\n    with open(image_path+filename, \"wb\") as f:\n        f.write(requests.get(url).content)\n    url = lsr.api_url+'Image.json'\n    \n    files = {'image': (filename, open(image_path + filename, 'rb'), 'image/jpeg')}\n    payload = {'data': '{\"description\": \"' + description + '\", \"ordering\": ' + ordering +', \"itemID\": ' + item_id +'}'}\n    r = requests.post(url, files=files, data=payload, headers=lsr.headers)\n    #print(r.text)\n\ndef round_up(n, decimals=0):\n    multiplier = 10 ** decimals\n    return math.ceil(n * multiplier) / multiplier\n\n# %%\npage_link = \"\"\nwhile page_link != \"quit\":\n    print(\"Wine.com URL?\")\n    page_link = input()\n    if page_link ==\"quit\": break\n\n    print(\"Item ID from the LS URL?\")\n    item_id = input()\n    # print(\"System ID?\")\n    # system_id = input()\n    # Lookup system ID so we don't have to type it.\n    item = lsr.get(\"Item\", rid=item_id)\n    system_id = item[\"Item\"][\"systemSku\"]\n\n    image_path = 'D:\\\\Data\\\\CloudStation\\\\Vintage\\\\ECommerce\\\\Graphics\\\\Products\\\\'\n    # this is the url that we've already determined is safe and legal to scrape from.\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'}\n\n\n    page_response = requests.get(page_link, headers=headers, timeout=5)\n    # here, we fetch the content from the url, using the requests library\n    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n    #we use the html parser to parse the url content and store it in a variable.\n    textContent = []\n    # %%\n    # %% [markdown]\n    # # Build and Store Webscrape\n    # I am storing webscraped data in a webscrape table. This isn't an automated scrape, but I still want to put it in there. Also, since this is manually matched\n    # this data can be used in the future to train record matching algos.\n    #\n    if page_content.find(class_='icon icon-screwcap prodAttr_icon prodAttr_icon-screwcap'):\n        closure = 'Screwcap'\n    else:\n        closure = ''\n\n    if page_content.find(class_='icon icon-glass-red prodAttr_icon prodAttr_icon-redWine'):\n        category = 'Red'\n    elif page_content.find(class_='icon icon-glass-white prodAttr_icon prodAttr_icon-whiteWine'):\n        category = 'White'\n    elif page_content.find(class_='icon icon-champagne prodAttr_icon prodAttr_icon-champagne'):\n        category = 'Sparkling'\n    elif page_content.find(class_='icon icon-glass-white prodAttr_icon prodAttr_icon-roseWine'):\n        category = 'Rose'\n    else:\n        category = ''\n\n    try:\n        price = float(re.findall('[0-9.]+', page_content.find(class_='productPrice').text)[0].strip())\n    except AttributeError:\n        price = 0\n    \n    try:\n        msrp = round_up(float(re.findall('[0-9.]+', page_content.find(class_='productPrice_price-regWhole').text)[0].strip()))\n    except AttributeError:\n        msrp = price\n\n    try:\n        vintage = re.findall('[0-9]+', page_content.find(class_='pipName').text)[0].strip()\n    except IndexError:\n        vintage = 0\n\n    # %%\n    insert_webscrape = (\"INSERT INTO webscrapes \"\n                        \"(retail_systemID, is_match, best_match, _web_scraper_order, web_scraper_start_url, scrape_sourceID, sku, product, product_href, title, name, vintage, producer, brand, price, msrp, region, subregion, description, size, alcohol, closure, category, varietal) \"\n                        \"VALUES (%(retail_systemID)s, %(is_match)s, %(best_match)s, %(_web_scraper_order)s, %(web_scraper_start_url)s, %(scrape_sourceID)s, %(sku)s, %(product)s, %(product_href)s, %(title)s, %(name)s, %(vintage)s, %(producer)s, %(brand)s, %(price)s, %(msrp)s, %(region)s, %(subregion)s, %(description)s, %(size)s, %(alcohol)s, %(closure)s, %(category)s, %(varietal)s)\")\n\n    data = {\n        'retail_systemID': system_id,\n        'is_match': 1,\n        'best_match': 1,\n        '_web_scraper_order': system_id,\n        'web_scraper_start_url': page_link,\n        'scrape_sourceID': 7,\n        'sku':  page_content.find(attrs={'name':'productID'})[\"content\"],\n        'product': page_content.find(class_='pipName').text,\n        'product_href': page_link,\n        'title': page_content.find(class_='pipName').text,\n        'name':  re.findall('[A-z ]+', page_content.find(class_='pipName').text)[0].strip(),\n        'vintage': vintage,\n        'producer': page_content.find(class_='pipWinery_headlineLink').text,\n        'brand': page_content.find(class_='pipWinery_headlineLink').text,\n        'price': price,\n        'msrp': msrp,\n        'region': page_content.find(attrs={'name':'productVarietal'})[\"content\"],\n        'subregion':  page_content.find(attrs={'name':'productRegion'})[\"content\"],\n        'description':  page_content.find(class_='viewMoreModule_text').text,\n        'size': page_content.find(class_='prodAlcoholVolume_text').text,\n        'alcohol': page_content.find(class_='prodAlcoholPercent_percent').text,\n        'closure':  closure,\n        'category': category,\n        'varietal': page_content.find(attrs={'name':'productVarietal'})[\"content\"]  \n    }\n\n    cursor.execute(insert_webscrape, data)\n    conn.commit()\n\n    # %% [markdown]\n    # ## Store Reviews and Build Description\n    # \n    # We store reviews in the Reviews table, and use them to create an HTML description to post.\n\n    # %%\n    # Start the description so we can append reviews if any\n    item_content =\"\"\n    item_content = '' + page_content.find(class_='viewMoreModule_text').text\n\n    reviews = page_content.select('div.pipProfessionalReviews_list')\n    # If there are reviews, add a header to the content\n    if reviews:\n        item_content = item_content + 'Tasting Notes'\n    else:\n        item_content = item_content + ''\n\n    for review in reviews:\n        # Writing the HTML is failing, I am sure because I need to escape some things, but it isn't really needed so I am skipping it.\n        insert_review = (\"INSERT INTO wine_reviews \"\n                    \"(source, initials, rating, review_detail, wine_itemID, wine_systemSKU, url) \"\n                    \"VALUES (%(source)s, %(initials)s, %(rating)s, %(review_detail)s, %(wine_itemID)s, %(wine_systemSKU)s, %(url)s)\")\n        data = {\n            'source': review.select('div[class=\"pipProfessionalReviews_authorName\"]')[0].text,\n            'initials': review.select('span[class=\"wineRatings_initials\"]')[0].text,\n            'rating': int(review.select('span[class=\"wineRatings_rating\"]')[0].text),\n            'review_detail': review.select('div[class=\"pipProfessionalReviews_review\"]')[0].text,\n            'wine_itemID': int(item_id),\n            'wine_systemSKU': system_id,\n            'url': page_link\n            \n        }\n        item_content = item_content + \"\" + review.select('span[class=\"wineRatings_rating\"]')[0].text + \" \" + review.select('div[class=\"pipProfessionalReviews_authorName\"]')[0].text + \"\"\n        item_content = item_content + review.select('div[class=\"pipProfessionalReviews_review\"]')[0].text + \"\"\n        cursor.execute(insert_review, data)\n        conn.commit()\n\n\n    \n    # %%\n    # ## Build the data that gets pushed in to Lightspeed\n\n   \n    #item_content = '' + page_content.find(class_='pipWineNotes_copy viewMoreModule').text + 'Tasting Notes'\n    # \n    # If it got a good score, we want to hightlight it\"\n    if reviews and int(review.select('span[class=\"wineRatings_rating\"]')[0].text) >= 90:\n        item_description = reviews[0].select('span[class=\"wineRatings_rating\"]')[0].text + \"pts. \" + page_content.find(class_='viewMoreModule_text').text[:100]+\"...\"\n    else:\n        item_description = page_content.find(class_='viewMoreModule_text').text[:112]+\"...\"\n\n    # ## Write what little we can to Lightspeed\n    # Lightspeed won't let you write directly to Product in eCom for things that are omni, so we have to make sure they are not active for eCom in Retail, then write using the retail api, then activate them for eCom.\n\n    # %%\n#\"publishToEcom\": True,\n    data = {\"publishToEcom\": True,\n            \"Prices\":{\n                \"ItemPrice\":{\n                    \"amount\": msrp,\n                    \"useType\":\"MSRP\",\n                    \"useTypeID\": 2\n                    }\n                },\n            \"ItemECommerce\":{\n                \"shortDescription\":item_description,\n                \"longDescription\": item_content,\n                \"weight\": 48        \n                }\n            }\n    \n    lsr.update(\"Item\", item_id, data)\n\n    # %% [markdown]\n    # # Do the Images\n    # Gets the wine images off the page, catches all the information, stores it, and puts the image in Lightspeed via API\n\n    # %%\n    images = page_content.find_all(class_='pipThumb_image')\n    for image in images:\n        # urlparse will take the url from the image link...\n        a = urlparse(image.get('src'))\n        # Then we split out the filename and the extension\n        (sourcefilename, ext) = os.path.splitext(os.path.basename(a.path))\n        link = 'https://www.wine.com/product/images/'+ os.path.basename(a.path)\n        ordering = \"0\"\n\n        if \"Bottle\" in image.get('alt'):\n            if \"Front\" in image.get('alt'):\n                filename = system_id + '_btl'+ ext\n                ordering = \"1\"\n            elif \"Back\" in image.get('alt'):\n                filename = system_id + '_btlb'+ ext\n                ordering = \"3\"\n            else:\n                filename = system_id + '_btl_'+ sourcefilename + ext\n                ordering = \"5\"\n        elif \"Label\" in image.get('alt'):\n            if \"Front\" in image.get('alt'):\n                filename = system_id + '_lbl'+ ext\n                ordering = \"2\"\n            elif \"Back\" in image.get('alt'):\n                filename = system_id + '_lblb'+ ext\n                ordering = \"4\"\n            else:\n                filename = system_id + '_lbl_'+ sourcefilename + ext\n                ordering = \"6\"\n        else:\n            filename = system_id + sourcefilename + ext\n        process_image(link, image_path, filename, item_id, description = image.get('alt'), ordering = ordering)\n        print(f\"This is {image.get('alt')} from {a.path} saved as {filename}\")\n    \n\n\n\n\n", "446": "# To add a new cell, type '# %%'\n# To add a new markdown cell, type '# %% [markdown]'\n# %%\nimport requests\nimport os\nimport json\nimport sys\nfrom bs4 import BeautifulSoup\nimport urllib.parse\n\nimport mysql.connector\nimport re\nimport math\n\nfrom selenium import webdriver\n\n# %%\nsys.path.append('D:\\Development\\Repositories\\WineFoundry\\pyLightspeed')\nprint(sys.path)\n\nfrom lsretail import api_dev as lsretail\n#from lsecom import api as lsecom\n\n# %%https://www.wine.com/product/chateau-vitallis-pouilly-fuisse-2018/540407\n\n#establishing the connection\nconn = mysql.connector.connect(user='jamie', password='W!neL0ver', host='127.0.0.1', database='vintagewine')\n#Creating a cursor object using the cursor() method\ncursor = conn.cursor()\n\n#And set up the webdriver\ndriver = webdriver.Chrome('D:\\\\Development\\\\chromedriver.exe')\n# %%\nstore_datafile = 'D:\\\\Development\\\\.keys\\\\vintage_keys.json'\nlsr = lsretail.Connection(store_datafile)\n\n# %%\ndef process_image(url, image_path, filename, item_id, description = \"Image\", ordering = 0):\n    lsr._manage_rate()\n    with open(image_path+filename, \"wb\") as f:\n        f.write(requests.get(url).content)\n    url = lsr.api_url+'Image.json'\n    \n    files = {'image': (filename, open(image_path + filename, 'rb'), 'image/jpeg')}\n    payload = {'data': '{\"description\": \"' + description + '\", \"ordering\": ' + ordering +', \"itemID\": ' + item_id +'}'}\n    r = requests.post(url, files=files, data=payload, headers=lsr.headers)\n    #print(r.text)\n\ndef round_up(n, decimals=0):\n    multiplier = 10 ** decimals\n    return math.ceil(n * multiplier) / multiplier\n\n# %%\nitem_id=\"\"\nwhile item_id !=\"quit\":\n    item_id = input('Item ID from Lightspeed? : ')\n    if item_id ==\"quit\": break\n\n    item = lsr.get(\"Item\", rid=item_id)\n    system_id = item[\"Item\"][\"systemSku\"]\n\n    driver.get(f\"http://www.wine.com/search/{urllib.parse.quote(item['Item']['description'])}/0?showOutOfStock=true\")\n\n    if input(\"Have you found the right page? : \") == 'y':\n        page_link = driver.current_url\n    else:\n        continue\n \n\n    image_path = 'D:\\\\Data\\\\CloudStation\\\\Vintage\\\\ECommerce\\\\Graphics\\\\Products\\\\'\n    # this is the url that we've already determined is safe and legal to scrape from.\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'}\n\n\n    page_response = requests.get(page_link, headers=headers, timeout=5)\n    # here, we fetch the content from the url, using the requests library\n    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n    #we use the html parser to parse the url content and store it in a variable.\n    textContent = []\n    # %%\n    # %% [markdown]\n    # # Build and Store Webscrape\n    # I am storing webscraped data in a webscrape table. This isn't an automated scrape, but I still want to put it in there. Also, since this is manually matched\n    # this data can be used in the future to train record matching algos.\n    #\n    if page_content.find(class_='icon icon-screwcap prodAttr_icon prodAttr_icon-screwcap'):\n        closure = 'Screwcap'\n    else:\n        closure = ''\n\n    if page_content.find(class_='icon icon-glass-red prodAttr_icon prodAttr_icon-redWine'):\n        category = 'Red'\n    elif page_content.find(class_='icon icon-glass-white prodAttr_icon prodAttr_icon-whiteWine'):\n        category = 'White'\n    elif page_content.find(class_='icon icon-champagne prodAttr_icon prodAttr_icon-champagne'):\n        category = 'Sparkling'\n    elif page_content.find(class_='icon icon-glass-white prodAttr_icon prodAttr_icon-roseWine'):\n        category = 'Rose'\n    else:\n        category = ''\n\n    try:\n        price = float(re.findall('[0-9.]+', page_content.find(class_='productPrice').text)[0].strip())\n    except AttributeError:\n        price = 0\n    \n    try:\n        msrp = round_up(float(re.findall('[0-9.]+', page_content.find(class_='productPrice_price-regWhole').text)[0].strip()))\n    except AttributeError:\n        msrp = price\n\n    try:\n        vintage = re.findall('[0-9]+', page_content.find(class_='pipName').text)[0].strip()\n    except IndexError:\n        vintage = 0\n\n    # %%\n    insert_webscrape = (\"INSERT INTO webscrapes \"\n                        \"(retail_systemID, is_match, best_match, _web_scraper_order, web_scraper_start_url, scrape_sourceID, sku, product, product_href, title, name, vintage, producer, brand, price, msrp, region, subregion, description, size, alcohol, closure, category, varietal) \"\n                        \"VALUES (%(retail_systemID)s, %(is_match)s, %(best_match)s, %(_web_scraper_order)s, %(web_scraper_start_url)s, %(scrape_sourceID)s, %(sku)s, %(product)s, %(product_href)s, %(title)s, %(name)s, %(vintage)s, %(producer)s, %(brand)s, %(price)s, %(msrp)s, %(region)s, %(subregion)s, %(description)s, %(size)s, %(alcohol)s, %(closure)s, %(category)s, %(varietal)s)\")\n\n    data = {\n        'retail_systemID': system_id,\n        'is_match': 1,\n        'best_match': 1,\n        '_web_scraper_order': system_id,\n        'web_scraper_start_url': page_link,\n        'scrape_sourceID': 7,\n        'sku':  page_content.find(attrs={'name':'productID'})[\"content\"],\n        'product': page_content.find(class_='pipName').text,\n        'product_href': page_link,\n        'title': page_content.find(class_='pipName').text,\n        'name':  re.findall('[A-z ]+', page_content.find(class_='pipName').text)[0].strip(),\n        'vintage': vintage,\n        'producer': page_content.find(class_='pipWinery_headlineLink').text,\n        'brand': page_content.find(class_='pipWinery_headlineLink').text,\n        'price': price,\n        'msrp': msrp,\n        'region': page_content.find(attrs={'name':'productVarietal'})[\"content\"],\n        'subregion':  page_content.find(attrs={'name':'productRegion'})[\"content\"],\n        'description':  page_content.find(class_='viewMoreModule_text').text,\n        'size': page_content.find(class_='prodAlcoholVolume_text').text,\n        'alcohol': page_content.find(class_='prodAlcoholPercent_percent').text,\n        'closure':  closure,\n        'category': category,\n        'varietal': page_content.find(attrs={'name':'productVarietal'})[\"content\"]  \n    }\n\n    cursor.execute(insert_webscrape, data)\n    conn.commit()\n\n    # %% [markdown]\n    # ## Store Reviews and Build Description\n    # \n    # We store reviews in the Reviews table, and use them to create an HTML description to post.\n\n    # %%\n    # Start the description so we can append reviews if any\n    item_content =\"\"\n    item_content = '' + page_content.find(class_='viewMoreModule_text').text\n\n    reviews = page_content.select('div.pipProfessionalReviews_list')\n    # If there are reviews, add a header to the content\n    if reviews:\n        item_content = item_content + 'Tasting Notes'\n    else:\n        item_content = item_content + ''\n\n    for review in reviews:\n        # Writing the HTML is failing, I am sure because I need to escape some things, but it isn't really needed so I am skipping it.\n        insert_review = (\"INSERT INTO wine_reviews \"\n                    \"(source, initials, rating, review_detail, wine_itemID, wine_systemSKU, url) \"\n                    \"VALUES (%(source)s, %(initials)s, %(rating)s, %(review_detail)s, %(wine_itemID)s, %(wine_systemSKU)s, %(url)s)\")\n        data = {\n            'source': review.select('div[class=\"pipProfessionalReviews_authorName\"]')[0].text,\n            'initials': review.select('span[class=\"wineRatings_initials\"]')[0].text,\n            'rating': int(review.select('span[class=\"wineRatings_rating\"]')[0].text),\n            'review_detail': review.select('div[class=\"pipProfessionalReviews_review\"]')[0].text,\n            'wine_itemID': int(item_id),\n            'wine_systemSKU': system_id,\n            'url': page_link\n            \n        }\n        item_content = item_content + \"\" + review.select('span[class=\"wineRatings_rating\"]')[0].text + \" \" + review.select('div[class=\"pipProfessionalReviews_authorName\"]')[0].text + \"\"\n        item_content = item_content + review.select('div[class=\"pipProfessionalReviews_review\"]')[0].text + \"\"\n        cursor.execute(insert_review, data)\n        conn.commit()\n\n\n    \n    # %%\n    # ## Build the data that gets pushed in to Lightspeed\n\n   \n    #item_content = '' + page_content.find(class_='pipWineNotes_copy viewMoreModule').text + 'Tasting Notes'\n    # \n    # If it got a good score, we want to hightlight it\"\n    if reviews and int(review.select('span[class=\"wineRatings_rating\"]')[0].text) >= 90:\n        item_description = reviews[0].select('span[class=\"wineRatings_rating\"]')[0].text + \"pts. \" + page_content.find(class_='viewMoreModule_text').text[:100]+\"...\"\n    else:\n        item_description = page_content.find(class_='viewMoreModule_text').text[:112]+\"...\"\n\n    # ## Write what little we can to Lightspeed\n    # Lightspeed won't let you write directly to Product in eCom for things that are omni, so we have to make sure they are not active for eCom in Retail, then write using the retail api, then activate them for eCom.\n\n    # %%\n#\"publishToEcom\": True,\n    data = {\"publishToEcom\": True,\n            \"Prices\":{\n                \"ItemPrice\":{\n                    \"amount\": msrp,\n                    \"useType\":\"MSRP\",\n                    \"useTypeID\": 2\n                    }\n                },\n            \"ItemECommerce\":{\n                \"shortDescription\":item_description,\n                \"longDescription\": item_content,\n                \"weight\": 48        \n                }\n            }\n    \n    lsr.update(\"Item\", item_id, data)\n\n    # %% [markdown]\n    # # Do the Images\n    # Gets the wine images off the page, catches all the information, stores it, and puts the image in Lightspeed via API\n\n    # %%\n    images = page_content.find_all(class_='pipThumb_image')\n    for image in images:\n        # urlparse will take the url from the image link...\n        a = urllib.parse.urlparse(image.get('src'))\n        # Then we split out the filename and the extension\n        (sourcefilename, ext) = os.path.splitext(os.path.basename(a.path))\n        link = 'https://www.wine.com/product/images/'+ os.path.basename(a.path)\n        ordering = \"0\"\n\n        if \"Bottle\" in image.get('alt'):\n            if \"Front\" in image.get('alt'):\n                filename = system_id + '_btl'+ ext\n                ordering = \"1\"\n            elif \"Back\" in image.get('alt'):\n                filename = system_id + '_btlb'+ ext\n                ordering = \"3\"\n            else:\n                filename = system_id + '_btl_'+ sourcefilename + ext\n                ordering = \"5\"\n        elif \"Label\" in image.get('alt'):\n            if \"Front\" in image.get('alt'):\n                filename = system_id + '_lbl'+ ext\n                ordering = \"2\"\n            elif \"Back\" in image.get('alt'):\n                filename = system_id + '_lblb'+ ext\n                ordering = \"4\"\n            else:\n                filename = system_id + '_lbl_'+ sourcefilename + ext\n                ordering = \"6\"\n        else:\n            filename = system_id + sourcefilename + ext\n        process_image(link, image_path, filename, item_id, description = image.get('alt'), ordering = ordering)\n        print(f\"This is {image.get('alt')} from {a.path} saved as {filename}\")\n   \ndriver.close() \n\n\n", "447": "#import required lib\r\nimport requests \r\nfrom bs4 import BeautifulSoup\r\nimport concurrent.futures\r\nimport time\r\n\r\n#link = input(\"Enter url: \")\r\nlink=\"https://realpython.com/\"\r\n\r\nsitemap_lst=[]\r\ntitle_tag=[]\r\nh1_tag=[]\r\nh2_tag=[]\r\nimg_tag=[]\r\na_tag=[]\r\n\r\n\r\ndef get_sitemap(lnk):\r\n    sitemap_url= lnk+(\"sitemap.xml\")\r\n    sitemap_req= requests.get(sitemap_url)\r\n    soup_1 = BeautifulSoup(sitemap_req.content, 'html.parser')\r\n    #print(soup_1.prettify())\r\n    t1=time.time()\r\n    for link in soup_1.find_all(\"loc\"):\r\n            if link.text not in sitemap_lst:\r\n                sitemap_lst.append(link.text)\r\n                #print(link.text)\r\n\r\n    if len(sitemap_lst)==0 :\r\n        print(\"Sitemap is not available for this web-site.\")\r\n    else:\r\n        #print(\"List of sitemap : \",sitemap_lst)\r\n        print(\"Total sitemaps are \",len(sitemap_lst))\r\n\r\n    t2=time.time()\r\n    print(t2-t1)\r\n\r\nget_sitemap(link)\r\n\r\n\r\nsitemap_lst=sitemap_lst[:25]     #for time saving trim list\r\n\r\n\r\n#webscraping\r\ndef webscrape(sitemap):\r\n\r\n    #print(sitemaps)\r\n    req=requests.get(sitemap)\r\n    soup=BeautifulSoup(req.content,'html.parser')\r\n\r\n    for title in soup.find_all(\"title\"):\r\n        if None in title:\r\n            title_tag.append(None)    \r\n        else:\r\n            title_tag.append(title.text)\r\n\r\n    for h1 in soup.find_all(\"h1\"):\r\n        if None in h1:\r\n            h1_tag.append(None)    \r\n        else:\r\n            h1_tag.append(h1.text)\r\n    \r\n    for h2 in soup.find_all(\"h2\"):\r\n        if None in h2:\r\n            h2_tag.append(None)    \r\n        else:\r\n            h2_tag.append(h2.text)\r\n        \r\n    for img in soup.find_all(\"img\"):\r\n        if img.has_attr('alt'):\r\n            img_tag.append(img['alt'])\r\n\r\n    for a in soup.find_all('a', href=True): \r\n        if None in a: \r\n            a_tag.append(None)\r\n        else:\r\n            a_tag.append(a['href'])\r\n\r\n\r\n    \r\nt5=time.time()\r\nwith concurrent.futures.ThreadPoolExecutor() as executor:\r\n    executor.map(webscrape, sitemap_lst)\r\n\r\nprint(\"Total Title tags are \",len(title_tag))\r\nprint(\"Total h1 tags are \", len(h1_tag))\r\nprint(\"Total h2 tags are \",len(h2_tag))\r\nprint(\"Total img tags are \",len(img_tag))\r\nprint(\"Total a tags are \",len(a_tag))\r\n\r\n\r\n#print(\"List of Title tags: \",title_tag) \r\n#print(\"List of h1 tags:\",h1_tag)\r\n#print(\"List of h2 tags:\",h2_tag)\r\n#print(\"List of img tags:\",img_tag)\r\n#print(\"List of a tag :\", a_tag)\r\nt6=time.time()\r\nprint(\"Total time requiered to scrape site,\",t6-t5)\r\n        \r\n\r\n\r\n\r\n    ", "448": "from bs4 import BeautifulSoup\nimport requests\nimport yaml\n\nwith open(\"config.yml\", 'r') as ymlfile:\n    cfg = yaml.load(ymlfile)\n\nrows = []\nrows.append(['Source', 'Product Name', 'Price'])\nurlList = cfg['url']['urlList']\nproductClass = cfg['url']['productClass']\nprodNameClass = cfg['url']['prodNameClass']\nprodPriceClass = cfg['url']['prodPriceClass']\n\n\ndef getSoup(urlList):\n\t\"\"\"This function creates the soup for scraping\"\"\"\n\n\tresponse = requests.get(urlList)\n\tmy_soup = BeautifulSoup(response.text, \"html.parser\")\n\t#print(my_soup.prettify())\n\treturn my_soup\n\ndef getSoupText(urlList):\n\t\"\"\"Soup text is needed later in the output function, a snapshot of the html we scraped\"\"\"\n\tsoupText = getSoup(urlList).prettify()\t\n\treturn soupText\n\ndef webScrape(urlList):\n\t\"\"\"Scrapes the soup and creates the rows object for csv output\"\"\"\n\t\n\titemArray= getSoup(urlList).find_all('div', attrs={'class' :productClass})\n\titemLen = len(itemArray)\n\tprint(urlList)\n\tprint(\"number of products on first load= \" + str(itemLen))\n\t# print(itemArray[0].getText())\n\tfor a in range (0,itemLen):\n\t\tprodInfo = itemArray[a]\n\t\tprodBrand = prodInfo.find('div' , attrs={'class' : prodNameClass})\n\t\tprodPrice = prodInfo.find('span', attrs={'class' : prodPriceClass})\n\t\tif prodPrice:\n\t\t\tprice = prodPrice.getText().lstrip().rstrip()\n\t\telse:\n\t\t\tprice = \"No Price Information\"\n\n\t\trows.append([urlList,\n\t\t\tprodBrand.getText().lstrip().rstrip(),\n\t\t\tprice\n\n\t\t\t\t])\n\treturn rows\n\n\n# end of function\n\n\nfor i in urlList:\n\twebScrape(i)\n\n\n", "449": "# Import Libraries\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.chrome.options import Options\nfrom production.data import *\nimport sys\n\n# Configure chromedriver\nopt = Options()\n# opt.add_argument(\"--headless\")\nopt.add_argument(\"--disable-gpu\")\nopt.add_argument(\"--no-sandbox\")\ndriver = webdriver.Chrome(\"bin/chromedriver-mac\", chrome_options=opt)\ndriver.get(\"https://coursebook.utdallas.edu\")\n\n\ndef spawn_browser():\n    spawn = webdriver.Chrome(\"bin/chromedriver-win.exe\", chrome_options=opt)\n    return spawn\n\n\n# Scrape coursebook, given the formatted course tag (Ex: cs4337.001.20f)\n\ndef webscrape_single_section(course_tag):\n    driver.get(\n        f\"https://coursebook.utdallas.edu/clips/clip-coursebook.zog?id={course_tag}&action=info\")\n\n    course = set_inject_vars(driver)[\"course\"]\n    course_head = set_inject_vars(driver)[\"course_head\"]\n\n    course_info = scrape_data(course, course_head)\n    course_info = array_to_obj(course_info)\n    return course_info\n\n# Scrape all sections for a course on coursebook\n\n\ndef webscrape_all_sections(course_tag):\n    driver.get(f\"https://coursebook.utdallas.edu/search/{course_tag}\")\n    course_list = driver.find_elements_by_class_name(\"stopbubble\")\n    current_term = driver.find_element_by_class_name(\"directaddress\").text\n\n    course_list = add_elements_to_array(course_list)\n    course_list = course_list\n    current_term = current_term[-3:]\n    list_data = []\n\n    for course in course_list:\n        list_data.append(webscrape_single_section(\n            f\"{course.replace(' ', '').lower()}.{current_term}\"))\n        time.sleep(0.2)\n\n    return list_data\n\n\ndef scrape_prof_data(prof_list):\n    success = []\n    failed = []\n    for prof in prof_list:\n        # prof = prof_list[0]\n        try:\n            driver.get(\n                f\"https://www.utdallas.edu/directory/includes/directories.class.php?dirType=displayname&dirSearch={prof['name']}&dirAffil=faculty&dirDept=All&dirMajor=All&dirSchool=All\")\n            name = driver.find_element_by_class_name(\"fullname\").text\n            details = driver.find_element_by_class_name(\"output\").text\n            # print(name, file=sys.stderr)\n            details = details.split(\"\\n\")\n            details[4] = details[4].replace(\"Office - \", \"\")\n            details[5] = details[5].replace(\"Mailstop - \", \"\")\n\n            success.append({\"name\": name, \"email\": details[0], \"title\": details[1], \"department\": details[2],\n                            \"phone\": details[3], \"office\": details[4], \"mailstop\": details[5]})\n        except:\n            # print(f\"ERROR {prof['name']}\", file=sys.stderr)\n            failed.append(prof['name'])\n    return {\n        'success': success,\n        'failed': failed\n    }\n\n\ndef set_inject_vars(driver):\n    return {\"course\": driver.find_elements_by_class_name(\n            \"courseinfo__overviewtable__td\"),\n            \"course_head\": driver.find_elements_by_class_name(\n            \"courseinfo__overviewtable__th\")}\n", "450": "# Import Libraries\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.chrome.options import Options\nfrom .data import *\nimport os\n\n# Configure chromedriver\nopt = Options()\nopt.binary_location = os.environ.get(\"GOOGLE_CHROME_BIN\")\nopt.add_argument(\"--headless\")\nopt.add_argument(\"--disable-gpu\")\nopt.add_argument(\"--no-sandbox\")\ndriver = webdriver.Chrome(\n    executable_path=os.environ.get(\"CHROMEDRIVER_PATH\"), chrome_options=opt)\ndriver.get(\"https://coursebook.utdallas.edu\")\n\n# Scrape coursebook, given the formatted course tag (Ex: cs4337.001.20f)\n\n\ndef webscrape_single_section(course_tag):\n    driver.get(\n        f\"https://coursebook.utdallas.edu/clips/clip-cb11.zog?id={course_tag}&action=info\")\n\n    course = set_inject_vars(driver)[\"course\"]\n    course_head = set_inject_vars(driver)[\"course_head\"]\n\n    course_info = scrape_data(course, course_head)\n    course_info = array_to_obj(course_info)\n    return course_info\n\n# Scrape all sections for a course on coursebook\n\n\ndef webscrape_all_sections(course_tag):\n    driver.get(f\"https://coursebook.utdallas.edu/search/{course_tag}\")\n    course_list = driver.find_elements_by_class_name(\"stopbubble\")\n    current_term = driver.find_element_by_class_name(\"directaddress\").text\n\n    course_list = add_elements_to_array(course_list)\n    course_list = course_list\n    current_term = current_term[-3:]\n    list_data = []\n\n    for course in course_list:\n        list_data.append(webscrape_single_section(\n            f\"{course.replace(' ', '').lower()}.{current_term}\"))\n        time.sleep(0.2)\n\n    return list_data\n\n\ndef scrape_prof_data(prof_list):\n    success = []\n    failed = []\n    for prof in prof_list:\n        # prof = prof_list[0]\n        try:\n            driver.get(\n                f\"https://www.utdallas.edu/directory/includes/directories.class.php?dirType=displayname&dirSearch={prof['name']}&dirAffil=faculty&dirDept=All&dirMajor=All&dirSchool=All\")\n            name = driver.find_element_by_class_name(\"fullname\").text\n            details = driver.find_element_by_class_name(\"output\").text\n            # print(name, file=sys.stderr)\n            details = details.split(\"\\n\")\n            details[4] = details[4].replace(\"Office - \", \"\")\n            details[5] = details[5].replace(\"Mailstop - \", \"\")\n\n            success.append({\"name\": name, \"email\": details[0], \"title\": details[1], \"department\": details[2],\n                            \"phone\": details[3], \"office\": details[4], \"mailstop\": details[5]})\n        except:\n            # print(f\"ERROR {prof['name']}\", file=sys.stderr)\n            failed.append(prof['name'])\n    return {\n        'success': success,\n        'failed': failed\n    }\n\n\ndef set_inject_vars(driver):\n    return {\"course\": driver.find_elements_by_class_name(\n            \"courseinfo__overviewtable__td\"),\n            \"course_head\": driver.find_elements_by_class_name(\n            \"courseinfo__overviewtable__th\")}\n", "451": "import asyncio\nimport re\nfrom contextlib import closing\n\nimport click\nfrom bs4 import BeautifulSoup, Comment, Tag\nfrom requests import get\nfrom requests.exceptions import RequestException\n\nMENU_CHECK = ['sidebar', 'menu', 'dropdown', 'mail', 'social']\n\n@click.command()\n@click.option('-o', '--output-file', help='Name and location of the file')\n@click.argument('urls')\ndef scrape(urls, output_file=''):\n    webscrape(urls, output_file=output_file)\n\ndef webscrape(*urls, output_file=''):\n    \"\"\"\n    Gets all the text from a webpage.\n\n    Parameters\n    ----------\n    urls : str\n        Urls to scrape\n    output_file : str, optional\n        output file name/location, by default cwd\n    \"\"\"\n\n    for url in urls:\n        raw_html = _simple_get(url)\n\n        try:\n            soup = BeautifulSoup(raw_html, 'html.parser')\n            soup = soup.body\n\n            # Delete any comments\n            for comments in soup.findAll(text=lambda text: isinstance(text, Comment)):\n                comments.decompose()\n\n            # kill all script and style elements\n            for script in soup([\"header\", \"footer\", \"script\", \"style\", \"code\", \"form\"]):\n                script.decompose()    # rip it out\n\n            # Remove any menus from the html\n            for div in soup.find_all('div'):\n                if isinstance(div, Tag):\n                    if div.attrs:\n                        if 'class' in div.attrs:\n                            for menu_item in MENU_CHECK:                        \n                                if menu_item in \" \".join(div.attrs['class']):\n                                    div.decompose()\n                                    break\n            \n            # Clean up text from raw html a little\n            cleaned_content = list(map(lambda x: re.sub('\\s+', ' ', x).strip(), soup.find_all(text=True)))\n            \n            return (\" \".join(filter(lambda x: x != '', cleaned_content))).strip()\n\n        except:\n            return \"\"\n\n\ndef _simple_get(url):\n    \"\"\"\n    Attempts to get the content at `url` by making an HTTP GET request.\n    If the content-type of response is some kind of HTML/XML, return the\n    text content, otherwise return None.\n    \"\"\"\n    try:\n        with closing(get(url)) as resp:\n            if _is_good_response(resp):\n                return resp.content \n            else:\n                print(f\"Request failed for {url}\")\n                return None \n\n    except RequestException as e:\n        _log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n        return None\n\n\ndef _is_good_response(resp):\n    \"\"\"\n    Returns True if the response seems to be HTML, False otherwise.\n    \"\"\"\n    content_type = resp.headers['Content-Type'].lower()\n    return (resp.status_code == 200 \n            and content_type is not None \n            and content_type.find('html') > -1)\n\n\ndef _log_error(e):\n    \"\"\"\n    It is always a good idea to log errors. \n    This function just prints them, but you can\n    make it do anything.\n    \"\"\"\n    print(e)\n", "452": "import requests\nimport sqlite3\nfrom bs4 import BeautifulSoup\nimport pandas as pd \nimport plotly.express as go\n\nconn = sqlite3.connect('travel_table.sqlite')\ncur = conn.cursor()\n\ncreate_table = '''\n    CREATE TABLE IF NOT EXISTS \"Rate_Table1\" (\n        \"#\"                 INTEGER PRIMARY KEY AUTOINCREMENT UNIQUE,\n        \"Country\"           TEXT NOT NULL,\n        \"New Cases\"         INTEGER NOT NULL,\n        \"New Deaths\"        INTEGER NOT NULL,\n        \"1 Case/X Ppl\"      INTEGER NOT NULL,\n        \"1 Death/X Ppl\"     INTEGER NOT NULL\n    )\n'''\ncur.execute(create_table)\n\ncountries_list = [\n    'usa', 'india', 'brazil', 'russia', 'france', 'uk', 'italy', 'turkey', 'spain', \n    'argentina', 'colombia', 'germany', 'mexico', 'poland', 'iran', 'peru', 'ukraine', \n    'south africa', 'indonesia', 'netherlands', 'belgium', 'czechia', 'iraq', 'chile', \n    'romania', 'bangladesh', 'canada', 'philippines', 'pakistan', 'morocco', 'switzerland', \n    'saudi arabia', 'israel', 'portugal', 'austria', 'sweden', 'hungary', 'serbia', 'jordan', \n    'nepal', 'ecuador', 'panama', 'georgia', 'uae', 'bulgaria', 'japan', 'croatia', 'azerbaijan', \n    'belarus', 'dominican republic', 'costa rica', 'armenia', 'bolivia', 'lebanon', 'kuwait', \n    'kazakhstan', 'qatar', 'slovakia', 'guatemala', 'moldova', 'oman', 'greece', 'egypt', 'ethiopia', \n    'honduras', 'tunisia', 'denmark', 'palestine', 'myanmar', 'venezuela', 'bosnia and herzegovina', \n    'slovenia', 'paraguay', 'lithuania', 'algeria', 'kenya', 'libya', 'bahrain', 'malaysia', 'kyrgyzstan', \n    'ireland', 'uzbekistan', 'north macedonia', 'nigeria', 'singapore', 'ghana', 'afghanistan', 'albania', \n    's. korea', 'luxembourg', 'montenegro', 'el salvador', 'norway', 'sri lanka', 'finland', 'australia', \n    'uganda', 'latvia', 'cameroon', 'ivory coast', 'sudan', 'zambia', 'estonia', 'madagascar', 'senegal', \n    'mozambique', 'namibia', 'angola', 'french polynesia', 'cyprus', 'drc', 'guinea', 'maldives', 'tajikistan', \n    'botswana', 'french guiana', 'jamaica', 'cabo verde', 'zimbabwe', 'malta', 'mauritania', 'uruguay', 'haiti', \n    'cuba', 'gabon', 'belize', 'syria', 'guadeloupe', 'r\u00e9union', 'bahamas', 'hong kong', 'andorra', \n    'trinidad and tobago', 'eswatini', 'rwanda', 'malawi', 'congo', 'guyana', 'nicaragua', 'mali', 'djibouti', \n    'martinique', 'iceland', 'mayotte', 'suriname', 'equatorial guinea', 'aruba', 'car', 'somalia', 'thailand', \n    'burkina faso', 'gambia', 'cura\u00e7ao', 'togo', 'south sudan', 'benin', 'guinea-bissau', 'sierra leone', 'niger', \n    'lesotho', 'new zealand', 'yemen', 'channel islands', 'san marino', 'chad', 'liberia', 'liechtenstein', \n    'vietnam', 'sint maarten', 'gibraltar', 'sao tome and principe', 'mongolia', 'saint martin', 'turks and caicos', \n    'taiwan', 'burundi', 'papua new guinea', 'diamond princess', 'eritrea', 'monaco', 'comoros', 'faeroe islands', \n    'mauritius', 'tanzania', 'bhutan', 'bermuda', 'isle of man', 'cambodia', 'cayman islands', 'barbados', \n    'saint lucia', 'seychelles', 'caribbean netherlands', 'st. barth', 'brunei', 'antigua and barbuda', \n    'st. vincent grenadines', 'dominica', 'british virgin islands', 'grenada', 'macao', 'fiji', 'laos', 'new caledonia', \n    'timor-leste', 'vatican city', 'saint kitts and nevis', 'greenland', 'falkland islands', 'solomon islands', \n    'saint pierre miquelon', 'montserrat', 'western sahara', 'anguilla', 'ms zaandam', 'marshall islands', \n    'wallis and futuna', 'samoa', 'vanuatu', 'china']\n\ndef rate_webscrape():\n    '''\n    Below is the code for web scraping country-specific covid rate data,\n    which is then parsed, formatted, fit to a table, and extracted to \n    a file named 'covid_rates_table'.\n    '''\n    rate_url = 'https://www.worldometers.info/coronavirus/?utm_campaign=homeAdvegas1?\"%20%5CI%20\"countries\"'\n    rate_response = requests.get(rate_url)\n    rate_soup = BeautifulSoup(rate_response.text, 'html.parser')\n\n    table = rate_soup.find(\"table\", attrs={\"id\": \"main_table_countries_today\"})\n\n    columns = table.find_all(\"th\")\n    #for c in rate_soup.find_all('br'):\n    #    c.replace_with(' ')\n\n    column_names = []\n    for c in columns:\n        column_names.append(c.get_text())\n    #print(column_names)\n\n    rows = table.find(\"tbody\").find_all(\"tr\")\n\n    l = []\n    for tr in rows:\n        td = tr.find_all('td')\n        row = [tr.text for tr in td]\n        l.append(row)\n\n    df = pd.DataFrame(l, columns=column_names)\n   \n    for i in range(0, len(df)):  \n        insert_row = '''\n            INSERT INTO Rate_Table1\n            VALUES (NULL, ?, ?, ?, ?, ?)\n            '''\n        values_list = [\n            df.iloc[i]['Country,Other'], df.iloc[i]['NewCases'], df.iloc[i]['NewDeaths'], \n            df.iloc[i]['1 Caseevery X ppl'], df.iloc[i]['1 Deathevery X ppl']\n            ]\n        cur.execute(insert_row, values_list)\n    conn.commit()\n\ndef db_processing_and_graphs(input_response):\n    db_query = \"\"\"\n    SELECT *\n    FROM Rate_Table1\n    WHERE COUNTRY = '{country}'\n    \"\"\"\n    \n    if input_response in ['uk', 'usa']:\n        country = input_response.upper()\n    else:\n        country = input_response.capitalize()\n\n    cur.execute(db_query.format(country=country))\n    result = cur.fetchall()\n    answer = result[0]\n \n    print(\"Worldwide country rank, by total number of cases: \" + str(answer[0]))\n    print(\"New cases today: \" + str(answer[2]))\n    print(\"New deaths today: \" + str(answer[3]))\n    print(\"New case per \" + str(answer[4]) + \" number of people\")\n    print(\"New death per \" + str(answer[5]) + \" number of people\")\n    \n    \n    fig = go.bar(x=[\"New Cases\", \"New Deaths\"], y=[answer[2], answer[3]], labels=dict(x=\"Metric\", y=\"Incidence today\"))\n    fig.show()\n\ndef travel_webscrape(input_response):\n    '''\n    Below is the code for web scraping country-specific travel \n    restrictions (in the form of Levels 1-4) put in place due to COVID spread.\n    '''\n    base_travel_url = 'https://wwwnc.cdc.gov/travel/notices/covid-4/coronavirus-'\n    \n    if input_response in (\"uk\", \"UK\"):\n        input_response = \"united-kingdom\"\n    country_travel_url = input_response\n    travel_url = base_travel_url + country_travel_url\n    travel_response = requests.get(travel_url)\n    travel_soup = BeautifulSoup(travel_response.text, 'html.parser')\n    travel_level = travel_soup.find(\"div\", class_=\"notice-typename-covid-4 p-2\").get_text()\n\n    print(\"For your selected country, the CDC has designated it: \" + travel_level + \".\")\n\ndef interactive_prompt():\n    input_response = ''\n\n    while input_response != 'exit':\n        print(\"Welcome to the COVID travel safety check program, where you can compare the CDC recommended travel guideline with currend covid rate information!\")\n        print(\"Enter a country! Is it safe for you to travel there?\")\n        print(\"Note: For a list of countries in their proper search format, type 'countries'. For help, type in 'help'.\")\n        input_response = input('Choose a country: ')\n\n        if input_response == 'help':\n            print(\"Type in the name of any country. Note your spelling. Case sensitivity does not apply.\")\n            continue\n        elif input_response == 'countries':\n            print(\"Enter one of the following countries, spelled the same way: \") \n            print(countries_list)\n        elif input_response in ('us', 'US', 'usa', 'USA'):\n            print(\"Sorry, we do not currently have data on the United States. Try again with another country.\")\n        elif input_response.lower() in countries_list:\n            rate_webscrape()\n            travel_webscrape(input_response)\n            db_processing_and_graphs(input_response)\n            print(\"With this new information in mind, we urge you to consider the risk of contracting or spreading COVID-19 should you decide to travel. Stay safe!\")\n        else:\n            print(\"Invalid input. Please check your spelling and try again.\")\n    \n    else:\n        conn.close()\n        quit()\n\nif __name__==\"__main__\":\n    interactive_prompt()", "453": "from selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\nimport requests\nstart_url = \"https://exoplanets.nasa.gov/discovery/exoplanet-catalog/\"\nbrowser = webdriver.Chrome(\"/Users/tejasoberoi/Downloads/chromedriver\")\nbrowser.get(start_url)\ntime.sleep(1)\n\nheaders = [\"name\",\"light years from earth\", \"planet mass\", \"stellar magnitude\", \"discovery date\", \"hyperlinks\", \"planet_type\", \"planet_mass\", \"planet_radius\", \"orbital_radius\", \"orbital_period\", \"eccentricity\"]\nplanetData = []\ndef webScrape():\n    for i in range(202):\n        bs = BeautifulSoup(browser.page_source,\"html.parser\")\n        for ultag in bs.find_all(\"ul\",attrs = {\"class\",\"exoplanet\"}):\n            litags = ultag.find_all(\"li\")\n            temp_list = []\n            for index,li in enumerate(litags):\n                if(index == 0):\n                    temp_list.append(li.find_all(\"a\")[0].contents[0])\n\n                else:\n                    try:\n                        temp_list.append(li.contents[0])\n                    except:\n                        temp_list.append(\"\")\n            planetData.append(temp_list)\n            hyperlink_li_tag = litags[0]\n            temp_list.append(\"https://exoplanets.nasa.gov\"+hyperlink_li_tag.find_all(\"a\", href=True)[0][\"href\"])\n        browser.find_element_by_xpath('//*[@id=\"primary_column\"]/footer/div/div/div/nav/span[2]/a').click()\nplanetData2 = []\ndef moreScraping(hyperlink):\n    page = requests.get(hyperlink)\n    soup = BeautifulSoup(page.content, \"html.parser\")\n\n    temp_list = []\n    for trtags in soup.find_all(\"tr\",attrs = {\"class\",\"fact_row\"}):\n        tdTags = trtags.find_all(\"td\")\n        for tdtag in tdTags:\n            try:\n                temp_list.append(tdtag.find_all(\"div\",attrs={\"class\",\"value\"})[0].contents[0])\n            except:\n                temp_list.append(\"\")\n    planetData2.append(temp_list)\n    moreScraping(hyperlink)\nwebScrape()\nfor index,data in enumerate(planetData):\n    moreScraping(data[5])\nfinalData = []\nfor index,data in enumerate(planetData):\n    newData = planetData2[index]\n    newData = [element.replace(\"\\n\",\"\") for element in newData]\n    print(newData)\n    newData = newData[:7]\n    finalData.append(data+newData)\nwith open(\"planets.csv\",\"w\") as f:\n    writer = csv.writer(f)\n    writer.writerow(headers)\n    writer.writerows(finalData)\n\n", "454": "'''\ntitle: GUI for Window\nauthors: Palaash & Shaishav\ndate created: 2020-06-11\n'''\n\nfrom tkinter import *\nfrom GUI import topStocksGUI\nfrom GUI import searchBarGUI\nfrom GUI import graphGUI\nfrom GUI import newsGUI\nfrom GUI import stockinfoGUI\nfrom matplotlib import rcParams\nimport matplotlib.pyplot as plt\nfrom webscraping import *\n\nclass Engine:\n    def __init__(self):\n\n        # The basics of creating a window here... Nothing fancy\n        self.window = Tk()\n        self.window.minsize(1028, 720)\n        self.window.title(\"Stock Trader\")\n        self.window.configure(bg=\"#1c1c1c\")\n        self.window.resizable(False, False) # Keeping it non-resizeable so it doesn't alter how the content will look\n        self.currentTicker = 'SPY' # Default stock shown\n        self.changed = False # To identify if the stock has been changed\n        self.window.iconphoto(False, PhotoImage(file='favicon.png'))\n        #\n\n        # Search\n        self.searchBar = searchBarGUI.SearchBarGUI(self.window,self)\n\n        # Top Stocks\n        self.topStocks = topStocksGUI.TopStocksGUI(self.window,self)\n\n        # Graph -- Much of the graph config has to be done here since the variables have to be accessed in this file.\n\n            # Colors\n\n        rcParams['axes.labelcolor'] = 'white'\n        rcParams['xtick.color'] = 'white'\n        rcParams['axes.titleweight'] = \"bold\"\n        rcParams['ytick.color'] = 'white'\n        rcParams['text.color'] = 'white'\n\n            # Configs\n        self.fig = plt.figure(figsize=(7, 3), dpi=100) # Basically a 700x300 i,age\n        self.fig.patch.set_facecolor(\"#1c1c1c\")\n        self.graph = self.fig.add_subplot(1, 1, 1)\n        self.graph.set_facecolor('#454444')\n\n            # Graph Gui\n        self.graphGUI =  graphGUI.GraphGUI(self.window,self.fig,self.graph)\n\n        # Stock info\n        self.urlFinviz = \"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker) # URL FOR FINVIZ\n        self.urlYahoo = 'https://ca.finance.yahoo.com/quote/{0}'.format(self.currentTicker) # URL FOR YAHOO FINANCE\n\n        self.pageFinviz = webScrapeURL(self.urlFinviz) # webscrape the finviz page\n        self.pageYahoo= webScrapeURL(self.urlYahoo) # webscrape the yahoo page\n\n        # News feed GUI\n        self.newsfeed = newsGUI.NewsfeedGUI(self.window, self.pageFinviz) # create the news feed\n\n        # Stock info GUI\n        self.stockInfo = stockinfoGUI.StockInfo(self.window, self.pageFinviz)\n\n    # Modify Methods #\n\n    def changeTicker(self,ticker): # Self-explanatory\n        self.currentTicker = ticker\n        self.graphGUI.ticker = ticker\n\n    def updateEverything(self): # Re-Init stuff which changes based on stocks\n        self.newsfeed.__init__(self.window,webScrapeURL(\"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker)))\n        self.stockInfo.__init__(self.window,webScrapeURL(\"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker)))\n\n    # Accessor Methods #\n\n    def getWindow(self):\n        return self.window\n    def getGraphClass(self):\n        return self.graphGUI\n    def getTicker(self):\n        return self.currentTicker", "455": "\"\"\"\nDjango settings for PumpWebscrape project.\n\nGenerated by 'django-admin startproject' using Django 4.1.1.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.1/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/4.1/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/4.1/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = \"django-insecure-)4-0c(m7gdbqx=_=gky7u^-wom-pfry(oxenclr*moeya9=ac^\"\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n# Application definition\n\nINSTALLED_APPS = [\n    \"django.contrib.admin\",\n    \"django.contrib.auth\",\n    \"django.contrib.contenttypes\",\n    \"django.contrib.sessions\",\n    \"django.contrib.messages\",\n    \"django.contrib.staticfiles\",\n    'rest_framework',\n    \"corporation\"\n]\n\nMIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n]\n\nROOT_URLCONF = \"PumpWebscrape.urls\"\n\nTEMPLATES = [\n    {\n        \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n        \"DIRS\": [BASE_DIR / 'templates']\n        ,\n        \"APP_DIRS\": True,\n        \"OPTIONS\": {\n            \"context_processors\": [\n                \"django.template.context_processors.debug\",\n                \"django.template.context_processors.request\",\n                \"django.contrib.auth.context_processors.auth\",\n                \"django.contrib.messages.context_processors.messages\",\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \"PumpWebscrape.wsgi.application\"\n\n# Database\n# https://docs.djangoproject.com/en/4.1/ref/settings/#databases\n\n#  Could change this to mysql if needed\nDATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.sqlite3\",\n        \"NAME\": BASE_DIR / \"db.sqlite3\",\n    }\n}\n\n# Password validation\n# https://docs.djangoproject.com/en/4.1/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.UserAttributeSimilarityValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.MinimumLengthValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.CommonPasswordValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.NumericPasswordValidator\",\n    },\n]\n\n# Internationalization\n# https://docs.djangoproject.com/en/4.1/topics/i18n/\n\nLANGUAGE_CODE = \"en-us\"\n\nTIME_ZONE = \"UTC\"\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/4.1/howto/static-files/\n\nSTATIC_URL = \"static/\"\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/4.1/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = \"django.db.models.BigAutoField\"\n\nREST_FRAMEWORK = {\n    #  default pagination\n    'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.LimitOffsetPagination',\n    'PAGE_SIZE': 50,\n}\n", "456": "from ui import *\r\nfrom webscr_maps import *\r\nimport sys\r\n\r\n# GUI Application \r\nclass WebScr_Project(Ui_MainWindow):\r\n    \r\n    def __init__(self,window):\r\n        self.setupUi(window)\r\n        self.pushButton.clicked.connect(self.click)\r\n        \r\n    def click(self):\r\n        if self.lineEdit_2.text() == '':\r\n            address = self.lineEdit.text()\r\n        else:  address = self.lineEdit_2.text() + \", \" + self.lineEdit.text() \r\n        \r\n        res, rating, dir = WebScrape.pickFood(\r\n            self.lineEdit_2.text() + \" \" + self.lineEdit.text() ,WebScrape.driver)\r\n        self.setup_popup(address, res, rating, dir)\r\n    \r\n    \r\napp = QtWidgets.QApplication(sys.argv)\r\nMainWindow = QtWidgets.QMainWindow()\r\n\r\nui = WebScr_Project(MainWindow)\r\n\r\nMainWindow.show()\r\napp.exec_()\r\n", "457": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Tue Sep  7 17:07:54 2021\r\n\r\n@author: Kiran B\r\n\"\"\"\r\n\r\nfrom bs4 import BeautifulSoup #converts the contents of a page into a proper format\r\nimport requests #used to get the content from a web page\r\nimport spacy\r\n\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n\r\n#Defining Functions for Webscraping and Text Visualization\r\n\r\ndef Webscrape(URL, div_id):\r\n    '''This function scrapes the website from the URL given to it.\\\r\n    It collects the entire website data and stores the data in the html format '''\r\n    \r\n    HEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})\r\n    \r\n    # Making the HTTP Request\r\n    webpage = requests.get(URL, headers=HEADERS)\r\n  \r\n    # Creating the Soup Object containing all data\r\n    soup = BeautifulSoup(webpage.content, \"html.parser\")\r\n\r\n    results = soup.find(id=div_id)\r\n\r\n    print(results.get_text())\r\n    \r\n    return results.get_text()\r\n\r\n\r\ndef Macro_Visualize(data):\r\n    '''Visualize the entire extracted data using Parse Trees'''\r\n    \r\n    spacy.displacy.serve(data, style=\"dep\")\r\n    \r\n\r\ndef Micro_Visualize(data):\r\n    '''Visualize the extracted data sentence by sentence using Parse Trees'''\r\n    \r\n    sentence_spans = list(data.sents)\r\n    spacy.displacy.serve(sentence_spans, style=\"dep\")\r\n\r\n\r\ndef POS_Tag(data):\r\n    '''Tag Parts of Speech to the Extracted data and visualize'''\r\n    \r\n    spacy.displacy.serve(data, style='ent')\r\n    \r\n    \r\n#Obtaining the email category input from the user\r\n\r\ndef Collect_input():\r\n    print(\"    1. Sick Leave Email Template\\n\\\r\n    2. Vacation Leave Email Template\\n\\\r\n    3. Birthday Wishes Email Template\\n\\\r\n    4. Marketing Reply Email Template\\n\\\r\n    5. Out of Town Email Template\\n\\\r\n    6. Interview Application Email Template\\n\\n\")\r\n\r\n    val = input(\"Enter your desrired category(1-6) for the Email Template:\\n>>> \")\r\n    return val\r\n\r\n\r\nval = int(Collect_input())\r\n    \r\nwhile val <= 0 or val >= 7:\r\n    print(\"That is not a valid input. Numbers between 1 and 6 only please!\\n\\n\")\r\n    val = int(Collect_input())\r\n        \r\nif val == 1:\r\n    print(\"\\nYou have chosen to generate a Sick Leave Email Template:\\n\\n\")\r\n    URL = \"https://www.thebalancecareers.com/formal-leave-of-absence-letter-request-example-2060597\"\r\n    \r\n    div_id = \"mntl-sc-block-callout-body_1-0-3\"\r\n    extract = Webscrape(URL, div_id)\r\n            \r\n    nlp = spacy.load('en_core_web_lg')\r\n        \r\n    # Parse the text with spaCy\r\n    spacy_text = nlp(extract)\r\n\r\n    # Print out all the named entities that were detected\r\n    print(\"\\n\\n*Tagged POS entities of the Extracted Text*\")\r\n    for entity in spacy_text.ents:\r\n        print(f\"{entity.text} ({entity.label_})\")\r\n        \r\n    # Parse Trees\r\n    # Macro_Visualize(spacy_text)\r\n        \r\n    # Parts of Speech Tagging\r\n    # POS_Tag(spacy_text)\r\n    \r\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n", "458": "from django.shortcuts import render\r\nfrom .serializer import EventsSerializers\r\n# Create your views here.\r\n\r\nfrom django.http import HttpResponse\r\nfrom .scrape import WebScrape\r\nfrom .models import EventData\r\nfrom .service import DataProcess\r\nfrom rest_framework import viewsets\r\nimport json\r\nfrom .search import Search\r\nfrom .recommend import Recommend\r\nfrom .Nearby import Nearby\r\nfrom django.http import JsonResponse\r\nfrom .UserSuggestion import UserRecommend\r\nimport time\r\n\r\n\r\ndef processwebdata(request):\r\n\r\n    event_list = WebScrape.scrapeweb()\r\n    time.sleep(2)\r\n    eventdatalist = DataProcess.saveeventdata(event_list)\r\n    \r\n    return HttpResponse(eventdatalist)\r\n\r\n\r\ndef search(req,inputstr = 'Dub'):\r\n    \r\n    print(inputstr)\r\n    event_list = Search.searchtry(inputstr)\r\n\r\n    print(len(event_list))\r\n    \r\n  \r\n    events = list(map(lambda x: to_json(x), event_list))\r\n    #print(events)\r\n    return HttpResponse(events)\r\n\r\n\r\ndef recommendations(req,inputstr = 'Book of Kells'):\r\n    print(inputstr)\r\n    recommendations_list = Recommend.eventsrecommendations(inputstr)\r\n\r\n    recomendedEventsList = DataProcess.fetchEventsByTitle(recommendations_list)\r\n\r\n    events = list(map(lambda x: to_json(x), recomendedEventsList))\r\n\r\n    return JsonResponse(events, safe=False)\r\n\r\n\r\ndef nearby(req,inputstr = 'Book of Kells'):\r\n    print(inputstr)\r\n    nearby_list = Nearby.eventsNearBy(inputstr)\r\n\r\n    nearByEventsList = DataProcess.fetchEventsByTitle(nearby_list)\r\n\r\n    events = list(map(lambda x: to_json(x), nearByEventsList))\r\n\r\n    return JsonResponse(events, safe=False)\r\n\r\n\r\ndef usersrecommend(req, inputstr = ''):\r\n    print(inputstr)\r\n    usersrecommend_list = UserRecommend.userRecommendations(inputstr)\r\n   \r\n    #users = list(map(lambda x: users_json(x), usersrecomment_list))\r\n    print(usersrecommend_list)\r\n    return JsonResponse(usersrecommend_list, safe= False)\r\n\r\n\r\ndef deletePastEvents(request):\r\n\r\n    print('begin delete')\r\n\r\n    DataProcess.deletePastEvents()\r\n    print(\"finish delete\")\r\n\r\n    return HttpResponse(\"events deleted successfully\")\r\n\r\n\r\n\r\n\r\ndef to_json(x):\r\n    return {\r\n            'title': x.title,\r\n            'time': x.time,\r\n            'location': x.location,\r\n            'summary': x.summary,\r\n            'img': x.img,\r\n            'startdate': x.startdate,\r\n            'enddate': x.enddate,\r\n            'price': x.price,\r\n            #'address': x.address,\r\n            'read_more': x.read_more,\r\n            'category': x.category,\r\n            'latitude': x.latitude,\r\n            'longitude': x.longitude\r\n        }\r\n\r\n\r\n\r\n\r\n\r\n", "459": "import sys\nimport urlparse\n\nimport lxml.html\nfrom lxml.etree import tostring\nimport requests\nimport re\n\nimport httplib, urllib, urllib2, cookielib\nimport re\nimport base64\nimport os\nimport json\nimport markdown\nimport html2text\nfrom datetime import datetime\nfrom optparse import OptionParser\nfrom ConfigParser import RawConfigParser\nfrom urllib import urlretrieve\nfrom bs4 import BeautifulSoup\nimport urllib2\nimport csv\n#time is needed to pause while fetching the data, as it may affect the performance of server.\nimport time\n\nuser_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'\nheaders = { 'User-Agent' : user_agent}\n\ndef webscrape(urlToScrape): # returns content of the webpage as html object for the given url\n    req = urllib2.Request(urlToScrape, headers={'User-Agent' : \"Mozilla Firefox\"}) \n    response = urllib2.urlopen( req )\n    html = response.read()\n    return html\n\ndef read_file(self):\n    with open(self.file, 'r') as f:\n        data = [row for row in csv.reader(f.read().splitlines())]\n    return data\n\ndef main():\n\n    fname = 'ceolisting_final.csv'\n    reader = csv.reader(open(fname, 'rU'), dialect='excel')\n    fopen = csv.writer(open(\"ceoprofile_13Apr.csv\",'w'))\n\n\n    for ceolink in reader:\n        items = []\n        if ceolink[3] is not '':            \n            print 'Crawling people from :' + ceolink[3] \n            item = []\n            name = ''\n            jobTitle = ''\n            worksFor = ''\n            alumniOf = ''\n            age = ''\n            totalCompensation = ''\n            desc = ''\n            workLocation = ''\n            affiliationsStr = ''\n\n            profile_html = webscrape(ceolink[3])\n            soup = BeautifulSoup(profile_html)\n            keyExecs = soup.find(\"div\", {'itemtype' : 'http://schema.org/Person'})\n            if keyExecs :\n                name = keyExecs.find(\"h1\",{'itemprop':\"name\"}).text.strip().encode('utf8')\n                jobTitle = keyExecs.find(\"span\",{'itemprop':\"jobTitle\"}).text.strip().encode('utf8')\n                worksFor = keyExecs.find(\"a\",{'itemprop':\"worksFor\"}).text.strip().encode('utf8')\n                if keyExecs.find(\"div\",{'itemprop':\"alumniOf\"}):\n                    alumniOf = keyExecs.find(\"div\",{'itemprop':\"alumniOf\"}).text.strip().encode('utf8')\n                largeDetail = keyExecs.findAll(\"td\",{'class':\"largeDetail\"})\n                if largeDetail :\n                    age = largeDetail[0].text.strip().encode('utf8')\n                    totalCompensation = largeDetail[1].text.strip().split('As of')[0].strip('$').replace(',','')\n                description = keyExecs.find(\"p\",{'itemprop':\"description\"})\n                moredescription = keyExecs.find(\"span\",{\"id\":\"hidden\"})\n                if description :\n                    desc = description.text.strip().encode('utf8') + ' '\n                if moredescription :\n                    desc += moredescription.text.strip().encode('utf8')    \n                if keyExecs.find(\"div\",{'itemprop':\"workLocation\"}) :\n                    workLocation = keyExecs.find(\"div\",{'itemprop':\"workLocation\"}).text.strip().encode('utf8')\n                affiliations = keyExecs.findAll(\"a\",{'itemprop':\"affiliation\"})\n                affiliationList = []\n                for affiliation in affiliations :\n                    affiliationList.append(affiliation.text.strip().encode('utf8'))\n                affiliationsStr = \"|\".join(affiliationList)            \n                              \n                item.append(ceolink[1])\n                item.append(name)\n                item.append(jobTitle)\n                item.append(worksFor)\n                item.append(alumniOf)\n                item.append(age)\n                item.append(totalCompensation)\n                item.append(desc)\n                item.append(workLocation)\n                item.append(affiliationsStr)\n                items.append(item)\n                \n                fopen.writerows(items)\n                time.sleep(3)        \n\nif __name__ == '__main__':\n    main()", "460": "from django.shortcuts import render\r\nfrom django.http import HttpResponse\r\nfrom bs4 import BeautifulSoup\r\nimport random\r\nfrom selenium import webdriver\r\n\r\n# Create your views here.\r\n\r\ndef webscrape(request):\r\n    adesc=''\r\n    alink=''\r\n    aprice=''\r\n    arating=''\r\n    aimg=''\r\n    amaz=False\r\n\r\n    fdesc=''\r\n    flink=''\r\n    fprice=''\r\n    frating=''\r\n    fimg=''\r\n    flip=False\r\n\r\n    sdesc=''\r\n    slink=''\r\n    sprice=''\r\n    srating=''\r\n    simg=''\r\n    snap=False\r\n    rate=[4.1,4.2,4.3,4.4,3.8,3.9,4,3.5]\r\n\r\n\r\n    if request.method == 'POST':\r\n        book=request.POST.get('price')\r\n        driver = webdriver.Chrome(executable_path=r\"C:\\Users\\Priyanshi\\Downloads\\chromedriver\")\r\n\r\n    #AMAZON WEB SCRAPE\r\n        atemp='https://www.amazon.in/s?k={}&ref=nb_sb_noss_2'\r\n        book=book.replace(' ','+')\r\n        atemp=atemp.format(book)\r\n        driver.get(atemp)\r\n        soup=BeautifulSoup(driver.page_source,'html.parser')\r\n        results=soup.find_all('div',{'data-component-type':'s-search-result'})\r\n        if len(results)==0:\r\n            amaz=True\r\n        else:\r\n            item=results[0]\r\n            adesc=item.h2.a.text.strip()\r\n            alink=\"https://www.amazon.in\"+item.h2.a.get('href')\r\n            aprice=item.find('span','a-price').find('span','a-offscreen').text\r\n            arating=random.choice(rate)\r\n            aimg=item.img.get('src')\r\n\r\n    #FLIPKART WEB SCRAPE\r\n        ftemp='https://www.flipkart.com/search?q={}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'\r\n        ftemp=ftemp.format(book)\r\n        driver.get(ftemp)\r\n        soup=BeautifulSoup(driver.page_source,'html.parser')\r\n        fresults=soup.find_all('div',{'class':'_13oc-S'})\r\n        if len(fresults)==0:\r\n            flip=True\r\n        else:\r\n            fitem=fresults[0]\r\n            fdesc=fitem.img.get('alt')\r\n            flink=\"https://www.flipkart.com\"+fitem.a.get('href')\r\n            #frating=fitem.find('div',{'class':'_3LWZlK'}).text\r\n            frating=random.choice(rate)\r\n            fprice=fitem.find('div',{'class':'_30jeq3'}).text\r\n            fimg=fitem.img.get('src')\r\n\r\n    #SNAPDEAL WEB SCRAPE\r\n        stemp='https://www.snapdeal.com/search?keyword={}&santizedKeyword=&catId=&categoryId=0&suggested=false&vertical=&noOfResults=20&searchState=&clickSrc=go_header&lastKeyword=&prodCatId=&changeBackToAll=false&foundInAll=false&categoryIdSearched=&cityPageUrl=&categoryUrl=&url=&utmContent=&dealDetail=&sort=rlvncy'\r\n        stemp=stemp.format(book)\r\n        driver.get(stemp)\r\n        soup=BeautifulSoup(driver.page_source,'html.parser')\r\n        sresults=soup.findAll('div',{'class':'product-tuple-listing'})\r\n        if len(sresults)==0:\r\n            snap=True\r\n        else:\r\n            sitem=sresults[0]\r\n            slink=sitem.a.get('href')\r\n            simg=sitem.img.get('src')\r\n            sdesc=sitem.p.get('title')\r\n            sprice=sitem.find('span','product-price').text\r\n            srating=random.choice(rate)\r\n\r\n\r\n    return render(request, 'web_scrape/webscrape.html',{'aimg':aimg,'arating':arating,'aprice':aprice,'adesc':adesc,'alink':alink,'fimg':fimg,'frating':frating,'fprice':fprice,'fdesc':fdesc,'flink':flink,'simg':simg,'srating':srating,'sprice':sprice,'sdesc':sdesc,'slink':slink,'snap':snap,'flip':flip,'amaz':amaz})\r\n", "461": "from django.contrib import admin\n\n# Register your models here.\n\nfrom scrapes_data_app.models import * \n\nclass WebscrapeAdmin(admin.ModelAdmin):\n\n    list_display = ('name','price', 'hour', 'twenty_hours','seven_days','market_cap','volume','circulating_supply')\n\n\nadmin.site.register(Webscrap,WebscrapeAdmin)\n", "462": "from setuptools import find_packages\nfrom setuptools import setup\n\nwith open('requirements.txt') as f:\n    content = f.readlines()\nrequirements = [x.strip() for x in content if 'git+' not in x]\n\nsetup(name='webscrape-metacritic-videogames',\n      version=\"1.0\",\n      description=\"Project Description\",\n      packages=find_packages(),\n      install_requires=requirements,\n      test_suite='tests',\n      # include_package_data: to install data from MANIFEST.in\n      include_package_data=True,\n      scripts=['scripts/webscrape-metacritic-videogames-run'],\n      zip_safe=False)\n", "463": "from urllib.request import urlopen\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport csv\n\nclass WebScrape():\n    def __init__(self):\n        self.cities = []\n        self.dictionary = {}\n    def scrapeCities(self):\n        \n        url = 'https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population'\n        website_url = requests.get(url).text\n        soup = BeautifulSoup(website_url, \"lxml\") \n        \n        My_table = soup.find('table',{'class':'wikitable sortable'})\n        new = My_table.findAll('tr')\n        \n        for i in range(1, len(new)):\n            string = str(new[i])\n            start = string.find(\"title=\")+7\n            end = string.find(\"\\\"\", start, 900)\n            altEnd = string.find(\",\", start, 900)\n            if(end", "464": "\"\"\"\nFor all data pivots in HAWC, get an SVG and PNG for each data pivot:\n\n```bash\nexport \"HAWC_USERNAME=foo@bar.com\"\nexport \"HAWC_PW=foobar\"\n\ncd ~/dev/hawc/hawc\npython ../scripts/scrape_data_pivots.py get-pivot-objects\npython ../scripts/scrape_data_pivots.py webscrape https://hawcproject.org\n```\n\"\"\"\nimport os\nimport sys\nimport time\nfrom pathlib import Path\n\nimport click\nimport django\nimport pandas as pd\nfrom selenium import webdriver\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.webdriver.chrome.options import Options\n\nFN = \"data-pivots.pkl\"\nROOT = str(Path(Path(__file__).parents[0] / \"../hawc\").resolve())\nos.chdir(ROOT)\nsys.path.append(ROOT)\n\n\n@click.group()\ndef cli():\n    pass\n\n\n@cli.command()\ndef get_pivot_objects():\n\n    os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"main.settings.dev\"\n    django.setup()\n\n    from summary.models import DataPivot\n\n    data = []\n    for d in DataPivot.objects.all().order_by(\"assessment_id\"):\n        data.append((d.id, d.get_absolute_url(), False, False, False, False))\n\n    df = pd.DataFrame(data=data, columns=(\"id\", \"url\", \"loaded\", \"error\", \"png\", \"svg\"))\n    df.to_pickle(FN)\n\n\n@cli.command()\n@click.argument(\"base_url\")\ndef webscrape(base_url: str):\n    df = pd.read_pickle(FN)\n\n    max_sleep = 60 * 10  # 10 min\n\n    chrome_options = Options()\n    chrome_options.add_argument(\"--headless\")\n    driver = webdriver.Chrome(options=chrome_options)\n    driver.set_window_size(2000, 1500)\n    driver.implicitly_wait(max_sleep)\n\n    url = f\"{base_url}/user/login/\"\n    driver.get(url)\n\n    driver.find_element_by_id(\"id_username\").clear()\n    driver.find_element_by_id(\"id_username\").send_keys(os.environ[\"HAWC_USERNAME\"])\n    driver.find_element_by_id(\"id_password\").clear()\n    driver.find_element_by_id(\"id_password\").send_keys(os.environ[\"HAWC_PW\"])\n    driver.find_element_by_id(\"submit-id-login\").submit()\n\n    for key, data in df.iterrows():\n        if data.loaded is True:\n            continue\n\n        driver.implicitly_wait(max_sleep)\n        url = f\"{base_url}{data.url}\"\n        print(f\"Trying {key+1} of {df.shape[0]}: {url}\")\n        driver.get(url)\n        el = driver.find_element_by_id(\"dp_display\")\n        loading_div = driver.find_element_by_id(\"loading_div\")\n        while True:\n            if not loading_div.is_displayed():\n                driver.implicitly_wait(10)\n                try:\n                    svg = driver.find_element_by_xpath(\"//*[local-name()='svg']\")\n                    if svg:\n                        df.loc[key, \"loaded\"] = True\n                        df.loc[key, \"error\"] = False\n                        df.loc[key, \"png\"] = svg.screenshot_as_png\n                        df.loc[key, \"svg\"] = svg.get_attribute(\"innerHTML\")\n\n                except NoSuchElementException:\n                    df.loc[key, \"loaded\"] = True\n                    df.loc[key, \"error\"] = True\n                    df.loc[key, \"svg\"] = el.get_attribute(\"innerHTML\")\n\n                df.to_pickle(FN)\n                break\n\n            time.sleep(0.1)\n\n\nif __name__ == \"__main__\":\n    cli()\n", "465": "from bs4 import BeautifulSoup\r\nimport requests\r\nimport csv\r\nimport re\r\n\r\nedicoes = [1527]\r\nlinks = []\r\nnum_gravados = 0\r\n\r\ndef webscrape_edicoes(numeros):\r\n    global num_gravados\r\n    url = f'http://www.uel.br/revistas/uel/index.php/informacao/issue/view/{numeros}'\r\n    source = requests.get(url).text\r\n    soup = BeautifulSoup(source, 'lxml')\r\n    lidos = 0\r\n   \r\n    for texto in soup.find_all('table', class_='tocArticle'):\r\n        link = texto.find('div', class_='tocTitle').a\r\n        link2 = link['href']\r\n        titulo = texto.find('div', class_='tocTitle').text.strip()\r\n        autor = texto.find('div', class_='tocAuthors').text.strip().replace('\\t', '')\r\n        print(f'T\u00c3\u00adtulo do trabalho: {titulo}')\r\n        print(f'Autores: {autor}')\r\n        print(f'Link: {link2}')\r\n        print()\r\n        lidos += 1\r\n        num_gravados += 1\r\n    return lidos > 0\r\n\r\nfor item in edicoes:\r\n    webscrape_edicoes(item)\r\n\r\nprint(links)\r\n\r\nprint(f'Total de artigos analisados: {num_gravados}')\r\n\r\n", "466": "# WHAT WE DOIN HERE\n# by pubins.taylor\n# v0.1\n# created DD MMM YYYY\n# lastUpdate DD MMM YYYY\n\nfrom enum import Enum\n\n\ndirHQ = \"/Users/Shared/BaseballHQ/\"\n\n\nclass FGSystem(Enum):\n    DC_RoS = \"rfangraphsdc\"\n    ZiPS_RoS = \"rzips\"\n    Steamer_RoS = \"steamerr\"\n    ATC = \"atc\"\n\n\nclass FGPosGrp(Enum):\n    HIT = 'bat'\n    PIT = 'pit'\n\n\nclass Savant(Enum):\n    xStats = \"expected_statistics\"\n    barrels = \"statcast\"\n    rolling = \"rolling\"\n    rankings = \"percentile-rankings\"\n\n\nclass SavantDownload(Enum):\n    xStats = \"expected_stats.csv\"\n    barrels = \"exit_velocity.csv\"\n    # rolling does not download .csv; need to webscrape\n    rankings = \"percentile-rankings.csv\"\n\n\nclass SavantPosGrp(Enum):\n    HIT = \"batter\"\n    PIT = \"pitcher\"\n", "467": "from playwright.sync_api import sync_playwright\nimport time\nimport sys\nimport re\n\nclass WebScrape(object):\n\n    def __init__(self, parcel = None):\n        self.parcel = parcel\n\n    def insert_scraping(self):\n        with sync_playwright() as p:\n            browser = p.chromium.launch(headless=False)\n            page = browser.new_page()\n            page.goto(\"https://permits.placer.ca.gov/CitizenAccess/Default.aspx\")\n            # time.sleep(10)\n            iframe = page.wait_for_selector('#ACAFrame').content_frame()\n            table = iframe.wait_for_selector('#ctl00_PlaceHolderMain_TabDataList_TabsDataList_ctl01_LinksDataList_ctl00_LinkItemUrl')\n            table.click()\n            iframe = page.wait_for_selector('#ACAFrame').content_frame()\n            # iframe.wait_for_load_state()\n            iframe.wait_for_selector('#ctl00_PlaceHolderMain_generalSearchForm_txtGSParcelNo')\n            iframe.fill('#ctl00_PlaceHolderMain_generalSearchForm_txtGSParcelNo', self.parcel)\n            table = iframe.wait_for_selector('#ctl00_PlaceHolderMain_btnNewSearch')\n            table.click()\n            iframe = page.wait_for_selector('#ACAFrame').content_frame()\n            table = iframe.wait_for_selector('#lnkMoreDetail')\n            table.click()\n            apply = iframe.wait_for_selector('#imgASI')\n            apply.click()\n            apply_2 = iframe.wait_for_selector('#imgParcel')\n            apply_2.click()\n            permit_number = iframe.wait_for_selector('#ctl00_PlaceHolderMain_lblPermitNumber').inner_text()\n            applicant_name = iframe.wait_for_selector('.contactinfo_fullname').inner_text()\n            work_location = iframe.wait_for_selector('.NotBreakWord').inner_text()\n            work_place = re.sub(r\"\\n\", \" \", work_location)\n            data={}\n            data['Permit Number']= permit_number\n            data['Applicant Name']= applicant_name\n            data['Work Location']= work_place\n\n            print(data)\n            # time.sleep(10)\n\n            print(table.inner_text())\n            print(page.title())\n            browser.close()\n\n\nif __name__ == \"__main__\":\n    print(\"### Building permit info for Placer County ###\")\n    print(\"Parcel number is : \", sys.argv[1])\n    print(\"Looking up, building permits...\")\n    parcel_number = str(sys.argv[1])\n    web_scrape = WebScrape(parcel = parcel_number)\n    web_scrape.insert_scraping()\n\n\n# print(\"### Building permit info for Placer County ###\")\n\n# parcel_number = input(\"please enter a parcel number : \")\n\n# print(\"Parcel number is : \", parcel_number)\n\n", "468": "import requests\nimport re\nfrom flask_cors import CORS, cross_origin\n\nfrom flask import Flask,jsonify\napp = Flask(__name__)\nCORS(app)\n\nclass WebScrape:\n    URL = 'https://www.time.com'\n    def getResponse(self):\n        response = requests.get(self.URL)\n        responseString = str(response.text)\n        return responseString\n    def getStoriesTitleandLink(self):\n        responseString = self.getResponse()\n        indexPartialStories = responseString.find('partial latest-stories')\n        responseString = responseString[indexPartialStories:indexPartialStories + 14000]\n        title = re.findall(']*>(.*?)', responseString)\n        link = re.findall('href=\"/([^\"]*)\"[^>]*>([\\s\\S]*?)', responseString)\n        requiredLinkArray = []\n        for i in range(len(link)):\n            requiredLinkArray.append(link[i][0])\n        return(title,requiredLinkArray)\n\n\n\n\n@app.route('/', methods=['GET'])\ndef getTimeStories():\n    data = WebScrape().getStoriesTitleandLink()\n    requiredJson = []\n    for i in range(len(data[0])):\n        jsonMap = {'title': data[0][i], 'link': 'http://www.time.com/'+ data[1][i]}\n        requiredJson.append(jsonMap)\n    return jsonify(requiredJson)\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=105)\n", "469": "import numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt \nimport re \nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\nsns.set_theme(style=\"whitegrid\")\n\nimport webscrape\n\nclass suggestion:\n\n    def __init__(self):\n        return \n    \n    def readdata(self, filename, sheetname):\n        '''clean data for this part should be a xlsx file so we must specify the filename and sheetname'''\n        #try:\n        graph = pd.read_excel(filename, sheet_name=sheetname, engine='openpyxl')\n        # except:\n        #     print(filename)\n        #     print(\"filename or sheetname is wrong, Please check your input\")\n        #     return \n            \n        graph = graph[:-6]  #delete the last six lines which is the comment\n        \n        graph['ESTIMATED ENERGY USAGE*'] = graph['ESTIMATED ENERGY USAGE*'].apply(lambda x: x.split(' ')[0])\n        graph['ESTIMATED ENERGY COSTS**'] = graph['ESTIMATED ENERGY COSTS**'].apply(lambda x: x.split(' ')[0][1:])\n        graph['ESTIMATED ENERGY COSTS**'] = graph['ESTIMATED ENERGY COSTS**'].apply(lambda x: x.split('/')[0])\n        \n        for i in range(len(graph['CATEGORY1'])):\n\n            # the column estimated energy costs is cleaned in this process\n            # print(graph['ESTIMATED ENERGY COSTS**'][i])\n            \n            if '$' in graph['ESTIMATED ENERGY COSTS**'][i]:\n                # print(\"I come into the if statment\")\n                tmp = graph['ESTIMATED ENERGY COSTS**'][i].split('\u2013$')\n                graph['ESTIMATED ENERGY COSTS**'][i] = (float(tmp[0]) + float(tmp[1])) / 2\n            elif graph['ESTIMATED ENERGY COSTS**'][i] == 'ess':            \n                # catch the exception when less than 0.01 per use\n                graph['ESTIMATED ENERGY COSTS**'][i] = 0.0\n            else:\n                graph['ESTIMATED ENERGY COSTS**'][i] = float(graph['ESTIMATED ENERGY COSTS**'][i])\n\n            # print(graph['ESTIMATED ENERGY USAGE*'][i])\n            \n            # process the estimated energy usage data\n            if '\u2013' in graph['ESTIMATED ENERGY USAGE*'][i]:\n                # print(\"I come into the if statment\")\n                tmp = graph['ESTIMATED ENERGY USAGE*'][i].split('\u2013')\n                graph['ESTIMATED ENERGY USAGE*'][i] = (float(tmp[0]) + float(tmp[1])) / 2\n            else:\n                graph['ESTIMATED ENERGY USAGE*'][i] = float(graph['ESTIMATED ENERGY USAGE*'][i])\n    \n        # print(graph['ESTIMATED ENERGY USAGE*'])\n        #transfer the data \n\n        return graph\n\n    def read_user_option(self, graph):\n        #initialize some of the variable used in this method\n        cat1 = graph['CATEGORY1'][0] +\" \" +graph['CATEGORY2'][0]\n        cate_min = ''\n        price_min = 9999999\n        total_save = 0\n        detail = {}\n\n        print(\"\\n\\nWelcome to the demo part for calculating your energy consumption and saving money!\\nYou can enter q to exit this flow\")\n        for i in range(len(graph['CATEGORY1'])):\n            # if categories change, then print all the options and let the user choose from them\n            if str(graph['CATEGORY1'][i]) +\" \" + str(graph['CATEGORY2'][i]) != cat1:\n                print(\"\\n**Category: \", cat1)\n                # print(detail)\n                for j in range(len(detail)):\n                    print(j+1, '. ', list(detail)[j])\n                choice = input(\"please Enter your choice or leave it blank: \")\n                if choice == 'q':\n                    break\n                if choice != '':\n                    choice = int(choice) - 1\n                    if list(detail.values())[choice][0]*list(detail.values())[choice][1] != price_min:\n                        print(\"Your choice is \", list(detail.keys())[choice], \", this would cost \", list(detail.values())[choice][0]*list(detail.values())[choice][1], \n                        \"dollar per hour\\n\", \"If you choose \", cate_min, \", you will save\", list(detail.values())[choice][0]*list(detail.values())[choice][1]-price_min,\n                        \"dollar per hour!\")\n                        total_save += list(detail.values())[choice][0]*list(detail.values())[choice][1]-price_min\n                    else:\n                        print(\"Congratulations! You have made the most energy saving choice!\")\n                    print(\"In this suggestion flow you have saved \", total_save, \"dollars per hour!!\")\n                    #demo the least one\n                else:\n                    print(\"Please choose \", cate_min, \" as your choice! This choice saves energy and will cost you \", price_min, \" dollar per hour!\\nOr you choosing not to involve this category\")\n                detail = {}\n                detail[graph['DETAIL'][i]] = [graph['ESTIMATED ENERGY USAGE*'][i], graph['ESTIMATED ENERGY COSTS**'][i]]\n                cate_min = graph['DETAIL'][i]\n                price_min = graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i]\n                cat1 = str(graph['CATEGORY1'][i]) + \" \" + str(graph['CATEGORY2'][i])\n            else:\n                detail[graph['DETAIL'][i]] = [graph['ESTIMATED ENERGY USAGE*'][i], graph['ESTIMATED ENERGY COSTS**'][i]]\n                if graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i] < price_min:\n                    cate_min = graph['DETAIL'][i]\n                    price_min = graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i] \n        print(\"\\nYou have saved \", total_save, \"dollars per hour in the whole program flow!!!\")\n        print(\"\\nThank you for using demo program to reduce your energy costs and your money!\")\n        #return total_save\n\n    def co2_emi_capita(self, filename, sheetname):\n        df1 = pd.read_excel(open(filename, 'rb'), sheet_name=sheetname) \n        df_USA_individual = df1.iloc[21232:21451]\n        df_USA_individual.plot(x=\"Year\", y='Per capita CO2 emissions')\n        plt.show()\n\n\n\nif __name__ == '__main__':\n    # you can enter the file location in this part to initial the class\n    filename = r'D:\\project_agg\\21S_DFP_project\\Big_project\\Project_Prototype_DFP_group12.xlsx' \n    sheetname_emission_per_capita = r'Worldbank CO2-emissions_capita'\n    sheetname_interactive = r'Appliance Energy Use_Clean'\n    signal = True\n    print(\"Scrape is starting, it may take a few seconds to finish\")\n    web = webscrape.ncdc_pre() #initial a new class from webscrape\n    df = web.clean_data(web.scrape())\n\n\n    # project begin\n    print(\"\\nWelcome to the demo project which could help you be a more environmental friendly person!\")\n    while signal:\n        print(\"\\nPlease make your choice:\\n1. Display the Global Land and Sea Yearly Temperature Anomalies Chart\\n2. Display the Top Ten Hottest Years on Record Chart\\n3. Display CO2 Emissions per Capita Chart\\n4. Suggestions for Your Energy Clean Choice\\n5. Quit\")\n        choice = input(\"Your choice is: \")\n        if choice == '1':\n            web.draw_temp_anomly(df)\n        elif choice == '2':\n            web.draw_top10(df)\n        elif choice == '3':\n            suggestion().co2_emi_capita(filename, sheetname_emission_per_capita)\n        elif choice == '4':\n            graph = suggestion().readdata(filename, sheetname_interactive)\n            #print(graph)\n            suggestion().read_user_option(graph)\n        elif choice == '5':\n            signal = False\n            print(\"Thank you for using the demo project!!\")\n        else:\n            print(\"\\nYour input is invalid! Please re-try!\")\n    \n\n", "470": "import numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt \nimport re \nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\nsns.set_theme(style=\"whitegrid\")\n\nimport webscrape\n\nclass suggestion:\n\n    def __init__(self):\n        return \n    \n    def readdata(self, filename, sheetname):\n        '''clean data for this part should be a xlsx file so we must specify the filename and sheetname'''\n        #try:\n        graph = pd.read_excel(filename, sheet_name=sheetname, engine='openpyxl')\n        # except:\n        #     print(filename)\n        #     print(\"filename or sheetname is wrong, Please check your input\")\n        #     return \n            \n        graph = graph[:-6]  #delete the last six lines which is the comment\n        \n        graph['ESTIMATED ENERGY USAGE*'] = graph['ESTIMATED ENERGY USAGE*'].apply(lambda x: x.split(' ')[0])\n        graph['ESTIMATED ENERGY COSTS**'] = graph['ESTIMATED ENERGY COSTS**'].apply(lambda x: x.split(' ')[0][1:])\n        graph['ESTIMATED ENERGY COSTS**'] = graph['ESTIMATED ENERGY COSTS**'].apply(lambda x: x.split('/')[0])\n        \n        for i in range(len(graph['CATEGORY1'])):\n\n            # the column estimated energy costs is cleaned in this process\n            # print(graph['ESTIMATED ENERGY COSTS**'][i])\n            \n            if '$' in graph['ESTIMATED ENERGY COSTS**'][i]:\n                # print(\"I come into the if statment\")\n                tmp = graph['ESTIMATED ENERGY COSTS**'][i].split('\u2013$')\n                graph['ESTIMATED ENERGY COSTS**'][i] = (float(tmp[0]) + float(tmp[1])) / 2\n            elif graph['ESTIMATED ENERGY COSTS**'][i] == 'ess':            \n                # catch the exception when less than 0.01 per use\n                graph['ESTIMATED ENERGY COSTS**'][i] = 0.0\n            else:\n                graph['ESTIMATED ENERGY COSTS**'][i] = float(graph['ESTIMATED ENERGY COSTS**'][i])\n\n            # print(graph['ESTIMATED ENERGY USAGE*'][i])\n            \n            # process the estimated energy usage data\n            if '\u2013' in graph['ESTIMATED ENERGY USAGE*'][i]:\n                # print(\"I come into the if statment\")\n                tmp = graph['ESTIMATED ENERGY USAGE*'][i].split('\u2013')\n                graph['ESTIMATED ENERGY USAGE*'][i] = (float(tmp[0]) + float(tmp[1])) / 2\n            else:\n                graph['ESTIMATED ENERGY USAGE*'][i] = float(graph['ESTIMATED ENERGY USAGE*'][i])\n    \n        # print(graph['ESTIMATED ENERGY USAGE*'])\n        #transfer the data \n\n        return graph\n\n    def read_user_option(self, graph):\n        #initialize some of the variable used in this method\n        cat1 = graph['CATEGORY1'][0] +\" \" +graph['CATEGORY2'][0]\n        cate_min = ''\n        price_min = 9999999\n        total_save = 0\n        detail = {}\n\n        print(\"\\n\\nWelcome to the demo part for calculating your energy consumption and saving money!\\nYou can enter q to exit this flow\")\n        for i in range(len(graph['CATEGORY1'])):\n            # if categories change, then print all the options and let the user choose from them\n            if str(graph['CATEGORY1'][i]) +\" \" + str(graph['CATEGORY2'][i]) != cat1:\n                print(\"\\n**Category: \", cat1)\n                # print(detail)\n                for j in range(len(detail)):\n                    print(j+1, '. ', list(detail)[j])\n                choice = input(\"please Enter your choice or leave it blank: \")\n                if choice == 'q':\n                    break\n                if choice != '':\n                    choice = int(choice) - 1\n                    if list(detail.values())[choice][0]*list(detail.values())[choice][1] != price_min:\n                        print(\"Your choice is \", list(detail.keys())[choice], \", this would cost \", list(detail.values())[choice][0]*list(detail.values())[choice][1], \n                        \"dollar per hour\\n\", \"If you choose \", cate_min, \", you will save\", list(detail.values())[choice][0]*list(detail.values())[choice][1]-price_min,\n                        \"dollar per hour!\")\n                        total_save += list(detail.values())[choice][0]*list(detail.values())[choice][1]-price_min\n                    else:\n                        print(\"Congratulations! You have made the most energy saving choice!\")\n                    print(\"In this suggestion flow you have saved \", total_save, \"dollars per hour!!\")\n                    #demo the least one\n                else:\n                    print(\"Please choose \", cate_min, \" as your choice! This choice saves energy and will cost you \", price_min, \" dollar per hour!\\nOr you choosing not to involve this category\")\n                detail = {}\n                detail[graph['DETAIL'][i]] = [graph['ESTIMATED ENERGY USAGE*'][i], graph['ESTIMATED ENERGY COSTS**'][i]]\n                cate_min = graph['DETAIL'][i]\n                price_min = graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i]\n                cat1 = str(graph['CATEGORY1'][i]) + \" \" + str(graph['CATEGORY2'][i])\n            else:\n                detail[graph['DETAIL'][i]] = [graph['ESTIMATED ENERGY USAGE*'][i], graph['ESTIMATED ENERGY COSTS**'][i]]\n                if graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i] < price_min:\n                    cate_min = graph['DETAIL'][i]\n                    price_min = graph['ESTIMATED ENERGY USAGE*'][i] * graph['ESTIMATED ENERGY COSTS**'][i] \n        print(\"\\nYou have saved \", total_save, \"dollars per hour in the whole program flow!!!\")\n        print(\"\\nThank you for using demo program to reduce your energy costs and your money!\")\n        #return total_save\n\n    def co2_emi_capita(self, filename, sheetname):\n        df1 = pd.read_excel(open(filename, 'rb'), sheet_name=sheetname) \n        df_USA_individual = df1.iloc[21232:21451]\n        df_USA_individual.plot(x=\"Year\", y='Per capita CO2 emissions')\n        plt.show()\n\n\n\nif __name__ == '__main__':\n    # you can enter the file location in this part to initial the class\n    filename = r'D:\\project_agg\\21S_DFP_project\\Big_project\\Project_Prototype_DFP_group12.xlsx' \n    sheetname_emission_per_capita = r'Worldbank CO2-emissions_capita'\n    sheetname_interactive = r'Appliance Energy Use_Clean'\n    signal = True\n    print(\"Scrape is starting, it may take a few seconds to finish\")\n    web = webscrape.ncdc_pre() #initial a new class from webscrape\n    df = web.clean_data(web.scrape())\n\n\n    # project begin\n    print(\"\\nWelcome to the demo project which could help you be a more environmental friendly person!\")\n    while signal:\n        print(\"\\nPlease make your choice:\\n1. Display the Global Land and Sea Yearly Temperature Anomalies Chart\\n2. Display the Top Ten Hottest Years on Record Chart\\n3. Display CO2 Emissions per Capita Chart\\n4. Suggestions for Your Energy Clean Choice\\n5. Quit\")\n        choice = input(\"Your choice is: \")\n        if choice == '1':\n            web.draw_temp_anomly(df)\n        elif choice == '2':\n            web.draw_top10(df)\n        elif choice == '3':\n            suggestion().co2_emi_capita(filename, sheetname_emission_per_capita)\n        elif choice == '4':\n            graph = suggestion().readdata(filename, sheetname_interactive)\n            #print(graph)\n            suggestion().read_user_option(graph)\n        elif choice == '5':\n            signal = False\n            print(\"Thank you for using the demo project!!\")\n        else:\n            print(\"\\nYour input is invalid! Please re-try!\")\n    \n\n", "471": "from operator import index\nimport requests, lxml, re, json\nfrom bs4 import BeautifulSoup\nimport os\n\nowd = os.getcwd()\nif os.path.isdir('images') is False:\n    os.makedirs('images')\nif os.path.isdir('images/dog') is False:\n    os.makedirs('images/dog')\n    os.makedirs('images/cat')\n    os.makedirs('images/rabbit')\n    os.makedirs('images/horse')\n    os.makedirs('images/fox')\n    os.makedirs('images/squirrel')\n    os.makedirs('images/bear')\n    os.makedirs('images/wolf')\n    os.makedirs('images/monkey')\n    os.makedirs('images/turtle')\n    os.makedirs('images/pig')\n    os.makedirs('images/deer')\n    os.makedirs('images/frog')\nan_face = {'\uac15\uc544\uc9c0\uc0c1':[\"\ubc15\ubcf4\uac80\",\"\uac15\ub2e4\ub2c8\uc5d8\",\"\ubc31\ud604\",\"\uc774\uc885\uc11d\",\"\uc190\uc608\uc9c4\",\"\ud55c\ud6a8\uc8fc\",\"\uc218\uc9c0\",\"\uc544\uc774\uc720\"],\\\n            '\uace0\uc591\uc774\uc0c1':[\"\ud55c\uc608\uc2ac\",\"\ud604\uc544\",\"\uc608\uc9c0\",\"\uc870\uc778\uc131\",\"\ud558\ub2c8\",\"\ud55c\uc18c\ud76c\",\"\uace0\uc544\ub77c\",\"\ud06c\ub9ac\uc2a4\ud0c8\"],\\\n            '\ud1a0\ub07c\uc0c1':[\"\ub098\uc5f0\",\"\ub824\uc6b1\",\"\uc544\uc774\ub9b0\",\"\uc720\ub098\",\"\uc774\ub098\uc740\",\"\uc815\uad6d\",\"\uc9c0\uc218\",\"\ud55c\uc9c0\ubbfc\",\"\ub3c4\uc601\",\"\uc5d0\uc2a4\ud30c\uc708\ud130\",\"\ud0dc\ubbfc\"],\\\n            '\ub9d0\uc0c1':[\"\uac15\ud0c0\",\"\uae40\uae30\uc218\",\"\ubcf4\uc544\",\"\uc81c\uc774\ud649\",\"\ud0dc\uc5f0\",\"\uc774\uad11\uc218\",\"\uc18c\uc9c0\uc12d\",\"\uc774\ubcd1\ud5cc\"],\\\n            '\uc5ec\uc6b0\uc0c1':[\"\uc11c\uc778\uad6d\",\"\uac00\uc778\",\"\uacbd\ub9ac\",\"\uc720\uc778\ub098\",\"\uc721\uc131\uc7ac\",\"\ucbd4\uc704\",\"\ucc44\ub839\",\"\uc0e4\uc774\ub2c8\ud0a4\"],\\\n            '\ub2e4\ub78c\uc950\uc0c1':[\"\uae40\uc131\uacbd\",\"\uc0ac\ub098\",\"\ubb38\ubcc4\",\"\uc870\ubcf4\uc544\",\"\uac15\ubbf8\ub098\",\"\ub85c\uaf2c\",\"\ub808\uc774\ub098\"],\\\n            \"\uacf0\uc0c1\":[\"\ub9c8\ub3d9\uc11d\",\"\uc154\ub204\",\"\uc2ac\uae30\",\"\uc2a4\uc719\uc2a4\",\"\uae40\uc724\uc11d\"],\\\n            \"\ub291\ub300\uc0c1\":[\"\uc138\ud6c8\",\"\ubdd4\",\"\ud669\uc778\uc5fd\",\"\uae40\uc601\ub300\",\"\uc11c\uac15\uc900\",\"\uad6c\uc900\ud68c\"],\\\n            \"\uc6d0\uc22d\uc774\uc0c1\":[\"\ubc15\uc9c4\uc601\",\"\ube48\uc9c0\ub178\",\"\uc591\uc138\ud615\",\"\ucf54\ub4dc\ucfe4\uc2a4\ud2b8\",\"\uc9c0\ub4dc\ub798\uace4\"],\\\n            \"\uac70\ubd81\uc774\uc0c1\":[\"\uc0e4\uc774\ub2c8\ubbfc\ud638\",\"\uc194\ub77c\",\"\uc608\ub9ac\",\"\uc720\uc815\",\"\ud314\ub85c\uc54c\ud1a0\"],\n            \"\ub3fc\uc9c0\uc0c1\":[\"\uac15\uc18c\ub77c\",\"\uac15\ud638\ub3d9\",\"\uae40\uc900\ud604\",\"\ubc15\ub098\ub798\",\"\uc11c\ud604\",\"\uc18c\uc720\",\"\uc190\ub098\uc740\"],\\\n            \"\uc0ac\uc2b4\uc0c1\":[\"\uae40\ucc44\uc6d0\",\"\ub80c\",\"\ub8e8\ud55c\",\"\uc0e4\uc774\ub2c8\ubbfc\ud638\",\"\uc1a1\uac15\",\"\uc724\uc544\",\"\ucc28\uc740\uc6b0\",\"\ud0dc\ubbfc\"],\\\n            \"\uac1c\uad6c\ub9ac\uc0c1\":[\"\uad8c\uc815\uc5f4\",\"\uae40\ubbfc\uc8fc\",\"\uc624\ub9c8\uc774\uac78\ube44\ub2c8\",\"\uc2e0\ubbfc\uc544\",\"\uc804\uc18c\ubbfc\",\"\ud558\ud604\uc6b0\"]}\nheaders = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\"}\n\ndef webscrape_images1():\n    for id,animal in enumerate(an_face.keys()):\n        print(\"1st loop\",animal,\"searching\")\n        url = \"https://search.aol.com/aol/image;_ylt=Awr9Hr57TuJh5GkAKeNjCWVH?q={}+\uc5f0\uc608\uc778&ei=UTF-8&s_it=sb_top&v_t=comsearch&imgty=photo&fr2=p%3As%2Cv%3Ai\".format(animal)\n        res = requests.get(url, headers=headers)\n        res.raise_for_status()\n        soup = BeautifulSoup(res.text, \"lxml\")\n        images = soup.find_all(\"a\", attrs={\"class\":\"img\"})\n        print(\"images length:\",len(images))\n        if animal == '\uac15\uc544\uc9c0\uc0c1':\n            os.chdir('images/dog')\n        elif animal == '\uace0\uc591\uc774\uc0c1':\n            os.chdir('images/cat')\n        elif animal == '\ud1a0\ub07c\uc0c1':\n            os.chdir('images/rabbit')\n        elif animal == '\ub9d0\uc0c1':\n            os.chdir('images/horse')\n        elif animal == '\uc5ec\uc6b0\uc0c1':\n            os.chdir('images/fox')\n        elif animal == '\ub2e4\ub78c\uc950\uc0c1':\n            os.chdir('images/squirrel')\n        elif animal == '\uacf0\uc0c1':\n            os.chdir('images/bear')\n        elif animal == '\ub291\ub300\uc0c1':\n            os.chdir('images/wolf')\n        elif animal == '\uc6d0\uc22d\uc774\uc0c1':\n            os.chdir('images/monkey')\n        elif animal == '\uac70\ubd81\uc774\uc0c1':\n            os.chdir('images/turtle')\n        elif animal == '\ub3fc\uc9c0\uc0c1':\n            os.chdir('images/pig')\n        elif animal == '\uc0ac\uc2b4\uc0c1':\n            os.chdir('images/deer')\n        elif animal == '\uac1c\uad6c\ub9ac\uc0c1':\n            os.chdir('images/frog')\n        cnt = 0\n        for i,image in enumerate(images):\n            cnt += 1\n            image_url = image[\"href\"]\n            image_res = requests.get(image_url)\n            try:\n                with open(\"{}{}.jpg\".format(animal,i), \"wb\") as f:\n                    f.write(image_res.content)\n            except image_res.raise_for_status():\n                continue\n        os.chdir(owd)\n\ndef webscrape_images2(dog_cnt,cat_cnt,rabbit_cnt,horse_cnt,fox_cnt,squirrel_cnt,bear_cnt,wolf_cnt,monkey_cnt,turtle_cnt,pig_cnt,deer_cnt,frog_cnt):\n    for idx1,names in enumerate(an_face.values()):\n        for name_ind,name in enumerate(names):\n            print(\"2nd loop\",name,\"searching\")\n            url1 = \"https://search.aol.com/aol/image;_ylt=Awr9Hr57TuJh5GkAKeNjCWVH?q={}+\uc5bc\uad74&ei=UTF-8&s_it=sb_top&v_t=comsearch&imgty=photo&fr2=p%3As%2Cv%3Ai\".format(name)\n            res1 = requests.get(url1, headers=headers)\n            res1.raise_for_status()\n            soup1 = BeautifulSoup(res1.text, \"lxml\")\n            face_images = soup1.find_all(\"a\", attrs={\"class\":\"img\"})\n            if idx1 == 0:\n                os.chdir('images/dog')\n                animal = '\uac15\uc544\uc9c0\uc0c1'\n                if name_ind > 0:\n                    dog_cnt += temp_cnt\n                cnt = dog_cnt\n            elif idx1 == 1:\n                os.chdir('images/cat')\n                animal = '\uace0\uc591\uc774\uc0c1'\n                if name_ind > 0:\n                    cat_cnt += temp_cnt\n                cnt = cat_cnt\n            elif idx1 == 2:\n                os.chdir('images/rabbit')\n                animal = '\ud1a0\ub07c\uc0c1'\n                if name_ind > 0:\n                    rabbit_cnt += temp_cnt\n                cnt = rabbit_cnt\n            elif idx1 == 3:\n                os.chdir('images/horse')\n                animal = '\ub9d0\uc0c1'\n                if name_ind > 0:\n                    horse_cnt += temp_cnt\n                cnt = horse_cnt\n            elif idx1 == 4:\n                os.chdir('images/fox')\n                animal = '\uc5ec\uc6b0\uc0c1'\n                if name_ind > 0:\n                    fox_cnt += temp_cnt\n                cnt = fox_cnt\n            elif idx1 == 5:\n                os.chdir('images/squirrel')\n                animal = '\ub2e4\ub78c\uc950\uc0c1'\n                if name_ind > 0:\n                    squirrel_cnt += temp_cnt\n                cnt = squirrel_cnt\n            elif idx1 == 6:\n                os.chdir('images/bear')\n                animal = '\uacf0\uc0c1'\n                if name_ind > 0:\n                    bear_cnt += temp_cnt\n                cnt = bear_cnt\n            elif idx1 == 7:\n                os.chdir('images/wolf')\n                animal = '\ub291\ub300\uc0c1'\n                if name_ind > 0:\n                    wolf_cnt += temp_cnt\n                cnt = wolf_cnt\n            elif idx1 == 8:\n                os.chdir('images/monkey')\n                animal = '\uc6d0\uc22d\uc774\uc0c1'\n                if name_ind > 0:\n                    monkey_cnt += temp_cnt\n                cnt = monkey_cnt\n            elif idx1 == 9:\n                os.chdir('images/turtle')\n                animal = '\uac70\ubd81\uc774\uc0c1'\n                if name_ind > 0:\n                    turtle_cnt += temp_cnt\n                cnt = turtle_cnt\n            elif idx1 == 10:\n                os.chdir('images/pig')\n                animal = '\ub3fc\uc9c0\uc0c1'\n                if name_ind > 0:\n                    pig_cnt += temp_cnt\n                cnt = pig_cnt\n            elif idx1 == 11:\n                os.chdir('images/deer')\n                animal = '\uc0ac\uc2b4\uc0c1'\n                if name_ind > 0:\n                    deer_cnt += temp_cnt\n                cnt = deer_cnt\n            elif idx1 == 12:\n                os.chdir('images/frog')\n                animal = '\uac1c\uad6c\ub9ac\uc0c1'\n                if name_ind > 0:\n                    frog_cnt += temp_cnt\n                cnt = frog_cnt\n            temp_cnt = 0\n            print(\"length= \",len(face_images))\n            for face_ind,face_image in enumerate(face_images):\n                print(face_ind)\n                temp_cnt += 1\n                cnt += 1\n                face_image_url = face_image[\"href\"]\n                face_image_res = requests.get(face_image_url)\n                try:\n                    with open(\"{}{}.jpg\".format(animal,cnt), \"wb\") as f:\n                        f.write(face_image_res.content)\n                except face_image_res.raise_for_status():\n                    continue\n            os.chdir(owd)\n            \n# webscrape_images2(36,46,36,23,40,44,21,14,38,11,26,24,34)", "472": "#@author Tim Hodson with credits to Beautiful Soup: https://www.crummy.com/software/BeautifulSoup/\n# Additional credits to this walkthrough: https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe\n\n# import libraries\nimport urllib2\nimport json\nfrom bs4 import BeautifulSoup #Remember to download and pip install beautifulsoup4\nimport csv\n\n# specify the url\n#crimePage = \"file:///Users/Tim/Documents/webScrape/jan1jan15.html\"\n\n# query the website and return the html to the variable page\npage = urllib2.urlopen(\"file:///Users/Tim/Documents/webScrape/endJan.html\").read()\n\n\n# parse the html using beautiful soap and store in variable soup\nsoup = BeautifulSoup(page, \"html.parser\")\n\ntable = soup.find('table') #finds the table for the page. You may need to use \"find all\" and a loop for multiple tables\n\ndata = []\nrows = table.findAll('tr') #find all rows of theh table\nfor row in rows: #for every row, extract the data\n    cols = row.find_all('td')\n    cols = [ele.text.strip() for ele in cols]\n    data.append([ele for ele in cols if ele]) # Get rid of empty values\n\nstr_list = []\nstr_list = filter(None, data) #Strip empty values\n\nnewList = []\ni = 0\n#iterate through str_list and combine array elements into a usable format, then seperate with a $ to use in excel cleanup. \nwhile i < (len(str_list)):\n    newList.append(str_list[i][0] + \"$ \" + str_list[i][1] + \"$ \" + str_list[i][2] + \"$ \" + str_list[i][3]+ \"$ \" + str_list[i][4]+ \"$ \" + str_list[i][5]+ \"$ \" + str_list[i][6]+ \"$ \" + str_list[i+1][0])\n    i = i+2\n\n#Write the data to CSV\nwith open(\"/Users/Tim/Documents/WebDesign/endJan.csv\",'w') as myfile:\n    wr = csv.writer(myfile)\n    header=['type','id','numberOfUpdates','isPingEnabled','lastUpdated']\n    for ele in newList:\n        wr.writerow([[ele]])", "473": "# Define here the models for your spider middleware\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\n\n# useful for handling different item types with a single interface\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass WebscrapeSpiderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(self, response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, or item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(self, response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Request or item objects.\n        pass\n\n    def process_start_requests(self, start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n\n\nclass WebscrapeDownloaderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_request(self, request, spider):\n        # Called for each request that goes through the downloader\n        # middleware.\n\n        # Must either:\n        # - return None: continue processing this request\n        # - or return a Response object\n        # - or return a Request object\n        # - or raise IgnoreRequest: process_exception() methods of\n        #   installed downloader middleware will be called\n        return None\n\n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n", "474": "import Oil_Price_Query\nimport Oil_webscrape_data\nimport opec\n", "475": "import json\nimport os\nimport sys\nimport django\n\n#  set the location of django settings to enable using the model\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"PumpWebscrape.settings\")\nsys.path.append(os.path.join(os.path.realpath(os.path.dirname(__file__)), \"..\", \"..\"))\ndjango.setup()\n\nfrom corporation.models import Corporation\n\n\ndef CreateCorperationWitJsonFile():\n    #  open the json file\n    with open(\"./petrol_pumps.json\", \"r\") as f:\n        for item in json.load(f):\n            name = item.get(\"name\")\n            address = item.get(\"address\")\n            phone = item.get(\"phone\")\n            open_hours = item.get(\"open_hours\")\n            link = item.get(\"link\")\n            #  not all has decription since i dont need to webscrape all\n            description = item.get(\"description\", \"No Description\")\n            try:\n                #  create the cooperating\n                Corporation.objects.create(\n                    name=name,\n                    address=address,\n                    phone=phone,\n                    open_hours=open_hours,\n                    link=link,\n                    description=description,\n                )\n            except Exception as a:\n                print(a)\n\n\nCreateCorperationWitJsonFile()\n", "476": "import json\r\nimport boto3\r\nimport logging\r\nimport threading\r\nimport sys\r\n\r\ns3 = boto3.client('s3')\r\n\r\ndef mask_entities_in_message(message, entity_list):\r\n  for entity in entity_list:\r\n      message = message.replace(entity['Text'], '#' * len(entity['Text']))\r\n  return message\r\n\r\ndef lambda_handler(event, context):\r\n  try:\r\n      #originalDataObject= s3.get_object(Bucket = 'assign2-scrape-bucket', Key= 'ScrapedFolder/webscrape.txt')\r\n      #Entityobject= s3.get_object(Bucket = 'assign2-scrape-bucket', Key= 'EntityExtractionOutput/webscrape.txt.txt')\r\n      #entities_json_data = json.loads(Entityobject['Body'].read())\r\n      #originalData = originalDataObject['Body'].read().decode()\r\n      \r\n      originalData = event['data']\r\n      entities_json_data = event['entities']\r\n      print(originalData, entities_json_data)\r\n      \r\n      masked_data = mask_entities_in_message(originalData, entities_json_data)\r\n      return masked_data\r\n     \r\n  except Exception as e:\r\n      logging.error('Exception: %s. Unable to get the data from s3' % e)\r\n      raise e", "477": "from bs4 import BeautifulSoup\r\nimport requests\r\nimport csv\r\nimport re\r\n\r\nedicoes = [1527, 1475, 1425, 1424]\r\nlinks = []\r\nnum_gravados = 0\r\n\r\ndef webscrape_edicoes(numeros):\r\n    global num_gravados\r\n    url = f'http://www.uel.br/revistas/uel/index.php/informacao/issue/view/{numeros}'\r\n    source = requests.get(url).text\r\n    soup = BeautifulSoup(source, 'lxml')\r\n    lidos = 0\r\n   \r\n    for texto in soup.find_all('table', class_='tocArticle'):\r\n        link = texto.find('a', attrs={'href': re.compile(\"http://\")})\r\n        link2 = str(link.get('href')[65:70])\r\n        links.append(link2)\r\n        titulo = texto.find('div', class_='tocTitle').text.strip()\r\n        autor = texto.find('div', class_='tocAuthors').text.strip().replace('\\t', '')\r\n        print(f'T\u00c3\u00adtulo do trabalho: {titulo}')\r\n        print(f'Autores: {autor}')\r\n        print(f'Link: {link2}')\r\n        print()\r\n        lidos += 1\r\n        num_gravados += 1\r\n    return lidos > 0\r\n\r\nfor item in edicoes:\r\n    webscrape_edicoes(item)\r\n\r\nprint(links)\r\n\r\nprint(f'Total de artigos analisados: {num_gravados}')\r\n\r\n", "478": "from selenium import webdriver \nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport csv\nimport urllib.request\nfrom lxml import html\nimport time\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.chrome.options import Options\nfrom pprint import pprint as pp\nfrom tabulate import tabulate\n\nscrape_file\nlogin_url\n\nwith open(scrape_file, encoding='UTF-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    scrape_file = list(reader)\n\nwritefile = open('Classic-Imports-and-Design\\python webscrape\\output.csv', 'w+', encoding='UTF8', newline='')\noutput_file = csv.writer(writefile, delimiter=\",\")\n\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option('excludeSwitches', ['enable-logging'])\ndriver = webdriver.Chrome('Classic-Imports-and-Design\\python webscrape\\chromedriver.exe', options=options)\n\n#login\ndriver.get(login_url)\nprint(\"Please log into the wholesale account.\")\ninput()\n\nfile_header = ['sku', 'Name', 'Brand', 'Categories', 'Wholesale', 'Retail', 'Description', 'Images', 'L', 'W', 'H']\noutput_file.writerow(file_header)\nfails = []\n\nfor product in scrape_file:\n    product[0] = product[0].strip()\n    product[1] = product[1].strip()\n    data = []\n    driver.get() \n    time.sleep(3)\n\n    pp(fails)\n    print_table = data.copy()\n    print_table[6] = 'desc...'\n    print_table[7] = 'images...'\n    print(tabulate([file_header] + [print_table]))\n    print(data[6])\n    print(data[7])\n    output_file.writerow(data)\n\npp(fails)\nwritefile.close()\n\n#element_present = EC.presence_of_element_located((By.CLASS_NAME, 'product-item-link'))\n#WebDriverWait(driver, 6).until(element_present)\n#driver.get(url)\n#time.sleep()\n#content = driver.page_source\n#soup = BeautifulSoup(content, \"html.parser\")\n#tree = html.fromstring(driver.page_source) \n\n#driver.find_elements(By.XPATH, XPATH)\n#driver.find_elements(By.CLASS_NAME, CLASSNAME)\n#soup.find(\"TAG-TYPE\", class_=\"CLASS-NAME\").text\n\n#urllib.request.urlretrieve(IMG-URL, FILENAME) \n\n#output_file.writerow(data)", "479": "import requests\nimport csv\nfrom bs3 import BeautifulSoup\n\nclass WebScrape:\n\tdef __init__(self):\n\t\tprint(\"WebScrape Imported\")\n\n\tdef lazada_scrape(self,head,category,url):\n\t\tlist_of_rows = []\n        url = \"http://www.lazada.com.ph/\"+ url +\"/\"\n        source_code = requests.get(url)\n        txt = source_code.text\n        soup = BeautifulSoup(txt, 'html.parser')\n        max_page = int(soup.select(\"span.pages > a:nth-of-type(6)\")[0].get_text())\n        page = 1\n        myfile = open(category + \".csv\", 'w', newline='')\n        writer = csv.DictWriter(myfile, fieldnames = [\"url\", \"product_name\", \"product_header\", \"product_category\", \"product_price\", \"product_sale\", \"product_old\", \"installment\", \"rating\"], delimiter=',')\n        writer.writeheader()\n        while page <= max_page:\n                print(page)\n                url = \"http://www.lazada.com.ph/shop-mobiles/?page=\" + str(page)\n                source_code = requests.get(url)\n                txt = source_code.text\n                soup = BeautifulSoup(txt,'html.parser')\n                for div in soup.find_all(\"div\", {\"class\":\"product-card\"}):\n                        mylist = []\n        \n                        for link in div.find_all(\"a\"):\n                                mylist.append(str(link.get(\"href\")))\n                        for title in div.find_all(\"span\", {\"class\":\"product-card__name\"}):\n                                mylist.append(str(title.text).replace(\"\\u200f\",\" \").replace(\"\\uFF08\",\"(\").replace(\"\\uff09\",\")\"))\n                                mylist.append(head)\n                                mylist.append(category)\n                        for price in div.find_all(\"div\", {\"class\":\"product-card__price\"}):\n                                mylist.append(str(price.text.replace(\"\\u20B1\",\"Php \")))\n\n                        sale = div.find_all(\"div\", {\"class\":\"product-card__sale\"})\n                        if not sale:\n                            mylist.append(\"0%\")\n                        else:                            \n                            for sales in sale:\n                                    mylist.append(str(sales.text))\n\n                        old = div.find_all(\"div\", {\"class\":\"old-price-wrap\"})\n                        if not old:\n                            mylist.append(\"Php 0.00\")\n                        else:                            \n                            for olds in old:\n                                    mylist.append(str(olds.text).replace(\"\\u20B1\",\"Php \").replace(\"\\n\",\"\"))\n\n                        installment = div.find_all(\"span\", {\"class\":\"installment-part\"})\n                        if not installment:\n                            mylist.append(\"Php 0.00\")\n                        else:\n                            for installments in installment:\n                                mylist.append(str(installments.text).replace(\"\\u20B1\",\"Php \"))\n\n                        rating = div.find_all(\"span\", {\"class\":\"rating__number\"})\n                        if not rating:\n                            mylist.append(\"(0 reviews)\")\n                        else:\n                            for ratings in rating:\n                                mylist.append(str(ratings.text))\n                                        \n                        list_of_rows.append(mylist)\n                page+=1\n        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n        wr.writerows(list_of_rows)", "480": "# Modules needed\nimport requests\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# from texttable import Texttable\n# import matplotlib\n\n\ndef webscrape():\n    \"\"\"Webscrapes the worldometers website for coronavirus statistics and saves information into textfile\"\"\"\n    url = 'https://www.worldometers.info/coronavirus/countries-where-coronavirus-has-spread/'\n\n    # gets the url's html\n    page_content = requests.get(url)\n    soup = BeautifulSoup(page_content.text, 'html.parser')\n\n    web_data = []\n\n    # soup.find_all('td') will scrape all elements in the url's table elements\n    data_iterator = iter(soup.find_all('td'))\n\n    # Loop that repeats until there is no data left available for the iterator\n    while True:\n        try:\n            country = next(data_iterator).text\n            confirmed = next(data_iterator).text\n            deaths = next(data_iterator).text\n            continent = next(data_iterator).text\n\n            # Replaces spaces with underscores in country and continent names to help with saving file content later\n            country = country.replace(' ', '_')\n            continent = continent.replace(' ', '_')\n            # This just adds stats into the list while also replacing the confirmed and deaths into ints\n            # NOTE: This creates a list of tuples\n            web_data.append((country,\n                             int(confirmed.replace(',', '')),  # This allows for numbers in millions\n                             int(deaths.replace(',', '')),\n                             continent\n                             ))\n\n        # StopIteration error occurs when there are no more elements to iterate through\n        except StopIteration:\n            break\n\n    #  Sorts the data by number of confirmed cases\n    web_data.sort(key=lambda row: row[1], reverse=True)\n\n    f = open('webscrape_data.txt', 'w')\n    for tuple_unit in web_data:\n        f.write(''.join(str(s) + ' ' for s in tuple_unit) + ' \\n')\n\n    f.close()\n\n\ndef deaths_of_country(data, country):\n    \"\"\"Returns the number of deaths caused by coronavirus of the chosen country\"\"\"\n    counter = 0\n    while country != data[counter][0]:\n        counter = counter + 1\n\n    return data[counter][2]\n\n\ndef cases_of_country(data, country):\n    \"\"\"Returns the number of coronavirus cases of the chosen country\"\"\"\n    counter = 0\n    while country != data[counter][0]:\n        counter = counter + 1\n\n    return data[counter][1]\n\n\ndef top_affected_countries(data):\n    \"\"\"Returns a list of names of the top 5 countries with the most coronavirus cases\"\"\"\n    countries = []\n    for tuple_value in data[:5]:\n        countries.append(tuple_value[0])\n\n    return countries\n\n\ndef read_data():\n    \"\"\"Reads data from textfile rather than consistently calling the webscrap function\"\"\"\n    f = open('webscrape_data.txt')\n    file_data = []\n\n    for line in f:\n        line = line.rstrip(' \\n')\n        # if line.isdigit():\n        #     line_edited = tuple(int(line))\n        # else:\n        line_edited = tuple(map(str, line.split(' ')))\n        file_data.append(line_edited)\n\n    return file_data\n\n\ndef quick_label(rects, ax):\n    \"\"\"Creates a text number label above each bar in *rects*, displaying totals.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\ndef graph_top_affected_countries_c(sent_data):\n    \"\"\"Creates a singular bar graph that displays the confirmed cases.\"\"\"\n    # matplotlib setup portion\n    country_labels = []\n    confirmed_bars = []\n    bar_width = 0.5\n    fig, ax1 = plt.subplots()\n    fig.suptitle('Top 5 Countries Most Affected By Coronavirus')\n    fig.set_size_inches(10, 7)\n\n    # Takes the first 5 countries from data list for now\n    for tuple_value in sent_data[:5]:\n        country_labels.append(tuple_value[0])\n        confirmed_bars.append(int(tuple_value[1]))\n\n    x = np.arange(len(country_labels))\n\n    # Total Confirmed Cases Portion on Bar Graph\n    rects1 = ax1.bar(x - bar_width / 2, confirmed_bars, bar_width, label='Confirmed', color=['teal'])\n    ax1.set_ylabel('Total Confirmed Cases (Millions)')\n    # ax1.set_title('Top 5 Countries most affected by Coronavirus')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(country_labels)\n    ax1.legend()\n    quick_label(rects1, ax1)\n\n    # Gets the final graph displayed\n    fig.tight_layout()\n    # plt.savefig('images/Coronavirus_Confirmed_Cases_Graph.png')\n    plt.show()\n\n\ndef graph_top_affected_countries_d(sent_data):\n    \"\"\"Creates a singular bar graph that displays the most affected countries' deaths caused by coronavirus.\"\"\"\n    # matplotlib setup portion\n    country_labels = []\n    deaths_bars = []\n    bar_width = 0.5\n    fig, ax1 = plt.subplots()\n    fig.suptitle('Top 5 Countries Most Affected By Coronavirus')\n    fig.set_size_inches(10, 7)\n\n    # Takes the first 5 countries from data list for now\n    for tuple_value in sent_data[:5]:\n        country_labels.append(tuple_value[0])\n        deaths_bars.append(int(tuple_value[2]))\n\n    x = np.arange(len(country_labels))\n\n    # Total Confirmed Cases Portion on Bar Graph\n    rects1 = ax1.bar(x - bar_width / 2, deaths_bars, bar_width, label='Confirmed', color=['red'])\n    ax1.set_ylabel('Total Confirmed Cases (Millions)')\n    # ax1.set_title('Top 5 Countries most affected by Coronavirus')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(country_labels)\n    ax1.legend()\n    quick_label(rects1, ax1)\n\n    # Gets the final graph displayed\n    fig.tight_layout()\n    # plt.savefig('images/Coronavirus_Death_Cases_Graph.png')\n    plt.show()\n\n\ndef graph_top_affected_countries_cd(sent_data):\n    \"\"\"Creates two bar graphs and displays the information side by side.\"\"\"\n    # matplotlib setup portion\n    country_labels = []\n    confirmed_bars = []\n    deaths_bars = []\n    bar_width = 0.45\n    fig, (ax1, ax2) = plt.subplots(1, 2)  # Note: fig is figure\n    fig.suptitle('Top 5 Countries Most Affected By Coronavirus')\n    fig.set_size_inches(10, 7)\n\n    # Takes the first 5 countries from data list for now\n    for tuple_value in sent_data[:5]:\n        country_labels.append(tuple_value[0])\n        confirmed_bars.append(int(tuple_value[1]))\n        deaths_bars.append(int(tuple_value[2]))\n\n    x = np.arange(len(country_labels))\n\n    # Total Confirmed Cases Portion on Bar Graph\n    rects1 = ax1.bar(x - bar_width / 2, confirmed_bars, bar_width, label='Confirmed', color=['teal'])\n    ax1.set_ylabel('Total Confirmed Cases (Millions)')\n    # ax1.set_title('Top 5 Countries most affected by Coronavirus')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(country_labels)\n    ax1.legend()\n\n    # Total Confirmed Death Cases Portion on Bar Graph\n    rects2 = ax2.bar(x + bar_width / 2, deaths_bars, bar_width, label='Deaths', color=['red'])\n    ax2.set_ylabel('Total Death Cases')\n    ax2.set_xticks(x)\n    ax2.set_xticklabels(country_labels)\n    ax2.legend()\n\n    quick_label(rects1, ax1)\n    quick_label(rects2, ax2)\n\n    # Gets the final graph displayed\n    fig.tight_layout()\n    #plt.savefig('images/Confirmed_and_Death_Cases_Graph.png')\n    plt.show()\n\n\n# arr = np.asarray(data)  # Converts list to numpy array\n# data = read_data()\n# graph_top_affected_countries_c(data)\n\n# Texttable code used to view data from initial web-scrape, used for testing purposes\n# create texttable object\n# table = Texttable()\n# table.add_rows([(None, None, None, None)] + data)  # Adds an empty row at the beginning for the headers\n# table.set_cols_align(('c', 'c', 'c', 'c'))  # 'l' = left, 'c' = center, 'r' = right\n# table.header((' Country ', ' Confirmed Cases ', ' Deaths ', ' Continent '))\n#\n# print(table.draw())\n", "481": "from datetime import datetime\n\nimport awswrangler as wr\nimport boto3\nfrom moto import mock_ses, mock_s3\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom nba_bbref_webscrape.utils import get_leading_zeroes\nfrom nba_bbref_webscrape.aws_functions import send_aws_email, execute_email_function, write_to_s3\n\n\n@mock_ses\ndef test_ses_email(aws_credentials):\n    ses = boto3.client(\"ses\", region_name=\"us-east-1\")\n    logs = pd.DataFrame({\"errors\": [\"ex1\", \"ex2\", \"ex3\"]})\n    send_aws_email(logs)\n    assert ses.verify_email_identity(EmailAddress=\"jyablonski9@gmail.com\")\n\n\n@mock_ses\ndef test_ses_execution_logs(aws_credentials):\n    ses = boto3.client(\"ses\", region_name=\"us-east-1\")\n    logs = pd.DataFrame({\"errors\": [\"ex1\", \"ex2\", \"ex3\"]})\n    execute_email_function(logs)\n    assert ses.verify_email_identity(EmailAddress=\"jyablonski9@gmail.com\")\n\n\n@mock_ses\ndef test_ses_execution_no_logs(aws_credentials):\n    ses = boto3.client(\"ses\", region_name=\"us-east-1\")\n    logs = pd.DataFrame({\"errors\": []})\n    send_aws_email(logs)\n    assert ses.verify_email_identity(EmailAddress=\"jyablonski9@gmail.com\")\n\n\n@mock_s3\ndef test_write_to_s3_validated(player_stats_data):\n    conn = boto3.resource(\"s3\", region_name=\"us-east-1\")\n    today = datetime.now().date()\n    month = datetime.now().month\n    month_prefix = get_leading_zeroes(month)\n    player_stats_data.schema = \"Validated\"\n    conn.create_bucket(Bucket=\"moto_test_bucket\")\n\n    write_to_s3(\"player_stats_data\", player_stats_data, bucket=\"moto_test_bucket\")\n    bucket = conn.Bucket(\"moto_test_bucket\")\n    contents = [_.key for _ in bucket.objects.all()]\n\n    assert (\n        contents[0]\n        == f\"player_stats_data/validated/{month_prefix}/player_stats_data-{today}.parquet\"\n    )\n\n\n@mock_s3\ndef test_write_to_s3_invalidated(player_stats_data):\n    conn = boto3.resource(\"s3\", region_name=\"us-east-1\")\n    today = datetime.now().date()\n    month = datetime.now().month\n    month_prefix = get_leading_zeroes(month)\n    player_stats_data.schema = \"Invalidated\"\n    conn.create_bucket(Bucket=\"moto_test_bucket\")\n\n    write_to_s3(\"player_stats_data\", player_stats_data, bucket=\"moto_test_bucket\")\n    bucket = conn.Bucket(\"moto_test_bucket\")\n    contents = [_.key for _ in bucket.objects.all()]\n\n    assert (\n        contents[0]\n        == f\"player_stats_data/invalidated/{month_prefix}/player_stats_data-{today}.parquet\"\n    )\n", "482": "# from Problem2 import problem2dhl as P2\nfrom Problem2 import problem2 as P2\nfrom Problem1 import p1Q1 as P1q1\nfrom Problem1 import p1Q2 as P1q2\nfrom Problem1 import p1Q4 as P1q4\nimport problem3 as P3\n# from Problem3 import problem3 as P3\n\nscore = {}\nCOURIER_NAME = [\"cityLE\", \"posLaju\", \"gdex\", \"jnt\", \"dhl\"]\n\n\ndef webScrape():\n    global score\n    score[COURIER_NAME[0]] = P2.cityLink(\n        \"http://autoworld.com.my/news/2020/09/25/city-link-express-takes-delivery-of-277-new-isuzu-trucks/\",\n        \"https://www.thestar.com.my/business/business-news/2015/01/05/citylink-mulls-main-market-listing-in-three-years\",\n        \"https://www.thesundaily.my/gear-up/isuzu-lorries--city-link-s-preferred-choice-AK729310\",\n    )\n\n    score[COURIER_NAME[1]] = P2.posLaju(\n        \"https://www.thestar.com.my/business/business-news/2021/02/22/pos-malaysia-records-rm233b-revenue-in-fy20\",\n        \"https://soyacincau.com/2020/08/15/pos-malaysia-e-consignment-notes-qr-code-available/\",\n        \"https://www.theborneopost.com/2020/07/08/poslaju-customers-urged-to-bear-with-longer-waiting-time/\",\n    )\n\n    score[COURIER_NAME[2]] = P2.gdex(\n        \"https://www.theedgemarkets.com/article/gdex-stands-benefit-pickup-ecommerce-activities-says-kenanga-research\",\n        \"https://www.theedgemarkets.com/article/gdex-look-creating-industrial-reit-part-next-growth-phase\",\n        \"https://www.theedgemarkets.com/article/gdex-2q-net-profit-down-absence-gain-warns-covid19-impact\",\n    )\n\n    score[COURIER_NAME[3]] = P2.jnt(\n        \"https://www.straitstimes.com/asia/se-asia/pandemic-fuelled-e-shopping-boom-spurs-courier-firms-growth\",\n        \"https://www.theedgemarkets.com/article/indonesias-jt-express-said-weigh-us1-billionplus-us-ipo\",\n        \"https://kr-asia.com/one-masters-two-apprentices-how-indonesias-jt-express-rose-in-china-on-the-back-of-pinduoduo\",\n    )\n\n    score[COURIER_NAME[4]] = P2.dhl(\n        \"https://www.theedgemarkets.com/article/tech-digitalisation-way-forward-dhl-express\",\n        \"https://www.theedgemarkets.com/article/dhl-predicts-strong-growth-b2b-ecommerce\",\n        \"https://www.theedgemarkets.com/article/special-report-rocky-road-ahead-logistics-operators-amid-pandemic\",\n    )\n\n\n\nP1q1.start()\nP1q2.start()\nP1q4.start()\n# webScrape()\n# P3.start(score)", "483": "from urllib.request import urlopen as uReq\nfrom bs4 import BeautifulSoup as soup    \nimport os, sys, datetime, time\n\ndef get_date(Datestr):\n\tDateList=Datestr.split('. ')\n\tif (len(DateList)==2):\n\t\tMonthNameDict = {\t\n\t\t'ledna' : 1,\n\t\t'\u00fanora' : 2,\n\t\t'b\u0159ezna' : 3,\n\t\t'dubna' : 4,\n\t\t'kv\u011btna' : 5,\n\t\t'\u010dervna' : 6,\n\t\t'\u010dervence' : 7,\n\t\t'srpna' : 8,\n\t\t'z\u00e1\u0159\u00ed' : 9,\n\t\t'\u0159\u00edjna' : 10,\n\t\t'listopadu' : 11,\n\t\t'prosince' : 12,\n\t\t}\n\t\tConvertedDateStr=str(DateList[0])+'|'+str(MonthNameDict[DateList[1]])+'|'+str(datetime.datetime.now().year)\n\t\treturn datetime.datetime.strptime(ConvertedDateStr,'%d|%m|%Y')\n\telif(len(DateList)>2):\n\t\tif(len(Datestr.split(','))>0):\n\t\t\treturn datetime.datetime.strptime(Datestr, '%d. %m. %Y, %H:%M')\n\n\telse:\n\t\tToday=datetime.datetime.now()\n\t\tDayNameDict = {\n\n\t\t'p\u0159edev\u010d\u00edrem':2,\n\t\t'v\u010dera':1,\n\t\t'dnes':0,\n\t\t}\n\n\t\tif(len(Datestr.split('.'))>1):\n\t\t\treturn datetime.datetime.strptime(Datestr,'%d.%m.%Y')\n\t\telse:\n\t\t\treturn datetime.datetime.now()-datetime.timedelta(days=DayNameDict[Datestr])\n\ndef Webscrape_head(page_soup):\n\tNazev = page_soup.find(\"h1\",{\"itemprop\":\"name\"}).text\n\tBookInfoFile=open(\"/mnt/minerva1/nlp/projects/sentiment9/Results/BookInfo.tsv\", \"a\",encoding=\"utf-8\")\n\n\tAutor = page_soup.find(\"span\",{\"itemprop\":\"author\"}).text\n\tZanry = page_soup.select('a[href*=\"zanr\"]')\n\n\ttry:\n\t\tAnotacePart1 = page_soup.find(\"span\",{\"class\":\"start_text\"})\n\t\tif (AnotacePart1 is not None):\n\t\t\tAnotacePart1=AnotacePart1.text.replace(chr(13),'').replace('\\n',' ').replace('  ','').strip()\n\t\t\tAnotacePart2 = page_soup.find(\"span\",{\"class\":\"end_text\"})\n\t\t\tAnotacePart2=AnotacePart2.text.replace(chr(13),'').replace('\\n',' ').replace('  ','').strip()\n\t\t\tAnotace = str(AnotacePart1)+str(AnotacePart2)\n\t\telse:\n\t\t\tAnotace = page_soup.find(\"p\",{\"id\":\"bdetdesc\"}).text.replace(chr(13),'').replace('\\n',' ').replace('  ','').strip()\n\n\texcept AttributeError:\n\t\tAnotace = '??'\n\n\tBookInfoFile.write(\"Nazev: \"+Nazev+'\\n')\n\tfor genre in Zanry:\n\t\tBookInfoFile.write(\"Zanr: \"+genre.text+'\\n')\n\tBookInfoFile.write(\"Autor: \"+Autor+'\\n')\n\tBookInfoFile.write(\"Anotace: \"+Anotace+'\\n')\n\n\tgenre_list=[]\n\tfor genre in Zanry:\n\t\tgenre_list.append(genre.text)\n\n\tBookInfoFile.write(Nazev+'\\t'+author+'\\t'+','.join(genre_list)+'\\t'+Anotace+'\\n')\n\n\tBookInfoFile.close()\n\n\ndef Webscrape_reviews(my_url):\n\tif('c=all' not in my_url):\n\t\tmy_url = my_url + '?c=all'\n\t#otevre url a precte html zadaneho url\n\tuClient = uReq(my_url)\n\tpage_html = uClient.read()\n\tuClient.close()\n\n\t#vyhledani hledanych dat v html\n\tpage_soup = soup(page_html, \"html.parser\")\n\n\tNazev = page_soup.find(\"h1\",{\"itemprop\":\"name\"}).text\n\n\t#Webscrape_head(page_soup)\n\n\tDatabazeKnihReviews = open(\"/mnt/minerva1/nlp/projects/sentiment9/Results/Reviews.tsv\", \"a\",encoding=\"utf-8\")\n\n\trewiev_page_count=1\n\tfor rewiev_page in range(1,rewiev_page_count+1):\n\t\tif(rewiev_page != 1):\n\t\t\t#zmena url a precteni\n\t\t\tmy_url=my_url+'&str='+str(rewiev_page)\n\t\t\t#vyhledani recenzi v html\n\t\t\tuClient = uReq(my_url)\n\t\t\tpage_html = uClient.read()\n\t\t\tuClient.close()\n\t\t\tpage_soup = soup(page_html, \"html.parser\")\n\t\treviews = page_soup.findAll(\"div\",{\"class\":\"komentars_user komover\"})\n\t\treviews+= page_soup.findAll(\"div\",{\"class\":\"komentars_user_last komover\"})\n\t\tfor review in reviews:\n\t\t\tusername=review.div.a.text\n\n\t\t\tlikes=review.div.div.em\n\t\t\tif(likes is not None):\n\t\t\t\tlikes=likes.text\n\t\t\telse:\n\t\t\t\tlikes=0\n\t\t\tdate=review.div.div.span\n\t\t\tif(date is not None):\n\t\t\t\tdate=get_date(date.text)\n\t\t\telse:\n\t\t\t\tdate='??'\n\n\t\t\trating=review.div.div.img\n\t\t\tif(rating is not None):\n\t\t\t\trating=rating[\"src\"][:-4]\n\t\t\t\trating=int(rating[-1])*20\n\t\t\telse:\n\t\t\t\trating='??'\n\t\t\t\n\t\t\tcomment=review.div.p.text.replace(chr(13),'').replace('\\n',' ').replace('  ','').strip()\n\n\t\t\tDatabazeKnihReviews.write(\"DatabazeKnih\"+'\\t'+Nazev+'\\t'+username+'\\t'+str(likes)+'\\t'+str(date)+'\\t'+str(rating)+'\\t'+comment+'\\n') \n\t\ttime.sleep(2)#delay mezi pristupy aby nespadl server\n\ttime.sleep(2)\n\n\tDatabazeKnihReviews.close()", "484": "'''\nCreated on Aug 9, 2021\n\n@author: Jacob Summers\n'''\nfrom webscrape.searchresults.categorydetails.gamedetail.gameparse.game_user_parser import game_user_parser\nfrom webscrape.searchresults.categorydetails.gamedetail.gamedisplay import game_view_review_detail\nfrom webscrape import clear\n#represents the parser that parses the review pages\nparser = 0\n\n#goes to the next page of reviews\ndef _next_page(page, results):\n    #check to see if the page exists\n    if (page > parser.get_last_page()):\n        print(\"Page does not exist\")\n        #view the results again\n        view_results(results)\n        return\n    \n    #move the parser to the next page\n    parser.move_to_page(page)\n    \n    #get the new review results from the parser\n    results = parser.get_results()\n    #view them\n    view_results(results)\n    \n#print the reviews that are on the given page\ndef print_page_reviews(results):\n    #clear the command line output\n    clear()\n    \n    #number of reviews on the page\n    number = 1\n    #loop through each review and print it\n    for result in results:\n        print(str(number) + \". \" + result.to_string())\n        number = number + 1\n    #if the number is still 1, then the for loop was never entered due to results being empty\n    if (number == 1):\n        print(\"There are no user reviews on this page\")\n        \n    #print the current page\n    print(\"Page \" + str(parser.get_current_page()) + \" of \" + str(parser.get_last_page()))  \n        \n#displays the reviews and prompts the user to navigate the pages of reviews\ndef view_results(results):\n    \n    #prints the reviews\n    print_page_reviews(results)\n    \n    #prompts the user to either enter a page number or view a review\n    number = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n    \n    #if the number is invalid\n    if (number.isdigit() == False):\n        number = -1\n    #if the number is zero then the user wants to navigate to a different page\n    if (number == \"0\"):\n        #prompt user for page number\n        page = input(\"Enter page number\\n\")\n        #if the page is not valid\n        if (page.isdigit() == False):\n            page = 0\n        \n        page =  int(page)\n        #go to the next page    \n        _next_page(page, results)\n        \n        return\n    #cast the user input into a number\n    number = int(number)\n    \n    #while the number is a valid number\n    while (number > 0 and number < len(results) + 1):\n        \n        #get the chosen review\n        chosenresult = results[number-1]\n        \n        #view the details of the review here\n        game_view_review_detail.view_review_detail(chosenresult)\n        \n        \n        print_page_reviews(results)\n        \n        \n        #prompts the user to either view a new page or to view a search result\n        number = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n            #if the number is invalid\n        if (number.isdigit() == False):\n            number = -1\n        \n        #if the number is zero then the user wants to navigate to a different page\n        if (number == \"0\"):\n            #prompt user for page number\n            page = input(\"Enter page number\\n\")\n            #if the page is not valid\n            if (page.isdigit() == False):\n                page = 0\n            \n            page =  int(page)\n                \n            #go to the next page \n            _next_page(page, results)\n        \n            return\n    \n        number = int(number)\n        \n\n#view the user reviews of the movie      \ndef view_game_user_reviews(user_link):\n    global parser\n    \n    #create the parser for the review\n    parser = game_user_parser(user_link)\n    \n    #get the reviews on the page\n    results = parser.get_results()\n    \n    #view the results\n    view_results(results)", "485": "'''\nCreated on Aug 10, 2021\n\n@author: Jacob Summers\n'''\nfrom webscrape.searchresults.categorydetails.tvdetail.tvparse.tv_user_parser import tv_user_parser\nfrom webscrape.searchresults.categorydetails.tvdetail.tvdisplay import tv_view_review_detail\nfrom webscrape import clear\n\n#represents the parser that parses the review pages\nparser = 0\n\n#goes to the next page of reviews\ndef _next_page(page, results):\n    #check to see if the page exists\n    if (page > parser.get_last_page()):\n        print(\"Page does not exist\")\n        #view the results again\n        view_results(results)\n        return\n    \n    #move the parser to the next page\n    parser.move_to_page(page)\n    \n    #get the new review results from the parser\n    results = parser.get_results()\n    #view them\n    view_results(results)\n    \n#print the reviews that are on the given page\ndef print_page_reviews(results):\n    #clears the command line output\n    clear()\n    #number of reviews on the page\n    number = 1\n    #loop through each review and print it\n    for result in results:\n        print(str(number) + \". \" + result.to_string())\n        number = number + 1\n    #if the number is still 1, then the for loop was never entered due to results being empty\n    if (number == 1):\n        print(\"There are no user reviews on this page\")\n        \n    #print the current page\n    print(\"Page \" + str(parser.get_current_page()) + \" of \" + str(parser.get_last_page()))  \n        \n#displays the reviews and prompts the user to navigate the pages of reviews\ndef view_results(results):\n    \n    #prints the reviews\n    print_page_reviews(results)\n    \n    #prompts the user to either enter a page number or view a review\n    number = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n    \n    #if the number is invalid\n    if (number.isdigit() == False):\n        number = -1\n    #if the number is zero then the user wants to navigate to a different page\n    if (number == \"0\"):\n        #prompt user for page number\n        page = input(\"Enter page number\\n\")\n        #if the page is not valid\n        if (page.isdigit() == False):\n            page = 0\n        \n        page =  int(page)\n        #go to the next page    \n        _next_page(page, results)\n        \n        return\n    #cast the user input into a number\n    number = int(number)\n    \n    #while the number is a valid number\n    while (number > 0 and number < len(results) + 1):\n        \n        #get the chosen review\n        chosenresult = results[number-1]\n        \n        #view the details of the review here\n        tv_view_review_detail.view_review_detail(chosenresult)\n        \n        \n        print_page_reviews(results)\n        \n        \n        #prompts the user to either view a new page or to view a search result\n        number = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n            #if the number is invalid\n        if (number.isdigit() == False):\n            number = -1\n        \n        #if the number is zero then the user wants to navigate to a different page\n        if (number == \"0\"):\n            #prompt user for page number\n            page = input(\"Enter page number\\n\")\n            #if the page is not valid\n            if (page.isdigit() == False):\n                page = 0\n            \n            page =  int(page)\n                \n            #go to the next page \n            _next_page(page, results)\n        \n            return\n    \n        number = int(number)\n        \n\n#view the user reviews of the movie      \ndef view_tv_user_reviews(user_link):\n    global parser\n    \n    #create the parser for the review\n    parser = tv_user_parser(user_link)\n    \n    #get the reviews on the page\n    results = parser.get_results()\n    \n    #view the results\n    view_results(results)", "486": "'''\nCreated on Aug 8, 2021\n\n@author: Jacob Summers\n'''\n\nfrom webscrape.searchresults.categorydetails.moviedetail.movieparse.movie_user_parser import movie_user_parser\nfrom webscrape.searchresults.categorydetails.moviedetail.moviedisplay import movie_view_review_detail\nfrom webscrape import clear\n\n#represents the parser that parses the review pages\nparser = 0\n\n#goes to the next page of reviews\ndef _next_page(page, results):\n    #check to see if the page exists\n    if (page > parser.get_last_page()):\n        print(\"Page does not exist\")\n        #view the results again\n        view_results(results)\n        return\n    \n    #move the parser to the next page\n    parser.move_to_page(page)\n    \n    #get the new review results from the parser\n    results = parser.get_results()\n    #view them\n    view_results(results)\n    \n#print the reviews that are on the given page\ndef print_page_reviews(results):\n    #clears the command line output\n    clear()\n    #number of reviews on the page\n    number = 1\n    #loop through each review and print it\n    for result in results:\n        print(str(number) + \". \" + result.to_string())\n        number = number + 1\n    #if the number is still 1, then the for loop was never entered due to results being empty\n    if (number == 1):\n        print(\"There are no user reviews on this page\")\n        \n    #print the current page\n    print(\"Page \" + str(parser.get_current_page()) + \" of \" + str(parser.get_last_page()))  \n        \n#displays the reviews and prompts the user to navigate the pages of reviews\ndef view_results(results):\n    \n    #prints the reviews\n    print_page_reviews(results)\n    \n    #prompts the user to either enter a page number or view a review\n    number = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n    \n    #if the number is invalid\n    if (number.isdigit() == False):\n        number = -1\n    #if the number is zero then the user wants to navigate to a different page\n    if (number == \"0\"):\n        #prompt user for page number\n        page = input(\"Enter page number\\n\")\n        #if the page is not valid\n        if (page.isdigit() == False):\n            page = 0\n        \n        page =  int(page)\n        #go to the next page    \n        _next_page(page, results)\n        \n        return\n    #cast the user input into a number\n    number = int(number)\n    \n    #while the number is a valid number\n    while (number > 0 and number < len(results) + 1):\n        \n        #get the chosen review\n        chosenresult = results[number-1]\n        \n        #view the details of the review here\n        movie_view_review_detail.view_review_detail(chosenresult)\n        \n        \n        print_page_reviews(results)\n        \n        \n        #prompts the user to either view a new page or to view a search result\n        number = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n            #if the number is invalid\n        if (number.isdigit() == False):\n            number = -1\n        \n        #if the number is zero then the user wants to navigate to a different page\n        if (number == \"0\"):\n            #prompt user for page number\n            page = input(\"Enter page number\\n\")\n            #if the page is not valid\n            if (page.isdigit() == False):\n                page = 0\n            \n            page =  int(page)\n                \n            #go to the next page \n            _next_page(page, results)\n        \n            return\n    \n        number = int(number)\n        \n\n#view the user reviews of the movie      \ndef view_movie_user_reviews(user_link):\n    global parser\n    \n    #create the parser for the review\n    parser = movie_user_parser(user_link)\n    \n    #get the reviews on the page\n    results = parser.get_results()\n    \n    #view the results\n    view_results(results)\n        \n        \n    ", "487": "'''\nCreated on Aug 10, 2021\n\n@author: Jacob Summers\n'''\nfrom webscrape.searchresults.categorydetails.albumdetail.albumparse.album_user_parser import album_user_parser\nfrom webscrape.searchresults.categorydetails.albumdetail.albumdisplay import album_view_review_detail\nfrom webscrape import clear\n\n#represents the parser that parses the review pages\nparser = 0\n\n#goes to the next page of reviews\ndef _next_page(page, results):\n    #check to see if the page exists\n    if (page > parser.get_last_page()):\n        print(\"Page does not exist\")\n        #view the results again\n        view_results(results)\n        return\n    \n    #move the parser to the next page\n    parser.move_to_page(page)\n    \n    #get the new review results from the parser\n    results = parser.get_results()\n    #view them\n    view_results(results)\n    \n#print the reviews that are on the given page\ndef print_page_reviews(results):\n    #clear command line output\n    clear()\n    #number of reviews on the page\n    number = 1\n    #loop through each review and print it\n    for result in results:\n        print(str(number) + \". \" + result.to_string())\n        number = number + 1\n    #if the number is still 1, then the for loop was never entered due to results being empty\n    if (number == 1):\n        print(\"There are no user reviews on this page\")\n        \n    #print the current page\n    print(\"Page \" + str(parser.get_current_page()) + \" of \" + str(parser.get_last_page()))  \n        \n#displays the reviews and prompts the user to navigate the pages of reviews\ndef view_results(results):\n    \n    #prints the reviews\n    print_page_reviews(results)\n    \n    #prompts the user to either enter a page number or view a review\n    number = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n    \n    #if the number is invalid\n    if (number.isdigit() == False):\n        number = -1\n    #if the number is zero then the user wants to navigate to a different page\n    if (number == \"0\"):\n        #prompt user for page number\n        page = input(\"Enter page number\\n\")\n        #if the page is not valid\n        if (page.isdigit() == False):\n            page = 0\n        \n        page =  int(page)\n        #go to the next page    \n        _next_page(page, results)\n        \n        return\n    #cast the user input into a number\n    number = int(number)\n    \n    #while the number is a valid number\n    while (number > 0 and number < len(results) + 1):\n        \n        #get the chosen review\n        chosenresult = results[number-1]\n        \n        #view the details of the review here\n        album_view_review_detail.view_review_detail(chosenresult)\n        \n        \n        print_page_reviews(results)\n        \n        \n        #prompts the user to either view a new page or to view a search result\n        number = input(\"Enter the number that corresponds with the result you would like to view. Enter invalid number to exit\\n OR \\nType 0 to change the page\\n\")\n            #if the number is invalid\n        if (number.isdigit() == False):\n            number = -1\n        \n        #if the number is zero then the user wants to navigate to a different page\n        if (number == \"0\"):\n            #prompt user for page number\n            page = input(\"Enter page number\\n\")\n            #if the page is not valid\n            if (page.isdigit() == False):\n                page = 0\n            \n            page =  int(page)\n                \n            #go to the next page \n            _next_page(page, results)\n        \n            return\n    \n        number = int(number)\n        \n\n#view the user reviews of the movie      \ndef view_album_user_reviews(user_link):\n    global parser\n    \n    #create the parser for the review\n    parser = album_user_parser(user_link)\n    \n    #get the reviews on the page\n    results = parser.get_results()\n    \n    #view the results\n    view_results(results)", "488": "from setuptools import setup\n\nsetup(\n    name=\"appg\",\n    version='0.0.1',\n    packages=['appg'],\n    install_requires=[\n        'Click',\n        'requests',\n        'lxml',\n        'pandas',\n        'beautifulsoup4',\n        'html5lib', \n        'tqdm'\n    ],\n    entry_points='''\n        [console_scripts]\n        appg=appg.cli:webscrape\n    ''',\n)", "489": "from classes import *\n\ndef func_faixae():\n    faixa_etaria = Faixa('obitos','fx_etaria','total')\n    faixa_etaria.get().columns = ['faixa etaria', 'obitos']\n    faixa_etaria.agrupar('faixa etaria')\n    return faixa_etaria\n\ndef c_diarios():\n    #fazendo dataframe apenas com data e casos confirmados\n    casos_diarios = Faixa('municipios','data','confirmados')\n    casos_diarios.get().columns = ['data','casos confirmados']\n    return casos_diarios\n\ndef o_diarios():\n    #fazendo dataframe apenas com data e obitos\n    obitos_diarios = Faixa('municipios','data','obitos')\n    return obitos_diarios\n\n\ndef funcao_webscrape():\n    populacao = WebScraping()\n\n    #adicionando nomes das colunas, porque o pandas n\u00e3o os via como string, ainda aproveitando pra remover o que n preciso\n    populacao.get().columns = ['Posicao', 'Municipio', 'Populacao', 'lixo']\n\n    #removendo todas as linhas que cont\u00e9m \"habitantes\"\n    populacao.dropColumnValue('Posicao','habitantes')\n\n    #removendo as colunas que n\u00e3o preciso\n    populacao.dropColumn('Posicao','lixo')\n\n    #Colocando valores em ordem alfabetica\n    populacao.sort('Municipio')\n\n    #tirando espa\u00e7o em branco dos numeros\n    populacao.rmSpaceInt('Populacao')\n\n    #renomeando algumas coisas escritas erradas.\n    populacao.replace(9,'Municipio', 'Ar\u00eas')\n    populacao.replace(10,'Municipio','A\u00e7u')\n    populacao.replace(18,'Municipio', 'Brejinho')\n    populacao.replace(96,'Municipio', 'Passa e Fica')\n    populacao.replace(133,'Municipio', 'S\u00e3o Bento do Trair\u00ed')\n    return populacao\n\n\ndef func_mediaCem():\n    #fazendo dataframe apenas com municipio e casos confirmados\n    cemMil = Media100mil('municipios','mun_residencia','obitos')\n    cemMil.get().columns = ['Municipio','obitos']\n\n\n    #agrupar todos os valores, sort para deixar em ordem alfabetica baseada nos munic\u00edpios e resetar o \u00edndice\n    cemMil.agruparSort('Municipio')\n\n    cemMil.replace(10,'Municipio','Campo Grande')\n    cemMil.replace(55,'Municipio','Boa Sa\u00fade')\n\n    cemMil.sort('Municipio')\n\n    populacao = funcao_webscrape()\n\n    #concatenar a coluna do dataframe obtido pelo webscraping\n    cemMil.createColumn('Populacao',populacao.get(),'Populacao')\n    cemMil.createColumnMedia('Mortes por 100 mil','obitos',populacao.get(),'Populacao')\n    #cemMil.get().head()\n    return cemMil", "490": "import cv2\r\nimport numpy as np\r\nfrom PIL import ImageGrab\r\nimport pytesseract\r\nfrom webscrape import findWord\r\nimport pydirectinput\r\nimport time\r\n\r\n#get string from picture\r\ndef OCR(yest):\r\n    custom_config = r'-l eng --oem 3 --psm 6'\r\n    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\r\n    text = pytesseract.image_to_string(yest, config=custom_config)\r\n    return text\r\n\r\n#types a word from string?\r\ndef typeword(word):\r\n    print(word)\r\n    try:\r\n        for letter in word:\r\n            pydirectinput.write(letter)\r\n    except TypeError:\r\n        return(\"rbeoiwbghbewioh\")\r\n    pydirectinput.press('enter')\r\n'''sing happy birthday to someone'''\r\n\r\n#just loops. Too lazy to use while loop\r\nfor i in range(0,1000000):\r\n    #parameters for color mask for text\r\n    lower = np.array([10, 10, 30])\r\n    upper = np.array([25, 25, 45])\r\n    \r\n    #grabs image where the roblox prompt is\r\n    img1 = ImageGrab.grab(bbox=(200, 660, 480, 720))\r\n    img2 = ImageGrab.grab(bbox=(200, 390, 480, 450))\r\n    \r\n    #img into numpy array\r\n    img_np1 = np.array(img1)\r\n    img_np2 = np.array(img2)\r\n    \r\n    #convers numpy image to grayscale\r\n    frame1 = cv2.cvtColor(img_np1, cv2.COLOR_BGR2GRAY)\r\n    frame2 = cv2.cvtColor(img_np2, cv2.COLOR_BGR2GRAY)\r\n    \r\n    #creates mask\r\n    mask1 = cv2.inRange(img_np1, lower, upper)\r\n    mask2 = cv2.inRange(img_np2, lower, upper)\r\n\r\n    #inverts color\r\n    mask1 = cv2.bitwise_not(mask1)\r\n    mask2 = cv2.bitwise_not(mask2)\r\n    \r\n    #i copy and pasted this from somewhere on the internet\r\n    if cv2.waitKey(1) & 0Xff == ord('q'):\r\n        break\r\n    \r\n    #grabs text from mask\r\n    text1 = OCR(mask1)\r\n    text2 = OCR(mask2)\r\n    \r\n    #formats text\r\n    fragment1 = text1.strip().lower()\r\n    fragment2 = text2.strip().lower()\r\n    \r\n    #finds word in Webscrape.py\r\n    if len(fragment1) != 0:\r\n        final1 = findWord(fragment1)\r\n    if len(fragment2) != 0:\r\n        final2 = findWord(fragment2)\r\n\r\n    #types words\r\n    try:\r\n        print(fragment1)\r\n        print(fragment2)\r\n        typeword(final1)\r\n        typeword(final2)\r\n    except NameError:\r\n        continue\r\n\r\n\r\ncv2.destroyAllWindows()\r\n\r\n", "491": "# %%\ndef webscrape_DRG(*code):\n    \n    #Load required libraries\n    import pandas as pd\n    import requests\n    from bs4 import BeautifulSoup\n    import re\n\n    #Set max table column width and allow text wrapping. \n    pd.set_option('display.max_colwidth', 0)\n\n    #Define the DRG codes that will be webscraped from the website\n    DRG_codes = [*code]\n\n    #Base url that will be used to generate the final url in the build_url function. \n    baseurl = \"https://www.findacode.com/tools/code-print.php?set=DRG&c=\"\n\n    #Define the function to create the url that will be scraped.\n    def build_url(DRGcode):\n        return baseurl + DRGcode\n    urls = []\n    for DRGcode in DRG_codes:\n        urls.append(build_url(DRGcode))\n\n    #Table to display the results    \n    data = {\"URLS\": urls[:]}\n\n    df = pd.DataFrame(data)\n\n    #Print the table in the output\n    df\n    \n    #Create empty lists that will hold data from webscraped DRG codes\n    code = []\n    FCP = []\n    FOP = []\n\n    #Web scraping using a for loop that passes through all URLs\n    for url in urls[:]:\n        page = requests.get(url).text\n        soup = BeautifulSoup(page, 'html.parser')\n        name = soup.find('blockquote')\n        name.br.extract()\n        code.append(name.text[1:-213])\n        FOP.append(soup.find(id=\"drg_t_op_pmt\").getText()) #id in the DRG website for the Federal Operating Payment\n        FCP.append(soup.find(id=\"drg_t_cp_pmt\").getText()) #id in the DRG website for the Federal Capital Payment\n\n    FCP = [item.strip() for item in FCP]\n    FOP = [item.strip() for item in FOP]\n\n    #Column names for the df table \n    df[\"DRG Codes\"] = code\n    df[\"Federal Capital Payment\"] = FCP\n    df[\"Federal Operating Payment\"] = FOP\n\n    intFCP = []\n\n    #Extracting only the numbers from the extracted data. \n    for element in FCP:\n        x= re.findall(r\"\\$[^\\]]+\", element)\n        x = ''.join(x)\n        x = x.replace(\"$\",\"\").replace(\",\",\"\")\n        intFCP.append(float(x))\n\n    intFOP = []\n\n    for element in FOP:\n        x= re.findall(r\"\\$[^\\]]+\", element)\n        x = ''.join(x)\n        x= x.replace(\"$\",\"\").replace(\",\",\"\")\n        intFOP.append(float(x))\n        \n    #Sum of the two numbers FOP + FCP\n    sum_list = [a + b for a, b in zip(intFCP, intFOP)]\n\n    #Empty list for the Total Federal Payment\n    TFP = []\n\n    #Apply the $ formatting to the number\n    for element in sum_list:\n        TFP.append(\"${:,.2f}\".format(element))\n    \n    #Column name     \n    df[\"Total Federal Payment\"] = TFP\n\n    #Return the data table\n    return(df)\n\n#%%\nwebscrape_DRG(\"453\",\"570\",\"946\",\"947\", \"642\", \"543\", \"948\")\n", "492": "from bs4 import BeautifulSoup\nimport requests\nimport csv\n\nurl = \"https://en.wikipedia.org/wiki/List_of_brightest_stars_and_other_record_stars\"\nwebpage = requests.get(url)\nsoup = BeautifulSoup(webpage.content, 'html.parser')\n\nheaders = ['NAME', 'DISTANCE', 'MASS', 'RADIUS']\n\n\ndef webscrape():\n\n    data = []\n    # Accessing table\n    table = soup.find(\"table\", attrs={'class', 'wikitable'})\n    table_body = table.find('tbody')\n    # Table Row Tags\n    tr_tags = table_body.find_all('tr')\n    for i in tr_tags:\n        td_tags = i.find_all('td')\n        temp = []\n        # Looping through row\n        for index, j in enumerate(td_tags):\n            # For  tag value\n            if(index == 0):\n                a_tag = j.find_all('a')\n                # Some of them are plain text, not \n                if(a_tag):\n                    temp.append(a_tag[0].contents[0])\n                else:\n                    temp.append(j.text)\n            #Distance, Mass, Radius\n            elif(index == 5 or index == 7 or index == 8):\n                temp.append(j.text.strip(\"0\").strip('\\n'))\n\n        data.append(temp)\n        print(data)\n        with open('data.csv', 'w') as f:\n            fs = csv.writer(f)\n            fs.writerow(headers)\n            fs.writerows(data)\n\n\nwebscrape()\n", "493": "from bs4 import BeautifulSoup\r\nimport requests\r\nimport csv\r\nimport re\r\n\r\nedicoes = [1527]\r\nnum_gravados = 0\r\n\r\ndef webscrape(numeros):\r\n    global num_gravados\r\n    url = f'http://www.uel.br/revistas/uel/index.php/informacao/issue/view/{numeros}'\r\n    source = requests.get(url).text\r\n    soup = BeautifulSoup(source, 'lxml')\r\n    lidos = 0\r\n   \r\n    for texto in soup.find_all('table', class_='tocArticle'):\r\n        link = texto.find('a', attrs={'href': re.compile(\"http://\")})\r\n        link2 = str(link.get('href')[65:70])\r\n        titulo = texto.find('div', class_='tocTitle').text.strip()\r\n        autor = texto.find('div', class_='tocAuthors').text.strip().replace('\\t', '')\r\n        print(f'T\u00edtulo do trabalho: {titulo}')\r\n        print(f'Autores: {autor}')\r\n        print(f'Link: {link2}')\r\n        print()\r\n        csv_writer.writerow([titulo, autor])\r\n        lidos += 1\r\n        num_gravados += 1\r\n    return lidos > 0\r\n\r\n\r\n\r\n# realiza opera\u00e7\u00f5es com arquivos csv\r\narquivo_csv = open('InfoeInfo.csv', 'w', encoding='utf-8')\r\ncsv_writer = csv.writer(arquivo_csv)\r\ncsv_writer.writerow(['titulo', 'autor'])\r\n\r\nfor item in edicoes:\r\n    webscrape(item)\r\n\r\narquivo_csv.close()\r\n\r\nprint(f'Total de registros gravados: {num_gravados}')\r\n\r\n", "494": "# Generated by Django 2.1.7 on 2019-03-24 19:17\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\nimport django.utils.timezone\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('webscrape', '0003_auto_20190323_1501'),\n        ('dataprocess', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='TempProcLandData',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('price', models.IntegerField(default=0, null=True)),\n                ('latitude', models.FloatField(default=0)),\n                ('longitude', models.FloatField(default=0)),\n                ('land_type', models.CharField(max_length=20, null=True)),\n                ('land_size', models.FloatField(default=0, null=True)),\n                ('land_availability', models.BooleanField(default=0)),\n                ('date_collected', models.DateTimeField(default=django.utils.timezone.now, null=True)),\n                ('raw', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, to='webscrape.RawLandData')),\n            ],\n        ),\n    ]\n", "495": "from bs4 import BeautifulSoup\r\nimport requests\r\nimport csv\r\n\r\nedicoes = [34617]\r\nnum_gravados = 0\r\n\r\ndef webscrape(numeros):\r\n    global num_gravados\r\n    url = f'http://www.uel.br/revistas/uel/index.php/informacao/article/view/{numeros}'\r\n    source = requests.get(url).text\r\n    soup = BeautifulSoup(source, 'lxml')\r\n    lidos = 0\r\n   \r\n    for texto in soup.find_all('div', id='content'):\r\n        titulo = texto.find('div', id='articleTitle').text.strip()\r\n        autor = texto.find('div', id='authorString').text.strip()\r\n        resumo = texto.find('div', id='articleAbstract').text.strip().replace('Resumo', '').replace('\\n', '')\r\n        keywords = texto.find('div', id='articleSubject').text.strip().replace('Palavras-chave', '').replace('\\n', '')\r\n        print(f'T\u00edtulo do trabalho: {titulo}')\r\n        print(f'Autores: {autor}')\r\n        print(f'Resumo: {resumo}')\r\n        print(f'Palavras-Chave: {keywords}')\r\n        print()\r\n        #csv_writer.writerow([titulo, autor])\r\n        lidos += 1\r\n        num_gravados += 1\r\n    return lidos > 0\r\n\r\n\r\n\r\n# realiza opera\u00e7\u00f5es com arquivos csv\r\narquivo_csv = open('InfoeInfoArticles.csv', 'w', encoding='utf-8')\r\ncsv_writer = csv.writer(arquivo_csv)\r\ncsv_writer.writerow(['titulo', 'autor'])\r\n\r\nfor item in edicoes:\r\n    webscrape(item)\r\n\r\narquivo_csv.close()\r\nprint(f'Total de registros gravados: {num_gravados}')", "496": "import requests\nfrom bs4 import BeautifulSoup\n\nfrom handleCSV import HandleCSV\n\nclass Movie():\n    def __init__(self, url: str, csv: HandleCSV):\n        # add check to verify the domain is \"imdb.com\"\n        self.url = url\n        print(f\"Processing: {self.url}\")\n        self.repeat = False\n        if csv.isRepeat(self.url):\n            self.repeat = True\n            return\n\n        self.csv = csv\n        self.attributes = {}\n\n        self.page = requests.get(self.url) # get page HTML through request\n        self.soup = BeautifulSoup(self.page.content, \"html.parser\") # parse content\n\n        self.repeat = False # have webscraped this movie already\n\n        self.webScrape()\n\n    def setMovie(self, url: str):\n        # add check to verify the domain is \"imdb.com\"\n        self.url = url\n        self.webScrape()\n\n    def webScrape(self):\n        # may need to add try-except block\n        self.attributes[\"url\"] = self.url # identifier\n        self.attributes[\"title\"] = self.soup.select(\".TitleHeader__TitleText-sc-1wu6n3d-0\")\n        self.attributes[\"rating\"] = self.soup.select(\".AggregateRatingButton__RatingScore-sc-1ll29m0-1\")\n        self.attributes[\"numOfRatings\"] = self.soup.select(\".AggregateRatingButton__TotalRatingAmount-sc-1ll29m0-3\")\n        self.attributes[\"popularity\"] = self.soup.select(\".TrendingButton__TrendingScore-bb3vt8-1\")\n        self.attributes[\"numOfAwards\"] = self.soup.select(\"li[data-testid=\\\"award_information\\\"] div ul li span\")\n        self.attributes[\"parentalRating\"] = self.soup.select(\".TitleBlock__TitleMetaDataContainer-sc-1nlhx7j-2 ul li a\")\n        self.attributes[\"yearMade\"] = self.soup.select(\".TitleBlock__TitleMetaDataContainer-sc-1nlhx7j-2 ul li a\")\n        self.attributes[\"genre\"] = self.soup.select(\"div[data-testid=\\\"genres\\\"] a span\")\n        self.attributes[\"language\"] = self.soup.select(\"li[data-testid=\\\"title-details-languages\\\"] div ul li a\")\n        self.attributes[\"length\"] = self.soup.select(\"li[data-testid=\\\"title-techspec_runtime\\\"] div ul li span\")\n        self.attributes[\"gross\"] = self.soup.select(\"li[data-testid=\\\"title-boxoffice-cumulativeworldwidegross\\\"] div ul li span\")\n        # ...\n\n        self.parse()\n\n        self.addRowToPandas()\n\n    \"\"\" format attributes \"\"\"\n    def parse(self):\n        # title\n        if len(self.attributes[\"title\"]) == 0:\n            self.attributes[\"title\"] = None\n        else:\n            self.attributes[\"title\"] = str(self.attributes[\"title\"][0].text)\n\n        # rating\n        if len(self.attributes[\"rating\"]) == 0:\n            self.attributes[\"rating\"] = None\n        else:\n            self.attributes[\"rating\"] = float(self.attributes[\"rating\"][0].text)\n\n        # numOfRatings\n        if len(self.attributes[\"numOfRatings\"]) == 0:\n            self.attributes[\"numOfRatings\"] = None\n        else:\n            self.attributes[\"numOfRatings\"] = str(self.attributes[\"numOfRatings\"][0].text).lower()\n            if 'k' in self.attributes[\"numOfRatings\"]:\n                self.attributes[\"numOfRatings\"] = float(self.attributes[\"numOfRatings\"][:-1]) * 1000\n            elif 'm' in self.attributes[\"numOfRatings\"]:\n                self.attributes[\"numOfRatings\"] = float(self.attributes[\"numOfRatings\"][:-1]) * 1_000_000\n            else:\n                self.attributes[\"numOfRatings\"] = float(self.attributes[\"numOfRatings\"])\n\n        # popularity\n        if len(self.attributes[\"popularity\"]) == 0:\n            self.attributes[\"popularity\"] = None\n        else:\n            self.attributes[\"popularity\"] = self.attributes[\"popularity\"][0].text\n            self.attributes[\"popularity\"] = int(self.attributes[\"popularity\"].replace(',', \"\"))\n\n        # numOfAwards\n        if len(self.attributes[\"numOfAwards\"]) == 0:\n            self.attributes[\"numOfAwards\"] = None\n        else:\n            self.attributes[\"numOfAwards\"] = str(self.attributes[\"numOfAwards\"][0].text)\n\n            wins = self.attributes[\"numOfAwards\"].find(\"wins\")\n            if wins == -1:\n                wins = 0\n            else:\n                wins = int(self.attributes[\"numOfAwards\"][:wins - 1])\n\n            nominations = self.attributes[\"numOfAwards\"].find(\"nominations\")\n            if nominations == -1:\n                nominations = 0\n            else:\n                if wins == 0: # \"x wins\" does not exist in string\n                    nominations = self.attributes[\"numOfAwards\"][:nominations - 1]\n                else:\n                    start = self.attributes[\"numOfAwards\"].find(\"&\") + 2\n                    nominations = int(self.attributes[\"numOfAwards\"][start:nominations - 1])\n            \n            self.attributes[\"numOfAwards\"] = [wins, nominations]\n\n        # parentalRating\n        if len(self.attributes[\"parentalRating\"]) == 0:\n            self.attributes[\"parentalRating\"] = None\n        else:\n            self.attributes[\"parentalRating\"] = str(self.attributes[\"parentalRating\"][1].text)\n\n        # yearMade\n        if len(self.attributes[\"yearMade\"]) == 0:\n            self.attributes[\"yearMade\"] = None\n        else:\n            self.attributes[\"yearMade\"] = int(self.attributes[\"yearMade\"][0].text)\n\n        # genre\n        if len(self.attributes[\"genre\"]) == 0:\n            self.attributes[\"genre\"] = None\n        else:\n            # there might be multiple genres!\n            temp = []\n            for element in self.attributes[\"genre\"]:\n                temp.append(str(element.text))\n            self.attributes[\"genre\"] = temp\n\n        # language\n        if len(self.attributes[\"language\"]) == 0:\n            self.attributes[\"language\"] = None\n        else:\n            # there might be multiple languages!\n            temp = []\n            for element in self.attributes[\"language\"]:\n                temp.append(str(element.text))\n            self.attributes[\"language\"] = temp\n\n        # length\n        if len(self.attributes[\"length\"]) == 0:\n            self.attributes[\"length\"] = None\n        else:\n            self.attributes[\"length\"] = str(self.attributes[\"length\"][0].text)\n            \n            hours = self.attributes[\"length\"].find('h')\n            if hours == -1:\n                hours = 0\n            else:\n                hours = int(self.attributes[\"length\"][:hours])\n            \n            minutes = self.attributes[\"length\"].find(\"min\")\n            if minutes == -1:\n                minutes = 0\n            else:\n                if hours == 0: # \"h\" doesn't exist\n                    minutes = int(self.attributes[\"length\"][:minutes])\n                else:\n                    start = self.attributes[\"length\"].find('h') + 2\n                    minutes = int(self.attributes[\"length\"][start:minutes])\n\n            self.attributes[\"length\"] = (hours * 60) + minutes # total number of minutes\n\n        # gross\n        if len(self.attributes[\"gross\"]) == 0:\n            self.attributes[\"gross\"] = None\n        else:\n            self.attributes[\"gross\"] = str(self.attributes[\"gross\"][0].text)\n\n            self.attributes[\"gross\"] = self.attributes[\"gross\"].replace(',', \"\")\n            self.attributes[\"gross\"] = int(self.attributes[\"gross\"].replace('$', \"\"))\n\n    def setAttribute(self, attribute: str, value):\n        # may need to include try-except block\n        self.attributes[attribute] = value\n\n    def getAttribute(self, attribute: str):\n        # may need to include try-except block\n        return self.attributes[attribute]\n\n    def prettyPrint(self):\n        # header\n        text = \"Here's what I found about \\\"{}\\\" on IMDb:\".format(self.attributes[\"title\"])\n        length = len(text)\n        print(text)\n        print(('_' * length) + '\\n')\n\n        print(\"URL: {}\".format(self.attributes[\"url\"]))\n        print(\"Title: {}\".format(self.attributes[\"title\"]))\n        print(\"Rating (_/10): {}\".format(self.attributes[\"rating\"]))\n        print(\"Number of Ratings: {}\".format(self.attributes[\"numOfRatings\"]))\n        print(\"Popularity (see IMDb): {}\".format(self.attributes[\"popularity\"]))\n        print(\"Number of Awards [wins, nominations]: {}\".format(self.attributes[\"numOfAwards\"]))\n        print(\"Parental Rating: {}\".format(self.attributes[\"parentalRating\"]))\n        print(\"Made in: {}\".format(self.attributes[\"yearMade\"]))\n        print(\"Genre(s): {}\".format(self.attributes[\"genre\"]))\n        print(\"Language(s): {}\".format(self.attributes[\"language\"]))\n        print(\"Length: {} minutes\".format(self.attributes[\"length\"]))\n        print(\"Gross Worldwide: ${}\".format(self.attributes[\"gross\"]))\n\n    def addRowToPandas(self):\n        try:\n            self.csv.addRow(self.attributes)\n        except:\n            pass\n\n    def findSimilar(self) -> list:\n        queue = []\n        temp = self.soup.select(\"section[data-testid=\\\"MoreLikeThis\\\"] div div div div a\")\n        for item in temp:\n            item = str(item[\"href\"])\n\n            end = item.find(\"/?\") # clean url\n            if end == -1:\n                end = len(item)\n\n            item = item[:end]\n            item = \"https://imdb.com\" + item\n            queue.append(item)\n\n        return queue", "497": "import tkinter\nfrom tkinter import *\nimport tkinter.font as font\nfrom webscrape import *\n\nglobal_Label = None\nglobal label2\n\nroot = Tk()\nroot.geometry(\"500x550\")\nroot.resizable(False, False)\nroot.title(\"College Database Webscraper\")\n\n# Creates a frame for bottom widgets, then packs them to main screen\n\n\nframe = Frame(root)\nframe2 = Frame(root)\nframe3 = Frame(root)\n\nf = font.Font(family='Helvetica', size=16, weight=\"bold\")\n\nlabel_list = []\n\nnew_L = Label(frame2)\nnew_L.config(font=(\"Montserrat\", 12))\nnew_L.pack(anchor='w', side=LEFT)\n\n\ndef getInput():\n    college_name = u_input_box.get()\n\n    global global_Label\n\n    # check for empty input\n    if (len(college_name) != 0):\n        u_input_box.delete(0, END)\n\n        label2.pack_forget()\n        getData(college_name)\n    else:\n        label2.config(text=\"Input cannot be empty\", font=(\"Montserrat\", 12, font.BOLD), foreground=\"red\")\n        label2.pack()\n\n\ndef getData(college_name):\n    ret_val = webscrape((college_name))\n\n    if (ret_val == -1):\n        label2.config(text=\"COLLEGE NOT FOUND!\", font=(\"Montserrat\", 12, font.BOLD), foreground=\"red\")\n        label2.pack()\n    else:\n        blank = \"\\n\"\n        array_string = blank.join(ret_val)\n        new_L.configure(text=array_string)\n\n\n# title text\nmyLabel = Label(frame3, text=\"College Database Webscraper\")\nmyLabel.config(font=(\"Merriweather\", 16, font.BOLD))\nmyLabel.pack()\n\nlabel2 = Label(frame3, text=\"Input cannot be empty\")\nlabel2.pack_forget()\n\n# input box\nu_input_box = Entry(frame, width=30, font=(\"Montserrat\", 12))\nu_input_box.pack(padx=10, pady=10, side=tkinter.LEFT)\n\n# fetch button\nbut1 = Button(frame, text=\"Fetch\", command=getInput)\nbut1.config(width=10)\nbut1.pack(side=tkinter.LEFT)\n\nframe3.pack(side=tkinter.TOP)\nframe2.pack()\nframe.pack(side=tkinter.BOTTOM)\n\n\nroot.mainloop()\n", "498": "from gettext import install\nfrom twisted.internet import reactor\n\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\nimport time\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nfrom scrapy.utils.project import get_project_settings\nimport os\nfrom crochet import setup\n#setup()\nfrom items import WebscrapeItem\n\nclass TechSpider(scrapy.Spider):\n    name = 'tech'\n    allowed_domains = ['techcrunch.com']\n    start_urls = ['https://techcrunch.com/2022/04/01/staten-island-amazon-workers-vote-to-unionize/']\n    \n    def parse(self, response):\n        #c=response.css('.content a::attr(href)').getall()\n        #print(type(c))\n        #for i in c:\n        #i=response.urljoin(i)\n        parse_content(response)   \n        #yield response.follow(i,self.parse_content)\n           \n         \n    \n\n\n    def parse_content(self,response):\n        q={}\n        ittem = WebscrapeItem()\n        item = {}\n        Title=response.css('h1.article__title::text').extract()\n        Author=response.css('div.article__byline a::text').extract()\n        post=response.css('div.article-content ::text').extract()\n            \n        item['title']= Title\n        item['author']=Author\n        item['post']=post\n\n        \n          \n        a = item['title']\n        #print(type(a))\n\n        b = item['author']\n        \n\n        listt = list(item['post'])\n        result= ''\n        for element in listt:\n            result += str(element)\n        c=[]\n        c.append(result)\n        if len(a)==1 :\n            ittem['title']= a[0]\n            ittem['author']=b[0]\n            ittem['post']=c[0]\n        \n        \n                     \n        yield ittem\n        \n\n\n#if __name__==\"__main__\":\n#def s():\n\n    #process = CrawlerProcess(get_project_settings())\n    # 'followall' is the name of one of the spiders of the project.\n    #process.crawl(TechSpider)\n    #process.start(stop_after_crawl=True) # the script will block here until the crawling is finished\n   \n    #configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n    #runner = CrawlerRunner(get_project_settings())\n    #runner.crawl(TechSpider)\n    #d.addBoth(lambda _: reactor.stop())\n    #reactor.run()\n#s()\n#print(c)\n#def run_spider(spiderName):\n    #module_name=\"first_scrapy.spiders.{}\".format(spiderName)\n    #scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   \n    #spiderObj=scrapy_var.mySpider()           #get mySpider-object from spider module\n    #crawler = CrawlerRunner(get_project_settings())   #from Scrapy docs\n    #crawler.crawl(spiderObj)  \n\ndef crawl(runner):\n    d = runner.crawl(TechSpider)\n    d.addBoth(lambda _: reactor.stop())\n    return d\n\n\ndef loop_crawl():\n    runner = CrawlerRunner(get_project_settings())\n    crawl(runner)\n    reactor.run()\n    \ndef startspider():\n    loop_crawl()\n", "499": "from django.shortcuts import render\nfrom .models import PatternTable, Category\nfrom django.http import HttpResponseRedirect, HttpResponse\nfrom django.urls import reverse\nfrom django import forms\nclass Search(forms.Form):\n    item = forms.CharField(widget=forms.TextInput(attrs={'class' : 'myfieldclass', 'placeholder': 'Search'}))\n# Create your views here.\n\ndef webScrape():\n    import bs4\n    from urllib.request import urlopen as uReq\n    from bs4 import BeautifulSoup as soup\n    import sqlite3\n    my_url = 'https://www.amigurumi.com/search/free/'\n  \n    for page in range (1,11): #parse 10 pages\n        new_url = my_url + str(page) + '/'\n        uClient = uReq(new_url)\n        page_html = uClient.read()\n        uClient.close()\n        page_soup=soup(page_html, \"html.parser\")\n        containers = page_soup.findAll(\"div\", {\"class\":\"item\"})\n        \n        for i in range(len(containers)): #Go through each item\n            container = containers[i]\n            # Name of pattern\n            name = container.img.get('title')\n            # Link to pattern\n            link = container.a.get('href')\n\n            #Parse each link to pattern\n            newClient = uReq(link)\n            new_html = newClient.read()\n            newClient.close()\n            pattern_soup = soup(new_html, \"html.parser\")\n\n            # Description of pattern\n            description = pattern_soup.findAll(\"div\", {\"id\": \"patterndescription\"})\n            \n            if len(description)!=0:\n                des = description[0]\n                des_text = des.find('p').text.strip()\n                new_des_text=\"\".join(des_text.splitlines())\n            else: \n                new_des_text = \" \"\n            # What category pattern belongs in\n            category = pattern_soup.findAll(\"span\", {\"itemprop\": \"title\"})\n            \n            group = category[1].text\n            obj, created = Category.objects.get_or_create(cate=group)\n            p = PatternTable(name = name, link = link, description = new_des_text, category = obj)\n            p.save()\n\n\ndef index(request):\n    #webScrape()\n    #return(HttpResponse(\"hello\"))\n    if request.GET.get('q'):\n        query = request.GET.get(\"q\", \"\")\n        return(search(request, query))\n    else:\n        return render(request, \"patterns/index.html\", {\"patterns\": PatternTable.objects.all()})\n\n\ndef pattern(request, pattern_id):\n    pattern = PatternTable.objects.get(id=pattern_id)\n    return render(request, \"patterns/pattern.html\", {\"pattern\":pattern})\n\ndef search(request, query):\n    patterns = PatternTable.objects.all()\n    results=[]\n    for pattern in patterns:\n        if query.lower() == pattern.name.lower():\n            return render(request, \"patterns/pattern.html\",{\"pattern\": pattern})\n        if query.lower() in pattern.name.lower() or query.lower() in pattern.description.lower():\n            results.append(pattern)\n\n    return render(request,\"patterns/search.html\", {\"results\":results})   \n\ndef categories(request):\n    results = Category.objects.all()\n    return render(request, \"patterns/categories.html\",{\"results\": results})\n\ndef category(request,cate_id):\n    #patterns_match = PatternTable.objects.filter(category=cate)\n    c = Category.objects.get(id = cate_id)\n    all_patterns = c.patterns_incategory.all()\n    return render(request, \"patterns/category.html\", {\"all_patterns\": all_patterns, \"c\":c})", "500": "#!c:\\webscrape\\webscrape\\scripts\\python.exe\r\n\"\"\"\nThe registry server listens to broadcasts on UDP port 18812, answering to\ndiscovery queries by clients and registering keepalives from all running\nservers. In order for clients to use discovery, a registry service must\nbe running somewhere on their local network.\n\"\"\"\nfrom plumbum import cli\nfrom rpyc.utils.registry import REGISTRY_PORT, DEFAULT_PRUNING_TIMEOUT\nfrom rpyc.utils.registry import UDPRegistryServer, TCPRegistryServer\nfrom rpyc.lib import setup_logger\n\n\nclass RegistryServer(cli.Application):\n    mode = cli.SwitchAttr([\"-m\", \"--mode\"], cli.Set(\"UDP\", \"TCP\"), default = \"UDP\",\n        help = \"Serving mode\")\n    \n    ipv6 = cli.Flag([\"-6\", \"--ipv6\"], help=\"use ipv6 instead of ipv4\")\n\n    port = cli.SwitchAttr([\"-p\", \"--port\"], cli.Range(0, 65535), default = REGISTRY_PORT, \n        help = \"The UDP/TCP listener port\")\n    \n    logfile = cli.SwitchAttr([\"--logfile\"], str, default = None, \n        help = \"The log file to use; the default is stderr\")\n    \n    quiet = cli.SwitchAttr([\"-q\", \"--quiet\"], help = \"Quiet mode (only errors are logged)\")\n    \n    pruning_timeout = cli.SwitchAttr([\"-t\", \"--timeout\"], int, \n        default = DEFAULT_PRUNING_TIMEOUT, help = \"Set a custom pruning timeout (in seconds)\")\n\n    def main(self):\n        if self.mode == \"UDP\":\n            server = UDPRegistryServer(host = '::' if self.ipv6 else '0.0.0.0', port = self.port, \n                pruning_timeout = self.pruning_timeout)\n        elif self.mode == \"TCP\":\n            server = TCPRegistryServer(port = self.port, pruning_timeout = self.pruning_timeout)\n        setup_logger(self.quiet, self.logfile)\n        server.start()\n\n\nif __name__ == \"__main__\":\n    RegistryServer.run()\n\n", "501": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Tue Jul 26 13:47:51 2022\r\n\r\n@author: josh.smith\r\n\"\"\"\r\nfrom bs4 import BeautifulSoup\r\nimport requests\r\nimport sys\r\nfrom automate_email import email_alert\r\nimport datetime\r\n\r\n#today's date\r\nx=datetime.datetime.now()\r\nweekDays = (\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\")\r\n#get the first three characters from todays weekday\r\ntoday = weekDays[x.weekday()][:3]\r\n#find tomorrow too so we can parse the correct stuff\r\ntomorrow = weekDays[x.weekday()+1][:3]\r\ndef scrapin(url,user):\r\n    res = requests.get(str(url),headers = user)\r\n    res.status_code == requests.codes.ok\r\n    #USED TO HALT A BAD DOWNLOAD\r\n    try:\r\n        res.raise_for_status()\r\n    except Exception as exc:\r\n        print('There was a problem: %s' % (exc))\r\n    res_web = BeautifulSoup(res.text,'html.parser')\r\n    return res_web\r\ndef webscrapeWeather(city):\r\n    #define our user agent for scraping the website\r\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\r\n\r\n    city = city +' weather'\r\n    city=city.replace(' ','+')\r\n    #find the weather for the city using google\r\n    read = scrapin(f'https://www.google.com/search?q={city}&oq={city}&aqs=chrome.0.35i39l2j0l4j46j69i60.6128j1j7&sourceid=chrome&ie=UTF-8',headers)\r\n    \r\n    try:\r\n        #get all of the correct data from the wbsite\r\n        location = read.select('#wob_loc')[0].getText().strip()  \r\n        #info = read.select('#wob_dc')[0].getText().strip() \r\n        #weather = read.select('#wob_tm')[0].getText().strip()\r\n        #this is where the high and lows are located\r\n        more_Weather = read.select('#wob_dp')[0].getText().strip()\r\n\r\n        degree_sign = u'\\N{DEGREE SIGN}'\r\n        #parse the string to find todays temperatures\r\n        high = more_Weather[more_Weather.find(today)+3:more_Weather.find(tomorrow)][:2]+degree_sign\r\n        low = more_Weather[more_Weather.find(today)+3:more_Weather.find(tomorrow)][5:7]+degree_sign\r\n        #put it all together \r\n        weather_up = location +' Weather\\n'+len(location+' Weather')*'-'+'\\nHigh: '+high+'\\nLow: '+low+'\\n'\r\n\r\n    except: \r\n        print('invalid search')\r\n    return(weather_up)\r\n\r\n#here we want to scrape the news for the day, the goal is to find 5 articles\r\ndef webscrapeNews():\r\n    try:\r\n        \r\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\r\n        #website of choise\r\n        read_news = scrapin('https://www.allsides.com/',headers)\r\n        hyper_set=[]\r\n        title_set = []\r\n        #compile list of titles from the entire website\r\n        url = 'https://www.allsides.com'\r\n        articles = read_news.find_all('a',class_='main-link')\r\n        #read what the news was from yesterday\r\n        with open('C:\\\\Users\\josh.smith\\Desktop\\yesterdays_news.txt','r') as f:\r\n            yesterdays_news=f.read()\r\n\r\n        for article in articles:\r\n            #we're shooting for five articles and we want to make sure to not repeat yesterdays news\r\n            if len(title_set) < 5 and article.getText() not in yesterdays_news:    \r\n                #we also dont want to insert the wrong urls (this could be added to the above line instead)\r\n                if url not in article['href']:\r\n                    #print(len(title_set))\r\n                    title_set.append(article.getText())\r\n                    hyper_set.append(url+article['href'])\r\n        #save the news articles we found for today for later use\r\n        with open('C:\\\\Users\\josh.smith\\Desktop\\yesterdays_news.txt','w') as f:\r\n            for title in title_set:\r\n                f.write(str(title+'\\n'))\r\n    #just in case anything goes wrong\r\n    except Exception as err: \r\n        print(err)\r\n        print('invalid search')\r\n    return(hyper_set,title_set)\r\n\r\n#we are finding the quote of the day here\r\ndef getQuote():\r\n    #same drill, define user agents and scrape the website\r\n    user_agent={'User-agent': 'Mozilla/5.0'}\r\n    quotes_web = scrapin('https://www.brainyquote.com/quote_of_the_day',user_agent)\r\n\r\n    try:\r\n        #find the quote\r\n        qotd= quotes_web.find('div',class_='grid-item qb clearfix bqQt').getText()\r\n    #if anything goes wrong we insert this qotd instead\r\n    except Exception as err:\r\n        qotd = '\\nNo quote today.'\r\n        email_alert('Error with qotd','File: '+__file__+'\\n'+str(err),None,['josh.smith@kennypipe.com'])\r\n        #print(err)\r\n    return qotd\r\ndef newCity(city,emails):\r\n    #use func from above\r\n    weather_info = webscrapeWeather(city)\r\n    links,titles = webscrapeNews()\r\n    qotd = getQuote()\r\n    \r\n    #this counter is to number the articles\r\n    counter = 1\r\n    email_stuff = []\r\n    #run through all the links and titles to compile the email\r\n    for title in zip(titles,links):\r\n         \r\n        email = '\\n'+str(counter)+'. '+title[0]+' '\r\n        email_stuff.append(email)\r\n        dashes=len(str(title[0]))*'-'\r\n        counter +=1\r\n    #further compiling\r\n    email_stuff.insert(0,''+weather_info+'\\n')\r\n    email_stuff.append('\\n\\n'+qotd.replace('\\n\\n\\n','').replace('.','.\\n-')+'')\r\n    email_alert('Daily News', 'placeholder',','.join(email_stuff).replace(',',''), emails)\r\n    \r\n#send it all out! the above func makes it easy to add people from different places\r\n#newCity('Chicago',email_list1)\r\n#newCity('New York',email_list2)\r\n#newCity('Los Angeles',email_list3)\r\n\r\n\r\n", "502": "import mysql.connector\nimport http.client\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef getMovie(movieName, year):\n    url = \"https://movie-database-imdb-alternative.p.rapidapi.com/\"\n    movieName = movieName.replace(\"&\", \"&\")\n    movieName = movieName.replace(\"-\", \"\")\n    querystring = {\"s\":movieName,\"page\":\"1\",\"y\":year,\"r\":\"json\"}\n\n    headers = {\n        'x-rapidapi-host': \"movie-database-imdb-alternative.p.rapidapi.com\",\n        'x-rapidapi-key': \"8ca55b21e6msha37e66df719a38ap1fc3c2jsn802dc9396eed\"\n    }\n\n    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n\n    data = response.text\n    data_dict = json.loads(data)\n    try:\n        results = data_dict['Search']\n    except KeyError:\n        imdb = \"failed\"\n        return imdb, data_dict\n    movie = results[0]\n    imdb = movie['imdbID']\n\n    conn = http.client.HTTPSConnection(\"movie-database-imdb-alternative.p.rapidapi.com\")\n\n    headers = {\n        'x-rapidapi-host': \"movie-database-imdb-alternative.p.rapidapi.com\",\n        'x-rapidapi-key': \"8ca55b21e6msha37e66df719a38ap1fc3c2jsn802dc9396eed\"\n        }\n\n    request = \"/?i=\" + imdb + \"&r=json\"\n    conn.request(\"GET\", request, headers=headers)\n\n    res = conn.getresponse()\n    data = res.read().decode(\"utf-8\")\n    data_dict = json.loads(data)\n\n    print(movieName + \" details have been found\")\n\n    return imdb, data_dict\n\n\ndef intoDB(mydb, imdb, data_dict):\n    mycursor = mydb.cursor()\n\n    sql = \"INSERT INTO Movie(MovieID, Title, ReleaseYear, Rating, Runtime, Metascore, imdbRating) \" \\\n          \"VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n    val = (imdb, data_dict['Title'], data_dict['Year'], data_dict['Rated'], data_dict['Runtime'],\n           data_dict['Metascore'], data_dict['imdbRating'])\n\n    try:\n        mycursor.execute(sql, val)\n        mydb.commit()\n    except mysql.connector.errors.DatabaseError:\n        return \"failed\"\n\n    sql = \"INSERT INTO Directors(Name, MovieID) VALUES (%s, %s)\"\n    split = data_dict['Director'].split(',')\n    values = []\n    for x in split:\n        x = x.replace(\" \", \"\")\n        val = (x, imdb)\n        values.append(val)\n\n    mycursor.executemany(sql, values)\n    mydb.commit()\n\n    sql = \"INSERT INTO Writers(Name, MovieID) VALUES (%s, %s)\"\n    split = data_dict['Writer'].split(',')\n    values = []\n    for x in split:\n        x = x.replace(\" \", \"\")\n        val = (x, imdb)\n        values.append(val)\n\n    mycursor.executemany(sql, values)\n    mydb.commit()\n\n    sql = \"INSERT INTO Genres(Name, MovieID) VALUES (%s, %s)\"\n    split = data_dict['Genre'].split(',')\n    values = []\n    for x in split:\n        x = x.replace(\" \", \"\")\n        val = (x, imdb)\n        values.append(val)\n\n    mycursor.executemany(sql, values)\n    mydb.commit()\n\n    sql = \"INSERT INTO Actors(Name, MovieID) VALUES (%s, %s)\"\n    split = data_dict['Actors'].split(',')\n    values = []\n    for x in split:\n        x = x.replace(\" \", \"\")\n        val = (x, imdb)\n        values.append(val)\n\n    mycursor.executemany(sql, values)\n    mydb.commit()\n\n    return \"\"\n\n\ndef webScrape():\n    movies = []\n    response = requests.get(\"https://www.boxofficemojo.com/year/world/2020/?sort=domesticGrossToDate&ref_=bo_ydw__resort#table\")\n    soup = BeautifulSoup(response.text, 'html.parser')\n    titles = soup.findAll('td', attrs={\"a-text-left mojo-field-type-release_group\"})\n    for title in titles:\n        tmp = str(title)\n        parsed = tmp.split('>')\n        remaining = parsed[2]\n        clean = remaining.split('<')\n        movies.append(clean[0])\n\n    return movies\n\ndef fillDB(year):\n    movies = webScrape()\n    failed = []\n    mydb = mysql.connector.connect(\n            host = \"localhost\",\n            user = \"root\",\n            password = \"\",\n            database = \"MovieFinder\"\n        )\n\n    for movie in movies[:50]:\n        print(\"Currently adding: \" + movie)\n        imbd, data = getMovie(movie, year)\n        if (imbd == \"failed\"):\n            failed.append(movie)\n            print(\"Could not find: \" + movie)\n        else:\n            status = intoDB(mydb, imbd, data)\n            if (status == \"failed\"):\n                failed.append(movie)\n                print(\"Could not add: \" + movie)\n            else:\n                print(\"Successfully added: \" + movie)\n\n    print(\"Program failed to add the following movies:\")\n    for f in failed:\n        print(f)\n\n    mydb.close()\n\n\ndef addMovie(title):\n    mydb = mysql.connector.connect(\n        host=\"localhost\",\n        user=\"root\",\n        password=\"\",\n        database=\"MovieFinder\"\n    )\n\n    imbd, data = getMovie(title)\n    intoDB(mydb, imbd, data)\n\n    mydb.close()\n\nfillDB(\"2020\")", "503": "# Filename: novel_alerts_model.py\n\n\"\"\"Model that runs operations on data that is fed in through the controller.\"\"\"\n\n# Import csv for reading, appending, and writing csv files\nimport csv\n# Import smtplib and ssl for sending emails\nimport smtplib\nimport ssl\n# Import callable to type annotate functions\nfrom typing import Callable, Union\n# Import requests and BeautifulSoup libraries to web scrape URL's\nfrom urllib.request import Request, urlopen\nfrom bs4 import BeautifulSoup as soup\n\n\nclass NovelAlertsModel:\n    \"\"\"\n    A class that represents the model for the Model-View-Controller(MVC) design pattern.\n\n    :param FIELD_NAMES: List of dict string key headings\n    :type FIELD_NAMES: List[str]\n    :param _URL_file_path: File path of URL data\n    :type _URL_file_path: str\n    :param EMAIL_FILE_PATH: File path of email data\n    :type EMAIL_FILE_PATH: str\n    :param _user_email: Users email\n    :type _user_email: str\n    :param _url_data: List of dictionaries in the format: {\"URL\": \"url_Link, \"latestChapter\": \"chapter\"}\n    :type _url_data: list[dict[str, str]]\n    :param _password: users email password\n    :type _password: str\n    :param _message_box: GUI error msg method that brings up a message box\n    :type _message_box: NovelAlertsView method\n    \"\"\"\n\n    FIELD_NAMES = [\"URL\", \"latestChapter\"]\n\n    def __init__(self, message_box: Callable=print, URL_path: str=\"data/URL_log.csv\", email_path: str=\"data/email.txt\") -> None:\n        \"\"\"Model Initializer\"\"\"\n\n        self._URL_file_path = URL_path\n        self._email_file_path = email_path\n        self._user_email = self._load_email()\n        self._url_data = self._load_URL_Data()\n        self._password = \"\"\n        self._message_box = message_box \n        \n        # Initializes the csv file with column headers if there was no previous data.\n        if not self._url_data:\n            self._write_URL_data_to_file()\n\n    # Add function that figures out which website is added and get call different versions of latest chapter functions\n    def _get_Latest_Chapter_URL_Filtered(self, URL: str) -> Union[str, None]:\n        \"\"\"Choose a different version of the get latest chapter function by using the URL\"\"\"\n\n        if \"wln\" in URL and \"series-id\" in URL: \n            return self._webscrape_WLN_Latest_Chapter(URL)\n        elif \"novelupdates\" in URL and \"series\" in URL:\n            return self._webscrape_Novelupdates_Latest_Chapter(URL)\n        else:\n            self._message_box(\"ERROR: URL is not correct or from wlnupdates or novelupdates domain\")\n\n    def _webscrape_WLN_Latest_Chapter(self, URL: str) -> Union[str, None]:\n        \"\"\"Web scrapes the latest chapter from the URL link and must be from domain https://www.wlnupdates.com/\"\"\"\n\n        try:\n            # Title: How to Web Scrape using Beautiful Soup in Python without running into HTTP error 403\n            # Author: Raiyan Quaium\n            # Availability: https://medium.com/@raiyanquaium/how-to-web-scrape-using-beautiful-soup-in-python-without-running-into-http-error-403-554875e5abed\n\n            # Requests the URL data with disguised headers \n            req = Request(URL, headers={\"User-Agent\": \"Mozilla/5.0\"})\n            # Opens the url and reads the html as a string\n            webpage = urlopen(req).read()\n            # Creates Bs4 object with arguments consisting of html to be parsed and which parser to use.\n            page_soup = soup(webpage, \"html.parser\")\n            # Uses the soup object to find 'h5' tags within the html\n            # .text is used to grab the text within the tag and nothing else.\n            # [17:] is used to splice the text string to not include \"Latest release - \"\n            # Ex. Latest release - vol 2.0  chp. 351.0\n            latest_chapter = page_soup.find(\"h5\").text[17:]\n            return latest_chapter\n        except Exception:\n            # Returns None if latest chapter could not be found\n            self._message_box(\"ERROR: Could not find the latest chapter and was not entered into the data.\")\n\n    def _webscrape_Novelupdates_Latest_Chapter(self, URL: str) -> Union[str, None]:\n        \"\"\"Web scrapes the latest chapter from the URL link and must be from domain https://www.novelupdates.com/\"\"\"\n\n        try:\n            req = Request(URL, headers={\"User-Agent\": \"Mozilla/5.0\"})\n            webpage = urlopen(req).read()\n            page_soup = soup(webpage, \"html.parser\")\n            # Uses the soup object to find all 'a' tags with the class 'chp-release'\n            # Uses the bracket to access the first result which is the latest chp\n            # .text is used to grab the text within the tag and nothing else.\n            # Ex.  text \n            latest_chapter = page_soup.findAll(\"a\", \"chp-release\")[0].text\n            return latest_chapter\n        except Exception:\n            self._message_box(\"ERROR: Could not find the latest chapter and was not entered into the data.\")\n\n    def _integrate_Updated_URLS(self, updated_URLS: list[str]) -> str:\n        \"\"\"Integrate updated urls into a string\"\"\"\n\n        message = \"\"\"\"\"\"\n\n        for url in updated_URLS:\n            message += f\"\"\"{url}\\n\"\"\"\n\n        return message\n\n    def _send_Email(self, updated_URLS: list[str]) -> None:\n        \"\"\"Sends a email to user with a list of URL's that have new updates\"\"\"\n\n        # Title: Sending Emails with Python\n        # Author: Joska de Langen\n        # Availability: https://realpython.com/python-send-email/\n\n        # For SSL\n        port = 465 \n\n        message = self._integrate_Updated_URLS(updated_URLS)\n\n        # default context validates host name, certificates, and optimizes security of connection\n        context = ssl.create_default_context()\n\n        try:\n            # Initiates a TLS-encrypted connection\n            with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n                try:\n                    server.login(self._user_email, self._password)\n                    server.sendmail(self._user_email, self._user_email, message)\n                except Exception:\n                    self._message_box(\"ERROR: Email or password is incorrect!\")\n        except Exception:\n            self._message_box(\"ERROR: Server connection could not be established!\")\n\n    def _compile_updated_URLS(self) -> list[str]:\n        \"\"\"Compiles a list of updated URLS by comparing current chapters with new chapters\"\"\"\n\n        updated_URLS = []\n        for dict_ in self._url_data:\n            # Gets the latestchapter and compare it to the current one in object\n            # If it is less than the latest chapter then append URL to list of updated URL's and set new chapter into object\n            # No need to check for None from function call because webscraping the URL has worked if it is already in data structure.\n            latest_chapter = self._get_Latest_Chapter_URL_Filtered(dict_[self.FIELD_NAMES[0]])\n            if latest_chapter == None:\n                continue\n\n            if dict_[self.FIELD_NAMES[1]] < latest_chapter:\n                updated_URLS.append(dict_[self.FIELD_NAMES[0]])\n                dict_[self.FIELD_NAMES[1]] = latest_chapter\n\n        return updated_URLS\n\n    def _webscrape_Check(self) -> Union[int, None]:\n        \"\"\"Check if there are new updates and sends that data to _send_Email\"\"\"\n\n        if self._url_data:\n            try:\n                updated_URLS = self._compile_updated_URLS()\n        \n                # If there were new updated novels\n                if updated_URLS:\n                    # write the new latest chapter data into the csv file.\n                    self._write_URL_data_to_file()\n                    self._send_Email(updated_URLS)\n            except Exception:\n                self._message_box(\"Error: Webscraper did not work. If this continues then restart program.\")\n\n    def setEmail(self, email: str) -> None:\n        \"\"\"Set the new email and saves it to email file\"\"\"\n\n        self._user_email = email\n\n        with open(self._email_file_path, \"w\") as email_file:\n            email_file.write(self._user_email)\n\n    def _load_email(self) -> str:\n        \"\"\"Loads the email from email file into class property\"\"\"\n\n        with open(self._email_file_path, \"r\") as email_file:\n            return email_file.read()\n\n    def getEmail(self) -> str:\n        \"\"\"Returns email of the user\"\"\"\n\n        return self._user_email\n    \n    def setPassword(self, password: str) -> None:\n        \"\"\"Set the password\"\"\"\n\n        self._password = password\n\n    def getPassword(self) -> str:\n        \"\"\"Gets the password\"\"\"\n\n        return self._password\n\n    def _load_URL_Data(self) -> list[dict[str, str]]:\n        \"\"\"Opens csv file to be read into a list of dictionarys\"\"\"\n\n        with open(self._URL_file_path, mode='r') as csv_file:\n            reader = csv.DictReader(csv_file)\n            return list(map(dict, reader))\n\n    def addURLData(self, URL: str) -> None:\n        \"\"\"Add the new URL to the dictionary and csv file\"\"\"\n\n        for dict_ in self._url_data:\n            if dict_[\"URL\"] == URL:\n                self._message_box(\"ERROR: URL is already in data structure\")\n                return\n        \n        # Gets the latest chapter and if return type is None, then function call did not get latest chapter and doesn't add it to data.\n        latest_chapter = self._get_Latest_Chapter_URL_Filtered(URL)\n        if latest_chapter == None: \n            return\n        \n        dict_row = {self.FIELD_NAMES[0]: URL, self.FIELD_NAMES[1]: latest_chapter}\n\n        self._url_data.append(dict_row)\n\n        with open(self._URL_file_path, mode='a') as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=self.FIELD_NAMES)\n            writer.writerow(dict_row)\n\n    def _get_URL_data(self) -> list[dict[str, str]]:\n        \"\"\"Gets the current URL data within the object\"\"\"\n        return self._url_data\n\n    def _set_URL_data(self, URL_data: list[dict[str, str]]) -> None:\n        \"\"\"Sets the URL data to the object and file\"\"\"\n        self._url_data = URL_data\n        self._write_URL_data_to_file()\n\n    def _write_URL_data_to_file(self) -> None:\n        \"\"\"Writes current object _url_data into the csv file\"\"\"\n\n        with open(self._URL_file_path, mode='w') as csv_file:\n            # DictWriter object that allows for file output with dictionary keys as fieldnames/columns/headers\n            writer = csv.DictWriter(csv_file, fieldnames=self.FIELD_NAMES)\n\n            writer.writeheader()\n            for dict_ in self._url_data:\n                writer.writerow(dict_)\n\n    def deleteURLData(self, URL: str) -> None:\n        \"\"\"Delete the URL from the class object and rewrite data into the csv file \"\"\"\n\n        for dict_ in self._url_data:\n            if dict_[self.FIELD_NAMES[0]] == URL:\n                self._url_data.remove(dict_)\n                self._message_box(\"Success\")\n                self._write_URL_data_to_file()\n                return\n        \n        # Calls msgBox because URL data has been iterated through and match was not found\n        self._message_box(\"Error: URL is not within existing data or not correct!\")\n", "504": "from openpyxl import Workbook\nfrom Bs4 import WebScrape, Webscrape2\n\n\n# old version\ndef scrapper1(link):\n    \"\"\"\n    First version of this project. Kind of buggy, and sometimes puts the results into wrong columns\n    because the site doesn't always provide the same details. For example sometime the fuel type is missing thus\n    putting the next detail into the fuel type column.\n    \"\"\"\n    wb = Workbook()\n    ws = wb.active\n    ws.title = 'Vehicles'\n    print('Excel file has been created!')\n\n    webscraping = WebScrape(link)\n    webscraping.results()\n    webscraping.main_results()\n\n    result_dictionay = webscraping.get_dict()\n\n    headings = ['car0'] + list(result_dictionay['car1'].keys()) + ['soodushind']\n\n    ws.append(headings)\n\n    for vehicle in result_dictionay:\n        values = list(result_dictionay[vehicle].values())\n        ws.append([vehicle] + values)\n\n    wb.save('Auto-24-Py.xlsx')\n    print('Excel file has been saved and is ready to use!')\n\n\n# improved version:\ndef scrapper2(link):\n    \"\"\"\n    Second version of this project. Works flawlessly.\n    \"\"\"\n    wb = Workbook()\n    ws = wb.active\n    ws.title = 'Vehicles'\n    print('Excel file has been created!')\n\n    webscraping = Webscrape2(link)\n    webscraping.results()\n    webscraping.main_results()\n\n    result_dictionay = webscraping.get_dict()\n    headings = [0] + list(webscraping.get_example_dict().keys())\n\n    ws.append(headings)\n\n    print('Converting Dictionary -> Excel file!')\n    for vehicle in result_dictionay:\n        values = list(result_dictionay[vehicle].values())\n        ws.append([vehicle] + values)\n    wb.save('Auto-24-Py.xlsx')\n    print('Excel file has been saved and is ready to use!')\n\n# example links:\n# https://www.auto24.ee/kasutatud/nimekiri.php?b=2&ae=2&bw=301&f2=1991&f1=1987&ssid=13712424\n# https://www.auto24.ee/kasutatud/nimekiri.php?b=2&ae=2&bw=2136&f2=2018&f1=2011&ssid=14029839\n# https://www.auto24.ee/kasutatud/nimekiri.php?b=23&ae=2&bw=720&f2=2011&f1=2007&ssid=14030034\n# https://www.auto24.ee/kasutatud/nimekiri.php?b=4&ae=2&bw=809&f2=2010&f1=2007&ssid=14030330\n\nlink = 'https://www.auto24.ee/kasutatud/nimekiri.php?bn=2&a=100&b=35&g2=4500&ae=8&af=50&ssid=30064259'\nscrapper2(link)\n", "505": "from bs4 import BeautifulSoup\nimport requests\nimport os\nimport webbrowser\n\n#table dimensions\ntableDict=[]\ntotalTableCount=0\nwidthPerTable=0\ntotalHeight = 0\nheightPerTable = 0\n\n#array to store dimensions\ndimensionArray=[]\n\n#length of a given text file with stop time data \nfileLength = 0\n\n#declarations so values can be used later as globals in other functions\ncompleteFileName = \"\"\nsoup = None\n\n#main web scraping function - finds all needed data and writes to text file\ndef webScrape(routeURL, routeName):\n\tglobal completeFileName\n\tglobal soup\n\n\t#gets and requests data from the url\n\turl = routeURL\n\turlRequest = requests.get(url)\n\tsoup = BeautifulSoup(urlRequest.content, 'html5lib') \n\n\t#configures file and path naming\n\ttimeFileName = routeName+\".txt\"\n\tdataFileName = routeName+\"Data.txt\"\n\tpathName = (os.getcwd()+'/timetableFiles/txtFiles')\n\tcompleteFileName = os.path.join(pathName, timeFileName)\n\t#finds all elements with tag 'td' and writes them into the text file\n\ttdValue = 0\n\ttdContainer = soup.findAll('td')\n\n\topenFile=open(completeFileName, 'w')\n\t# goes through each td in the list and writes each value to a new line on the text file\n\tfor td in tdContainer:\n\t\topenFile.write((tdContainer[tdValue].text+'\\n'))\n\t\ttdValue+=1\n\topenFile.close()\n\n\ttotalTables()\n\ttableHeight()\n\ttableWidth()\n\tlineCount()\n\n\ttableData = [str(widthPerTable)+\"\\n\", str(heightPerTable)+\"\\n\", str(fileLength)]\n\n\ttableDataFile = (os.path.join(pathName, dataFileName))\n\topenFile = open(tableDataFile, 'w')\n\topenFile.writelines(tableData)\n\topenFile.close()\n\n#function to find the height of a first bus table - not including notes row\ndef tableHeight():\n\tglobal totalHeight\n\tglobal heightPerTable\n\ttableHeightContainer = soup.findAll('tr', attrs={\"class\": \"table_alt\"})\n\tfor tr in tableHeightContainer:\n\t\ttotalHeight+=1\n\ttableHeightContainer = soup.findAll('tr', attrs={\"class\" : \"table_alt\"})\n\tfor tr in tableHeightContainer:\n\t\ttotalHeight+=1\n\n\t#calculates the average height of each table\n\tfirstColumns = soup.findAll(\"td\", attrs={\"class\":\"first-column\"})\n\tfor td in firstColumns:\n\t\theightPerTable+=1\n\theightPerTable=int(heightPerTable/totalTableCount)\n\n#counts the total number of columns in each table\ndef tableWidth():\n\tglobal widthPerTable\n\ttotal=0\n\tfirstRow = soup.find(\"span\", attrs={\"class\":\"magenta_on\"})\n\tfor i in range (1, (len(firstRow)+1)):\n\t\ttotal+=1\n\ttotal+=(totalTableCount)\n\twidthPerTable = total+3\n\n#finds total amount of tables on a page - not used in end product (not needed)\ndef totalTables():\n\tglobal totalTableCount\n\ttotalTableCount = 0\n\ttableCount = soup.findAll(\"tr\", attrs={\"class\":\"bodytextbold\"})\n\tfor tr in tableCount:\n\t\ttotalTableCount+=1\n\n#creates dictionary of dimensions for all tables on a page - not used in end product (couldnt get accurate values)\ndef fileInput():\n\tglobal tableDict\n\tglobal totalTableCount\n\tglobal heightPerTable\n\ttableDict = []\n\tlineTracker = 0\n\tfile = open(completeFileName)\n\tfileContent = file.readlines()\n\t#makes array 3D for amount of tables\n\tfor i in range(0,totalTableCount):\n\t\ttableDict.append([])\n\n\t#makes the smaller arrays, one for each table line\n\tfor i in range(0, totalTableCount):\n\t\tfor x in range(0,heightPerTable):\n\t\t\ttableDict[i].append([])\n\t\t\tfor b in range(0,11):\n\t\t\t\ttableDict[i][x].append(fileContent[lineTracker])\n\t\t\t\twhile lineTracker != fileLength-1:\n\t\t\t\t\tlineTracker+=1\n\t\t\t\t\tbreak\n\n#counts total lines of data in a given text file\ndef lineCount():\n\tglobal fileLength\n\twith open(completeFileName, 'r') as openFile:\n\t\tfileLength = len(openFile.readlines())\n\n#makes empty arrays to function as dimensions for each table - not used in end product\ndef tableDimensions():\n\tglobal dimensionArray\n\tglobal heightPerTable\n\tfor i in range(0, totalTableCount):\n\t\tdimensionArray.append([])\n\t\tdimensionArray[i].append(widthPerTable)\n\t\tdimensionArray[i].append(heightPerTable)\n\tprint(dimensionArray)\n\n#used instaed of scraping the web to ensure low WIFI usage - this would cut bandwidth as its only accessed if/when needed\n#table on the website is also extremely difficult to read through - making issues during development and data hard to display\ndef cancellations():\n\t#takes user to website in order to \n\twebbrowser.open_new(\"https://www.firstbus.co.uk/doncaster/plan-journey/timetables/journey-cancellations\")\n\n'''\n#finds out overall data \nlineCount()\ntotalTables()\n\n#calculates dimensions of timetables\ntableHeight()\ntableWidth()\ntableDimensions()\n\n#inputs timetable data to file and 3D array\nfileInput()\n'''\n\n#^^functions were called to test - no longer needed to be called, messes with the program when importing module\n\n'''\nwebScrape(\"https://www.firstbus.co.uk/doncaster/plan-journey/timetables/?day=1&source_id=2&service=72%2F72x%2F73%2F73x&routeid=23936179&operator=34&source=sp\", \"73\")\ntotalTables()\ntableWidth()\ntableHeight()\n'''\n#webScrape(\"https://www.firstbus.co.uk/doncaster/plan-journey/timetables/?day=1&source_id=2&service=87%2F87a%2F87b&routeid=23936181&operator=34&source=sp\", \"87\")\n#webScraper was running automatically with 73 route, slowing system down\n\n#print(widthPerTable)\n#print(heightPerTable)\n\n#^^ test to show working web scraping", "506": "from flask import Flask, send_file, make_response, request\r\nfrom webscrape import present_prices\r\napp = Flask(__name__)\r\n\r\n@app.route(\"/\", methods=['GET', 'POST'])\r\ndef index():\r\n    if request.method == 'POST':\r\n        keyword = request.data\r\n        present_prices(keyword)\r\n        return send_file(\"static/js/amazon-output.json\")\r\n\r\n    return send_file(\"templates/index.html\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    app.run(host=\"0.0.0.0\")\r\n", "507": "import os\nimport uuid\n\nimport base64\nimport boto3\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\nRANDOM_NUMBER=str(uuid.uuid4()).lower().replace('-','')\nSECRET_KEY = os.getenv('SECRET_KEY', RANDOM_NUMBER)\n\nDEBUG = os.getenv('DEBUG', 'on') == 'on'\nVARS_ENCRYPTED = os.getenv('VARS_ENCRYPTED', 'off') == 'on'\n\nALLOWED_HOSTS = os.getenv('ALLOWED_HOSTS', 'localhost,*').split(',')\n\nINSTALLED_APPS = [\n    'django.contrib.staticfiles',\n    'storages',\n    'mw',\n    ]\nAPPEND_SLASH = True\n\nMIDDLEWARE = [\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'mw_webscrape.urls'\n\nWSGI_APPLICATION = 'mw_webscrape.wsgi.application'\n\nDATABASES = {}\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n    },\n]\n\nCHROM_VERBOSITY = int(os.getenv('CHROM_VERBOSITY', '0'))  # 99 was the default\nURL = os.getenv('URL')\nUSER = os.getenv('USER')\nPW = os.getenv('PW')\nDASHBOARD = os.getenv('DASHBOARD') # Can not navigate directly\nORDERS = os.getenv('ORDERS')\n\nFRAMEWORK = os.getenv('FRAMEWORK', '')\nif FRAMEWORK == 'Zappa' and VARS_ENCRYPTED:\n    # settings for website to scrape\n    USER = boto3.client('kms').decrypt(CiphertextBlob=base64.b64decode(USER))['Plaintext'].decode('utf8')\n    PW = boto3.client('kms').decrypt(CiphertextBlob=base64.b64decode(PW))['Plaintext'].decode('utf8')\n\nif not FRAMEWORK == 'Zappa' and not VARS_ENCRYPTED:\n    # collectstatic, not zappa, but do, yes, interfere\n    # so, only use when running collectstatic on the cli\n\n    S3_USER_ACCESS_KEY_ID = os.getenv('S3_USER_ACCESS_KEY_ID')\n    S3_SECRET_ACCESS_KEY = os.getenv('S3_SECRET_ACCESS_KEY')\n    AWS_S3_REGION_NAME = os.getenv('REGION_NAME')\n    AWS_ACCESS_KEY_ID = S3_USER_ACCESS_KEY_ID\n    AWS_SECRET_ACCESS_KEY = S3_SECRET_ACCESS_KEY\n\n## collectstatic\nAWS_STORAGE_BUCKET_NAME = os.getenv('BUCKET_NAME')\n# Tell django-storages the domain to use to refer to static files, for templates\nAWS_S3_CUSTOM_DOMAIN = '%s.s3.amazonaws.com' % AWS_STORAGE_BUCKET_NAME\n\n# Tell the staticfiles app to use S3Boto3 storage when writing the collected static files (when\n# you run `collectstatic`).\n# https://www.caktusgroup.com/blog/2014/11/10/Using-Amazon-S3-to-store-your-Django-sites-static-and-media-files/\nSTATICFILES_LOCATION = 'static'\nSTATICFILES_STORAGE = 'custom_storages.StaticStorage'\n\nRESULTFILES_LOCATION = 'results' # where it writes to in S3\nDEFAULT_FILE_STORAGE = 'custom_storages.ResultStorage'\n\n# Take Bucket ACLs\nAWS_DEFAULT_ACL = None\n\n# FunWithStorage\nFUN_WITH_STORAGE = 'fun_with_storage'\n\nRESULTFILE_NAME = os.getenv('RESULTFILE_NAME', 'name_for_dev_file_name')\n# vim: ai et ts=4 sw=4 sts=4 nu ru\n", "508": "", "509": "# Author : Sahil Sharma, Bidya Dash\r\n# import the necessary packages\r\nfrom imutils import paths\r\nimport argparse\r\nimport requests\r\nimport cv2\r\nimport os\r\nimport random\r\nfrom sklearn.model_selection import train_test_split\r\nimport numpy as np\r\n\r\n\r\nclass WebScrape:\r\n    def __init__(self, in_path, out_path):\r\n        self.rows = open(in_path).read().strip().split(\"\\n\")\r\n        self.out_path = out_path\r\n\r\n    def download_image(self):\r\n        # print(rows)\r\n        total = 0\r\n        # loop the URLs\r\n        for url in self.rows:\r\n            try:\r\n                # try to download the image\r\n                r = requests.get(url, timeout=60)\r\n\r\n                # save the image to disk\r\n                p = os.path.sep.join([self.out_path + \"{}.jpg\".format(str(total).zfill(8))])\r\n                f = open(p, \"wb\")\r\n                f.write(r.content)\r\n                f.close()\r\n\r\n                # update the counter\r\n                print(\"[INFO] downloaded: {}\".format(p))\r\n                total += 1\r\n\r\n            # handle if any exceptions are thrown during the download process\r\n            except:\r\n                print(\"[INFO] error downloading {}...skipping\".format(p))\r\n\r\n    def delete_invalid_image(self):\r\n        # loop over the image paths we just downloaded\r\n        for imagePath in paths.list_images(self.out_path):\r\n            # initialize if the image should be deleted or not\r\n            delete = False\r\n\r\n            # try to load the image\r\n            try:\r\n                image = cv2.imread(imagePath)\r\n\r\n                # if the image is `None` then we could not properly load it\r\n                # from disk, so delete it\r\n                if image is None:\r\n                    delete = True\r\n\r\n            # if OpenCV cannot load the image then the image is likely\r\n            # corrupt so we should delete it\r\n            except:\r\n                print(\"Except\")\r\n                delete = True\r\n\r\n            # check to see if the image should be deleted\r\n            if delete:\r\n                print(\"[INFO] deleting {}\".format(imagePath))\r\n                os.remove(imagePath)\r\n\r\n\r\nclass DataPreprocess:\r\n    def resize(self, out_path, write_loc):\r\n        # loop over the image paths we just downloaded\r\n        count = 0\r\n        for imagePath in paths.list_images(out_path):\r\n            try:\r\n                image = cv2.imread(imagePath)\r\n                image = cv2.resize(image, (224, 224))\r\n                cv2.imwrite(write_loc+str(count)+\".jpg\", image)\r\n                count += 1\r\n            except Exception as e:\r\n                print(e)\r\n                print(\"couldn't write this image, skipping\")\r\n\r\n\r\n    def prepare(self, file_loc, test_size=0.25):\r\n\r\n        # grab the image paths and randomly shuffle them\r\n        imagePaths = sorted(list(paths.list_images(file_loc)))\r\n        random.seed(42)\r\n        random.shuffle(imagePaths)\r\n        data = []\r\n        labels = []\r\n        # count = 0\r\n        label1 = 0\r\n        label0 = 0\r\n        label2 = 0\r\n        label3 = 0\r\n        label4 = 0\r\n        total = 0\r\n        # loop over the input images\r\n        for imagePath in imagePaths:\r\n            # load the image, pre-process it, and store it in the data list\r\n            image = cv2.imread(imagePath)\r\n            data.append(image)\r\n\r\n            # extract the class label from the image path and update the\r\n            # labels list\r\n            label = imagePath.split(os.path.sep)[-2]\r\n            # print(label)\r\n            total += 1\r\n            if label == \"Batman\":\r\n                label = 0\r\n                label0 += 1\r\n            elif label == \"Superman\":\r\n                label = 1\r\n                label1 += 1\r\n            elif label == \"Wonderwoman\":\r\n                label = 2\r\n                label2 += 1\r\n            elif label == \"Joker\":\r\n                label = 3\r\n                label3 += 1\r\n            elif label == \"Persons\":\r\n                label = 4\r\n                label4 += 1\r\n            labels.append(label)\r\n        # print(count)\r\n        data = np.array(data)\r\n        labels = np.array(labels)\r\n        print(\"Batman : \" + str(label0))\r\n        print(\"Superman : \" + str(label1))\r\n        print(\"Wonderwoman : \" + str(label2))\r\n        print(\"Joker : \" + str(label3))\r\n        print(\"Persons : \" + str(label4))\r\n        print(\"Total images : \" + str(total))\r\n        # partition the data into training and testing splits using 75% of\r\n        # the data for training and the remaining 25% for testing\r\n        (trainX, testX, trainY, testY) = train_test_split(data,\r\n                                                          labels, test_size=test_size, random_state=42)\r\n        print(\"Split : \" + str(1-test_size) + \" : \" + str(test_size))\r\n        print(\"Splitting the set into Training : \" + str(int(total * (1-test_size))) + \" Test : \" + str(int(total * test_size)))\r\n        return trainX, testX, trainY, testY\r\n\r\n\r\ndef main():\r\n    in_path = \"url/persons.txt\"\r\n    out_path = \"Output/Persons/\"\r\n    write_loc = \"Data/Persons/\"\r\n    file_loc = \"Data\"\r\n    ws = WebScrape(in_path, out_path)\r\n    # ws.download_image()\r\n    # ws.delete_invalid_image()\r\n    process = DataPreprocess()\r\n    process.resize(out_path, write_loc)\r\n    # print(process.prepare(\"Data\"))\r\n    # process.prepare(\"Data\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()", "510": "import os\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlretrieve\nimport codecs\nfrom io import StringIO\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.pdfpage import PDFPage\n\n\n# [FILE DESCRIPTION]\n# python file for scraping websites\n# Input: URL\n# Output: array of txts of the pdfs we scrape the url\n\n# Within each website, possible formats for where content is stored is\n# 1) PDFs (lecture notes, textbooks)\n# 2) Google Slides \"....docs.google.com\"\n# 3) HTML / online textbooks \".....html\" files\n\ndef webscrape(url):\n    \"\"\" Grabs all the pdf files in a website and converts them into text files, storing them in an array that preserves\n    order.\n\n    Inputs: website url\n    Output: array of text <- CAN BE CHANGED CORRESPONDINGLY\n\n    \"\"\"\n    # connect to website and get list of all pdfs\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    links = soup.select(\"a[href$='.pdf']\")\n\n    # clean the pdf link names\n    url_list = []\n    for el in links:\n        if el['href'].startswith('http'):\n            url_list.append(el['href'])\n        else:\n            url_list.append(url + el['href'])\n\n    # TODO: have to change the pdf to text conversion here. (add the pdf2txt function)\n    # TO RITVIK: i had it so that we add in the text itself, but we can have it so that it appends the txt file\n    pdf_txts = []\n    i = 0\n    for url in url_list:\n        url_str = \"{}.pdf\".format(i) # should think about naming as well. Maybe just store it as numbers for now?\n        urlretrieve(url, url_str)\n        txt = pdf2txt(url_str)\n        pdf_txts.append(txt)\n        i = i + 1\n\n    return pdf_txts\n\n#helper to extract text from link of pdf\ndef pdf2txt(url):\n    rsrcmgr = PDFResourceManager()\n    retstr = StringIO()\n    codec = 'utf-8'\n    laparams = LAParams()\n    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n    fp = open(url, 'rb')\n    interpreter = PDFPageInterpreter(rsrcmgr, device)\n    password = \"\"\n    maxpages = 0\n    caching = True\n    pagenos=set()\n\n    i = 1\n    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n        #print(i)\n        interpreter.process_page(page)\n        i = i + 1\n\n    text = retstr.getvalue()\n\n    fp.close()\n    device.close()\n    retstr.close()\n    return text\n\nwebscrape(\"http://www.google.com\")\n", "511": "from django.apps import AppConfig\n\n\nclass WebscrapeConfig(AppConfig):\n    name = 'webscrape'\n", "512": "# Generated by Django 3.2.3 on 2022-02-10 01:50\n\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('webscrape', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.DeleteModel(\n            name='SavedProducts',\n        ),\n    ]\n", "513": "import tkinter as tk\nfrom tkinter.filedialog import askopenfilename\nfrom tkinter.ttk import Progressbar\n\nfrom webscrape.webscrape.SeleniumSD.automate_servicedesk import AutoSD  # importing my own script class\nimport threading\nimport asyncio\nimport time\nimport subprocess\nimport emoji\n\ndef text(index, string):\n    # I do all 3 so it stays disabled again like default after inserting\n    text1.configure(state='normal')\n    text1.insert(index, string)\n    text1.configure(state='disabled')\n\n\n# Controlled by start_submit_thread\ndef submit():\n    row = groupUnassigned_entry.get() # previously hardcoded '0', now gets value from input field\n    print('Script running...')\n\n    text(0.0, '\\n \\n ')\n    text(1.0, '---- Group Unassigned, row:' + ' ' + row + '----' + '\\n')\n    text(2.0, 'Script running...' + '\\n')\n    progressbar.grid(column=0, row=0, sticky=\"w\")  # Start showing the progress bar\n   # window.iconify() # the app will minimize\n    bot = AutoSD()\n    output = bot.main(row)  # return from main function, pass it the row number\n\n    text(3.0, '====================================' + '\\n')\n    text(4.0, output + '\\n')\n    text(5.0, '====================================' + '\\n')\n    text(6.0, 'Script ended.      '\n              '                 ' + '\\n ')\n    progressbar.grid_forget()  # Hide the progress bar\n\n\ndef start_submit_thread(event):\n    global submit_thread\n    submit_thread = threading.Thread(target=event)\n    submit_thread.daemon = True\n    progressbar.start()\n    submit_thread.start()\n    window.after(20, check_submit_thread)\n\n\ndef check_submit_thread():\n    if submit_thread.is_alive():\n        window.after(20, check_submit_thread)\n    else:\n        # progressbar.stop()\n        progressbar.stop()\n\n\ndef close_app():\n    window.destroy()\n\n\ndef print_val():\n    print(inputDc_entry.get())  # gets the value from inputDc input field from user\n    text1.insert(0.0, \"Shazaam\")\n\ndef hello():\n    text(0.0, \"hello!\")\n\ndef open_logs():\n    # filename = askopenfilename(parent=window) # this is how you open File Explorer\n    import os\n    os.system('C:\\\\Users\\\\antonoium\\\\Desktop\\\\venv\\\\webscrape\\\\webscrape\\\\SeleniumSD\\\\logs.txt') # this is how you open a file directly\n\n\nwindow = tk.Tk()\nwindow.title('CSI Quick Tools')\nwindow.geometry(\"400x630\")\nwindow.resizable(\"true\", \"true\")  # (hor, ver.)\nwindow.columnconfigure(0, weight=1)\nwindow.rowconfigure(3, weight=1)\n\n\n#Menu bar\nmenubar = tk.Menu(window)\nwindow.config(menu=menubar)\nfilemenu = tk.Menu(menubar, tearoff=0)\nmenubar.add_cascade(label=\"File\", menu=filemenu)\nfilemenu.add_command(label=\"Logs\", command=lambda: start_submit_thread(open_logs))\nfilemenu.add_command(label=\"ServiceDesk - Get Cookies/ Log In\")\nfilemenu.add_separator()\nfilemenu.add_command(label=\"Exit\", command=window.quit)\n\n\n# Splitting the window in 2 zones\ncenter_frame = tk.Frame(window, borderwidth=2, pady=5)\nbottom_frame = tk.Frame(window, borderwidth=2, pady=5)\n\n# Attributing them to grid\ncenter_frame.grid(row=0, column=0, sticky='n')\nbottom_frame.grid(row=1, column=0, sticky='s')\n\nbottom_frame_1 = tk.Frame(bottom_frame, borderwidth=2)\nbottom_frame_2 = tk.Frame(bottom_frame, borderwidth=2)\n# Header\n\n\nheader = tk.Label(center_frame, text=\"Web Login\", bg=\"grey\", fg=\"black\", height=\"1\", width=\"34\", font={\"Helvetica 14 bold\"})\nheader.pack(fill=\"x\", pady=\"20 0\")\n\n\n\n\n\n# Center frame split into two frames\n\nframe_main_1 = tk.Frame(center_frame, borderwidth=2, relief=\"sunken\")\nframe_main_2 = tk.Frame(center_frame, borderwidth=2, relief=\"sunken\")\nframe_main_3 = tk.Frame(center_frame, borderwidth=2, relief=\"sunken\")\nframe_main_4 = tk.Frame(center_frame, relief=\"sunken\")\nframe_main_5 = tk.Frame(center_frame, borderwidth=2)\n\n\n# First row - Domain Controller\ninputDc = tk.Label(frame_main_1, text=\"Domain Controller \")\ninputDc_StringVar = tk.StringVar()\ninputDc_entry = tk.Entry(frame_main_1, textvariable=inputDc, width=\"25\")  # this is what holds data\ninputDc_entry.insert(0, 'e.g masthaven niuadmin')\ndomainController_button = tk.Button(frame_main_1,\n                                    text=\"Open\",\n                                    command=print_val,\n                                    bg=\"dark green\",\n                                    fg=\"white\",\n                                    relief=\"raised\",\n                                    width=\"10\",\n                                    font=\"Helvetica 10 bold\"\n                                    )\n\n# VDI Broker\nvdiBroker = tk.Label(frame_main_2, text=\"VDI Broker\")\nvdiBroker_StringVar = tk.StringVar()\nvdiBroker_entry = tk.Entry(frame_main_2, textvariable=vdiBroker, width=\"33\")  # this is what holds data\nvdiBroker_entry.insert(0, 'e.g cambridge')\nvdiBroker_button = tk.Button(frame_main_2,\n                             text=\"Open\",\n                             command=print_val,\n                             bg=\"dark green\",\n                             fg=\"white\",\n                             relief=\"raised\",\n                             width=\"10\",\n                             font=\"Helvetica 10 bold\"\n                             )\n\n# Group Unassigned\ngroupUnassigned = tk.Label(frame_main_3, text=\"Group Unassigned\")\ngroupUnassigned_StringVar = tk.StringVar()\ngroupUnassigned_entry = tk.Entry(frame_main_3, textvariable=groupUnassigned, width=\"18\")\ngroupUnassigned_entry.insert(0, '0')\ngroupUnassigned_button = tk.Button(frame_main_3,\n                                   text=\"Assign Row\",\n                                   command=lambda: start_submit_thread(submit),\n                                   bg=\"orange\",\n                                   fg=\"white\",\n                                   relief=\"raised\",\n                                   width=\"15\",\n                                   font=\"Helvetica 10 bold\"\n                                   )\n\ntext1 = tk.Text(frame_main_4,  bg=\"gray\", fg=\"black\", wrap=\"word\", state='disabled')\n\n\n# Packing the things visually\n# here you define padding from frame to frame\nframe_main_1.pack(fill=\"x\", pady=\"2\")\nframe_main_2.pack(fill=\"x\", pady=\"2\")\nframe_main_3.pack(fill=\"x\", pady=\"2\")\nframe_main_4.pack(fill=\"both\", expand = True)\nframe_main_5.pack(fill=\"x\", pady=\"2\")\n\nbottom_frame_1.pack(fill=\"x\", pady=\"2\")\nbottom_frame_2.pack(fill=\"x\", pady=\"2\")\n\n\ninputDc.pack(side=\"left\", padx=\"5\", expand=False)\ninputDc_entry.pack(side=\"left\", padx=\"5\", expand=False)\ndomainController_button.pack(side=\"left\", padx=\"2\", expand=False)\n\nvdiBroker.pack(side=\"left\", padx=\"5\", expand=False)\nvdiBroker_entry.pack(side=\"left\", padx=\"5\", expand=False)\nvdiBroker_button.pack(side=\"left\", padx=\"2\", expand=False)\n\ngroupUnassigned.pack(side=\"left\", padx=\"5\", expand=False)\ngroupUnassigned_entry.pack(side=\"left\", padx=\"5\", expand=False)\ngroupUnassigned_button.pack(side=\"left\", padx=\"2\", expand=False)\n\ntext1.pack(side=\"top\", expand=True)  # without .pack() there is no render on screen\nprogressbar = Progressbar(frame_main_5, mode='indeterminate', length=100)\n\nheart = emoji.emojize(\":green_heart:\")\nfooter = tk.Label(bottom_frame_1, text=\"Made with \" + heart + \" by Madalin\", fg=\"#333333\", font=(\"Courier\", 8), )\nfooter.pack(fill=\"x\", pady=\"35\", side=\"right\")\n\n\n#tk.Button(bottom_frame, text=\"Open Logs\", command=lambda: start_submit_thread(None)).grid(column=0, row=0, sticky=\"S\") # sticky is cardinal points\n\n\nwindow.mainloop()  # End\n\n\n# model, don't delete # tk.Button(frame_bottom_3, text=\"Group Unassigned\", command=lambda: start_submit_thread(None)).grid(column=0, row=2, sticky=\"E\") # this is how you call function on a separate thread\n# tk.Button(frame_bottom_3, text=\"Tickets Number Dropdown\", command=lambda: start_submit_thread(None)).grid(column=0, row=2, sticky=\"E\") # this is how you call function on a separate thread\n", "514": "'''\nCreated on Aug 5, 2021\n\n@author: Jacob Summers\n'''\n\nfrom webscrape.searchresults import prompt_user_search_results_script\n\nfrom webscrape import clear\n\n#insert the spaces into the url search\ndef insertSpaces(name):\n    \n    for i in range(len(name)):\n        if (name[i] == \" \"):\n            num = i + 1\n            name = name[:i] + \"%20\" + name[num:]  \n    \n    ##print(name)\n    newN = name\n    return newN\n    \n    \n\n\n#prompt the user to enter their search choices\ndef prompt_user():\n    print(\"Welcome to Metacritic Webscraping!\\n\\n\")\n   \n    \n    \n    categories = [\"movie\", \"tv\", \"game\", \"album\", \"all\"]\n    print(\"1. Movies\")\n    print(\"2. TV Shows\")\n    print(\"3. Video Games\")\n    print(\"4. Album\")\n    print(\"5. All\")\n    #prompt the user to enter their search category\n    index = input(\"Enter the number that corresponds with the category you would like to search in\\nOr\\nEnter invalid number to exit\\n\")\n    \n    if (index.isdigit() == False):\n        index = -1\n    index = int(index)\n    \n    while(index > 0 and index <= len(categories)):\n        \n        category = categories[index - 1]\n        \n        #convert the user input to lower-case for consistency\n        name = input(\"Enter the name of what you wish to search\\n\")\n        name = name.lower()\n        \n        searchName = name\n        if \" \" in name:\n            searchName = insertSpaces(name)\n         \n         \n        #clear output here\n        clear()\n         \n        if (searchName != None):\n            prompt_user_search_results_script.find_results(searchName, category)\n        \n        #clear output here\n        clear()\n        \n        print(\"1. Movies\")\n        print(\"2. TV Shows\")\n        print(\"3. Video Games\")\n        print(\"4. Album\")\n        print(\"5. All\")\n        \n        index = input(\"Enter the number that corresponds with the category you would like to search in\\nOr\\nEnter invalid number to exit\\n\")\n        #check to see if the index is a valid number\n        if (index.isdigit() == False):\n            index = -1\n        \n        index = int(index)\n        \n    \n    print(\"Thanks for using my program!\")\n    \n    \n\n\n    \n", "515": "from src.config import Config\nfrom src.sql import SQL\n\nfrom src import webscrape\n\nimport praw\nimport os\nimport re\nimport time\n\n# setup config file and SQL\nconfig = Config()\nsql = SQL()\n\n# create reddit instance\nreddit = praw.Reddit(\n    client_id = config.getItem('ACCOUNT', 'CLIENT_ID'),\n    client_secret = config.getItem('ACCOUNT', 'CLIENT_SECRET'),\n    user_agent = config.getItem('ACCOUNT', 'USER_AGENT'),\n    username = config.getItem('ACCOUNT', 'USERNAME'),\n    password = config.getItem('ACCOUNT', 'PASSWORD')\n)\n\ndef findCourses(string):\n    # [A-Z]{4}  match a 4 character string from A to Z\n    # ' '?      match space or no space\n    # [0-9]{4}  match 4 digit number from 0 to 9\n    matches = re.findall(r'[A-Z]{4} ?[0-9]{4}', string.upper())\n\n    # get all the courses and seperate sub and number (COMP2160 -> COMP 2160)\n    for key, value in enumerate(matches):\n        if (len(matches[key].split()) != 2):\n            matches[key] = ' '.join(value[i:i + 4] for i in range(0,len(value), 4))\n\n    return matches\n\ndef log(logType, message):\n    # store time and prepare log message\n    t = time.gmtime()\n    output = f'[{t.tm_year}-{t.tm_mon}-{t.tm_mday} {t.tm_hour}:{t.tm_min}:{t.tm_sec}][{logType}] {message}'\n\n    # open log file for writing (append)\n    f = open('f.log', 'a')\n\n    # write log and print\n    f.write(output + '\\n')\n    print(output)\n\n    # clean up our toys\n    f.close()\n\n# https://www.reddit.com/r/redditdev/comments/7jng5a/whats_the_best_way_to_monitorreply_to_comments/dr7qn4p/\n# solution from another user to having a stream of comments and submissions\ndef customStream(subreddit, **kwargs):\n    # create array of comments and submissions\n    results = []\n    results.extend(subreddit.new(**kwargs))\n    results.extend(subreddit.comments(**kwargs))\n    results.sort(key = lambda post: post.created_utc, reverse = True)\n\n    return results\n\n# continous stream, anything in this is included in the loop\ndef __run__():\n    doReply = False\n\n    stream = praw.models.util.stream_generator(lambda **kwargs: submissions_and_comments(reddit.subreddit('umanitoba'), **kwargs), skip_existing = True)\n    for newPost in stream:\n        entirePostContent = None\n\n        if (type(newPost) is praw.models.Submission):\n            entirePostContent = newPost.title + ' ' + newPost.selftext\n            log('READ', 'Reading submission ' + str(newPost))\n        elif (type(newPost) is praw.models.Comment):\n            # TODO find comments with [ABCD 1234]\n            #entirePostContent = newPost.body\n            log('READ', 'Reading comment ' + str(newPost))\n            entirePostContent = None\n\n        if (entirePostContent != None):\n            courses = findCourses(entirePostContent)\n            replyCourseInfo = []\n\n            for course in courses:\n                courseSplit = course.split()\n\n                result = webscrape.getAuroraCourse(courseSplit[0], courseSplit[1])\n                if (result == None):\n                    continue\n\n                doReply = True\n\n                get = sql.getCourseInfo(courseSplit[0], courseSplit[1])\n\n                if (get != None):\n                    if (get['last_update'] + (60 * 1) < time.time()):\n                        sql.updateCourseInfo(get['id'], result['title'], result['desc'], result['notHeld'], result['preReq'])\n                        replyCourseInfo.append((result['title'], result['desc'], result['notHeld'], result['preReq']))\n                    else:\n                        replyCourseInfo.append((get['title'], get['desc'], get['notHeld'], get['preReq']))\n                else:\n                    sql.insertCourseInfo(courseSplit[0], courseSplit[1], result['title'], result['desc'], result['notHeld'], result['preReq'])\n                    replyCourseInfo.append((result['title'], result['desc'], result['notHeld'], result['preReq']))\n                    \n            if (doReply):\n                log('REPLY', 'Replying to ' + str(new))\n\n                doReply = False\n                \n                replyStr = ''\n\n                for courseInfo in replyCourseInfo:\n                    replyStr = replyStr + '\\n' + courseInfo[0] + '|' + courseInfo[1] + '|' + courseInfo[2] + '|' + courseInfo[3]\n\n                newPost.reply('Course|Description|Not Held With|Prerequisite(s)\\n-|-|-|-' + replyStr)\n\n__run__()\n", "516": "#!/usr/bin/env python\n# coding: utf-8\n\nimport pandas as pd\nfrom tabulate import tabulate\nimport matplotlib.pyplot as plt\n\n\ndef winexp(t1, t2):\n    we2 = 1 / (1 + 10 ** ((t1 - t2) / 400))  # winning expectancy of team 2\n    we1 = 1-we2\n    return round(we1, 2), round(we2, 2)\n\n\ndef webscrape(week, tab):\n    tab2 = tab[week - 1]\n    tab2.dropna(inplace=True)  # cleaning data\n    tab2.drop(tab2.columns[[3, 4]], axis=1, inplace=True)  # removing NaNs\n    tab2.reset_index(drop=True, inplace=True)\n    tab2.columns = ['Team1', 'Score1', 'Score2', 'Team2']\n    return tab2\n\n\ndef elocalc():\n    k = 47  # k-factor\n    while True:\n        region = input(\"Ranking kt\u00f3rej ligi chcesz modyfikowa\u0107? \")\n        try:\n            season = pd.read_html('https://lol.fandom.com/%s/2021_Season/Spring_Season' % region,\n                                  # getting table with results\n                                  attrs={'class': 'wikitable2 matchlist'})\n        except Exception as e:\n            print(e)\n            continue\n        break\n    df = pd.DataFrame(index=['Rating'])\n    week = 1\n    while True:\n        try:\n            results = webscrape(week, season)\n        except (IndexError, ValueError):\n            break\n        for i in range(len(results.index)):\n            team1 = results.loc[i, 'Team1'].replace('\\u2060', '')\n            team2 = results.loc[i, 'Team2'].replace('\\u2060', '')\n            if team1 not in df.columns:\n                df[team1] = 1000\n            if team2 not in df.columns:\n                df[team2] = 1000\n            win1, win2 = winexp(df.loc['Rating', team1], df.loc['Rating', team2])\n\n            if results.loc[i, 'Score1'] > results.loc[i, 'Score2']:\n                df.loc['Rating', team1] += k*(1-win1)\n                df.loc['Rating', team2] += k*(0-win2)\n            else:\n                df.loc['Rating', team1] += k*(0-win1)\n                df.loc['Rating', team2] += k*(1-win2)\n        week += 1\n\n    df = df.T\n    df = df.sort_values(by='Rating', ascending=False)\n    dfprint = df.reset_index()  # dataframe for pretty print\n    dfprint.index += 1\n    dfprint.rename(columns={'index': 'Team'}, inplace=True)\n    df = df.round(1)\n    print(tabulate(dfprint, headers='keys', tablefmt=\"fancy_grid\", floatfmt='.1f'))\n    df.to_csv(region + '.csv', index='True')\n\n\ndef stats():\n    while True:\n        region = input(\"Dla kt\u00f3rej ligi chcesz statystyki: \").upper()\n        try:\n            df = pd.read_csv(region + '.csv', index_col=0)\n        except FileNotFoundError as e:\n            print(e)\n            continue\n        break\n\n    print(df.describe())\n\n\ndef main():\n    print(\"\"\"\\t\\t\\tWitaj w programie ESPORTS ELO\n    Co chcesz zrobi\u0107?\n    1. Policzy\u0107 ranking ELO\n    2. Inne\n        \"\"\")\n    option = int(input(\"\"))\n    if option == 1:\n        elocalc()\n    elif option == 2:\n        stats()\n\n    input()\n\n\nmain()\n", "517": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Aug  3 18:47:25 2020\n\n@author: Rowena\n\nRough and ready web scraping script to determine the number of times fungi are mentioned in UK biology undergraduate modules at Russell Group universities and the number of courses on specifically fungi (relative to the other eukaryote kingdoms of animals and plants)\n\"\"\"\n\nfrom googlesearch import search\nimport requests\nimport re\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n##Webscrape of eukaryote kingdom mentions in Russell university biology modules##\n\n#Make list of Russell Group universities\nrussell_group_list = [\"birmingham\", \"bristol\", \"cambridge\", \"cardiff\", \"durham\", \"edinburgh\", \"exeter\", \"glasgow\", \"imperial\", \"kings\", \"leeds\", \"liverpool\", \"lse\", \"manchester\", \"newcastle\", \"nottingham\", \"oxford\", \"queen mary\", \"belfast\", \"sheffield\", \"southampton\", \"ucl\", \"warwick\", \"york\"]\n\n#Make empty variables to add results to\nrussell_links = []\nbiology_df = []\n\n#For each uni (and its index) in the Russell group list...\nfor  idx, uni in enumerate([s + \" biology undergraduate modules\" for s in russell_group_list]):\n        \n    #Find the top google result for biology modules\n    for url in search(query=uni,\n                      tld = 'com',\n                      lang = 'en',\n                      num = 1,\n                      start = 0,\n                      stop = 1,\n                      pause = 2.0):\n        \n        #Add the link to the results list (in case you want to check manually for sensible links)\n        russell_links.append(url)\n        #Get the page details\n        page = requests.get(url).text\n        \n        #For each of the eukaryote kingdoms...\n        for euk in [\"animal\", \"plant\", \"fungi\"]:\n            #Add the count of uni modules to the results list\n            biology_df.append({'uni' : russell_group_list[idx],\n                           'eukaryote' : euk, \n                           'count' : len(re.findall(euk, page))})\n\n#Convert the module count results list to a dataframe\nbiology_df = pd.DataFrame(biology_df, columns=['uni', 'eukaryote', 'count'])\n\n#Export the dataframe to csv\nbiology_df.to_csv('D:\\\\Documents\\\\Kew\\\\Outreach\\\\Understudied fungi\\\\uk_biology_modules.csv', index=False)\n\n\n##Webscrape of eukaryote kingdom mentions in courses listed on whatuni.com##\n\n#Make empty variable to add results to\ncourses_df = []\n\n#For each of the eukaryote kingdoms...\nfor euk in [\"animal\", \"plant\", \"fungi\"]:\n    #Reset empty results variable\n    results = []\n    #Search for the kingdom on the whatuni website\n    url = 'https://www.whatuni.com/degree-courses/search?q=' + euk\n    page = requests.get(url)\n    #Parse the results with BeautifulSoup\n    soup = BeautifulSoup(page.content, 'html.parser')\n    #Search for the html class which contains number of hits\n    results = soup.find(class_=\"result-head respar2\")\n    #If the class exists (i.e. successful search)...\n    if results:\n        #Add the number of course hits to the results list\n        courses_df.append({'eukaryote' : euk,\n                           'count' : results.text.split('courses')[0].split()[-1]})\n        #If the class doesn't exist (i.e. unsuccessful search)...\n    else:\n        #Add 0 course hits to the results list\n        courses_df.append({'eukaryote' : euk,\n                           'count' : 0})\n    \n#Convert the course count results list to a dataframe\ncourses_df = pd.DataFrame(courses_df, columns=['eukaryote', 'count'])\n\n#Export the dataframe to csv\ncourses_df.to_csv('D:\\\\Documents\\\\Kew\\\\Outreach\\\\Understudied fungi\\\\whatuni_courses.csv', index=False)", "518": "import os\n\nfrom audio import Audio\nfrom bs4 import BeautifulSoup\nfrom cpod_scrape import ScrapeCpod\nfrom terminal_opts import TerminalOptions\nfrom web_scrape import WebScrape\nfrom word_scrape import WordScrape\nfrom write_file import WriteFile\n\n\nclass LessonScrape:\n    def __init__(self, session, dictionary, file_list):\n        self.session = session\n        self.file_list = file_list\n        self.dictionary = dictionary\n        self.start()\n\n    def start(self):\n        wb = WebScrape(self.session)\n        wb.init_driver()\n        word_expansion = []\n\n        for c_lesson in self.file_list:\n            wb.run_webdriver(c_lesson)\n            lesson = wb.get_source()\n            tempfile_path = WriteFile.write_file(\n                \"./data/temp/temp.html\", lesson[\"source\"]\n            )\n            data = open(tempfile_path, \"r\")\n            soup = BeautifulSoup(data, \"html.parser\")\n            wcpod = ScrapeCpod(soup)\n            wcpod.scrape_dialogues()\n            dialogues = wcpod.get_dialogues()\n            if len(dialogues) > 0:\n                self.dictionary.add_sentences(dialogues)\n\n            if \"Vocabulary\" not in lesson[\"not_available\"]:\n                words = wcpod.scrape_lesson_vocab()\n                non_dup_words = [\n                    word.chinese\n                    for word in words\n                    if not self.dictionary.check_for_dup(word, False)\n                ]\n                word_expansion.extend(non_dup_words)\n            if \"Expansion\" not in lesson[\"not_available\"]:\n                wcpod.scrape_expansion()\n                expand = wcpod.get_expansion()\n                self.dictionary.add_sentences(expand)\n\n            if \"Grammar\" not in lesson[\"not_available\"]:\n                wcpod.scrape_lesson_grammar()\n                grammar = wcpod.get_grammar()\n                self.dictionary.add_sentences(grammar)\n\n            try:\n                os.remove(tempfile_path)\n            except OSError as error:\n                raise RuntimeError(error)\n        wb.close()\n        sent_csv_path = WriteFile.write_to_csv(\n            \"./out/lessons.csv\", self.dictionary.get_all_sentences()\n        )\n        sent_audio = TerminalOptions(\n            [\"Yes\", \"No\"], \"Do You Download the Audio for the Sentences?\"\n        ).get_selected()\n        if sent_audio == \"Yes\":\n            Audio(sent_csv_path, \"sentences\")\n        if len(word_expansion) > 0:\n            save_exp_vocab = TerminalOptions(\n                [\"Yes\", \"No\"], \"Do you want add the Lesson Vocabs?\"\n            ).get_selected()\n            if save_exp_vocab == \"Yes\":\n                keepAll = TerminalOptions(\n                    [\"Yes\", \"No\"], \"Keep All of the Words?\"\n                ).get_selected()\n                if keepAll == \"No\":\n                    word_list = TerminalOptions(\n                        [word for word in word_expansion],\n                        \"Select the Words You Want to Keep:\",\n                        True,\n                    ).get_selected()\n                    WordScrape(self.session, self.dictionary, word_list)\n                else:\n                    WordScrape(self.session, self.dictionary, word_expansion)\n", "519": "from selenium import webdriver \nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport csv\nimport urllib.request\nfrom lxml import html\nimport time\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom pprint import pprint as pp\n\nlogin_url = 'https://catalog.jonathancharlesfurniture.com/'\n\nwritefile = open('Classic-Imports-and-Design\\python webscrape\\output.csv', 'w+', encoding='UTF8', newline='')\noutput_file = csv.writer(writefile, delimiter=\",\")\n\ndriver = webdriver.Chrome('Classic-Imports-and-Design\\python webscrape\\chromedriver.exe')\n\n#login\ndriver.get(login_url)\nprint(\"Please log into the wholesale account.\")\nprint('mklatsky')\nprint('23Monday!01')\ninput()\n\ndata = ['SKU', 'Catagory']\noutput_file.writerow(data)\nfails = []\nactive = True\n\nwhile active == True:\n    cat = input('Select a catagory, then provide a name. OR type 000 to terminate: ')\n    if cat == '000':\n        active == False\n        break\n\n    catalog = driver.find_elements(By.CLASS_NAME, 'catalog-item')\n    index = 0\n    for element in catalog:\n        data = []\n        element = driver.find_elements(By.CLASS_NAME, 'catalog-item')[index]\n        all_info = element.find_elements(By.TAG_NAME, 'li')\n        for text in all_info:\n            if ' OH' in text.text:\n                on_hand = int( text.text.strip().rstrip(' OH') )\n            if ' IT' in text.text:\n                in_transit = int( text.text.strip().rstrip(' IT') )\n        if on_hand > 0 or in_transit > 0:\n            sku_link = element.find_element(By.CLASS_NAME, 'catalog-item-number')\n            sku = sku_link.text\n            if sku[-1] == '.' and sku[-2] == '.' and sku[-3] == '.':\n                sku_link.click()\n                try:\n                    element_present = EC.presence_of_element_located((By.CLASS_NAME, 'item-name'))\n                    WebDriverWait(driver, 6).until(element_present)\n                except TimeoutException:\n                    print(sku + ' TOOK TO LONG')\n                sku = driver.find_element(By.CLASS_NAME, 'item-name').find_element(By.TAG_NAME, 'h2').text\n                driver.back()\n                \n            data.append(sku)\n            data.append(cat)\n            pp(data)\n            output_file.writerow(data)        \n        index = index + 1\n\nwritefile.close()\n\n#driver.get(url)\n#time.sleep()\n#content = driver.page_source\n#soup = BeautifulSoup(content, \"html.parser\")\n#tree = html.fromstring(driver.page_source) \n\n#driver.find_elements(By.XPATH, XPATH)\n#driver.find_elements(By.CLASS_NAME, CLASSNAME)\n#soup.find(\"TAG-TYPE\", class_=\"CLASS-NAME\").text\n\n#urllib.request.urlretrieve(IMG-URL, FILENAME) \n\n#output_file.writerow(data)", "520": "# Generated by Django 3.2.3 on 2022-03-13 13:32\n\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('webscrape', '0004_trendsearch'),\n    ]\n\n    operations = [\n        migrations.RenameField(\n            model_name='trendsearch',\n            old_name='linK',\n            new_name='link',\n        ),\n    ]\n", "521": "from django.contrib import admin\r\nfrom django.urls import path\r\nfrom web_scrape import views\r\n\r\n\r\napp_name='web_scrape'\r\n\r\nurlpatterns = [\r\n     path('webscrape', views.webscrape, name='webscrape'),\r\n]\r\n", "522": "\"\"\"\nDjango settings for FinalScraper project.\n\nGenerated by 'django-admin startproject' using Django 3.2.3.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/3.2/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/3.2/ref/settings/\n\"\"\"\n\nimport os\nimport dj_database_url\nfrom pathlib import Path\nfrom django.contrib.messages import constants as messages\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/3.2/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure--ddjw+c@@a6^wj7-adov+uz0&e+6(0^ra)6ijph8d(#*=#3yqp'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = False\n\nALLOWED_HOSTS = ['finalprojectscraper.herokuapp.com', '127.0.0.1']\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'webscrape.apps.WebscrapeConfig',\n    'crispy_forms',\n\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'whitenoise.middleware.WhiteNoiseMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\n# colour code for messages\nMESSAGE_TAGS = {\n    messages.DEBUG: 'alert-secondary',\n    messages.INFO: 'alert-info',\n    messages.SUCCESS: 'alert-success',\n    messages.WARNING: 'alert-warning',\n    messages.ERROR: 'alert-danger',\n}\n\nROOT_URLCONF = 'FinalScraper.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [BASE_DIR / 'templates']\n        ,\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'FinalScraper.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/3.2/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': 'FinalScraper',\n        'USER': 'postgres',\n        'PASSWORD': 'Oreoluwa',\n        'HOST': '127.0.0.1',\n        'PORT': '5432',\n    }\n}\n\ndb_from_env = dj_database_url.config(conn_max_age=600)\nDATABASES['default'].update(db_from_env)\n\n\n# Password validation\n# https://docs.djangoproject.com/en/3.2/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/3.2/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/3.2/howto/static-files/\n\nSTATIC_URL = '/static/'\nSTATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\nBASE_URL = 'https://finalprojectscraper.herokuapp.com/'\n\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, 'static')\n]\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/3.2/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n\n# copied and pasted\nCRISPY_TEMPLATE_PACK = 'bootstrap4'\n\nLOGIN_REDIRECT_URL = 'search-page'\n\nLOGIN_URL = 'login'\n\nLOGOUT_REDIRECT_URL = 'search-page'\n\nLOGOUT_URL = 'logout'\n", "523": "from selenium import webdriver\r\nfrom selenium.webdriver.common.keys import Keys\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom selenium.common.exceptions import TimeoutException\r\nfrom selenium.webdriver.common.by import By\r\nfrom bs4 import BeautifulSoup\r\nimport numpy as np\r\nimport pandas as pd\r\nimport time\r\nimport requests\r\n\r\n\r\ndef initiateDriver():\r\n    webdriver_path = 'C:\\\\IEDriverServer.exe'\r\n    driver = webdriver.Ie(webdriver_path)\r\n    driver.maximize_window()\r\n    return driver\r\n\r\ndef login(driver, user, pwd):\r\n\r\n    # Open IE browser and login to e-portal\r\n    driver.get(\"http://***\")\r\n    id = driver.find_element_by_name(\"textfield32\")\r\n    password = driver.find_element_by_name(\"textfield33\")\r\n    id.send_keys(user)\r\n    password.send_keys(pwd)\r\n    password.send_keys(Keys.RETURN)\r\n\r\n    # Dismiss alert if any popup alert happens\r\n    try:\r\n        WebDriverWait(driver, 2).until(EC.alert_is_present(), 'Timed out waiting for popup to appear')\r\n        driver.switch_to.alert.dismiss()\r\n        print(\"alert dismissed!\")\r\n    except TimeoutException:\r\n        print(\"no dismissed\")\r\n\r\n\r\ndef webScrape(driver, start, end):\r\n    id = [ j + 1 for j in range(start, end)]\r\n    columns = ['\u54e1\u5de5\u7de8\u78bc', '\u59d3\u540d', 'First Name', 'Last Name', \r\n               '\u8fa6\u516c\u5ba4\u96fb\u8a71', 'EMAIL', '\u90e8\u9580\u4ee3\u78bc', '\u90e8\u9580\u540d\u7a31', '\u8077\u7a31']\r\n    df = pd.DataFrame(np.zeros((end - start, 9)), columns=columns, index=id)\r\n    for i in range(start, end):\r\n        url = \"http://***?ID=\" + str(i + 1)\r\n        req = requests.get(url)\r\n\r\n        # Check if url works\r\n        if req.status_code == requests.codes.ok: #pylint: disable=no-member\r\n            try:\r\n                driver.get(url)\r\n                soup = BeautifulSoup(driver.page_source, \"html.parser\")\r\n\r\n            # Invalid ID\r\n            except:\r\n                WebDriverWait(driver, 2).until(EC.alert_is_present(), 'Timed out waiting for popup to appear')\r\n                driver.switch_to.alert.dismiss()\r\n                dataRow = [\"InvalidID\"] * 9\r\n                df.iloc[i - start, :] = dataRow\r\n            else:\r\n                data = soup.find_all(\"nobr\")\r\n                if len(data) == 18:\r\n                    dataRow = [data[2 * j + 1].text for j in range(9)]\r\n                    df.iloc[i - start, :] = dataRow\r\n\r\n                # No data\r\n                else:\r\n                    dataRow = [None] * 9\r\n                    df.iloc[i - start, :] = dataRow\r\n        else:\r\n            print(\"url invalid for id \" + str(i + 1))\r\n\r\n    df.to_excel(\"Employee List.xlsx\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    tStart = time.time()\r\n    driver = initiateDriver()\r\n    login(driver, \"userid\", \"password\")\r\n    webScrape(driver, 0, 4000)\r\n    driver.quit()\r\n    tEnd = time.time()\r\n    print(\"Total running time: \" + str(tEnd - tStart))", "524": "from selenium import webdriver\r\nimport re\r\n\r\n# Set the URL you want to webscrape from\r\n\r\nsite = \"\"\r\ndriver = webdriver.Chrome()\r\ndriver.get(site)\r\nsource = driver.page_source\r\nregex = \"[\\w\\.-]+@[\\w\\.-]+\"\r\nemails = re.findall(regex, source)\r\nfor n, m in enumerate(emails):\r\n    print(n, ') ', m)\r\n\r\ndriver.close()\r\n", "525": "import requests\nimport re\nimport time\n# import urllib.request offline\nfrom bs4 import BeautifulSoup\n# Set the URL you want to webscrape from\nsearch=input('Enter your Search \\n')\nurl = 'https://github.com/search?q={}&type=Users&p='.format(search)\ndata = {}\nfor page in range(10):\n    r = requests.get(url + str(page))\n    print(url + str(page))\n    soup = BeautifulSoup(r.content, \"html.parser\")\n    for link in soup.find_all('a',attrs={'class':'muted-link'}):\n        print(link.get('href'))", "526": "import scrapy\r\nfrom ..items import WebscrapeItem\r\n\r\n\r\nclass ToScrapeCSSSpider(scrapy.Spider):\r\n    name = \"toscrape-css\"\r\n    start_urls = [\r\n        'http://quotes.toscrape.com/',\r\n    ]\r\n\r\n    def parse(self, response):\r\n        items = WebscrapeItem()\r\n        for quote in response.css(\"div.quote\"):\r\n\r\n                items['text'] = quote.css('span.text::text').extract()\r\n                items['author']= quote.css('small.author::text').extract()\r\n                items['tags'] = quote.css('div.tags > a.tag::text').extract()\r\n\r\n                yield items\r\n\r\n        next_page_url = response.css(\"li.next > a::attr(href)\").extract_first()\r\n        if next_page_url is not None:\r\n            yield scrapy.Request(response.urljoin(next_page_url))\r\n", "527": "# Webscrapter for getting updated data for use from api guide url.\n\ndef webscrape_url_apiguide(savepath=''):\n    '''Webscraper function for getting updated api statics from guide url and save to file.'''\n\n    ", "528": "import requests, bs4, re \n\nclass WebScrape:\n\n    objects = []\n    def __init__(self, url, header):\n        self.url = url\n        self.response = requests.get(self.url, headers = header)\n        self.objects.append(self)\n        self.scraper = bs4.BeautifulSoup(self.response.text, features=\"html.parser\")\n\n\n\n", "529": "\"\"\"\nA time helper to get the current hours for a particular location\n--> Modularized in case we want to webscrape the data in the future, instead of hard coding it\n\"\"\"\n\n# HOURS mappings from \n#   int Day --> [datetime startTime, datetime endTime]\n#   the Day int is a dependency on the strftime('%w') interface\nWEIGAND_HOURS = {\n  '0': [], # SUN\n  '1': [], \n  '2': [],\n  '3': [], # WED\n  '4': [],\n  '5': [],\n  '6': [] # SAT\n}\n\n# Returns a timestamp representing the current time\ndef getTodaysHours(locationToken, opening=True):\n  today = datetime.date.today().strftime('%w')\n  \n  # if token == '':\n  return ", "530": "import numpy as np\nimport pymongo\nfrom flask import Flask, jsonify, render_template,redirect,url_for,request,make_response\n\n\napp = Flask(__name__)\n\n\n@app.route('/')\ndef home():\n    import webscrape as ws\n    data = ws.scrapeData()\n    return render_template('index.html', out=data)\n\n\n# @app.route('/update', methods=['POST'])\n# def update():\n#     data = scrape()\n#     return jsonify({\"jsdata\": data})\n\n\n\nif __name__ == '__main__':\n    app.run(debug=True)", "531": "from datetime import datetime\r\n\r\ndef sample_responses(input_text):\r\n\tuser_message = str(input_text).lower()\r\n\t\r\n\tif user_message in( 'hello', 'hi'):\r\n\t\treturn \"hello there!\"\r\n\tif user_message in( 'who are you?'):\r\n\t\treturn \"I am webscrape bot!\"\r\n\t\t\r\n\tif user_message in( 'time'):\r\n\t\tnow = datetime.now()\r\n\t\tdate_time = now.strftime(\"%d/%m/%y, %H:%M:%S \")\r\n\t\treturn date_time\r\n\treturn 'I dont understand what you just said'\r\n", "532": "import webscraper.webscrapeGymgrossisten as wsg\nimport json\n\nimport data_handling.dataHandler as dh\n\n\ndef main():\n    dh.wsgToJson()\n    dh.wsmToJson()\n\n\nif __name__ == \"__main__\":\n    main()\n", "533": "# retrieves links from csv within directory with prefered sizes for items and trigger price for alerts\r\n# current supported stores: centauro, amazon\r\n# imports\r\nfrom bs4 import BeautifulSoup\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.common.exceptions import TimeoutException\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\nfrom datetime import datetime\r\nimport pandas as pd\r\nfrom time import sleep\r\nimport smtplib\r\nimport ssl\r\nfrom email.mime.text import MIMEText\r\nfrom email.mime.multipart import MIMEMultipart\r\nfrom email.utils import formataddr\r\n\r\ndelay = 20  # seconds\r\nlog = \"\"\r\nshopping_list = pd.read_csv('shopping_list_v2.csv', sep=';')\r\nshopping_list.fillna('0', inplace=True)\r\n\r\n\r\ndef webscrape_centauro():\r\n    global log\r\n    selector = '_3teszy'\r\n    sources_list = []\r\n    browser = webdriver.Chrome(ChromeDriverManager().install())\r\n    browser.get(url)\r\n    browser.fullscreen_window()\r\n\r\n    try:\r\n        # wait for data to be loaded\r\n        WebDriverWait(browser, delay).until(\r\n            EC.presence_of_element_located((By.CLASS_NAME, selector))\r\n        )\r\n        # list of colors of item\r\n        btns = browser.find_elements(By.CLASS_NAME, 'box-color')\r\n\r\n        # iterate through all colors and get html\r\n        for btn in btns:\r\n            # WebDriverWait(browser, delay).until(\r\n            #     EC.element_to_be_clickable((By.CLASS_NAME, 'box-color')))\r\n\r\n            sleep(3)\r\n            btn.click()\r\n            sources_list.append(browser.page_source)\r\n\r\n    except TimeoutException:\r\n        log = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Link not working for {url}\\n'\r\n        print(\"Loading took too much time!\")\r\n\r\n    finally:\r\n        sleep(3)\r\n        browser.quit()\r\n\r\n    prices_log = pd.read_excel('itens.xlsx')\r\n\r\n    for source in sources_list:\r\n\r\n        # scrape properties from html\r\n        soup = BeautifulSoup(source, features=\"lxml\")  # turns into BeatifulSoup object\r\n        span_tags = soup.find('span', {'class': '_3teszy'})\r\n        curr_price = float(span_tags.text.strip('R$ ').replace(',', '.'))\r\n        item_name = soup.find('h1', {'class': '_gjoabl'}).text\r\n        sizes = [x.text for x in soup.find_all('div', {'class': '_1uax8x0'})]\r\n        my_size = sum([x in sizes for x in target_sizes]) > 0\r\n        curr_color = soup.find('h3', {'class': '_3lyjer color-selected-label'}).text[5:]\r\n        curr_datetime = datetime.now()\r\n        mail_check = False\r\n\r\n        # check if item and color is already in the file\r\n        if len(prices_log.query('name == @item_name and color == @curr_color').index) == 0:\r\n            price_dict = {'name': item_name, 'my_size': my_size, 'color': curr_color,\r\n                          'last_price': curr_price, 'date_last_price': curr_datetime,\r\n                          'high_price': curr_price, 'date_high_price': curr_datetime,\r\n                          'low_price': curr_price, 'date_low_price': curr_datetime,\r\n                          'store': 'centauro'\r\n                          }\r\n            prices_log = prices_log.append(price_dict, ignore_index=True)\r\n            log = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Added item {item_name} - {curr_color}\\n'\r\n            continue\r\n\r\n        # fill values\r\n        current_item = (prices_log['name'] == item_name) & (prices_log['color'] == curr_color)\r\n        position = prices_log[current_item].index\r\n        prices_log.at[position[0], 'last_price'] = curr_price\r\n        prices_log.at[position[0], 'date_last_price'] = curr_datetime\r\n        prices_log.at[position[0], 'my_size'] = my_size\r\n\r\n        # checks for new low price\r\n        if target_price >= curr_price:\r\n            if not mail_check:\r\n                try:\r\n                    send_mail(url, item_name, curr_price, my_size, curr_color, curr_store)\r\n                    mail_check = True\r\n\r\n                except Exception as erro:\r\n                    print(f'Error {erro.__class__}')\r\n\r\n        if float(prices_log.loc[current_item, 'low_price']) > curr_price:\r\n\r\n            prices_log.loc[current_item, 'low_price'] = curr_price\r\n            prices_log.loc[current_item, 'date_low_price'] = curr_datetime\r\n\r\n            log = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - New low price on item {item_name} - {curr_color}\\n'\r\n\r\n            if not mail_check:\r\n                try:\r\n                    send_mail(url, item_name, curr_price, my_size, curr_color, curr_store)\r\n                    mail_check = True\r\n\r\n                except Exception as erro:\r\n                    print(f'Error {erro.__class__}')\r\n\r\n            print('alert new low price')\r\n\r\n        elif float(prices_log.loc[current_item, 'high_price']) < curr_price:\r\n\r\n            prices_log.loc[current_item, 'high_price'] = curr_price\r\n            prices_log.loc[current_item, 'date_high_price'] = curr_datetime\r\n\r\n    prices_log.to_excel('itens.xlsx', index=False)\r\n\r\n\r\ndef webscrape_amazon():\r\n    \"\"\"\r\n    Scrapes the price, item name, color/model and checks if prefered size is available. Register current price,\r\n    and size check to excel file \"itens\" in the same directory.\r\n\r\n    Compares current price with lower price registered. Alerts in case of new low price.\r\n    \"\"\"\r\n\r\n    global log\r\n    selector = 'productTitle'\r\n    browser = webdriver.Chrome(ChromeDriverManager().install())\r\n    browser.get(url)\r\n    browser.fullscreen_window()\r\n\r\n    try:\r\n        # wait for data to be loaded\r\n        WebDriverWait(browser, delay).until(\r\n            EC.presence_of_element_located((By.ID, selector))\r\n        )\r\n        sleep(3)\r\n        source = browser.page_source\r\n\r\n    except TimeoutException:\r\n        log = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Link not working for {url}\\n'\r\n        print(\"Loading took too much time!\")\r\n        return\r\n\r\n    finally:\r\n        sleep(1)\r\n        browser.quit()\r\n\r\n    prices_log = pd.read_excel('itens.xlsx')\r\n\r\n    # scrape properties from html\r\n    soup = BeautifulSoup(source, features=\"lxml\")  # turns into BeatifulSoup object\r\n    try:\r\n        span_tags = soup.find('div', {'id': 'corePrice_desktop'}).select('span[class*=\"apexPriceToPay\"]')[0].find('span', {\"aria-hidden\": \"true\"})\r\n    except IndexError:\r\n        item_name = soup.find('span', {'id': 'productTitle'}).text.strip()\r\n        log = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Item {item_name} is unavailable on {curr_store.capitalize()}\\n'\r\n        return\r\n    except AttributeError:\r\n        item_name = soup.find('span', {'id': 'productTitle'}).text.strip()\r\n        log = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Couldnt locate price for item {item_name} on {curr_store.capitalize()}\\n'\r\n        return\r\n\r\n    curr_price = float(span_tags.text.strip('R$ ').replace(',', '.'))\r\n    item_name = soup.find('span', {'id': 'productTitle'}).text.strip()\r\n    # sizes = [x.text for x in soup.find_all('div', {'class': '_1uax8x0'})]\r\n    my_size = True\r\n    try:\r\n        curr_color = soup.find('div', {'id': 'variation_color_name'}).find('span', {'class': 'selection'}).text\r\n    except AttributeError:\r\n        curr_color = ' '\r\n\r\n    curr_datetime = datetime.now()\r\n    mail_check = False\r\n\r\n    # check if item and color is already in the file\r\n    if len(prices_log.query('name == @item_name and color == @curr_color').index) == 0:\r\n        price_dict = {'name': item_name, 'my_size': my_size, 'color': curr_color,\r\n                      'last_price': curr_price, 'date_last_price': curr_datetime,\r\n                      'high_price': curr_price, 'date_high_price': curr_datetime,\r\n                      'low_price': curr_price, 'date_low_price': curr_datetime,\r\n                      'store': 'amazon'\r\n                      }\r\n        prices_log = prices_log.append(price_dict, ignore_index=True)\r\n        log = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - Added item {item_name} - {curr_color}\\n'\r\n        prices_log.to_excel('itens.xlsx', index=False)\r\n        return\r\n\r\n    # fill values\r\n    current_item = (prices_log['name'] == item_name) & (prices_log['color'] == curr_color)\r\n    position = prices_log[current_item].index\r\n    prices_log.at[position[0], 'last_price'] = curr_price\r\n    prices_log.at[position[0], 'date_last_price'] = curr_datetime\r\n    prices_log.at[position[0], 'my_size'] = my_size\r\n\r\n    # checks for new low price\r\n    if target_price >= curr_price:\r\n        if not mail_check:\r\n            try:\r\n                send_mail(url, item_name, curr_price, my_size, curr_color, curr_store)\r\n                mail_check = True\r\n\r\n            except Exception as erro:\r\n                print(f'Error {erro.__class__}')\r\n\r\n    if float(prices_log.loc[current_item, 'low_price']) > curr_price:\r\n\r\n        prices_log.loc[current_item, 'low_price'] = curr_price\r\n        prices_log.loc[current_item, 'date_low_price'] = curr_datetime\r\n\r\n        log = log + f'{datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")} - New low price on item {item_name} - {curr_color}\\n'\r\n\r\n        if not mail_check:\r\n            try:\r\n                send_mail(url, item_name, curr_price, my_size, curr_color, curr_store)\r\n                mail_check = True\r\n\r\n            except Exception as erro:\r\n                print(f'Error {erro.__class__}')\r\n\r\n        print('alert new low price')\r\n\r\n    elif float(prices_log.loc[current_item, 'high_price']) < curr_price:\r\n\r\n        prices_log.loc[current_item, 'high_price'] = curr_price\r\n        prices_log.loc[current_item, 'date_high_price'] = curr_datetime\r\n\r\n    prices_log.to_excel('itens.xlsx', index=False)\r\n\r\n\r\ndef send_mail(link, product, price, size, color, store):\r\n    \"\"\"\r\n    Sends an email through gmail account alerting of a price found on item.\r\n    \"\"\"\r\n    port = 465  # For SSL\r\n    password = \"password\"  # generated app password\r\n\r\n    sender_email = \"email@gmail.com\"\r\n    receiver_email = \"email@provider.com\"\r\n\r\n    message = MIMEMultipart(\"alternative\")\r\n    message[\"Subject\"] = f\"Alert for {product} on {store.capitalize()}\"\r\n    message[\"From\"] = formataddr(('Your WebScraper', sender_email))\r\n    message[\"To\"] = receiver_email\r\n\r\n    # send both HTML and plain text version of the same email\r\n    text = f\"\"\"\\\r\n        Hi,\r\n        I found the item {product} {color} for R$ {price}! Your size {\"is\" if size else \"isn't\"} available\r\n        Click the link below for more information\r\n        {link}\r\n        \"\"\"\r\n\r\n    mail_html = f\"\"\"\\\r\n        \r\n          \nHi,\r\n               I found the item {product} {color} for R$ {price}! Your size {\"is\" if size else \"isn't\"} available.  \r\n               Click the link for more information.\r\n            \r\n          \r\n        \r\n        \"\"\"\r\n\r\n    part1 = MIMEText(text, \"plain\")\r\n    part2 = MIMEText(mail_html, \"html\")\r\n\r\n    message.attach(part1)\r\n    message.attach(part2)\r\n\r\n    # Create secure connection with server and send email\r\n    context = ssl.create_default_context()\r\n    with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\r\n        server.login(sender_email, password)\r\n        server.sendmail(\r\n            sender_email, receiver_email, message.as_string()\r\n        )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    for label, row in shopping_list.iterrows():\r\n        url = row['link']\r\n        target_sizes = row['size(s)'].split(',')\r\n        target_price = row['buy_below']\r\n        curr_store = row['store']\r\n\r\n        print(f'checking link {url}')\r\n        if curr_store.strip().lower() == 'centauro':\r\n            try:\r\n                webscrape_centauro()\r\n\r\n            except Exception as erro:\r\n                print(f'Error {erro.__class__}')\r\n                continue\r\n\r\n        elif curr_store.strip().lower() == 'amazon':\r\n            webscrape_amazon()\r\n\r\n    # writes log to file\r\n    with open(\"Log.txt\", \"a\") as text_file:\r\n        text_file.write(log)\r\n\r\n", "534": "from operator import ilshift\nfrom sqlite3 import Date\nfrom bs4 import BeautifulSoup\nimport requests, time, re, csv, S3_Upload\nfrom sqlalchemy import null\n\n\ndef Intro():\n    print(\" \")\n    print(\">>>>>>>>>>>>> 0-60 Time import by Kordelle Walker <<<<<<<<<<<<\")\n    print(\"This Script webscrapes 0-60 times for every make and model car and\")\n    print(\"sends the output to local directory in multiple .csv format files\")\n    while True:\n       answer = input('Do you want to create/update all csv files?: (y/n)')\n       if answer.lower().startswith(\"y\"):\n          print(\"Creating/Updating csv files now...\")\n          break\n       elif answer.lower().startswith(\"n\"):\n          print(\"Creation/Update Cancelled\")\n          exit()\n\nIntro()\n\ni = -1\n#        0       1      2       3         4         5        6        7     8      9       10         11       12    13     14       15       16       17          18          19       20        21     22      23      24       25         26          \ncar = ['audi','acura','bmw','bugatti','cadillac','dodge','ferrari','ford','gmc','honda','hyundai','infiniti','jeep','kia','lexus','lincoln','mazda','mercedes','mitsubishi','nissan','pontiac','porsche','ram','subaru','tesla','toyota','volkswagen']\nurl = f\"https://www.zeroto60times.com/vehicle-make/{''.join(car[i])}-0-60-mph-times/\"   \nresult = requests.get(url)\ndoc = BeautifulSoup(result.text, \"html.parser\")\nlist = doc.find_all('div',\"stats__list__accordion__body__stat\" )\n\ndef time_func():\n       for j in list:\n            jam = j.find_all('span', \"stats__list__accordion__body__stat__top__right__stat-time\")\n            fire = j.find_all('div', \"stats__list__accordion__body__stat__top__title\")\n            if not jam:\n                continue\n            mash = [jam[0].text.replace(\"[\",\"\")]\n            separator = \", \"\n            hot = separator.join(mash)\n            potato = [fire[0].text.strip()]\n            chocolate = separator.join(potato)\n            vanilla = separator.join(chocolate.split(\" \", maxsplit=1))\n            #cheese = [jam[1].text]\n            dataset = [hot,vanilla[0:4],car[i].upper(),vanilla[6:].upper()]\n            writer.writerow(dataset)\n\ndirectory = f\"/Users/kordellewalker/Documents/GitHub/DEProjects/Webscrape-DWH/Results_test/{''.join(car[i])}.csv\"\nmaster_directory = f\"/Users/kordellewalker/Documents/GitHub/DEProjects/Webscrape-DWH/Results/master.csv\"\n# Overwriting initial CSV Document\nprint(\"Writing individual [model.csv] files...\")\nwhile i < 26:\n    i += 1\n    print(f\"Writing {''.join(car[i])}.csv...\",i,\"/26\")\n    with open(f\"Resultss/{''.join(car[i])}.csv\", \"w\", newline= '') as csvfile:\n        writer = csv.writer(csvfile)\n        header = ['0_60_time','year','manufacturer','model']\n        writer.writerow(header)\n        car = ['audi','acura','bmw','bugatti','cadillac','dodge','ferrari','ford','gmc','honda','hyundai','infiniti','jeep','kia','lexus','lincoln','mazda','mercedes','mitsubishi','nissan','pontiac','porsche','ram','subaru','tesla','toyota','volkswagen']\n        url = f\"https://www.zeroto60times.com/vehicle-make/{''.join(car[i])}-0-60-mph-times/\"\n        result = requests.get(url)\n        doc = BeautifulSoup(result.text, \"html.parser\")\n        list = doc.find_all('div',\"stats__list__accordion__body__stat\" )\n        time_func()\nprint(\"Individual [model.csv] files created\")\ntime.sleep(2)\n\n# Writining and Appending a new master file\nprint(\"Now Writing and Appending [master_table.csv] file...\")\ntime.sleep(2)\nwith open(\"Resultss/master_table.csv\", \"w\", newline ='') as csvfile:\n    writer = csv.writer(csvfile)\n    header = ['0_60_time','year','manufacturer','model']\n    writer.writerow(header)\n\ni = -1\nwhile i < 26:\n    i += 1\n    print(\"Writing to master_table.csv...\",i,\"/26\",f\"(Appending {''.join(car[i])}.csv)\")\n    with open(\"Resultss/master_table.csv\", \"a\", newline= '') as csvfile:\n        writer = csv.writer(csvfile)\n        car = ['audi','acura','bmw','bugatti','cadillac','dodge','ferrari','ford','gmc','honda','hyundai','infiniti','jeep','kia','lexus','lincoln','mazda','mercedes','mitsubishi','nissan','pontiac','porsche','ram','subaru','tesla','toyota','volkswagen']\n        url = f\"https://www.zeroto60times.com/vehicle-make/{''.join(car[i])}-0-60-mph-times/\"\n        result = requests.get(url)\n        doc = BeautifulSoup(result.text, \"html.parser\")\n        list = doc.find_all('div',\"stats__list__accordion__body__stat\" )\n        time_func()   \n\nprint(\"--------Creation/Update Complete-------\")\n\nwhile True:\n    answer = input('Do you want to update Amazon AWS S3 Bucket?: (y/n)')\n    if answer.lower().startswith(\"y\"):\n        print(\"Uploading csv files now...\")\n        S3_Upload\n        break\n    elif answer.lower().startswith(\"n\"):\n        print(\"AWS S3 Update Cancelled\")\n        print(\"Goodbye\")\n        exit()\n\nprint(\"--------AWS S3 Upload Complete-------\")\nprint(\"Goodbye\")\n\n\n\n\n", "535": "import requests\nfrom bs4 import BeautifulSoup\n\nclass Webscrape():\n    \n    def __init__(self):\n        self.unrated = {\n            'Not Rated',\n            'Unrated',\n        }\n\n    # year, imdb rating, intended audience, runtime\n    def get_info(self, imdb_num):\n        url = f'https://www.imdb.com/title/tt{imdb_num}/'\n        page = requests.get(url) # Replace with other imdb link\n        soup = BeautifulSoup(page.content, 'html.parser')\n\n        li = [x.get_text() for x in soup.find_all(class_='ipc-inline-list__item')]\n        x = 0\n        while not li[x][-4:].isnumeric():\n            x += 1\n        # shift = -1 if li[2][-1].isnumeric() else 0 # some movies don't have trivia\n        has_audience = 1 if not li[x+1][0].isnumeric() else 0 # some movies don't have an \"Unrated\" option...\n        '''\n        0 Cast & crew\n        1 User reviews\n        2 Trivia\n        3 Episode aired Apr 6, 1997\n        4 PGPG\n        5 1h 47m\n        '''\n        # print(li[:6])\n        # print(f'date: {x}', f'missing_audience: {has_audience}')\n        year = li[x][-4:]\n        if has_audience: audience = li[x+1][:len(li[x+1])//2]\n        else: audience = audience = 'Unrated'\n        runtime = self.string_to_minutes(li[has_audience + x + 1])\n\n        imdb_rating = soup.find_all(class_='sc-7ab21ed2-1 jGRxWM')[0].get_text() # rating from imdb\n        \n        # print(year, imdb_rating, audience, runtime)\n        return year, imdb_rating, self.unrated_helper(audience), runtime\n\n    def string_to_minutes(self, timestring):\n        # 1h 48m\n        # 2h 03,\n        hours_mins = timestring.strip().split()\n        if (len(hours_mins) == 2) : return int(hours_mins[0][-2]) * 60 + int(hours_mins[1][-3:-1])\n        if (hours_mins[0][-1] == 'm'): return int(hours_mins[0][:-1]) # if only minutes runtime\n        return int(hours_mins[0][-2]) * 60\n\n    def unrated_helper(self, s):\n        if s in self.unrated:\n            return 'Unrated'\n        return s\n\n'''\ncheck\n'''\nif __name__ == '__main__':\n    li = [\n        '0114117', # 1997 7.6 Unrated 107\n        # '0112896', # 1995 5.8 Unrated 88\n        # '0114709', # 1995 8.3 Unrated 81\n        # '0114916', # 1995 6.6 Unrated 94\n        # '0114814', # 1995 8.5 Unrated 106\n        # '0113819', # 1995 7.0 Unrated 95\n        # '0114916', # 1995 6.6 Unrated 94\n        # '0110299', # 1994 7.5 Unrated 116\n        # '0113247', # 1995 8.1 Unrated 98\n        # '0113283', # 1995 6.6 Unrated 106\n        # '0106473', # 1992 7.6 TV-14 93\n        '0109890',\n        '0109891',\n        '114709',\n    ] # problems\n\n\n    scraper = Webscrape()\n    for x in li:\n        scraper.get_info(x)\n        print()\n\n", "536": "# TODO: webscrape off https://oscar.gatech.edu/bprod/bwckctlg.p_disp_dyn_ctlg\n\n", "537": "import ClointFusion as cf\nimport helium\nimport locale\nimport time\nimport os \nlocale.setlocale( locale.LC_ALL, 'en_US.UTF-8' ) \n\n#scraping info on the given website to notepad\ncf.browser_activate(\"link\")\npath = os.path.join(os.getcwd(),\"generated_files\")\ncf.scrape_save_contents_to_notepad(folderPathToSaveTheNotepad=path)\ncf.browser_quit_h()\n\n\nclass WebScrape:\n    \n    def get_converted_value(value_list:list[list[str,str,int]])->list[int]:\n        \"\"\"Get the converted values for passed required conversions in a list\n        Args : list containing the conversion details \n        returns conversion values for the given list\"\"\"\n\n        cf.browser_activate()\n        this_list = []\n        for val_list in value_list:\n            helium.go_to(f\"https://www.xe.com/currencyconverter/convert/?Amount={val_list[2]}&From={val_list[0]}&To={val_list[1]}\")\n            value = cf.browser_locate_element_h(selector=r\"//*[@id='__next']/div[2]/div[2]/section/div[2]/div/main/form/div[2]/div[1]/p[2]\",get_text = True)\n            this_list.append(round(locale.atof(value.split()[0]),3))\n        cf.browser_quit_h()\n        return this_list\n    \n    \n\nclass ExcelSheet:\n   \n    def __init__(self,pathtofolder:str,filename:str,sheet_name:str,pathtosourcefolder=None)->None:\n        \"\"\"Initialising function to create an excel doc at the given location (which acts as an object of the class throughout)\n            if source folder path is mentioned then it will be copied into the newly created excel document\n            Params : pathtofolder : Path where the excel sheet has to be created\n                    filename : filename of the excel document\n                    sheet_name :Sheet name of the sheet created in excel sheets by default\n                    pathtosourcefolder :Path from which the excel sheet is to be copied (None by default)\n\n            Returns : None\n                    \"\"\"\n        self.path = cf.os.path.join(pathtofolder, filename)\n        self.name = sheet_name\n  \n        cf.excel_create_excel_file_in_given_folder(fullPathToTheFolder=pathtofolder,excelFileName=filename,sheet_name=sheet_name)\n        print(f\"New Excel Sheet created at {pathtofolder} with name {self.name}\")\n        if pathtosourcefolder:\n            cf.shutil.copyfile(pathtosourcefolder,self.path)\n\n\n    def insert_data(self,data_list:list)->None:\n        \"\"\"Inserts the given data in dictionary format into the excel sheet\n            Params : data list\n            Returns : None\"\"\"\n        row_count,_ = cf.excel_get_row_column_count(excel_path = self.path,sheet_name = self.name,header = 0)\n    \n        if row_count==1:\n            for i in range(len(data_list)):\n                cf.excel_set_single_cell(excel_path = self.path, sheet_name = self.name,header = 0, columnName = \"\",cellNumber = i,setText = data_list[i])\n\n#opening the notepad in which the content is scraped and filtering that content for the data\nwith open(path) as file:\n    content = file.readlines()\n    file.close()\n#filtering\nfiltered_content = [content[i].strip().split() for i in range(len(content)) if i%2==0 and i>2]\n\n#getting the converted values into a list\nvalue_list = WebScrape.get_converted_value(filtered_content)\n\n#merging the converted values\nfor i in range(len(filtered_content)):\n    filtered_content[i].append(value_list[i])\n#converting the amount to int\nflitered_content =  [int(i[2]) for i in filtered_content]\n\n#excel file generation\nfiltered_content.insert(0,['From',\"To\",\"Amount\",\"Converted\"])\nexcel_path  = os.path.join(path,\"Conversions.xlsx\")\ncf.excel_copy_paste_range_from_to_sheet(excel_path=excel_path, startCol=1, startRow=1, endRow=6, endCol=4, copiedData=filtered_content)\n\n\n", "538": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom wheel.cli import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "539": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom flask.cli import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "540": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom wheel.cli import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "541": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom wheel.cli import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "542": "# import urllib.request\n# from urllib.request import urlopen\nfrom html.parser import HTMLParser\n#import re\nimport requests\nfrom requests_html import HTMLSession\nfrom requests_html import HTML\nimport pandas as pd\n\n# Problem 1 (2 points)\n# Assign the 'name' variable an object that is your name of type str.\nname: str = \"Dan Fawcett\"\n\n# Problem 2 (2 points)\n# Create a class and implement it for your problem of interest\nclass Deuce():\n  \"\"\" This class serves as a storage mechanism for list of urls\"\"\"  \n  # Constant URL http://www.deucegym.com/community/2021-12-01/\n  DEUCE_URL = \"http://www.deucegym.com/community/\" # Webscrape\n  A_BLOG_XPATH = '/html/body/div[1]/main/center/article/div/a'\n  DIV_WODBLOCK_XPATH = '/html/body/div[1]/main/center/article/div/div[3]'\n  DEUCE_ATHLETICS_GPP = 'DEUCE ATHLETICS GPP'\n  DEUCE_GARAGE_GPP = 'DEUCE GARAGE GPP'\n  \n  def __init__(self, gpp_type=DEUCE_GARAGE_GPP, workout_dates=['2021-12-01', '2021-12-02']):\n    self.gpp_type = gpp_type\n    self.workout_dates = workout_dates\n    self.wod_urls = []\n    self.get_wod_url()\n    self.web_data = WebData()\n    #self.cycle_wods_json = self._web_data.cycle_wods_json()\n\n  def add_wod_url(self, a_href: str):\n    self.wod_urls.append(a_href)\n\n  def get_wod_url(self) -> None:\n    # TODO: get this working for singleton then loop it => for wod_date in workout_dates:\n    wod_date = self.workout_dates[0]\n    wod_url_base = Deuce.DEUCE_URL + wod_date\n    self.web_data = WebData(wod_url_base)\n    obj_html = self.web_data.html\n    sel_url = obj_html.xpath(Deuce.A_BLOG_XPATH)\n    #list_wod_links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", xhtml)\n    wod_url = sel_url[0].links.pop()\n    # for url in list_wod_links:\n    #     if wod_url_base in url:\n    #       wod_url = url\n    #       break\n    #print(\"wod_url => \", wod_url)\n    #a_href =  url_get_contents(wod_url[0]).decode('utf-8')\n    self.add_wod_url(wod_url)\n\n# Problem 3 (2 points)\n# Create another class and implement it for your problem of interest\nclass WebData:\n    \"\"\" This class serves as a storage mechanism for an HTTPResponse object data decoded to utf-8 string\"\"\"    \n    def __init__(self, url: str = ''):\n        self.url = url\n        if url != '':\n          self.html: HTML = self.webscrape_html_data(self.url)\n\n    def webscrape_html_data(self, url) -> HTML:\n        \"\"\" Opens a website and read its binary contents (HTTP Response Body)\"\"\"   \n        try:\n            session = HTMLSession()\n            raw_html_data = session.get(url)     \n            clean_html_data = HTML(html=replace_chars(raw_html_data.text))\n        except requests.exceptions.RequestException as e:\n            print(e)\n        return clean_html_data\n\n# Problem 4 (2 points)\n# Create another class and implement it for your problem of interest\nclass HTMLDeuceParser(HTMLParser):\n    \"\"\" This class serves as a html Deuce GPP parser. It extends HTMLParser and is able to parse div\n    tags containing class=\"wod_block\" which you feed in. You can access the result per .wod_table field.\n    \"\"\"\n    def __init__(self, html: HTML):\n        self.recording = False\n        self.html = html\n        self.wodblock = self.html.xpath(Deuce.DIV_WODBLOCK_XPATH) \n        self.tag = ''\n        self.wod_table = []\n        #self.convert_charrefs = False\n        # initialize the base class\n        HTMLParser.__init__(self)         \n\n    def handle_starttag(self, tag, attrs):      \n        if tag == 'div':\n            self.tag = 'th'\n            # for value in attrs:\n                # print(value)\n                # print(\"Encountered the beginning of a %s tag\" % tag)\n            self.recording = True \n        elif tag == 'h2':\n            self.tag = 'th'\n            # for value in attrs:\n                # print(value)\n                # print(\"Encountered the beginning of a %s tag\" % tag)\n            self.recording = True  \n        elif tag == 'p':\n            self.tag = 'tr'\n            # for value in attrs:\n                # print(value)\n                # print(\"Encountered the beginning of a %s tag\" % tag)\n            self.recording = True                 \n        elif tag == 'span':\n            self.tag = 'td'\n            # for value in attrs:\n                # print(value)\n                # print(\"Encountered the beginning of a %s tag\" % tag)\n            self.recording = True                            \n        else:\n            self.recording = True\n            return\n\n    def handle_endtag(self, tag):\n        if tag == 'div' and self.recording == True:\n            self.tag = 'th'\n            self.recording = False \n            # print(\"Encountered the end of a %s tag\" % tag)\n        elif tag == 'h2' and self.recording == True:\n            self.tag = 'th'\n            self.recording = False \n            # print(\"Encountered the end of a %s tag\" % tag)     \n        elif (tag == 'b' or tag == 'br') and self.recording == True:\n            pass # FIXME: Does this break the table?\n            # self.recording = False\n        elif tag == 'p' and self.recording == True:\n            self.tag = 'tr'\n            self.recording = False \n            # print(\"Encountered the end of a %s tag\" % tag)       \n        elif tag == 'span' and self.recording == True:\n            self.tag = 'td'\n            self.recording = False \n            # print(\"Encountered the end of a %s tag\" % tag)              \n        else:\n            return # We don't want               \n\n    def handle_data(self, data):\n        if self.recording == True:\n            self.wod_table.append('<' + self.tag + '>' + data + '' + self.tag + '>')\n\n# If you need to, you can create any additional classes or functions here as well.\n# Replace \\n and \\t and \\r embedded \\0 strings with empty string\ndef replace_chars(s: str) -> str:\n    s = s.replace('\\n', '').replace('\\t', '').replace('\\r', '').replace('\\0', '')\n    return s  \n\ndef create_table(table_data: list) -> str:\n    table_data[0] = '' + str(table_data[0]) + ''\n    garage_idx = table_data.index('' + Deuce.DEUCE_GARAGE_GPP + '')\n    table_data[garage_idx] = '' + Deuce.DEUCE_GARAGE_GPP + ''\n    table_data[-1] = str(table_data[-1]) + ''\n    created_table = ''.join(table_data)\n    return created_table\n# Problem 5 (2 points)\n# Assign a variable named 'obj_1' an example instance of one of your classes\nobj_1 = Deuce()    \n    \n# Problem 6 (2 points)\n#  Assign a variable named 'obj_2' an example instance of another one of your\n#  classes\nobj_2 = WebData(obj_1.wod_urls[0])\n\n# Problem 7 (2 points)\n#  Assign a variable named 'obj_3' an example instance of one of your classes\n#  that extends another class\nobj_3 = HTMLDeuceParser(obj_2.html)\nobj_3.feed(obj_3.wodblock[0].html)\nhtml_table = create_table(obj_3.wod_table)\ndf_wod = pd.read_html(html_table)\nprint(df_wod)\n# obj_3 = HTMLDeuceParser()\n# obj_3.feed(obj_2.html_data\n# #print(obj_3.wod_data)\n# obj_3.close()", "543": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom chardet.cli.chardetect import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "544": "# webscrape script for\n# finding github user profile pic\n\nimport requests\nfrom bs4 import BeautifulSoup as bs\n\ngithub_user = input(\"Input Github user: \")\nurl = \"https://github.com/\" + github_user\nr = requests.get(url)\n\n# prints HTML file\nsoup = bs(r.content, \"html.parser\")\n# print(soup)\n\n# find img tag with alt: Avatar and retrieve src link\nprofile_pic = soup.find(\"img\", {\"alt\": \"Avatar\"})[\"src\"]\nprint(profile_pic)\n", "545": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom setuptools.command.easy_install import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "546": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom setuptools.command.easy_install import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "547": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom setuptools.command.easy_install import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "548": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom pip._internal.cli.main import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "549": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom pip._internal.cli.main import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "550": "#!/home/drpi/Python/WebScrapeRM/webscrape-rm/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom pip._internal.cli.main import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "551": "'''\ntitle: GUI for Window\nauthors: Palaash & Shaishav\ndate created: 2020-06-11\n'''\n\nfrom tkinter import *\nfrom GUI import topStocksGUI\nfrom GUI import searchBarGUI\nfrom GUI import graphGUI\nfrom GUI import newsGUI\nfrom GUI import stockinfoGUI\nfrom matplotlib import rcParams\nimport matplotlib.pyplot as plt\nfrom webscraping import *\n\nclass Engine:\n    def __init__(self):\n\n        # The basics of creating a window here... Nothing fancy\n        self.window = Tk()\n        self.window.minsize(1028, 720)\n        self.window.title(\"Stock Trader\")\n        self.window.configure(bg=\"#1c1c1c\")\n        self.window.resizable(False, False) # Keeping it non-resizeable so it doesn't alter how the content will look\n        self.currentTicker = 'SPY' # Default stock shown\n        self.changed = False # To identify if the stock has been changed\n        self.window.iconphoto(False, PhotoImage(file='favicon.png'))\n        #\n\n        # Search\n        self.searchBar = searchBarGUI.SearchBarGUI(self.window,self)\n\n        # Top Stocks\n        self.topStocks = topStocksGUI.TopStocksGUI(self.window,self)\n\n        # Graph -- Much of the graph config has to be done here since the variables have to be accessed in this file.\n\n            # Colors\n\n        rcParams['axes.labelcolor'] = 'white'\n        rcParams['xtick.color'] = 'white'\n        rcParams['axes.titleweight'] = \"bold\"\n        rcParams['ytick.color'] = 'white'\n        rcParams['text.color'] = 'white'\n\n            # Configs\n        self.fig = plt.figure(figsize=(7, 3), dpi=100) # Basically a 700x300 i,age\n        self.fig.patch.set_facecolor(\"#1c1c1c\")\n        self.graph = self.fig.add_subplot(1, 1, 1)\n        self.graph.set_facecolor('#454444')\n\n            # Graph Gui\n        self.graphGUI =  graphGUI.GraphGUI(self.window,self.fig,self.graph)\n\n        # Stock info\n        self.urlFinviz = \"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker) # URL FOR FINVIZ\n        self.urlYahoo = 'https://ca.finance.yahoo.com/quote/{0}'.format(self.currentTicker) # URL FOR YAHOO FINANCE\n\n        self.pageFinviz = webScrapeURL(self.urlFinviz) # webscrape the finviz page\n        self.pageYahoo= webScrapeURL(self.urlYahoo) # webscrape the yahoo page\n\n        # News feed GUI\n        self.newsfeed = newsGUI.NewsfeedGUI(self.window, self.pageFinviz) # create the news feed\n\n        # Stock info GUI\n        self.stockInfo = stockinfoGUI.StockInfo(self.window, self.pageFinviz)\n\n    # Modify Methods #\n\n    def changeTicker(self,ticker): # Self-explanatory\n        self.currentTicker = ticker\n        self.graphGUI.ticker = ticker\n\n    def updateEverything(self): # Re-Init stuff which changes based on stocks\n        self.newsfeed.__init__(self.window,webScrapeURL(\"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker)))\n        self.stockInfo.__init__(self.window,webScrapeURL(\"https://finviz.com/quote.ashx?t={0}\".format(self.currentTicker)))\n\n    # Accessor Methods #\n\n    def getWindow(self):\n        return self.window\n    def getGraphClass(self):\n        return self.graphGUI\n    def getTicker(self):\n        return self.currentTicker", "552": "# To add a new cell, type '# %%'\n# To add a new markdown cell, type '# %% [markdown]'\n# %%\nimport requests\nimport os\nimport json\nimport sys\nfrom bs4 import BeautifulSoup\nimport urllib.parse\n\nimport mysql.connector\nimport re\nimport math\n\nfrom selenium import webdriver\n\n# %%\nsys.path.append('D:\\Development\\Repositories\\WineFoundry\\pyLightspeed')\nprint(sys.path)\n\nfrom lsretail import api_dev as lsretail\n#from lsecom import api as lsecom\n\n# %%https://www.wine.com/product/chateau-vitallis-pouilly-fuisse-2018/540407\n\n#establishing the connection\nconn = mysql.connector.connect(user='jamie', password='W!neL0ver', host='127.0.0.1', database='vintagewine')\n#Creating a cursor object using the cursor() method\ncursor = conn.cursor()\n\n#And set up the webdriver\ndriver = webdriver.Chrome('D:\\\\Development\\\\chromedriver.exe')\n# %%\nstore_datafile = 'D:\\\\Development\\\\.keys\\\\vintage_keys.json'\nlsr = lsretail.Connection(store_datafile)\n\n# %%\ndef process_image(url, image_path, filename, item_id, description = \"Image\", ordering = 0):\n    lsr._manage_rate()\n    with open(image_path+filename, \"wb\") as f:\n        f.write(requests.get(url).content)\n    url = lsr.api_url+'Image.json'\n    \n    files = {'image': (filename, open(image_path + filename, 'rb'), 'image/jpeg')}\n    payload = {'data': '{\"description\": \"' + description + '\", \"ordering\": ' + ordering +', \"itemID\": ' + str(item_id) +'}'}\n    r = requests.post(url, files=files, data=payload, headers=lsr.headers)\n    #print(r.text)\n\ndef round_up(n, decimals=0):\n    multiplier = 10 ** decimals\n    return math.ceil(n * multiplier) / multiplier\n\ndef mark_as_added(itemID, status='true'):\n    #changes the local copy of the item to show that the item has been added to ecom incase the script starts overdef round_up(n, decimals=0):\n    update_ecom = f\"UPDATE item SET publishToEcom = '{status}' WHERE itemID = {itemID}\"\n    cursor.execute(update_ecom)\n    conn.commit()\n# %%\n# Build the list of items that have not been added to eCom. This omits beers and food.\n# TODO: This should turn into a function that returns a list.\nquery_items_to_add = \"\"\"SELECT\n                        item.itemID\n                        FROM item\n                        INNER JOIN itemshop\n                            ON item.itemID = itemshop.itemID\n                        INNER JOIN category\n                            ON item.categoryID = category.categoryID\n                        WHERE itemshop.qoh > 0\n                        AND item.archived = 'false'\n                        AND item.publishToEcom = 'false'\n                        AND itemshop.shopID = 1\n                        AND category.parentID <> 178\n                        AND category.categoryID NOT IN (177,178)\n                        ORDER BY item.createTime DESC\"\"\"\ncursor.execute(query_items_to_add)\n#Pulls results in to list of rows that look like (123,), and passes it into a list\nitems_to_add = [x[0] for x in cursor.fetchall()]\n\n\n# %%\n#This loops through the list and prompts for updates.\n#TODO: This should also be rewritten to move the actual look up and scrape to a function\n\nitem_id=\"\"\nfor item_id in items_to_add:\n    #item_id = input('Item ID from Lightspeed? : ')\n    #if item_id ==\"quit\": break\n\n    item = lsr.get(\"Item\", rid=item_id)\n    system_id = item[\"Item\"][\"systemSku\"]\n\n    print(f\"I am looking for : {item['Item']['description']}\")\n    driver.get(f\"http://www.wine.com/search/{urllib.parse.quote(item['Item']['description'])}/0?showOutOfStock=true\")\n\n    if input(\"Have you found the right page? : \") == 'y':\n        page_link = driver.current_url\n    else:\n        mark_as_added(item_id, status=\"fail\")\n        continue\n \n\n    image_path = 'D:\\\\Data\\\\CloudStation\\\\Vintage\\\\ECommerce\\\\Graphics\\\\Products\\\\'\n    # this is the url that we've already determined is safe and legal to scrape from.\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'}\n\n\n    page_response = requests.get(page_link, headers=headers, timeout=5)\n    # here, we fetch the content from the url, using the requests library\n    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n    #we use the html parser to parse the url content and store it in a variable.\n    textContent = []\n    # %%\n    # %% [markdown]\n    # # Build and Store Webscrape\n    # I am storing webscraped data in a webscrape table. This isn't an automated scrape, but I still want to put it in there. Also, since this is manually matched\n    # this data can be used in the future to train record matching algos.\n    #\n    if page_content.find(class_='icon icon-screwcap prodAttr_icon prodAttr_icon-screwcap'):\n        closure = 'Screwcap'\n    else:\n        closure = ''\n\n    if page_content.find(class_='icon icon-glass-red prodAttr_icon prodAttr_icon-redWine'):\n        category = 'Red'\n    elif page_content.find(class_='icon icon-glass-white prodAttr_icon prodAttr_icon-whiteWine'):\n        category = 'White'\n    elif page_content.find(class_='icon icon-champagne prodAttr_icon prodAttr_icon-champagne'):\n        category = 'Sparkling'\n    elif page_content.find(class_='icon icon-glass-white prodAttr_icon prodAttr_icon-roseWine'):\n        category = 'Rose'\n    else:\n        category = ''\n\n    try:\n        price = float(re.findall('[0-9.]+', page_content.find(class_='productPrice').text)[0].strip())\n    except AttributeError:\n        price = 0\n    \n    try:\n        msrp = round_up(float(re.findall('[0-9.]+', page_content.find(class_='productPrice_price-regWhole').text)[0].strip()))\n    except AttributeError:\n        msrp = price\n\n    try:\n        vintage = re.findall('[0-9]+', page_content.find(class_='pipName').text)[0].strip()\n    except IndexError:\n        vintage = 0\n\n    # %%\n    insert_webscrape = (\"INSERT INTO webscrapes \"\n                        \"(retail_systemID, is_match, best_match, _web_scraper_order, web_scraper_start_url, scrape_sourceID, sku, product, product_href, title, name, vintage, producer, brand, price, msrp, region, subregion, description, size, alcohol, closure, category, varietal) \"\n                        \"VALUES (%(retail_systemID)s, %(is_match)s, %(best_match)s, %(_web_scraper_order)s, %(web_scraper_start_url)s, %(scrape_sourceID)s, %(sku)s, %(product)s, %(product_href)s, %(title)s, %(name)s, %(vintage)s, %(producer)s, %(brand)s, %(price)s, %(msrp)s, %(region)s, %(subregion)s, %(description)s, %(size)s, %(alcohol)s, %(closure)s, %(category)s, %(varietal)s)\")\n\n    data = {\n        'retail_systemID': system_id,\n        'is_match': 1,\n        'best_match': 1,\n        '_web_scraper_order': system_id,\n        'web_scraper_start_url': page_link,\n        'scrape_sourceID': 7,\n        'sku':  page_content.find(attrs={'name':'productID'})[\"content\"],\n        'product': page_content.find(class_='pipName').text,\n        'product_href': page_link,\n        'title': page_content.find(class_='pipName').text,\n        'name':  re.findall('[A-z ]+', page_content.find(class_='pipName').text)[0].strip(),\n        'vintage': vintage,\n        'producer': page_content.find(class_='pipWinery_headlineLink').text,\n        'brand': page_content.find(class_='pipWinery_headlineLink').text,\n        'price': price,\n        'msrp': msrp,\n        'region': page_content.find(attrs={'name':'productVarietal'})[\"content\"],\n        'subregion':  page_content.find(attrs={'name':'productRegion'})[\"content\"],\n        'description':  page_content.find(class_='viewMoreModule_text').text,\n        'size': page_content.find(class_='prodAlcoholVolume_text').text,\n        'alcohol': page_content.find(class_='prodAlcoholPercent_percent').text,\n        'closure':  closure,\n        'category': category,\n        'varietal': page_content.find(attrs={'name':'productVarietal'})[\"content\"]  \n    }\n\n    cursor.execute(insert_webscrape, data)\n    conn.commit()\n\n    # %% [markdown]\n    # ## Store Reviews and Build Description\n    # \n    # We store reviews in the Reviews table, and use them to create an HTML description to post.\n\n    # %%\n    # Start the description so we can append reviews if any\n    item_content =\"\"\n    item_content = '' + page_content.find(class_='viewMoreModule_text').text\n\n    reviews = page_content.select('div.pipProfessionalReviews_list')\n    # If there are reviews, add a header to the content\n    if reviews:\n        item_content = item_content + 'Tasting Notes'\n    else:\n        item_content = item_content + ''\n\n    for review in reviews:\n        # Writing the HTML is failing, I am sure because I need to escape some things, but it isn't really needed so I am skipping it.\n        insert_review = (\"INSERT INTO wine_reviews \"\n                    \"(source, initials, rating, review_detail, wine_itemID, wine_systemSKU, url) \"\n                    \"VALUES (%(source)s, %(initials)s, %(rating)s, %(review_detail)s, %(wine_itemID)s, %(wine_systemSKU)s, %(url)s)\")\n        data = {\n            'source': review.select('div[class=\"pipProfessionalReviews_authorName\"]')[0].text,\n            'initials': review.select('span[class=\"wineRatings_initials\"]')[0].text,\n            'rating': int(review.select('span[class=\"wineRatings_rating\"]')[0].text),\n            'review_detail': review.select('div[class=\"pipProfessionalReviews_review\"]')[0].text,\n            'wine_itemID': int(item_id),\n            'wine_systemSKU': system_id,\n            'url': page_link\n            \n        }\n        item_content = item_content + \"\" + review.select('span[class=\"wineRatings_rating\"]')[0].text + \" \" + review.select('div[class=\"pipProfessionalReviews_authorName\"]')[0].text + \"\"\n        item_content = item_content + review.select('div[class=\"pipProfessionalReviews_review\"]')[0].text + \"\"\n        cursor.execute(insert_review, data)\n        conn.commit()\n\n\n    \n    # %%\n    # ## Build the data that gets pushed in to Lightspeed\n\n   \n    #item_content = '' + page_content.find(class_='pipWineNotes_copy viewMoreModule').text + 'Tasting Notes'\n    # \n    # If it got a good score, we want to hightlight it\"\n    if reviews and int(review.select('span[class=\"wineRatings_rating\"]')[0].text) >= 90:\n        item_description = reviews[0].select('span[class=\"wineRatings_rating\"]')[0].text + \"pts. \" + page_content.find(class_='viewMoreModule_text').text[:100]+\"...\"\n    else:\n        item_description = page_content.find(class_='viewMoreModule_text').text[:112]+\"...\"\n\n    # ## Write what little we can to Lightspeed\n    # Lightspeed won't let you write directly to Product in eCom for things that are omni, so we have to make sure they are not active for eCom in Retail, then write using the retail api, then activate them for eCom.\n\n    # %%\n#\"publishToEcom\": True,\n    data = {\"publishToEcom\": True,\n            \"Prices\":{\n                \"ItemPrice\":{\n                    \"amount\": msrp,\n                    \"useType\":\"MSRP\",\n                    \"useTypeID\": 2\n                    }\n                },\n            \"ItemECommerce\":{\n                \"shortDescription\":item_description,\n                \"longDescription\": item_content,\n                \"weight\": 48        \n                }\n            }\n    \n    lsr.update(\"Item\", item_id, data)\n    mark_as_added(item_id)\n\n    # %% [markdown]\n    # # Do the Images\n    # Gets the wine images off the page, catches all the information, stores it, and puts the image in Lightspeed via API\n\n    # %%\n    images = page_content.find_all(class_='pipThumb_image')\n    for image in images:\n        # urlparse will take the url from the image link...\n        a = urllib.parse.urlparse(image.get('src'))\n        # Then we split out the filename and the extension\n        (sourcefilename, ext) = os.path.splitext(os.path.basename(a.path))\n        link = 'https://www.wine.com/product/images/'+ os.path.basename(a.path)\n        ordering = \"0\"\n\n        if \"Bottle\" in image.get('alt'):\n            if \"Front\" in image.get('alt'):\n                filename = system_id + '_btl'+ ext\n                ordering = \"1\"\n            elif \"Back\" in image.get('alt'):\n                filename = system_id + '_btlb'+ ext\n                ordering = \"3\"\n            else:\n                filename = system_id + '_btl_'+ sourcefilename + ext\n                ordering = \"5\"\n        elif \"Label\" in image.get('alt'):\n            if \"Front\" in image.get('alt'):\n                filename = system_id + '_lbl'+ ext\n                ordering = \"2\"\n            elif \"Back\" in image.get('alt'):\n                filename = system_id + '_lblb'+ ext\n                ordering = \"4\"\n            else:\n                filename = system_id + '_lbl_'+ sourcefilename + ext\n                ordering = \"6\"\n        else:\n            filename = system_id + sourcefilename + ext\n        process_image(link, image_path, filename, item_id, description = image.get('alt'), ordering = ordering)\n        print(f\"This is {image.get('alt')} from {a.path} saved as {filename}\")\n   \ndriver.close() \n\n\n", "553": "import joblib\nimport json \nimport random as rand\nimport requests \nimport os\nimport re\n\nfrom flask import Flask\nfrom geolib import geohash\nfrom google.cloud import firestore\nfrom label import LabelNews\nfrom newspaper import Article, ArticleException\n#===============================================================================\n\n\n# information needed to make the request\nsubscription_key = \"6b62615906f84b91b3cceb985b4011e6\"\nsearch_term = \"Microsoft\"\nsearch_url = \"https://api.cognitive.microsoft.com/bing/v7.0/news/search\"\ncities = [\"long-beach\",\"lakewood\",\"cerritos\", \"bellflower\"]\n\n#helper function to strip html element tags --- used in webscrape()\ndef striphtml(data):\n    p = re.compile(r'')\n    return p.sub('', data)\n\ndef webscrape(city, key, url):\n    \"\"\" Creates a request for news article that contain a city, gets the \n    response in JSON format, and both adds and removes information based \n    on what we need.\n    \n    RETURNS\n    -------\n    A list of headline items, each item is a dictionary.\n    \"\"\"\n    \n    # header contains info to give us access to the site at url\n    headers = {\"Ocp-Apim-Subscription-Key\": key}  \n    params = {\"q\":city,\"textDecorations\": True,\n              \"textFormat\":\"HTML\",\"freshness\":\"Week\"}  \n    print(\"pulling from Microsoft ...\\n\")\n    response = requests.get(url, headers=headers, params=params)\n    response.raise_for_status()\n    response_json = response.json()\n    # clean up response json - only keep 'value' tag\n    print(\"isolating 'value' data ...\\n\")\n    tags_to_remove = ['_type','readLink','queryContext',\n                      'totalEstimatedMatches','sort']\n    for tag in tags_to_remove:\n        response_json.pop(tag)\n    # articles are under the 'value\" key in response_json\n    articles = response_json['value']  \n    # get summaries\n    print(\"retrieving summaries ...\\n\")\n    for item in articles :\n        try:\n            news = Article(item[\"url\"])\n            news.download()\n            news.parse()\n            news.nlp()\n            item[\"summary\"] = news.summary\n        except ArticleException:\n            print(\"Forbidden Article\")\n            item[\"summary\"] = \"no summary\" #-!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n    important_tags = [\"city\",'datePublished','name',\n                      'organization','summary','url']\n    # clean each article\n    print(\"\\ncollecting payload ...\\n\")\n    payload = []\n    for item in articles :\n        # want to save organization name from 'provider' key, ... \n        # ... then remove it and save under key 'organization'\n        organization = item['provider'][0]['name']\n        item.pop('provider')\n        item['organization'] = organization\n        item[\"city\"] = city\n        # remove other unneccessary tags in headline name \n        item[\"name\"] = striphtml(item['name'])\n        \n        money = dict((k, item[k]) for k in important_tags)\n\n        payload.append(money)\n                \n    return payload\n#===============================================================================\ndef gen_id(news_item) :\n    '''generates a unique id for a news headline by concatenating the article's\n    geohash with its publication date.\n    \n    ARGS\n    ---\n    news_item (dict) : news headline object in format ...\n    \n    RETURNS\n    ---\n    unique id as a string\n    '''\n    return news_item[\"geohash\"] + '_' + news_item[\"datePublished\"]\n    \ndef locate(news_lst, city) :\n    '''Temporary geohasher assigns random locations based on city parameter; also \n    assigns unique id's\n    \n    ARGS\n    ---\n    news_lst : list of news articles in format ...\n    city (string) : collection id\n    \n    RETURNS\n    ---\n    list of news articles in format ...\n    '''\n    #long beach values\n    if (city == \"long-beach\"):\n        max_lat = 33.8765\n        min_lat = 33.7664\n        max_lon = -118.0997\n        min_lon = -118.2042\n\n    #cerritos values\n    elif (city == \"cerritos\"):\n        max_lat = 33.8879\n        min_lat = 33.8459\n        max_lon = -118.0287\n        min_lon = -118.1085\n    #\n    #bellflower values\n    elif (city == \"bellflower\"):\n        max_lat = 33.9105\n        min_lat = 33.8656\n        max_lon = -118.1067\n        min_lon = -118.1514\n    #\n    #lakewood values\n    elif (city == \"lakewood\"):\n        max_lat = 33.8692\n        min_lat = 33.8202\n        max_lon = -118.0590\n        min_lon = -118.1677\n\n\n    rand.seed()\n    #\n    for item in news_lst :\n        lati = min_lat + (max_lat-min_lat)*rand.random()\n        lon = min_lon + (max_lon-min_lon)*rand.random()\n        ghash = geohash.encode(lati, lon, 7)\n        loncoord = float(geohash.decode(ghash).lon)\n        latcoord = float(geohash.decode(ghash).lat)\n        item[\"geohash\"] = ghash\n        item[\"id\"] = gen_id(item)\n        item[\"coordinates\"] = firestore.GeoPoint(latcoord, loncoord)\n    return news_lst\n#===============================================================================\n\n\n\napp = Flask(__name__) \n#\nlabeler = joblib.load(\"labeler_02.joblib\")\nmodel = joblib.load(\"mnb-model_02.joblib\")\nvectorizer = joblib.load(\"vectorizer_02.joblib\")\n\n\n@app.route('/')\ndef collect() :\n    '''docstring'''\n    text = []\n    #\n    for c in cities:\n        # scrape\n        news = webscrape(c, subscription_key, search_url)\n        # geohash and id\n        news_lst = locate(news,c)\n        # label\n        ln = LabelNews(labeler, model, news_lst, vectorizer)\n        news_lst = ln.assign_topics()\n        #\n        # Project ID determined by GCLOUD_PROJECT environment variable \n        print(\"writing to Firestore ...\\n\")\n        db = firestore.Client()    \n        for item in news_lst :\n            doc_ref = db.collection(\"Testing Collections\").document(c).collection(\"Articles\").document(item[\"id\"])\n            doc_ref.set({\n                \"datePublished\" : item[\"datePublished\"],\n                \"name\" : item[\"name\"],\n                \"organization\" : item[\"organization\"],\n                \"summary\" : item[\"summary\"],\n                \"url\" : item[\"url\"],\n                \"coordinates\" : item[\"coordinates\"],\n                \"topic\" : item[\"topic\"],\n                \"g\" : {\n                    \"geohash\" : item[\"geohash\"],\n                    \"geopoint\" : item[\"coordinates\"]\n                }\n            })\n        #    \n    for i in news_lst :\n        text.append(i[\"topic\"] + \" : \" + i[\"name\"])\n    #    \n    text = \";_____\".join(text)\n    name = os.environ.get('NAME', text)\n    return \"{}\".format(name)\n\n\n\nif __name__ == \"__main__\" :\n    app.run(debug=True, host=\"0.0.0.0\", port=os.environ.get(\"PORT\", 8080))\n    #\n    print(\"\\nDONE!\")\n", "554": "#!/usr/bin/env python3\n# Amos \n#2/4/2022\n\n#this section imports external functions\nfrom fileinput import close\nimport requests\nimport re\n\n#this section defines the function called scrape\ndef scrape(url):\n    #requests one url\n    r = requests.get(url)\n    #uses regex to parse through all data for valid URLs\n    matches = re.findall(r'href=[\\'|\\\"](\\S*)[\\'|\\\"]', r.text)\n    fixed = []\n    for x in matches:\n        try:\n            #turns relative url paths into absolute paths\n            if x[0] == '/':\n                fixed.append(f'{url}{x}')\n            # looks for urls starting with http and includes https\n            elif x[0:4] == 'http':\n                fixed.append(x)\n            #passes on broken or invalid links\n            else:\n                pass\n        except IndexError:\n            pass\n\n    # returs results into a list called fixed\n    return fixed\n\n#the target url\nweb = 'http://example.com'\n#results from the first scrape of the target url \nres = scrape(web)\n#defines fin as a set \n#sets dont duplicate\nfin = set()\n#creates a list called res2 \nres2 = []\n#specifies one item in the list res in a loop \nfor x in res:\n    #adds the results of the first webscrape into the fin set\n    fin.add(x)\n    #adds the results of the first scrape into the list res2\n    res2.extend(scrape(x))\n\n#creates a list called res3\nres3 = []\n#specifies one item in the list res2 in a loop\nfor x in res2:\n    #adds the 2nd results of the first webscrape into the fin set\n    fin.add(x)\n    #adds the 2nd results of the first scrape into the list res2\n    res3.extend(scrape(x))\n#specifies one item in the list res3 in a loop \nfor x in res3:\n    ##adds the 3rd results of the first scrape into the fin set \n    fin.add(x)\n\n#specifies one item in the set fin and initialises a loop \nfor x in fin:\n    #opens the file OP1.txt in append mode \n    f = open(\"OP1.txt\", \"a\")\n    #writes the output for x, then moves to the next line\n    f.write(str(x+'\\n'))\n    #closes the file \n    f.close\n\n\n", "555": "from bs4 import BeautifulSoup\nimport requests\n\n\nclass WebScrape:\n\n    def scrape(self, URL):\n        \"\"\"function for getting the web content\"\"\"\n        html_text = requests.get(URL).text\n        html_content = BeautifulSoup(html_text, \"lxml\").text\n        return html_content\n\n\n\n\n", "556": "import os\nimport time\nimport requests \nfrom bs4 import BeautifulSoup \nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options  \n\n\ndef webscrape():\n\n\tpath = os.environ['PATH_CHROMEDRIVER']\n\n\tchrome_options = Options()  \n\tchrome_options.add_argument(\"--headless\") #headless browsing\n\n\t#updates the driver object\n\tdriver = webdriver.Chrome(executable_path=path, chrome_options=chrome_options)\n\n\tdriver.get('https://pogoapi.net/Is_It_Shiny/') #async\n\n\ttime.sleep(1) #forces 1-second wait for driver.get to complete\n\t\t\n\thtml_rendered = driver.page_source #page source sould be rendered by now\n\n\t#Beautiful Soup: Build a Web Scraper With Python https://realpython.com/beautiful-soup-web-scraper-python/\n\tsoup = BeautifulSoup(html_rendered, 'html.parser') \n\tresults = soup.find(id='pokemonList') # pull out html for pokemonList id that lists all pokemon currently in the game and their shiny status\n\n\t\n\t#\tprint(\"this is results: \", results.prettify()) # nicely format with .prettify() dataquest.io/blog/web-scraping-tutorial-python/\n\t#extract the text of all  tags within results \"Finding all instances of a tag at once\" dataquest.io/blog/web-scraping-tutorial-python/\n\t# Beautifulsoup loop through HTML https://stackoverflow.com/questions/38519975/beautifulsoup-loop-through-html\n\n\ttd_tags = results.find_all('td')\n\n\tlist_pokemon_and_shiny_status = []\n\n\tfor td_tag in td_tags:\n\t\n\t\t#\"Once we\u00e2\u20ac\u2122ve isolated the tag, we can use the get_text method to extract all of the text inside the tag\" dataquest.io/blog/web-scraping-tutorial-python/\n\n\t\t#The strip() method returns a copy of the string with both leading and trailing characters removed (based on the string argument passed).\n\t\t#e.g. empty spaces\n\t\t#https://towardsdatascience.com/top-5-beautiful-soup-functions-7bfe5a693482#:~:text=One%20of%20them%20is%20Beautiful,order%20to%20get%20data%20easily.&text=The%20basic%20process%20goes%20something,it%20any%20way%20you%20want.\n\t\tlist_pokemon_and_shiny_status.append(td_tag.get_text().strip())\n\n\n\tdict_pokemon_shinystatus = {}\n\n\t#slice list for pokemon names https://www.xspdf.com/resolution/52771228.html\n\tpokemon_list = list_pokemon_and_shiny_status[1::3]\n\n\t#slice list for shiny status\n\tshiny_status = list_pokemon_and_shiny_status[2::3]\n\n\t## Merge the two lists to create a dictionary\n\t#https://thispointer.com/python-6-different-ways-to-create-dictionaries/\n\tdict_pokemon_shinystatus = dict(zip(pokemon_list, shiny_status))\n\n\tlist_shinies_newer = [key for key,value \\\n\tin dict_pokemon_shinystatus.items() if value == 'Yes'] \n\n\n\twith open('shiny_pokemon.txt', 'a+') as shiny_pokemon_file:\n\t\tshiny_pokemon_file.seek(0)\n\n\t\t#read WHOLE file to see what's in it, first. first time, should be empty list\n\t\tlines = shiny_pokemon_file.readlines()\n\n\t   \t#clean up \\n's\n\t\tlist_stripped = [line.strip() for line in lines]\n\t\tprint(f\"list_stripped:\\n {list_stripped}\")\n\n\n\t\tfor pokemon in list_shinies_newer:\n\t   \t\tif pokemon not in list_stripped: \n\n\t   \t\t\tprint(f\"{pokemon} not in file currently!\")\n\n\n\t   \t\t\tmessage = Mail(\n\t\t\t\t\tfrom_email=email_from, \n\t\t\t\t\tto_emails=email_to,\n\t\t\t\t\tsubject='Test: PokemonGo Shinies new (via SendGrid)',\n\t\t\t\t\thtml_content=f\"and easy to do anywhere, {pokemon} even with Python\")\n\t\t\t\t\n\t\t\t\ttry:\n\t\t\t\t\tsg = SendGridAPIClient(os.environ.get('SENDGRID_API_KEY'))\n\t\t\t\t\tresponse = sg.send(message)\n\t\t\t\t\tprint(response.status_code)\n\t\t\t\t\tprint(response.body)\n\t\t\t\t\tprint(response.headers)\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tprint(e)\n\n\n\t   \t\t\tshiny_pokemon_file.write(f\"{pokemon}\\n\")\n\n\n\ndef getPokemonList():\n\n\n\n\tprint(\"next steps...\")\n\n\nwebscrape()\ngetPokemonList()\n\n", "557": "import requests\nfrom bs4 import BeautifulSoup\nimport holidays\nfrom datetime import date\nimport csv\n\ndef webscrape():\n    \"\"\"\n    Web scraping 3 character ISO and country name from holidays\n    documentation site\n\n    :return countries:  {3 character ISO: [Country, states]}\n    :type countries: dict\n\n    \"\"\"\n    url = 'https://pypi.org/project/holidays/'\n    countries = {}\n    r = requests.get(url)\n\n    soup = BeautifulSoup(r.text, 'html.parser')\n    iso_table = soup.find('table')\n\n    for country in iso_table.find_all('tbody'):\n        rows = country.find_all('tr')\n        for row in rows:\n            c = row.find_all('td')\n            if '/' not in c[1].text:\n                continue\n            else:\n                if len(c[1].text.split('/')[1]) == 3:\n                    prov = ''\n                    if c[2].text != None:\n                        prov = c[2].text.strip()\n                        if '\\n' in prov:\n                            prov = prov.replace('\\n', ' ')\n                        if ' (default)' in prov:\n                            prov = prov.replace(' (default)', '')\n                        if 'prov = ' in prov:\n                            prov = prov.replace('prov = ', '')\n                        if 'state = ' in prov:\n                            prov = prov.replace('state = ', '')\n                        if prov == '' or prov == 'None':\n                            prov = None\n\n                    countries[c[1].text.split('/')[1]] = [c[0].text, prov]\n    return countries\n\ndef getHolidays(countries):\n    \"\"\"\n    Get holiday dates for years in range 2016-2022 for\n    supported countries\n\n    :param countries:\n    :type countries: dict\n    :return country_holidays:\n    :type country_holidays: dict\n\n    \"\"\"\n    country_holidays = {}\n\n    for c in countries.keys():\n        country_holidays[c] = {}\n        for year in range(2016, 2022+1):\n            country_holidays[c][year] = holidays.CountryHoliday(countries[c][0], years=year)\n\n\n    return country_holidays\n\ndef writeHolidays(countries, country_holidays):\n    \"\"\"\n    Writes countries and their holidays to csv file holidays.csv\n\n    :param countries, country_holidays:\n    :type countries, country_holidays: dict\n\n    \"\"\"\n\n    with open('holidays.csv', mode='w', newline='') as csv_file:\n        headers = ['ISO', 'Country', 'States', '2016', '2017', '2018', '2019', '2020', '2021', '2022']\n        writer = csv.DictWriter(csv_file, fieldnames=headers)\n        writer.writeheader()\n\n        for c in country_holidays:\n            holidays16 = [k for k in country_holidays[c][2016]]\n            holidays17 = [k for k in country_holidays[c][2017]]\n            holidays18 = [k for k in country_holidays[c][2018]]\n            holidays19 = [k for k in country_holidays[c][2019]]\n            holidays20 = [k for k in country_holidays[c][2020]]\n            holidays21 = [k for k in country_holidays[c][2021]]\n            holidays22 = [k for k in country_holidays[c][2022]]\n            writer.writerow({'ISO': c, 'Country': countries[c][0], 'States': countries[c][1], '2016': str(holidays16), '2017': str(holidays17),\n            '2018': str(holidays18), '2019': str(holidays19), '2020': str(holidays20), '2021': str(holidays21), '2022': str(holidays22)})\n\ndef summary(country_holidays):\n    \"\"\"\n    Summary of number of holidays in each supported\n    country for each year in the range 2016-2022\n\n    :param country_holidays:\n    :type country_holidays: dict\n    :return nbr_holidays: {ISO: [2016, ..., 2022]\n    :type nbr_holidays: dict\n\n    \"\"\"\n\n    nbr_holidays = {}\n    for c in country_holidays:\n\n        if c != 'SWE' or c != 'NOR':\n            nbr_holidays[c] = [len(country_holidays[c][2016]), len(country_holidays[c][2017]), len(country_holidays[c][2018]), len(country_holidays[c][2019]), len(country_holidays[c][2020]), len(country_holidays[c][2021]), len(country_holidays[c][2022])]\n\n        # Remove regular Sundays from holidays count for Sweden and Norway\n        if c == 'SWE' or c == 'NOR':\n            nbr_holidays[c] = []\n            for y in range(2016, 2022+1):\n                nbr_holidays_y = len(country_holidays[c][y])\n                for dt in country_holidays[c][y]:\n                    date_ = dt.strftime(\"%Y-%m-%d\")\n                    if country_holidays[c][y].get(date_) == 'S\u00f6ndag' or country_holidays[c][y].get(date_) == 'S\u00f8ndag':\n                        nbr_holidays_y -= 1\n                nbr_holidays[c].append(nbr_holidays_y)\n\n    return nbr_holidays\n\ndef writeSummary(nbr_holidays):\n    \"\"\"\n    Writes summary of number of holidays to csv file nbr_holidays.csv\n\n    :param nbr_holidays:\n    :tye nbr_holidays: dict\n\n    \"\"\"\n\n    with open('nbr_holidays.csv', mode='w', newline='') as csv_file:\n        headers = ['ISO', '2016', '2017', '2018', '2019', '2020', '2021', '2022']\n        writer = csv.DictWriter(csv_file, fieldnames=headers)\n        writer.writeheader()\n\n        for c in nbr_holidays:\n            writer.writerow({'ISO': c, '2016': nbr_holidays[c][0], '2017': nbr_holidays[c][1], '2018': nbr_holidays[c][2], '2019': nbr_holidays[c][3], '2020': nbr_holidays[c][4], '2021': nbr_holidays[c][5], '2022': nbr_holidays[c][6]})\n\n\n\ndef run():\n    countries = webscrape()\n    country_holidays = getHolidays(countries)\n    writeHolidays(countries, country_holidays)\n    nbr_holidays = summary(country_holidays)\n    writeSummary(nbr_holidays)\n \n\nrun()\n", "558": "import time\nimport ParseData as Pd\nimport WebScrape as Ws\n\n\nbadCalls = 0\ntitleNumber = 0\nproviderList = ['', 'hulu/', 'disney-plus/', 'hbo-max/', 'amazon-prime-video/']  # '' means netflix\nbadAPIcalls = []\n\n\ndef mainFunc():\n    provider = \"\"  # starts with netflix\n    pageNumber = 0  # Should start with 0\n\n    start = time.time()\n    Ws.loopThroughScrapePages(provider, pageNumber)\n    Pd.writeToJson(badAPIcalls)\n    end = time.time()\n    print('total time: ' + str(end - start) + ' seconds.')\n    print(\"BAD calls \" + str(badCalls))\n\n\nif __name__ == '__main__':\n    mainFunc()\n", "559": "from googlemapsgeocoding import geocode_single_location\nfrom otodomestateprices import otodom_web_scraper, otodom_page_scraper, how_many_pages_to_scrape\nfrom requests import post, get\nimport json\nimport sys\nimport time\nimport os\nimport schedule\n\ndef check_if_scraper_works():\n    TEST_URL = \"https://www.otodom.pl/pl/oferty/sprzedaz/mieszkanie/warszawa?limit=10&page=1\"\n    page_count = how_many_pages_to_scrape(TEST_URL)\n    if type(page_count) != int or page_count <= 0:\n        print('page count does not work properly')\n        return False\n    scraping_results = otodom_page_scraper(1,TEST_URL)\n    if len(scraping_results) <= 1:\n        print('scraping results are broken')\n        return False\n    print('Scraper passed the test')\n    return True\n\n\ndef post_data(city, results):\n    payload = {\"key\" : '1234','city': city, 'data' : results}\n    print(f'Length is {len(results)}')\n    data = json.dumps(payload)\n    print(f'json data in memory {sys.getsizeof(data)}')\n    r = post(f\"{os.environ.get('API_URL')}/api/\", json=data)\n    print(str(f'Status code {r.status_code}'))\n\n\ndef get_new_offers(cities):\n    for city,url in cities.items():\n        print(url)\n        print(city)\n        results = otodom_web_scraper(url)\n        post_data(city, results)\n\n\ndef geocode_offers(offers, city_geodata):\n    offers_geocoded = []\n    for offer in offers:\n        geodata = [data for data in city_geodata if data['location'] == offer['location'].lower()]\n        if len(geodata) == 0:\n            geodata = geocode_single_location(offer['location'].lower())\n            if geodata is None:\n                continue\n            lat = geodata['lat']\n            lng = geodata['lng']\n        else:\n            geodata = geodata[0]\n            lat = geodata['latitude']\n            lng = geodata['longtitude']\n        \n        geocoded_offer = offer\n        geocoded_offer['lat'] = lat\n        geocoded_offer['lng'] = lng\n        offers_geocoded.append(geocoded_offer)\n    return offers_geocoded\n\n\ndef get_city_geodata(city):\n    API_URL = os.environ.get('API_URL')\n    r = get(f\"{os.environ.get('API_URL')}/api/getgeodata/?city={city}\")\n    return json.loads(r.content)\n\n\ndef get_new_offers(cities):\n    for city,url in cities.items():\n        city_geodata = get_city_geodata(city);\n        print('got city_geodata')\n        offers = otodom_web_scraper(url)\n        print('offers scraped')\n        offers_geocoded = geocode_offers(offers, city_geodata)\n        post_data(city, offers_geocoded)\n\n\nif __name__ == \"__main__\":\n\n    CITIES_URL = {\n        'warszawa' : \"https://www.otodom.pl/pl/oferty/sprzedaz/mieszkanie/warszawa?limit=400&page=\",\n        'krakow' : \"https://www.otodom.pl/pl/oferty/sprzedaz/mieszkanie/krakow?limit=100&page=\",\n        'wroclaw' : \"https://www.otodom.pl/pl/oferty/sprzedaz/mieszkanie/wroclaw?limit=100&page=\"\n    }\n\n    current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n    print(current_time)\n\n    def webscrape():\n        if not check_if_scraper_works():\n            sys.exit(\"Webscraper stopped functioning properly. Possible changes at the scraped site\")\n        get_new_offers(cities=CITIES_URL)\n\n    schedule.every().day.at(\"12:30\").do(webscrape)\n\n    while True:\n        schedule.run_pending()\n        time.sleep(1)\n", "560": "# Define here the models for your spider middleware\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\n\n# useful for handling different item types with a single interface\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass WebscrapeSpiderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(self, response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, or item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(self, response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Request or item objects.\n        pass\n\n    def process_start_requests(self, start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n\n\nclass WebscrapeDownloaderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_request(self, request, spider):\n        # Called for each request that goes through the downloader\n        # middleware.\n\n        # Must either:\n        # - return None: continue processing this request\n        # - or return a Response object\n        # - or return a Request object\n        # - or raise IgnoreRequest: process_exception() methods of\n        #   installed downloader middleware will be called\n        return None\n\n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n", "561": "# Grace Foster\n# ITP 100-01\n# EXERCISE: 09\n# webscrape.py\n# ----------------------------------------------------------------\n\nimport re\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlopen\nurl = \"http://www.centralvirginia.edu\"\npage = urlopen(url)\nhtml = page.read().decode(\"utf-8\")\nsoup = BeautifulSoup(html, \"html.parser\")\n\nlinz = \"---\" * 35\nimages = soup.find_all('img')\nfor img in images:\n    print(img[\"src\"])\nprint(linz)\n\nlinz = \"---\" * 35\nfor img in images:\n    if re.search(\"KenArt\", img[\"src\"]):\n        print(img[\"src\"])\nprint(linz)\nprint(\"The Program Has Terminated Normally!\")\n", "562": "''\n\n\ndef Main():\n    info = 'PIA 063 PC E2022'\n    parser = argparse.ArgumentParser(info)\n\n    parser.add_argument('-opt', '--option', required = True, help = 'Opcion para elegir script a utilizar.', dest = 'opt')\n\n    \n    \n    #ENCRIPTAR\n    parser.add_argument('-enc', '--enc', help = 'Ruta del directorio a encriptar.')\n    #DESENCRIPTAR\n    parser.add_argument('-dec', '--dec', help = 'Ruta del directorio a desencriptar.')\n    #WEBSCRAPE\n    parser.add_argument('-ws', '--ws', help= 'Url de la pagina a escanear.')\n    #METADATA\n    parser.add_argument('-md', '--md', help= 'Ruta del imagen para extraer informacion. ')\n    #ESCANEO DE PUERTOS \n    parser.add_argument('-ip', '--ip', help= 'IP a escanear')\n    #ESCANEO DE PUERTOS POWERSHELL\n    parser.add_argument('-ep', '--ep', help= 'IP a escanear CON PS')\n    #APi \n    parser.add_argument('-api', '--api', help= 'API y busqueda de correos')\n    \n    \n   \n\n    args = parser.parse_args()\n    \n    \n        \n    if args.opt.upper()== 'ENCRIPTAR':\n         path_to_encrypt = args.enc\n         encrypt.main(path_to_encrypt)\n\n    if args.opt.upper()== 'DESENCRIPTAR':\n         path_to_encrypt = args.dec\n         decrypt.main(path_to_encrypt)\n     \n    if args.opt.upper()== 'ESCANEO_DE_PUERTOS':\n        target = args.ip\n        puertos.main(target)\n\n    if args.opt.upper()== 'WEBSCRAPE':\n         url = args.ws\n         webscrape.main(url)\n\n    if args.opt.upper()== 'METADATA':\n         directorio = args.md\n         metadata.main(directorio)\n      \n    if args.opt.upper()== 'API':\n         email = args.api\n         api.main(email)\n       \n\n    if args.opt.upper()== 'ESCANEOPUERTOSPOWERSHELL':\n         command = \"powershell -ExecutionPolicy ByPass -File EscaneoPuertosPowershell.ps1\"\n         print(command)\n         powerShellResult = subprocess.run(command)\n    \nif __name__ == '__main__':\n    import argparse\n    import encrypt\n    import decrypt\n    import puertos\n    import metadata\n    import api\n    import webscrape\n    import subprocess    \n    Main()\n    \n    \n    \n", "563": "#stockList=[\"GILD\",\"WMT\",\"UNP\", \"UTX\",\"HPQ\", \"V\", \"CSCO\", \"SLB\", \"AMGN\", \"BA\", \"TGT\", \"COP\", \"CMCSA\", \"BMY\", \"CVX\", \"VZ\", \"BP\", \"T\", \"UNH\", \"MCD\", \"PFE\", \"ABT\", \"FB\", \"DIS\", \"MMM\", \"XOM\", \"ORCL\", \"PEP\",\"HD\", \"JPM\", \"INTC\", \"WFC\", \"MRK\", \"KO\", \"AMZN\", \"PG\", \"BRKB\",\"GOOGL\", \"GM\", \"JNJ\", \"MO\", \"IBM\", \"GE\", \"MSFT\", \"AAPL\",\"NVDA\", \"AMD\", \"GE\", \"NTDOF\", \"SNE\"]\n\nfrom webscrape import *\nimport pygame\nfrom pygame.locals import *\nimport time\n\n\"\"\"\n\nInput:stockName (string)\n\nReturn: bool\n\nGiven a stok name will attempt to webscrape to its page, if sucessfull\nit will return True. If unsucessfull it will return false\n\n\"\"\"\n# Variable that holds stockName\n# stkName = \"\"\n\n\ndef search(stockName):\n    url = get_url(stockName.lower())\n    print(url)\n    price = current_price(url)\n    if price == \"Error. Can't find stock name. Make sure name is correct.\" or price == None:\n        print(\"Stock not Found\")\n        return False\n    else:\n        print(current_price(url))\n        print(\"THIS IS THECURRENT PRIce\")\n        print(\"Stock Full Name: \" + get_company_name(url))\n        # stockName = get_company_name(url)\n        t = time.localtime()\n        currentTime = time.strftime(\"%H:%M:%S\", t)\n        print(get_company_name(url) + \"Stock Data Updated at \" + currentTime)\n        return True\n\n\n\"\"\"\n\nReturn: Search Bar Button (pygame.Rect), Search Bar Font (pygame.font)\n\nInitializes the GUI structures for the search bar\n\n\"\"\"\n\n\ndef searchBarInitalize():\n    searchBarButton = pygame.Rect(198, 17, 983, 56)\n    font = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 50)\n    timeFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 30)\n    favFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 60)\n    verFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 30)\n    companyFont = pygame.font.Font(\n        \"../course-project-a8-mcm/Fonts/times.ttf\", 50)\n    t = time.localtime()\n    currentTime = time.strftime(\"%H:%M:%S\", t)\n    updatedTime = \"Stock Data Updated at \" + currentTime\n    return searchBarButton, font, updatedTime, timeFont, companyFont, favFont, verFont\n\n\n\"\"\"\n\nEvent handler for Search bar. On key press will delete a char from search bar text if char was \\b \nOtherwise will add char to the search bar text\n\n\"\"\"\n\n\ndef updateSearchBarOnKeyPress(theGuiEvent, searchBarText):\n    if theGuiEvent.unicode == \"\\b\":\n        return searchBarText[:-1]\n\n    elif theGuiEvent.unicode.isalpha() and len(searchBarText) <= 5 or theGuiEvent.unicode.isdigit() and len(searchBarText) <= 5:\n        return searchBarText+theGuiEvent.unicode\n    else:\n        if len(searchBarText) >= 5:\n            print(\"max length of string reached\")\n\n        return searchBarText\n\n\ndef timeStamp():\n    t = time.localtime()\n    currentTime = time.strftime(\"%H:%M:%S\", t)\n    updatedTime = \"Updated Stock Data at \" + str(currentTime)\n    return updatedTime\n", "564": "#stockList=[\"GILD\",\"WMT\",\"UNP\", \"UTX\",\"HPQ\", \"V\", \"CSCO\", \"SLB\", \"AMGN\", \"BA\", \"TGT\", \"COP\", \"CMCSA\", \"BMY\", \"CVX\", \"VZ\", \"BP\", \"T\", \"UNH\", \"MCD\", \"PFE\", \"ABT\", \"FB\", \"DIS\", \"MMM\", \"XOM\", \"ORCL\", \"PEP\",\"HD\", \"JPM\", \"INTC\", \"WFC\", \"MRK\", \"KO\", \"AMZN\", \"PG\", \"BRKB\",\"GOOGL\", \"GM\", \"JNJ\", \"MO\", \"IBM\", \"GE\", \"MSFT\", \"AAPL\",\"NVDA\", \"AMD\", \"GE\", \"NTDOF\", \"SNE\"]\n\nfrom webscrape import *\nimport pygame\nfrom pygame.locals import *\nimport time\n\n\"\"\"\n\nInput:stockName (string)\n\nReturn: bool\n\nGiven a stok name will attempt to webscrape to its page, if sucessfull\nit will return True. If unsucessfull it will return false\n\n\"\"\"\n# Variable that holds stockName\n# stkName = \"\"\n\n\ndef search(stockName):\n    url = get_url(stockName.lower())\n    print(url)\n    price=current_price(url)\n    if price == \"Error. Can't find stock name. Make sure name is correct.\" or price==None:\n        print(\"Stock not Found\")\n        return False\n    else:\n        print(current_price(url))\n        print(\"THIS IS THECURRENT PRIce\")\n        print(\"Stock Full Name: \" + get_company_name(url))\n        # stockName = get_company_name(url)\n        t = time.localtime()\n        currentTime = time.strftime(\"%H:%M:%S\", t)\n        print(get_company_name(url) + \"Stock Data Updated at \" + currentTime)\n        return True\n\n\n\"\"\"\n\nReturn: Search Bar Button (pygame.Rect), Search Bar Font (pygame.font)\n\nInitializes the GUI structures for the search bar\n\n\"\"\"\n\n\ndef searchBarInitalize():\n    searchBarButton = pygame.Rect(200, 10, 1000, 70)\n    font = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 50)\n    timeFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 30)\n    favFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 60)\n    verFont = pygame.font.Font(\"../course-project-a8-mcm/Fonts/times.ttf\", 30)\n    companyFont = pygame.font.Font(\n        \"../course-project-a8-mcm/Fonts/times.ttf\", 50)\n    t = time.localtime()\n    currentTime = time.strftime(\"%H:%M:%S\", t)\n    updatedTime = \"Stock Data Updated at \" + currentTime\n    return searchBarButton, font, updatedTime, timeFont, companyFont, favFont, verFont\n\n\n\"\"\"\n\nEvent handler for Search bar. On key press will delete a char from search bar text if char was \\b \nOtherwise will add char to the search bar text\n\n\"\"\"\n\n\ndef updateSearchBarOnKeyPress(theGuiEvent, searchBarText):\n    if theGuiEvent.unicode == \"\\b\":\n        return searchBarText[:-1]\n\n    elif theGuiEvent.unicode.isalpha() and len(searchBarText) <= 5 or theGuiEvent.unicode.isdigit() and len(searchBarText) <= 5:\n        return searchBarText+theGuiEvent.unicode\n    else:\n        if len(searchBarText) >= 5:\n            print(\"max length of string reached\")\n\n        return searchBarText\n\n\ndef timeStamp():\n    t = time.localtime()\n    currentTime = time.strftime(\"%H:%M:%S\", t)\n    updatedTime = \"Updated Stock Data at \" + str(currentTime)\n    return updatedTime\n", "565": "# Charles Young\r\n# 6/7/2020\r\n# This file will execute the program, getting data and\r\n# extracting information from it\r\nimport webScrape\r\nimport probabilities\r\nimport numpy as np\r\nimport tensorflow as tf\r\nlog = webScrape.WTFLogin()\r\nlog.login()\r\nlog.get_data(5000)\r\n# TIME_STEPS = 20\r\n#\r\n# data = probabilities.Data()\r\n# data.generate_data(\"input.txt\")\r\n# X, Y = data.split_sequence(TIME_STEPS)\r\n# train_x, train_y, test_x, test_y = data.split_data(X, Y, 100000)\r\n# train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\r\n# test_x = test_x.reshape((test_x.shape[0], test_x.shape[1], 1))\r\n# model = data.create_model(TIME_STEPS)\r\n# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\r\n# model.fit(train_x, train_y, epochs=30, verbose=1, callbacks=[callback])\r\n# model.save(\"LSTM_Model\")\r\n#\r\n# model.evaluate(test_x, test_y)\r\n\r\n# x_input = np.array([5.26, 9.01, 1.76, 3.19, 2.08, 7.68, 1.24, 1.6,  2.06, 5.3])\r\n# # 1\r\n# x_input = x_input.reshape((1, 10, 1))\r\n# yhat = model.predict(x_input, verbose=1)\r\n# print(yhat)\r\n\r\n# data.plot_data()", "566": "# Define here the models for your scraped items\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\nfrom multiprocessing import AuthenticationError\nfrom matplotlib.pyplot import title\n\n\n\nclass WebscrapeItem(scrapy.Item):\n    # define the fields for your item here like:\n     title = scrapy.Field()\n     author= scrapy.Field()\n     post = scrapy.Field()\n", "567": "\"\"\"\nFor all data pivots in HAWC, get an SVG and PNG for each data pivot:\n\n```bash\nexport \"HAWC_USERNAME=foo@bar.com\"\nexport \"HAWC_PW=foobar\"\n\ncd ~/dev/hawc/hawc\npython ../scripts/scrape_data_pivots.py get-pivot-objects\npython ../scripts/scrape_data_pivots.py webscrape https://hawcproject.org\n```\n\"\"\"\nimport os\nimport sys\nimport time\nfrom pathlib import Path\n\nimport click\nimport django\nimport pandas as pd\nfrom selenium import webdriver\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.webdriver.chrome.options import Options\n\nFN = \"data-pivots.pkl\"\nROOT = str(Path(Path(__file__).parents[0] / \"../hawc\").resolve())\nos.chdir(ROOT)\nsys.path.append(ROOT)\n\n\n@click.group()\ndef cli():\n    pass\n\n\n@cli.command()\ndef get_pivot_objects():\n\n    os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"main.settings.dev\"\n    django.setup()\n\n    from summary.models import DataPivot\n\n    data = []\n    for d in DataPivot.objects.all().order_by(\"assessment_id\"):\n        data.append((d.id, d.get_absolute_url(), False, False, False, False))\n\n    df = pd.DataFrame(data=data, columns=(\"id\", \"url\", \"loaded\", \"error\", \"png\", \"svg\"))\n    df.to_pickle(FN)\n\n\n@cli.command()\n@click.argument(\"base_url\")\ndef webscrape(base_url: str):\n    df = pd.read_pickle(FN)\n\n    max_sleep = 60 * 10  # 10 min\n\n    chrome_options = Options()\n    chrome_options.add_argument(\"--headless\")\n    driver = webdriver.Chrome(options=chrome_options)\n    driver.set_window_size(2000, 1500)\n    driver.implicitly_wait(max_sleep)\n\n    url = f\"{base_url}/user/login/\"\n    driver.get(url)\n\n    driver.find_element_by_id(\"id_username\").clear()\n    driver.find_element_by_id(\"id_username\").send_keys(os.environ[\"HAWC_USERNAME\"])\n    driver.find_element_by_id(\"id_password\").clear()\n    driver.find_element_by_id(\"id_password\").send_keys(os.environ[\"HAWC_PW\"])\n    driver.find_element_by_id(\"submit-id-login\").submit()\n\n    for key, data in df.iterrows():\n        if data.loaded is True:\n            continue\n\n        driver.implicitly_wait(max_sleep)\n        url = f\"{base_url}{data.url}\"\n        print(f\"Trying {key+1} of {df.shape[0]}: {url}\")\n        driver.get(url)\n        el = driver.find_element_by_id(\"dp_display\")\n        loading_div = driver.find_element_by_id(\"loading_div\")\n        while True:\n            if not loading_div.is_displayed():\n                driver.implicitly_wait(10)\n                try:\n                    svg = driver.find_element_by_xpath(\"//*[local-name()='svg']\")\n                    if svg:\n                        df.loc[key, \"loaded\"] = True\n                        df.loc[key, \"error\"] = False\n                        df.loc[key, \"png\"] = svg.screenshot_as_png\n                        df.loc[key, \"svg\"] = svg.get_attribute(\"innerHTML\")\n\n                except NoSuchElementException:\n                    df.loc[key, \"loaded\"] = True\n                    df.loc[key, \"error\"] = True\n                    df.loc[key, \"svg\"] = el.get_attribute(\"innerHTML\")\n\n                df.to_pickle(FN)\n                break\n\n            time.sleep(0.1)\n\n\nif __name__ == \"__main__\":\n    cli()\n", "568": "#!/usr/bin/env python\n# coding: utf-8\n\nimport pandas as pd\nimport requests\nimport json\nimport argparse\nimport pickle\nfrom pathlib import Path\n\n\n# ## DATA COLLECTION\n\n# import scraper modules\nimport webscrape_wiki\nimport webscrape_free4u\n\n# import api modules\nimport api_autocomplete\nimport api_geocode\nimport api_yelp\n\n# import sqlite for final db\nimport sqlite3\nimport sql_tables\nimport convert_boba_data\nimport create_fk_list\n\n# data visualization\nimport folium # need to \"conda install -c conda-forge folium\"\nfrom folium import plugins\nfrom folium.plugins import HeatMap\nimport maps #module\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport numpy as np\nimport seaborn as sns\n\n\n\ndef main():\n    \n    parser = argparse.ArgumentParser(description= 'This is main driver.')\n    # tried to use \"nargs = '1' \" but error\n    parser.add_argument(\"-source\", choices=[\"local\", \"remote\", \"test\"], required = True, help=\"where data should be gotten from\")\n    args = parser.parse_args()\n\n    location = args.source\n    \n    #if local: grab raw data from csv files\n    if location == \"local\":\n        #check if pickle files have been created before grabbing data\n        try:\n            # read pickle files as pandas df\n            data_folder = Path(\"./data/\")\n            college_data = pd.read_pickle(data_folder / \"college_pickle.pkl\")\n            boba_shops_data = pd.read_pickle(data_folder / \"boba_pickle.pkl\")\n        #if csv files do not exist, must run remote first to create csv files\n        except FileNotFoundError:\n            print('Please run meng_lisa.py on remote/test first to create data files.')\n            return\n    #if remote: grab raw data by webscraping and API requests\n    else:\n        # input websites that I want scrape \n        # collect data as dictionary\n        wiki_dict = webscrape_wiki.wiki_tables('https://en.wikipedia.org/wiki/List_of_colleges_and_universities_in_California')\n        free4u_dict = webscrape_free4u.free4u_table('https://www.free-4u.com/Colleges/California-Colleges.html')\n\n        # create CSV databases from raw data\n        webscrape_wiki.wiki_data(wiki_dict)\n        webscrape_free4u.free4u_data(free4u_dict)\n        \n        wiki_df = pd.read_csv('wiki_raw_data.csv')\n        free4u_df = pd.read_csv('free4u_raw_data.csv')\n        \n    \n        ## # DATA CLEANING\n        \n        # check for duplicate data from wiki\n        wiki_df[wiki_df.duplicated(['college_name']) == True]\n        # wiki data has no duplicates!\n        \n        # check for duplicate data from free4u\n        free4u_df[free4u_df.duplicated(['college_name'], keep=False) == True]\n        \n        # remove duplicate data!\n        #pd.set_option('display.max_rows', 300)\n        updated_free4u_df = free4u_df.drop_duplicates(['college_name'], keep = 'last')\n        \n        # use GoogleMap API \"Places Autocomplete\" on both raw dataframes to get full name of colleges and place IDs to normalize\n        wiki_normalize = api_autocomplete.normalize_college_name(wiki_df)\n        free4u_normalize = api_autocomplete.normalize_college_name(updated_free4u_df)\n    \n        # add full names and place IDs into individual dataframes\n        # turn off warning as I am aware I am operating on the copy of the dataframe, not original\n        pd.set_option('mode.chained_assignment', None)\n        wiki_df['normalize_name'] = wiki_normalize['normalize_name']\n        wiki_df['place_id'] = wiki_normalize['place_id']\n\n        updated_free4u_df['normalize_name'] = free4u_normalize['normalize_name']\n        updated_free4u_df['place_id'] = free4u_normalize['place_id']\n    \n        # remove rows from dataframe if place_id is None because can't be used with Yelp API to find nearby boba shops\n        new_wiki_df = wiki_df.dropna(subset=['place_id'])\n        new_free4u_df = updated_free4u_df.dropna(subset=['place_id'])\n        \n        # combine dataframes\n        # reset index\n        concat_df = pd.concat([new_wiki_df, new_free4u_df], axis=0, sort=False).reset_index(drop=True)\n\n        # clean duplicates by:\n        # 1. group by place_id (same location/campus)\n        # 2. combine duplicate rows by filling missing data in wiki with free4u data (url and tuition) (because wiki data is more up to date)\n        # will use enrollment data from free4u if none in wiki\n        # removed \"college_name\" column because no longer needed\n        final_df = concat_df.groupby('place_id').agg({'normalize_name':'first',\n                                                          'college_city':'first',\n                                                          'enrollment': 'first',\n                                                          'place_id':'first',\n                                                          'url':'last',\n                                                          'tuition':'last',\n                                                           'year_founded':'first'\n                                                     }).sort_values(by=['normalize_name']).reset_index(drop=True)\n    \n        # use GoogleMaps API \"Geocoding\" to get latitude, longitude\n        coordinates_list = api_geocode.coordinates(final_df)\n        \n        # add coordinates to dataframe\n        final_df['latitude'] = coordinates_list['latitude']\n        final_df['longitude'] = coordinates_list['longitude']\n\n        # impute missing values in city_names from city in normalized_name\n        final_df['college_city'] = final_df.apply(\n            lambda row: row['normalize_name'].split(\",\")[-3] if pd.isnull(row['college_city']) \n            else row['college_city'], axis=1)\n\n        # store cleaned college data as pickle file\n        final_df.to_pickle(\"college_pickle.pkl\")\n\n        # pandas read college pickle file to use data for Yelp API\n        college_data = pd.read_pickle(\"college_pickle.pkl\")\n\n        # use Yelp API with parameters: boba (business category) within 10 mile of colleges (radius)\n        # to get data on store names, rating, number of reviews, price, coordinates, and distance from colleges\n        # NOTE: Yelp maxes you out at 20 stores (so total stores don't matter)\n        boba_data = api_yelp.boba_shops(college_data)\n\n        # store boba_data (type = nested dictionaty) as pickle file\n        boba_pickle = open(\"boba_pickle.pkl\", \"wb\")\n        pickle.dump(boba_data, boba_pickle)\n        boba_pickle.close()\n\n        # pandas read boba pickle file \n        boba_shops_data = pd.read_pickle(\"boba_pickle.pkl\")\n\n    # use SQLite to insert both boba and college pickle dataframe into SQL data model due to relational data of colleges to boba shops\n    # create SQL tables\n    sql_tables.create_tables()\n    print(\"total_data.db as been created!\")\n\n    # create separate college data into separate panda df's with foreign/primary key \n    city_name = []\n    for name in college_data[\"college_city\"]:\n        if name not in city_name:\n            city_name.append(name)\n\n    # create foreign key lists\n    colleges_city_id = create_fk_list.fk_dict(college_data[\"college_city\"])\n\n    city_fk_id = create_fk_list.city_fk_list(college_data, colleges_city_id)\n\n    college_data = college_data.drop(columns=['college_city'])\n    college_data[\"city_id\"] = city_fk_id\n    \n    # add pandas df to sql Colleges and Cities table\n    conn = sqlite3.connect(\"total_data.db\")\n    cur = conn.cursor()\n    college_data.to_sql('Colleges', conn, if_exists='append', index = False)\n    for c_name in city_name:\n        cur.execute(\"INSERT INTO Cities (city_name) VALUES (?)\", (c_name,))\n    conn.commit()\n\n    # make df for Yelp boba data to insert into SQL table\n    boba_dict = convert_boba_data.boba_data_dict(boba_shops_data)   \n    boba_df = pd.DataFrame.from_dict(boba_dict)\n    \n    # create Boba df \n    boba_table = boba_df[['store_name', 'latitude', 'longitude']]\n    # filter out duplicates based on name and coordinates and fit into SQL table with primary key\n    boba_table = boba_table.groupby(['store_name', 'latitude', 'longitude']).size().reset_index(name='count')\n    boba_table = boba_table.rename(columns={\"store_name\":\"shop_name\"})\n    boba_table = boba_table.drop(columns=['count'])\n    # add primary key for None (to account for colleges with no shops nearby)\n    boba_table = boba_table.append({'shop_name': None}, ignore_index=True)\n    boba_table['shop_id'] = range(1,len(boba_table)+1)\n    boba_table.to_sql('Shops', conn, if_exists='replace', index = False)\n\n    # create foreign key list\n    shop_fkid = boba_table.drop(columns=['latitude','longitude'])\n    shop_fkid = shop_fkid.set_index(['shop_name'])\n    shop_fkid = shop_fkid.to_dict()\n\n    shop_fkid_list = []\n    for shop_name in boba_df['store_name']:\n        shop_fkid_list.append(shop_fkid['shop_id'][shop_name])\n\n    boba_df['store_name'] = shop_fkid_list\n    boba_df = boba_df.rename(columns={'store_name':'shop_id'})\n    boba_df.to_sql('Boba_near_Colleges', conn, if_exists='replace', index = False)\n\n    print(\"All clean data as been inserted into total_data.db!\")\n\n# ## DATA VISUALIZATIONS/ANALYSIS\n\n    # read sql db as pandas df and show tables\n    colleges_df = pd.read_sql_query(\"SELECT * from Colleges\", conn)\n    #colleges_df\n\n    cities_df = pd.read_sql_query(\"SELECT * from Cities\", conn)\n    #cities_df\n\n    shops_df = pd.read_sql_query(\"SELECT * from Shops\", conn)\n    #shops_df\n\n    boba_near_colleges_df = pd.read_sql_query(\"SELECT * from Boba_near_Colleges\", conn)\n    #boba_near_colleges_df\n\n\n    # Dot Map\n    maps.dot_map(colleges_df,shops_df)\n    print('Dot map has been saved as html file!')\n\n    '''My inital thought process was plotting the colleges on a map \n    and their nearby boba/coffee shops, with a 15 mile (~24140 meter) radius,\n    in order to visually see the proximity/distances between the colleges and shops,\n    and to see which colleges share the same shop. \n    But as you can see, the map looks very hectic, so I then decided to use a heat map.'''\n\n    # Heat Map\n    maps.heat_map(colleges_df,shops_df)\n    print('Heat map has been saved as html file!')\n\n    ''' Although at bird-eye view, the heat map also does not seem as useful because\n    I purposely gathered data about shops within a 15 mi radius of colleges, so it seems like \n    EVERYTHING near the colleges are red. But the heat map is more valuable once zooomed in. \n    Once zoomed in, you'll be able to identify more specific hotpots and notice that some\n    colleges don't have as high density of boba shops as compared to others.'''\n\n    # Merge college and boba_near_colleges df to make graphs\n    df = pd.merge(colleges_df, boba_near_colleges_df, on='college_id', how='inner')\n\n    # find corelation between how long school has been estsblished (year_founded) and amount of customers (will test with mean review count)\n    ave_reviews = df[['year_founded','review_count']].groupby(by=['year_founded']).mean()\n    ave_reviews.plot(title='Year colleges were founded vs Average review counts of nearby shops')\n\n    # find r^2 to determine percentage of correlation/dependence between variables\n    ave_reviews.reset_index().corr()\n    # r^s = -0.077 --> insignificant dependence\n\n    # determine if tuition or enrollment affect price of shops nearby\n    df['tuition'] = df['tuition'].replace('[\\$,]','', regex=True).astype(float)\n    df['enrollment'] = df['enrollment'].replace(',','', regex=True).astype(float)\n    price_vs_tuition = df[['price','tuition']].groupby(by=['tuition']).mean()\n    price_vs_enrollment = df[['price','enrollment']].groupby(by=['enrollment']).mean()\n\n    price_vs_tuition.plot(title='College tuition vs Price of drinks of nearby shops')\n    price_vs_tuition.reset_index().corr()\n    # r^2 = 0.039 --> insignificant\n\n\n    # for enrollment, there is an outlier of 107,170, so it will but cutoff from analysis\n    price_vs_enrollment.reset_index().plot(kind='scatter', \n                                        title = 'College enrollment vs Price of drinks of nearby shops',\n                                        x='enrollment',y='price', \n                                       xlim = (0,46000), ylim=(1,2), figsize=(7,7))\n    price_vs_enrollment.reset_index().corr()\n    # r^2 = -0.154 --> insignificant\n    ''' it is very noticable that there is a large cluster of data points for enrollment from 0-5000, \n        which shows randomess/insignificance, so will use 5000-9000 to check for correlation'''\n    price_vs_enrollment.reset_index().plot(kind='scatter', \n                                        title = 'College enrollment (up to 5000) vs Price of drinks of nearby shops',\n                                        x='enrollment',y='price', \n                                        xlim = (0,5000), ylim=(1,2), figsize=(7,7))\n\n    price_vs_enrollment = price_vs_enrollment.reset_index()\n    price_vs_enrollment = price_vs_enrollment[price_vs_enrollment['enrollment'] > 5000]\n    price_vs_enrollment = price_vs_enrollment[price_vs_enrollment['enrollment'] < 90000].groupby(by='enrollment').mean()\n    price_vs_enrollment.reset_index().corr()\n\n    price_vs_enrollment = price_vs_enrollment.reset_index()\n    sns.lmplot(x='enrollment',y='price',data=price_vs_enrollment,fit_reg=True)\n    price_vs_enrollment.corr()\n    # r^2 = 0.062 ---> still insignificant\n\n    # determine if tuiton or enrollment affect rating of nearby shops\n    rating_vs_tuition = df[['rating','tuition']].groupby(by=['tuition']).mean()\n    rating_vs_enrollment = df[['rating','enrollment']].groupby(by=['enrollment']).mean()\n\n    rating_vs_tuition.plot(title = 'College tuition vs Rating of nearby shops')\n    rating_vs_tuition.reset_index().corr()\n    # r^2 = 0.050 --> insignigicant\n\n    rating_vs_enrollment.reset_index().plot(kind='scatter', title = 'College enrollment vs Rating of nearby shops', x='enrollment', y='rating', xlim=(0,6000))\n    rating_vs_enrollment.reset_index().corr()\n    # r^2 = -0.007 --> insignigicant\n\n    ''' Assuming the more customers a shop has, the more reviews it will also have.\n        I am using review_count as a varible that translates to size of customer population'''\n    # determine if tuiton or enrollment affect review_count of nearby shops\n    review_count_vs_tuition = df[['review_count','tuition']].groupby(by=['tuition']).mean()\n    review_count_vs_enrollment = df[['review_count','enrollment']].groupby(by=['enrollment']).mean()\n\n    review_count_vs_tuition.reset_index().plot(kind='scatter', title = 'College tuition vs Number of reviews of nearby shops', x='tuition', y='review_count', xlim=(0,None))\n    review_count_vs_tuition.reset_index().corr()\n    # r^2 = -0.069 -- > insignificant\n\n    review_count_vs_enrollment.reset_index().plot(kind='scatter', title = 'College enrollment vs Number of reviews of nearby shops', x='enrollment', y='review_count', xlim=(0,None))\n    review_count_vs_enrollment.reset_index().corr()\n    # r^2 = -0.004 -- > insignificant\n\n    # determine if distance from college affects amount of customers for shop\n    ''' Because this graph plots the distances between colleges and each shop within a 15 mi radius, \n        it will generate too many lines on the graph, making it incomprehendable. Thus I will randomly\n        choose 5 colleges and 5 shops near though colleges. But if you would like to see the code that \n        plots all the schools and shops, here it is below:\n\n        review_count_vs_distance = df[['normalize_name','review_count','distance_from_college']].dropna().set_index('normalize_name')\n        review_count_vs_distance = review_count_vs_distance.sort_values(by = 'distance_from_college')\n\n        fig, ax = plt.subplots()\n        for name, group in review_count_vs_distance.groupby('normalize_name'):\n            group.plot('distance_from_college', y='review_count', ax=ax, label=name, xlim=(0,15000), ylim=(0,None))\n        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n        plt.show()'''\n    review_count_vs_distance = df[['normalize_name','review_count','distance_from_college']].dropna().set_index('normalize_name')\n\n    # shuffle df\n    review_count_vs_distance = review_count_vs_distance.sample(frac=1)\n\n    # randomly choose 5 shops from 5 colleges\n    sample_df = pd.concat(\n        [g.head(5) for rows, g in review_count_vs_distance.groupby(['normalize_name'], sort=False)][:5])\n\n    # sort values to make distance ascending to make line graph, then grouby \n    sample_df = sample_df.sort_values(['normalize_name', 'distance_from_college']).groupby('normalize_name')\n\n    fig, ax = plt.subplots()\n    for name, group in sample_df:\n        group.plot('distance_from_college', title = 'Distance from college vs Number of reviews of nearby shops', y='review_count', ax=ax, label=name)\n     \n    # move legend to outside of graph   \n    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\n    plt.savefig('review_count_vs_distance_linegraph.png')\n    plt.show()\n\n    ''' After all these graphs and analysis, it is clear that the attributes of a boba/coffee shop (rating, price, amount of customers('review count'))\n    are not significantly correlated or dependent on the attributes of colleges (year founded, tuition, enrillment, or distance from shops).\n    But as a final crap-shot, I made a scatterplot matrix to see if there are ANY possible correlations that I may have missed.\n    '''\n\n    total_df = pd.merge(df, cities_df, on='city_id', how='inner')\n    total_df = pd.merge(total_df, shops_df, on='shop_id', how='inner').drop(columns=['college_id','city_id','shop_id','place_id','url','latitude','longitude','latitude_x','longitude_x', 'latitude_y','longitude_y'])\n    #total_df\n\n    ''' I am aware that my total_df has NaNs, but out of curiousity and as a final Hail Mary, I would like to see what the scatterplot matrix looks like\n        to view the multiple variables at once and see if there is a correlatoin. Therefore I will silent this runtime warning.\n    '''\n    np.warnings.filterwarnings('ignore')\n\n    sns.pairplot(total_df, kind=\"scatter\")\n    #save the map as an html\n    plt.savefig('scatterplot_matrix.png')\n    print('Scatterplot matrix has been saved as png file!')\n    plt.show()\n\n    #sadly, the scatterplot seems nonsensical and does not show any relevant correlations.\n\n\n    \nif __name__ == \"__main__\":\n    main()\n\n", "569": "_author_ = \"Michael Matthews\"\n\n\"\"\"\nThis script is designed to go through each of the links within jod.mrm.org and\nwebscrape and then edit the contents of each site.\n\nAll of the code comes from online sources and class. Espcially in the formatting\nof the headers to send the html request\n\n\"\"\"\n\n#Importing libraries\nimport urllib.request\nimport re\nimport os\nfrom bs4 import BeautifulSoup\n\ncurrent_file = open('/Users/michael/Desktop/Volume.txt','wb')\n#there are 26 volumes of the JofD\nfor x in range(1,27):\n\n    #base url\n    url = \"http://jod.mrm.org/%d/\" % x\n\n    #I found this code online. Apparently this is how you need to encode your webscrape\n    #in the headers in order to get into the website\n    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n    headers = {'User-Agent':user_agent,}\n\n    #the format that the request needs to be in\n    request = urllib.request.Request(url,None,headers)\n    response = urllib.request.urlopen(request)\n    data = response.read()\n\n    #Regular expression given in class to find hyperlinks\n    all_links = re.findall(b'', data)\n\n    #looping through each of the sub links\n    #i.e. http://jod.mrm.org/1/1, http://jod.mrm.org/1/2, etc. (However not all the links are 1,2,3.. sometimes it goes 1,6,10, etc.)\n    for subLink in all_links:\n\n        #converting the return of bytes into string\n        subLink = str(subLink)\n\n        #removing the 'b'' within the sublink\n        subLink = re.sub('b\\'', \"\", subLink)\n\n        #resetting the url\n        url = \"http://jod.mrm.org/%s\" % subLink\n\n        #requesting data once again for that specific webiste\n        user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n        headers={'User-Agent':user_agent,}\n\n        #format to get the data\n        request=urllib.request.Request(url,None,headers)\n        response = urllib.request.urlopen(request)\n        data = response.read()\n\n        soup = BeautifulSoup(data, 'html.parser')\n        #formatting the code, found this online as well\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n\n        #getting text from file\n        text = soup.get_text()\n\n        #this is going through each line and splitting it up\n        lines = (line.strip() for line in text.splitlines())\n        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n\n        #getting rid of blank lines\n        text = '\\n'.join(chunk for chunk in chunks if chunk)\n\n\n        #reformatting the subLink to replace / and '\n        subLink = re.sub('\\/',\"-\",subLink)\n        subLink = re.sub('\\'',\"\",subLink)\n\n        #Write to folder and file of volume\n\n\n        #writting with specific encoding\n        current_file.write(text.encode(\"utf-8\"))\n\n#make sure to close the file\ncurrent_file.close()\n", "570": "import os\n\n\nos.system('python webscrape.py')\nos.system('python GolfDA-Scoring.py')\n", "571": "import hashlib\r\nimport base64\r\nimport logging\r\nimport uuid\r\nimport boto3\r\nimport json\r\n\r\ns3=boto3.client('s3')\r\nddb = boto3.client('dynamodb')\r\n\r\ndef timeout(event, context):\r\n    raise Exception('Execution is about to time out, exiting...')\r\n    \r\ndef store_deidentified_message(message, entity_map, ddb_table):\r\n    hashed_message = hashlib.sha3_256(message.encode()).hexdigest()\r\n    for entity_hash in entity_map:\r\n        ddb.put_item(\r\n            TableName=ddb_table,\r\n            Item={\r\n                'EntityHash': {\r\n                    'S': entity_hash\r\n                },\r\n                'Hash_Message': {\r\n                    'S': hashed_message\r\n                },\r\n                'Entity': {\r\n                    'S': entity_map[entity_hash]\r\n                }\r\n            }\r\n        )\r\n    return hashed_message\r\n    \r\ndef deidentify_entities_in_message(message, entity_list):\r\n    entity_map = dict()\r\n    for entity in entity_list:\r\n      salted_entity = entity['Text'] + str(uuid.uuid4())\r\n      hashkey = hashlib.sha3_256(salted_entity.encode()).hexdigest()\r\n      entity_map[hashkey] = entity['Text']\r\n      message = message.replace(entity['Text'], hashkey)\r\n    return message, entity_map\r\n    \r\ndef lambda_handler(event, context):\r\n    bucket = 'assign2-scrape-bucket'\r\n    try:\r\n        # Extract the entities and message from the event\r\n        #originalDataObject= s3.get_object(Bucket = bucket, Key= 'ScrapedFolder/webscrape.txt')\r\n        #Entityobject= s3.get_object(Bucket = bucket, Key= 'EntityExtractionOutput/webscrape.txt.txt')\r\n        #entities_json_data = json.loads(Entityobject['Body'].read())\r\n        #originalData = originalDataObject['Body'].read().decode()\r\n        \r\n        originalData = event['data']\r\n        entities_json_data = event['entities']\r\n        # Mask entities\r\n        deidentified_message, entity_map = deidentify_entities_in_message(originalData, entities_json_data)\r\n        hashed_message = store_deidentified_message(deidentified_message, entity_map, 'LookUp')\r\n        return deidentified_message\r\n    except Exception as e:\r\n      logging.error('Exception: %s. Unable to extract entities from message' % e)\r\n      raise e", "572": "# Define here the models for your spider middleware\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\n\n# useful for handling different item types with a single interface\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass WebscrapeSpiderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(self, response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, or item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(self, response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Request or item objects.\n        pass\n\n    def process_start_requests(self, start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n\n\nclass WebscrapeDownloaderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_request(self, request, spider):\n        # Called for each request that goes through the downloader\n        # middleware.\n\n        # Must either:\n        # - return None: continue processing this request\n        # - or return a Response object\n        # - or return a Request object\n        # - or raise IgnoreRequest: process_exception() methods of\n        #   installed downloader middleware will be called\n        return None\n\n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n", "573": "from selenium import webdriver\n\n\n\ndef categoryURLWebscraper(url, textDoc): \n    options = webdriver.ChromeOptions()\n    options.add_argument('--ignore-certificate-errors')\n    options.add_argument(\"--test-type\")\n    driver = webdriver.Chrome(executable_path=\"./chromedriver\",options=options)\n    driver.get(url)\n\n    menuElements = driver.find_elements_by_class_name('link-page')\n    menuElementLnks = set([element.get_attribute('href') for element in menuElements])\n\n    with open (textDoc, 'w') as f:\n        for line in menuElementLnks:\n            f.write(line)\n            f.write('\\n')\n    return\n    \n\nif __name__ == \"__main__\":\n    homeURL = \"https://www.grocerygateway.com/store/\"\n    textDoc='webscrapePages.txt'\n    categoryURLWebscraper(homeURL, textDoc)\n", "574": "from webscrape_backend.backend_runner import runner\n\ndef main():\n\n    runner()\n\nif __name__ == \"__main__\":\n    main()\n", "575": "import sys\nimport numpy as np\nimport os\nimport sentiment\nimport nlp\nimport re\nimport file_read\nimport print_comments\n\n#first read in what function to run\nopt = sys.argv[1]\n#===== conducts nlp scoring\nif opt == 'nlp':\n    print(\"running nlp script\")\n    mode = sys.argv[3]\n    if mode == 'overall':\n        nlp.nps_breakdown(sys.argv[2], sys.argv[3])\n    elif mode == 'product':\n        #we must first parse the product input and format it correctly\n        products = sys.argv[5]\n        #get rid of empty space then split into array via comments\n        products = re.sub('[ ]', '', products)\n        product_arr = np.array(re.split(',|, ', products))\n        #get effectiveness keywords ready to input into function\n        product_keywords = sys.argv[4]\n        product_keywords = re.sub('[ ]', '', product_keywords)\n        product_keywords = re.sub('[\\']', '', product_keywords)\n        keywords_arr = np.array(re.split(',|, ', product_keywords))\n        nlp.nps_breakdown(sys.argv[2], sys.argv[3], keywords_arr, product_arr)\n    elif mode == 'category':\n        #we must take a 2D array\n        category = sys.argv[5]\n        category_keywords = sys.argv[4]\n        #condense the keyword inputs, removing spaces and quotation marks from the inputs\n        category_keywords = re.sub('[\\']', '', category_keywords)\n        category_keywords = re.sub('[ ]', '', category_keywords)\n        #do the same for the category names\n        category = re.sub('[ ]', '', category)\n        category_names = np.array(re.split(',|, ', category))\n        category_split = np.array(re.split(';|; ', category_keywords))\n        keywords_arr = []\n        for i in category_split:\n            temp_arr = np.array(re.split(',|, ', i))\n            keywords_arr.append(temp_arr)\n        nlp.nps_breakdown(sys.argv[2], 'category', keywords_arr, category_names)\n#===== downloads comments with sentiment scoring\nelif opt == 'sentiment':\n    print(\"running sentiment script\")\n    mode = sys.argv[3]\n    if mode == 'overall':\n        df = sentiment.overall_sentiment(sys.argv[2])\n        df.to_csv('df_download.csv', index = False)\n        print('df_download.csv')\n    elif mode == 'product':\n        #====== ready keyword and category inputs\n        products = sys.argv[5]\n        #products\n        product_arr = [products]\n        #keywords\n        product_keywords = sys.argv[4]\n        if product_keywords == '':\n            keywords_arr = []\n        else:\n            product_keywords = re.sub('[ ]', '', product_keywords)\n            product_keywords = re.sub('[\\']', '', product_keywords)\n            keywords_arr = re.split(',|, ', product_keywords)\n        #====== download the one dataset that is specified\n        df_outputs = sentiment.product_sentiment(sys.argv[2], keywords_arr, product_arr)\n        df = df_outputs[sys.argv[5]]\n        # df = df.reset_index()\n        df.to_csv('df_download.csv', index = False)\n        print(\"df_download.csv\")\n    elif mode == 'category':\n        #====== ready keyword and category inputs\n        category = sys.argv[5]\n        #products\n        category_arr = [category]\n        #keywords\n        category_keywords = sys.argv[4]\n        if category_keywords == 'null':\n            keywords_arr = []\n        else:\n            category_keywords = re.sub('[ ]', '', category_keywords)\n            category_keywords = re.sub('[\\']', '', category_keywords)\n            keywords_arr = np.array(re.split(',|, ', category_keywords))\n            keywords_arr = [keywords_arr]\n        #====== download the one dataset that is specified\n        df_outputs = sentiment.category_sentiment(sys.argv[2], keywords_arr, category_arr)\n        df = df_outputs[sys.argv[5]]\n        # df = df.reset_index()\n        df.to_csv('df_download.csv', index = False)\n        print(\"df_download.csv\")\n\n#===== prints top negative and positive comments\nelif opt == 'print_comments':\n    print(\"running print comments script\")\n    print_comments.print_comments(sys.argv[2], sys.argv[3])\n#===== downloads webscraped comments into a csv file to be analyzed\nelif opt == 'webscrape_download':\n    print(\"running webscrape_download script\")\n    df = file_read.megajson_to_dataset(sys.argv[2], sys.argv[3])\n    df.to_csv('webscraped_download.csv', index = False)\n    print('webscraped_download.csv')\nelse:\n    print(\"invalid function option run\")\n", "576": "from webscrape import SCRAPE\n\n\nclass IMDB(SCRAPE):\n\n    def __init__(self) -> None:\n        super().__init__()\n\n", "577": "from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nimport re\n\nproductLink = \"\"\n\n\ndef webscrape(link):\n    productLink = link\n    print(productLink)\n    # we will be using a headless implementation of chrome\n    option = Options()\n    option.headless = True\n    driver = webdriver.Chrome(options=option)\n    driver.get(productLink)\n\n    # We're now going to find the availability tag which is a link tag with an href\n    availability = driver.find_element(\n        By.XPATH, '//link[contains(@href, \"http://schema.org\")]').get_attribute(\"href\")\n\n    if \"InStock\" in availability:\n        # we now have to send ourselves an email notification about the availability\n        exec(open(\"/home/mrugank/Documents/Python Projects/Monitoring-and-Alert-System/alert.py\", \"r\").read())\n        driver.quit()\n\n\ndef main():\n    with open(\"/home/mrugank/Documents/Python Projects/Monitoring-and-Alert-System/Monitor.txt\", 'r') as monitorFile:\n        lines = monitorFile.readlines()\n        regSearch = re.compile(r'https://www.bestbuy.c(a|om)(\\S)+(\\s)?')\n        for line in lines:\n            urlMatch = regSearch.search(line)\n            url = urlMatch.group(0) if urlMatch else None\n            if url is not None:\n                processedUrl = re.sub(\n                    '^(.*)(?=https://www.bestbuy.c(a|om))', \"\", url)\n                webscrape(processedUrl)\n\n\nif __name__ == '__main__':\n    main()\n", "578": "from flask import Flask, render_template, request, redirect, url_for\nimport smtplib\nfrom email.message import EmailMessage\nfrom urllib.request import urlopen as uReq\nfrom bs4 import BeautifulSoup as soup\nimport time\nimport sys\n\napp = Flask(__name__)\n\n# Scrapes Robinhood for price (just ETH for now)\ndef webScrape(ticker):\n\tif ticker == 'BTC' or ticker == 'ETH' or ticker == 'DOGE' or ticker == 'GME' or ticker == 'AMC' or ticker == 'TSLA' or ticker == 'BB':\n\t\tif ticker == 'BTC':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/BTC-USD?p=BTC-USD&.tsrc=fin-srch'\n\t\telif ticker == 'ETH':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/ETH-USD?p=ETH-USD&.tsrc=fin-srch'\n\t\telif ticker == 'DOGE':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/DOGE-USD?p=DOGE-USD&.tsrc=fin-srch'\n\t\telif ticker == 'GME':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/GME?p=GME&.tsrc=fin-srch'\n\t\telif ticker == 'AMC':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/AMC?p=AMC&.tsrc=fin-srch'\n\t\telif ticker == 'TSLA':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/TSLA?p=TSLA&.tsrc=fin-srch'\n\t\telif ticker == 'BB':\n\t\t\tmy_url = 'https://finance.yahoo.com/quote/BB?p=BB&.tsrc=fin-srch'\n\n\t\t#opening up connection and grab page\n\t\tuClient = uReq(my_url)\n\t\tpage_html = uClient.read()\n\t\tuClient.close()\n\n\t\t#html parsing\n\t\tpage_soup = soup(page_html, \"html.parser\")\n\n\t\t#grab price diff %\n\t\tif ticker == 'GME' or ticker == 'AMC' or ticker == 'TSLA' or ticker == 'BB':\n\t\t\tdata = str(page_soup.findAll(\"span\", {\"data-reactid\":\"51\"})[0].text)\n\t\telse:\n\t\t\tdata = str(page_soup.findAll(\"span\", {\"data-reactid\":\"34\"})[1].text)\n\n\t\tdata = data[:-2]\n\t\tnewData = \"\"\n\t\tfor element in reversed(data):\n\t\t\tif element == '(':\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tnewData += element\n\t\tnewData = newData[::-1]\n\t\treturn float(newData)\n\n\n\n\n# Handles sending the sms mesage\ndef sendMessage(body, subject, to):\n\t#use EmailMessage library and set variables based on function arguments\n\tmsg.set_content(body)\n\tmsg['subject'] = subject\n\tmsg['to'] = to\n\n\tserver.send_message(msg)\n\n\tdel msg['to']\n\tdel msg['subject']\n\n\n# Drives the program\ndef driver(ticker, number, carrier, threshold, duration):\n\t#email and login setup\n\tuser = \"your sent from email\"\n\tmsg['from'] = user\n\tpassword = \"your sent from email app password\"\n\tcarrierEmail = \"some email string\"\n\t\n\tif carrier == \"VERIZON\" or carrier == \"XFINITY\":\n\t\tcarrierEmail = \"vtext.com\"\n\telif carrier == \"AT&T\":\n\t\tcarrierEmail = \"txt.att.net\"\n\telif carrier == \"SPRINT\":\n\t\tcarrierEmail = \"messaging.sprintpcs.com\"\n\telif carrier == \"TMOBILE\":\n\t\tcarrierEmail = \"tmomail.net\"\n\telif carrier == \"VIRGIN\":\n\t\tcarrierEmail = \"vmobl.com\"\n\n\tto = number + \"@\" + carrierEmail\n\n\t#server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n\tserver.starttls()\n\tserver.login(user, password)\n\n\tcount = 0\n\n\n\n\t#get initial % change in price \n\tpriorChange = webScrape(ticker)\n\tsubject = 'INITIAL'\n\tbody = 'Welcome to Cereal! You will get alerts for {} when a change in daily price differs by 1%, starting at {}%.'.format(ticker, priorChange)\n\tsendMessage(body, subject, to)\n\n\n\n\t#enter while loop that will continue to run program\n\twhile count != (int(30 * duration)):\n\t\t#get current 24 hour % change\n\t\tcurrentChange = webScrape(ticker)\n\n\t\tif currentChange != priorChange:\n\t\t\tif priorChange > 0 and currentChange < 0:\n\t\t\t\t#output message saying it's now down today\n\t\t\t\tsubject = 'DOWN'\n\t\t\t\tbody = '{} is now down today at {}%'.format(ticker, currentChange)\n\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t#set prior to now be current\n\t\t\t\tpriorChange = currentChange\n\t\t\telif priorChange < 0 and currentChange > 0:\n\t\t\t\t#output message saying it's now up today\n\t\t\t\tsubject = 'UP'\n\t\t\t\tbody = '{} is now up today at {}%'.format(ticker, currentChange)\n\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t#set prior to now be current\n\t\t\t\tpriorChange = currentChange\n\t\t\telif currentChange > 0:\n\t\t\t\tif float(currentChange - priorChange) >= threshold:\n\t\t\t\t\t#output message saying that it's now up even more today at ___\n\t\t\t\t\tsubject = 'UP'\n\t\t\t\t\tbody = '{} is now up even more today at {}%'.format(ticker, currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\t\t\t\telif float(currentChange - priorChange) <= (-1 * threshold):\n\t\t\t\t\t#output message saying that it's now up less today at ___\n\t\t\t\t\tsubject = 'UP'\n\t\t\t\t\tbody = '{} is now up less today at {}%'.format(ticker, currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\t\t\telif currentChange < 0:\n\t\t\t\tif float(currentChange - priorChange) >= threshold:\n\t\t\t\t\t#ouptut message saying that it's now down even more today at ___\n\t\t\t\t\tsubject = 'DOWN'\n\t\t\t\t\tbody = '{} is now down less today at {}%'.format(ticker, currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\t\t\t\telif float(currentChange - priorChange) <= (-1 * threshold):\n\t\t\t\t\t#output message saying that it's now down less today at ___\n\t\t\t\t\tsubject = 'DOWN'\n\t\t\t\t\tbody = '{} is now down even more today at {}%'.format(ticker, currentChange)\n\t\t\t\t\tsendMessage(body, subject, to)\n\n\t\t\t\t\t#set prior to now be current\n\t\t\t\t\tpriorChange = currentChange\n\n\t\t#wait 2 minutes before entering loop again\n\t\ttime.sleep(120)\n\n\t\tcount = count + 1\n\n\t#exits while loop, we are done\t\t\t\n\tserver.quit()\n\n\n# Displays homepage\n@app.route(\"/\", methods =[\"GET\", \"POST\"])\ndef home():\n\tif request.method == \"POST\":\n\n\t\t#get info from html form\n\t\tticker = request.form.get(\"ticker\")\n\t\tnumber = request.form.get(\"number\")\n\t\tcarrier = request.form.get('carrier')\n\t\tthreshold = float(request.form.get('alerts'))\n\t\tduration = float(request.form.get('duration'))\n\n\t\tdriver(ticker, number, carrier, threshold, duration)\n\n\t\treturn render_template(\"buttonpressed.html\")\n\n\treturn render_template(\"home.html\")\n\n\n\n# Starts the program\nif __name__ == '__main__':\n\t#initialize variables\n\tmsg = EmailMessage()\n\tserver = smtplib.SMTP(\"smtp.gmail.com\", 587)\n\tcurrentChange = 1.0\n\tpriorChange = 1000.0\n\tsubject = 'This is a subject'\n\tbody = 'This is a body'\n\tto = \"filler send to number (string)\" #the number or email that we're sending the message to. Make this an option later\n\tcarrier = \"VERIZON\"\n\tthreshold = 1.0\n\tduration = 1.0\n\n\t#sys.exit()\n\n\tapp.run()\n\n\n\n", "579": "# WebScrape.py\r\nfrom bs4 import BeautifulSoup\r\nimport requests\r\n\r\n\r\nwebsite = \"https://subslikescript.com/movie/Titanic-120338\"\r\nresult = requests.get(website)\r\ncontent = result.text\r\nsoup = BeautifulSoup(content, \"lxml\")\r\nbox = soup.find('article', class_=\"main-article\")\r\n\r\ntitle = box.find('h1').get_text()\r\n\r\ntranscript = box.find('div', class_=\"full-script\")\r\ntranscript = transcript.get_text(strip=True, separator=\" \")\r\n\r\nwith open(f'{title}.txt', 'w', encoding='utf-8') as file:\r\n    file.write(transcript)\r\n", "580": "\"\"\"\nCreated on 2020-08-20\n\n  script to read event metadata from http://wikicfp.com\n  \n  use: python3 wikicfp.py [startId] [stopId] [threads]\n  @param startId\n  @param stopId\n  @param threads - number of threads the script should create to improve performance\n  \n  example: python3 wikicfp.py --startId 2000 --stopId 2999 10\n\n  @author:     svantje, wf\n  @copyright:  2020-2021 TIB Hannover, Wolfgang Fahl. All rights reserved.\n\n\"\"\"\nfrom corpus.datasources.webscrape import WebScrape\nfrom corpus.event import EventStorage,EventManager, EventSeriesManager\nimport datetime\nfrom enum import Enum\nimport glob\nimport re\nimport os\nimport sys\nimport threading\nimport time\nfrom argparse import ArgumentParser\nfrom argparse import RawDescriptionHelpFormatter\n#from lodstorage.jsonpicklemixin import JsonPickleMixin\nfrom lodstorage.storageconfig import StorageConfig\nimport corpus.datasources.wikicfp as wcfp\n#import jsonpickle\n\nclass CrawlType(Enum):\n    '''\n    possible supported storage modes\n    '''\n    EVENT = \"Event\"      \n    SERIES = \"Series\"\n    \n    @property\n    def urlPrefix(self):\n        baseUrl=\"http://www.wikicfp.com/cfp\"\n        if self.value is CrawlType.EVENT.value:\n            url= f\"{baseUrl}/servlet/event.showcfp?eventid=\"\n        elif self.value is CrawlType.SERIES.value:\n            url= f\"{baseUrl}/program?id=\"\n        return url\n    \n \n    @classmethod\n    def isValid(cls,value:str)->bool:\n        '''\n        check whether the given value is valid\n        \n        Args:\n            value(str): the value to check\n        \n        Return:\n            bool: True if the value is a valid value of this enum\n        '''\n        return value in [\"Event\",\"Series\"]\n\n    @classmethod\n    def ofValue(cls,value:str):\n        if value==\"Event\":\n            return CrawlType.EVENT\n        elif value==\"Series\":\n            return CrawlType.SERIES\n        else:\n            return None\n\nclass CrawlBatch(object):\n    '''\n    describe a batch of pages to fetch metadata from\n    '''\n    \n    def __init__(self,threads:int,startId:int,stopId:int,crawlTypeValue:str,threadIndex=None,):\n        '''\n        construct me with the given number of threads, startId and stopId\n        \n        Args:\n           threads(int): number of threads to use\n           startId(int): id of the event to start crawling with\n           stopId(int): id of the event to stop crawling\n           crawlTypeValue(str): the type of crawling (Event or Series)\n           threadIndex(int): the index of this batch\n        '''\n        self.threads=threads\n        self.threadIndex=threadIndex\n        self.startId=startId\n        self.stopId=stopId     \n        if startId <= stopId: \n            self.step = +1\n            self.total = stopId-startId+1\n        else: \n            self.step = -1\n            self.total = startId-stopId+1\n        self.batchSize = self.total // threads\n        if not CrawlType.isValid(crawlTypeValue):\n            raise Exception(f\"Invalid crawlType {crawlTypeValue}\")\n        self.crawlType=CrawlType.ofValue(crawlTypeValue)\n        \n    def split(self)->list:\n        '''\n        split me for my threads\n        '''\n        crawlBatches=[]\n        for threadIndex in range(self.threads):\n            s = self.startId + threadIndex * self.batchSize\n            e = s + self.batchSize-1\n            splitBatch=CrawlBatch(1,s, e,self.crawlType.value,threadIndex)\n            crawlBatches.append(splitBatch)\n        return crawlBatches\n      \n    \n    def __str__(self):\n        '''\n        get my string representation\n        Return\n            str: my string representation\n        '''\n        text=f\"WikiCFP {self.crawlType.value} IDs {self.startId} - {self.stopId} ({self.threads} threads of {self.batchSize} IDs each)\"\n        return text\n \nclass WikiCfpScrape(object):\n    '''\n    support events from http://www.wikicfp.com/cfp/\n    '''\n\n    def __init__(self,jsonEventManager,jsonEventSeriesManager,profile:bool=True,debug:bool=False,jsondir:str=None,limit=200000,batchSize=1000,showProgress=True):\n        '''\n        Constructor\n        \n        Args:\n            config(StorageConfig): the storage configuration to use\n            debug(bool): if True debug for crawling is on\n            limit(int): maximum number of entries to be crawled\n            batchSize(int): default size of batches\n            showProgress(bool): if True show Progress\n        '''\n        self.debug=debug\n        self.limit=limit\n        self.batchSize=batchSize\n        self.showProgress=showProgress\n        self.jsonEventManager=jsonEventManager\n        self.jsonEventSeriesManager=jsonEventSeriesManager\n        self.jsonManagers={\n            \"Event\": jsonEventManager,\n            \"Series\": jsonEventSeriesManager\n        }\n        self.profile=profile\n        if jsondir is not None:\n            self.jsondir=jsondir\n        else:\n            cachePath=self.jsonEventManager.config.getCachePath()\n            self.jsondir=f\"{cachePath}/wikicfp\"\n            if not os.path.exists(self.jsondir):\n                    os.makedirs(self.jsondir)\n        \n    def getManager(self,crawlType:CrawlType):\n        '''\n        get the manager for the given crawlType\n        \n        Args:\n           crawlType(CrawlType) the manager for the given CrawlType\n        '''\n        manager=self.jsonManagers[crawlType.value]\n        return manager\n    \n    def cacheToJsonManager(self,crawlType:CrawlType):\n        '''\n        cache the crawlType entities to my json manager for that crawlType\n        \n        Args:\n            crawlType(CrawlType): the CrawlType to work with\n            \n        Return:\n            EntityBasemanager: the entityManager\n            \n        '''\n        jsonEm=self.getManager(crawlType)\n        if jsonEm.isCached():\n            jsonEm.fromStore()\n        else:    \n            self.crawlFilesToJson(crawlType=crawlType,withStore=True)\n        return jsonEm    \n        \n    def crawlFilesToJson(self,crawlType:CrawlType,withStore=True):    \n        '''\n        convert the crawlFiles to Json\n        \n        Args:\n            crawlType(CrawlType): the CrawlType to work with\n            withStore(bool): True if the data should be stored after collecting\n        \n        Return:\n            jsonEm(EventBaseManager): the JSON - storage based EventBaseManager to use to collect the results\n        '''\n        # crawling is not done on startup but need to be done\n        # in command line mode ... we just collect the json crawl result files here\n        #self.crawl(startId=startId,stopId=stopId)\n        jsonEm=self.getManager(crawlType)\n        entityList=jsonEm.getList()\n        startTime=time.time()\n        jsonFiles=self.jsonFiles(crawlType)\n        if len(jsonFiles)==0:\n            if self.profile or self.debug:\n                print(f\"No wikiCFP crawl json backups for {crawlType.value} available\")\n        else:\n            # loop over all crawled files\n            for jsonFilePath in jsonFiles:\n                config=StorageConfig.getJSON(debug=self.debug)\n                config.cacheFile=jsonFilePath\n                if crawlType.value is CrawlType.EVENT.value:\n                    batchEm=EventManager(name=jsonFilePath,clazz=jsonEm.clazz,config=config)\n                elif crawlType.value is CrawlType.SERIES.value:\n                    batchEm=EventSeriesManager(name=jsonFilePath,clazz=jsonEm.clazz,config=config)\n                # legacy mode -file were created before ORM Mode\n                # was available - TODO: make new files available in ORM mode with jsonable\n                #jsonPickleEm=JsonPickleMixin.readJsonPickle(jsonFileName=jsonFilePath,extension='.json')\n                #jsonPickleEvents=jsonPickleEm['events']\n                #if self.showProgress:\n                #    print(\"%4d: %s\" % (len(jsonPickleEvents),jsonFilePath))\n                batchEm.fromStore(cacheFile=jsonFilePath)\n                for entity in batchEm.getList():\n                    if hasattr(entity, \"startDate\"):\n                        if entity.startDate is not None:\n                            entity.year=entity.startDate.year  \n                    doAdd=not(entity.deleted)\n                    # SPAM Filter\n                    if hasattr(entity, \"locality\"):\n                        if entity.locality.startswith(\"1\"):\n                            doAdd=False\n                    # Series Filter\n                    if hasattr(entity, \"title\"):\n                        if entity.title.startswith(\"WikiCFP : Call For Papers of Conferences, Workshops and Journals\"):\n                            doAdd=False\n                    \n                    if doAdd:\n                        entityList.append(entity)\n            if self.profile:\n                elapsed=time.time()-startTime\n                print (f\"read {len(entityList)} {crawlType.value} records in {elapsed:5.1f} s\")\n            if withStore:\n                jsonEm.store(limit=self.limit,batchSize=self.batchSize)\n        return jsonEm\n        \n    def jsonFiles(self,crawlType:CrawlType)->list:  \n        '''\n        get the list of the json files that have my data\n        \n        Args:\n            crawlType(CrawlType): the tpe of the files\n            \n        Return:\n            list: a list of json file names\n        '''\n        jsonFiles=sorted(glob.glob(f\"{self.jsondir}/wikicfp_{crawlType.value}*.json\"),key=lambda path:int(re.findall(r'\\d+',path)[0]))\n        return jsonFiles    \n        \n    def getJsonFileName(self,crawlBatch):\n        '''\n        get my the JsonFileName \n        \n        Args:\n            crawlBatch(CrawlBatch): the batch to crawl):\n            \n        Return:\n            str: the json file name for this batch\n        '''\n        jsonFilePath=f\"{self.jsondir}/wikicfp_{crawlBatch.crawlType.value}{crawlBatch.startId:06d}-{crawlBatch.stopId:06d}.json\"\n        return jsonFilePath\n    \n    def getBatchEntityManager(self,crawlBatch:CrawlBatch):\n        '''\n        get the batch Entity Manager for the given crawlBatch\n        \n        Args:\n            crawlBatch(CrawlBatch): the batch to crawl):\n            \n        Return:\n            EntityManager: either a Event or a Series Manager\n        '''\n        jsonFilepath=self.getJsonFileName(crawlBatch)\n        config=EventStorage.getStorageConfig(debug=self.debug, mode=\"json\")\n        config.cacheFile=jsonFilepath\n        crawlType=crawlBatch.crawlType\n        print(f\"CrawlBatch has crawlType {type(crawlType)}{crawlType}/{crawlType.value}\")\n        if crawlType.value is CrawlType.EVENT.value:\n            batchEm=wcfp.WikiCfpEventManager(config=config)\n        elif crawlType.value is CrawlType.SERIES.value:\n            batchEm=wcfp.WikiCfpEventSeriesManager(config=config)\n        else:\n            raise Exception(f\"Invalid crawlType {crawlType}\")\n        return batchEm\n        \n    def crawl(self,crawlBatch:CrawlBatch):\n        '''\n        see https://github.com/TIBHannover/confIDent-dataScraping/blob/master/wikicfp.py\n        \n        Args:\n            crawlBatch(CrawlBatch): the batch to crawl\n        '''\n       \n        print(f'crawling {crawlBatch}')\n        batchEm=self.getBatchEntityManager(crawlBatch)\n \n        # get all ids\n        crawlType=crawlBatch.crawlType\n        for eventId in range(int(crawlBatch.startId), int(crawlBatch.stopId+1), crawlBatch.step):\n            wEvent=WikiCfpEventFetcher(crawlType=crawlType)\n            retry=1\n            maxRetries=3\n            retrievedResult=False\n            while not retrievedResult:\n                try:\n                    rawEvent=wEvent.fromEventId(eventId)\n                    if crawlType.value is CrawlType.EVENT.value:\n                        event=wcfp.WikiCfpEvent()\n                        event.fromDict(rawEvent)\n                        title=\"? deleted: %r\" %event.deleted if not 'title' in rawEvent else event.title\n                        batchEm.getList().append(event)\n                    elif crawlType.value is CrawlType.SERIES.value:\n                        eventSeries=wcfp.WikiCfpEventSeries()\n                        eventSeries.fromDict(rawEvent)\n                        title=\"?\" if not 'title' in rawEvent else eventSeries.title\n                        batchEm.getList().append(eventSeries)\n                    retrievedResult=True\n                except Exception as ex:\n                    if \"HTTP Error 500\" in str(ex):\n                        print(f\"{eventId} inaccessible due to HTTP Error 500\")\n                        retrievedResult=True\n                    elif \"timed out\" in str(ex):\n                        print(f\"{eventId} access timed Out on retry attempt {retry}\")\n                        retry+=1\n                        if retry>maxRetries:\n                            raise ex\n                    else:\n                        raise ex\n                    pass\n                \n            print(f\"{eventId:06d}: {title}\")\n           \n           \n        batchEm.store()\n        return batchEm\n            \n    def threadedCrawl(self,crawlBatch:CrawlBatch):\n        '''\n        crawl with the given number of threads, startId and stopId\n        \n        Args:\n            crawlBatch(CrawlBatch): the batch to crawl\n        '''\n        # determine the eventId range for each threaded job\n        startTime=time.time()\n        \n        msg=f'Crawling {crawlBatch}'\n        print(msg)\n\n        # this list will contain all threads -> we can wait for all to finish at the end\n        jobs = []\n\n        # now start each thread with its id range and own filename\n        for crawlBatch in crawlBatch.split(): \n        \n            thread = threading.Thread(target = self.crawl, args=(crawlBatch,))\n            jobs.append(thread)\n            \n        for job in jobs:\n            job.start()   \n\n        # wait till all threads have finished before print the last output\n        for job in jobs:\n            job.join()\n\n        if self.debug:\n            elapsed=time.time()-startTime\n            print(f'crawling done after {elapsed:5.1f} s')\n               \n      \nclass WikiCfpEventFetcher(object):\n    '''\n    a single WikiCfpEentFetcher to fetch and event or series\n    '''\n    def __init__(self,crawlType=CrawlType.EVENT,debug=False,showProgress:bool=True,timeout=20):\n        '''\n        construct me\n        \n        Args:\n            showProgress(bool): if True show progress\n            timeout(float): the default timeout\n        \n        '''\n        self.debug=debug\n        self.crawlType=crawlType\n        self.showProgress=showProgress\n        self.progressCount=0\n        self.timeout=timeout\n            \n    def fromTriples(self,rawEvent,triples): \n        '''\n        get the rawEvent dict from the given triple e.g.:\n        \n        v:Event(v:summary)=IDC 2009\n        v:Event(v:eventType)=Conference\n        v:Event(v:startDate)=2009-06-03T00:00:00\n        v:Event(v:endDate)=2009-06-05T23:59:59\n        v:Event(v:locality)=Milano, Como, Italy\n        v:Event(v:description)= IDC  2009 : The 8th International Conference on Interaction Design and Children\n        v:Address(v:locality)=Milano, Como, Italy\n        v:Event(v:summary)=Submission Deadline\n        v:Event(v:startDate)=2009-01-19T00:00:00\n        v:Event(v:summary)=Notification Due\n        v:Event(v:startDate)=2009-02-20T00:00:00\n        v:Event(v:summary)=Final Version Due\n        v:Event(v:startDate)=2009-03-16T00:00:00\n        '''\n        recentSummary=None\n        for s,p,o in triples:\n            s=s.replace(\"v:\",\"\")\n            p=p.replace(\"v:\",\"\")\n            if self.debug:\n                print (\"%s(%s)=%s\" % (s,p,o)) \n            if recentSummary in ['Submission Deadline','Notification Due','Final Version Due']:             \n                key=recentSummary.replace(\" \",\"_\")\n            else:\n                key=p \n            if p.endswith('Date'):\n                dateStr=o\n                if dateStr==\"TBD\":\n                    o=None\n                else:    \n                    o=datetime.datetime.strptime(\n                        dateStr, \"%Y-%m-%dT%H:%M:%S\").date()\n            if not key in rawEvent: \n                rawEvent[key]=o    \n            if p==\"summary\": \n                recentSummary=o \n            else: \n                recentSummary=None\n                \n    @staticmethod       \n    def getUrl(cfpid,crawlType:CrawlType=CrawlType.EVENT)->str:\n        '''\n        Args:\n            cfpid(int): the WikiCFP id of the event or series\n            crawlType(CrawlType): Event or Series\n            \n        Returns:\n            the WikiCfP url\n        '''\n        url=f\"{crawlType.urlPrefix}{cfpid}\"\n        return url   \n    \n    @classmethod\n    def getLatestEvent(cls,debug=False,showProgress=True):\n        '''\n        get the latest Event doing a binary search\n        '''\n        wikicfp=WikiCfpEventFetcher(debug,showProgress=showProgress)\n        wikicfp.progressCount=0\n        wikicfp.getLatesEvetFromPair()\n        \n    def getHighestNonDeletedIdInRange(self,fromId:int,toId:int)->int:\n        '''\n        get the event with the highest id int the range fromId to toId that is not deleted\n        \n        Args:\n            fromId(int): minimum id to search from\n            toId(int): maximum id to search to\n            \n        Returns:\n            int: maxium id of an event that is not deleted or None if there is none in this range\n        '''\n        maxId=None\n        for eventId in range(fromId,toId+1):\n            if self.showProgress:\n                print(\".\",end='',flush=True)\n                self.progressCount+=1\n                if self.progressCount % 50 == 0:\n                    print(flush=True)\n            rawEvent=self.fromEventId(eventId)\n            if not rawEvent['deleted']:\n                maxId=eventId\n        return maxId\n    \n    def getLatesEvetFromPair(self,low=5000,high=300000,margin=40):\n        '''\n        get the latest Event doing a binary search\n        \n        Args:\n            low(int): lower index to search from\n            hight(int): upper index boundary\n        '''\n        if high-low>margin+1:\n            mid=(high+low)//2\n            midId=(self.getHighestNonDeletedIdInRange(mid-margin, mid))\n            if midId:\n                return self.getLatesEvetFromPair(mid+1,high)\n            else:\n                return self.getLatesEvetFromPair(low, mid-1)\n        else:\n            return mid\n        pass\n                           \n    def fromEventId(self,cfpid:int):\n        '''\n        see e.g. https://github.com/andreeaiana/graph_confrec/blob/master/src/data/WikiCFPCrawler.py\n        \n        Args:\n            cfpid(int): the wikicfp id to use\n        '''\n        url=WikiCfpEventFetcher.getUrl(cfpid,self.crawlType)\n        return self.fromUrl(url)\n    \n    def rawEventFromWebScrape(self,rawEvent:dict,triples:list,scrape:WebScrape):\n        '''\n        fill the given rawEvent with the data derived from the scrape \n        \n        Args:\n            rawEvent(dict): the event dictionary\n            triples(list): the triples found\n            scrape(WebScrape): the webscrape object to be used for parsing\n        '''\n        if len(triples)==0:\n            #scrape.printPrettyHtml(scrape.soup)\n            firstH3=scrape.fromTag(scrape.soup, 'h3')\n            if \"This item has been deleted\" in firstH3:\n                rawEvent['deleted']=True\n        else:        \n            self.fromTriples(rawEvent,triples)\n            # add series information\n            # Tag: International Semantic Web Conference\n            m,seriesText=scrape.findLinkForRegexp(r'/cfp/program\\?id=([0-9]+).*')\n            if m:\n                seriesId=m.group(1)\n                rawEvent['seriesId']=seriesId\n                rawEvent['series']=seriesText\n                pass\n                \n            if 'summary' in rawEvent:\n                rawEvent['acronym']=rawEvent.pop('summary').strip()\n            if 'description' in rawEvent:\n                rawEvent['title']=rawEvent.pop('description').strip()\n                \n    def rawEventSeriesFromWebScrape(self,rawEvent:dict,scrape:WebScrape):\n        '''\n        fill the given rawEventSeries with the data derived from the scrape \n        \n        Args:\n            rawEvent(dict): the event dictionary\n            scrape(WebScrape): the webscrape object to be used for parsing\n        '''\n        title=scrape.soup.find(\"title\")\n        rawEvent[\"title\"]=title.text.strip()\n        dblpM,_text=scrape.findLinkForRegexp(r'http://dblp.uni-trier.de/db/([a-zA-Z0-9/-]+)/index.html')\n        if dblpM:\n            dblpSeriesId=dblpM.group(1)\n            rawEvent['dblpSeriesId']=dblpSeriesId\n        pass\n \n    \n    def fromUrl(self,url:str)->dict:\n        '''\n        get the event form the given url\n        \n        Args:\n            url(str): the url to get the event from\n        \n        Returns:\n            dict: a raw event dict or None if an error occured\n        \n        '''\n        regexp=r\"^\"+self.crawlType.urlPrefix.replace(\"?\",\"\\?\")+\"(\\d+)$\"\n        m=re.match(regexp,url)\n        if not m:\n            raise Exception(\"Invalid URL %s\" % (url))\n        else:\n            cfpId=int(m.group(1))\n        rawEvent={}\n        if self.crawlType.value is CrawlType.EVENT.value:\n            rawEvent['eventId']=f\"{cfpId}\" \n        elif self.crawlType.value is CrawlType.SERIES.value:\n            rawEvent['seriesId']=f\"{cfpId}\" \n        rawEvent['url']=url\n        rawEvent['wikiCfpId']=cfpId\n        rawEvent['deleted']=False\n        scrape=WebScrape(debug=self.debug,timeout=self.timeout)\n        triples=scrape.parseRDFa(url)\n        if scrape.err:\n            raise Exception(f\"fromUrl {url} failed {scrape.err}\")\n        if self.crawlType.value is CrawlType.EVENT.value:\n            self.rawEventFromWebScrape(rawEvent, triples, scrape)\n        else:\n            self.rawEventSeriesFromWebScrape(rawEvent,scrape)\n        return rawEvent\n    \n__version__ = 0.4\n__date__ = '2020-06-22'\n__updated__ = '2021-08-09'    \n\nDEBUG = 1\n\n    \ndef main(argv=None): # IGNORE:C0111\n    '''main program.'''\n\n    if argv is None:\n        argv=sys.argv[1:]\n        \n    program_name = os.path.basename(__file__)\n    program_version = \"v%s\" % __version__\n    program_build_date = str(__updated__)\n    program_version_message = '%%(prog)s %s (%s)' % (program_version, program_build_date)\n    program_shortdesc = \"script to read event metadata from http://wikicfp.com\"\n    user_name=\"Wolfgang Fahl\"\n    program_license = '''%s\n\n  Created by %s on %s.\n  Copyright 2020-2021 TIB Hannover, Wolfgang Fahl. All rights reserved.\n\n  Licensed under the Apache License 2.0\n  http://www.apache.org/licenses/LICENSE-2.0\n\n  Distributed on an \"AS IS\" basis without warranties\n  or conditions of any kind, either express or implied.\n\nUSAGE\n''' % (program_shortdesc,user_name, str(__date__))\n\n    try:\n        # Setup argument parser\n        parser = ArgumentParser(description=program_license, formatter_class=RawDescriptionHelpFormatter)\n        parser.add_argument(\"-d\", \"--debug\", dest=\"debug\", action=\"count\", help=\"set debug level [default: %(default)s]\")\n        parser.add_argument('-V', '--version', action='version', version=program_version_message)\n        parser.add_argument('--startId', type=int, help='eventId to start crawling from', required=True)\n        parser.add_argument('--stopId', type=int, help='eventId to stop crawling at', required=True)\n        parser.add_argument('--crawlType',type=str,default=\"Event\",help=\"The crawlType - Event or Series\")\n        parser.add_argument('-p','--targetPath',type=str,help=\"targetPath (JSON directory) for crawl results\")\n        parser.add_argument('-t','--threads', type=int, help='number of threads to start', default=10)\n\n        # Process arguments\n        args = parser.parse_args(argv)\n        wikiCfp=wcfp.WikiCfp()\n        wikiCfpScrape=wikiCfp.wikiCfpScrape\n        wikiCfpScrape.jsondir=args.targetPath\n        wikiCfpScrape.debug=args.debug\n        crawlBatch=CrawlBatch(args.threads, args.startId, args.stopId,args.crawlType,None)\n        wikiCfpScrape.threadedCrawl(crawlBatch)\n        \n    except KeyboardInterrupt:\n        ### handle keyboard interrupt ###\n        return 1\n    except Exception as e:\n        if DEBUG:\n            raise(e)\n        indent = len(program_name) * \" \"\n        sys.stderr.write(program_name + \": \" + repr(e) + \"\\n\")\n        sys.stderr.write(indent + \"  for help use --help\")\n        return 2     \n    \nif __name__ == \"__main__\":\n    if DEBUG:\n        sys.argv.append(\"-d\")\n    sys.exit(main())\n", "581": "\"\"\"\nCreated on Sun Sep 27 14:40:55 2020\n\n@author: kianaamaral\n\"\"\"\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom pymol import cmd\nimport os\n\nprint('Enter pdbDirectory (This is where the input pdb files are saved): ')\npdbDirectory = input()\nprint('Enter outputDirectory (This is where the output .cif files will be saved): ')\noutputDirectory = input() \n\n\nclass protein3Dcompare:\n    \n    def __init__(self):\n        pass\n            \n    def testClass(self,mystring):\n        print(mystring)\n        \n        \n####### UNIPROT DATA #######      \n        \n    def webscrape_gene(self,uniprot_ids):\n        LIST_OF_GENES = [] \n        for items in uniprot_ids:\n            # WEB SCRAPES GENE \n            URL = 'https://www.uniprot.org/uniprot/' + items\n            page = requests.get(URL)\n            soup = BeautifulSoup(page.content, 'html.parser')\n            results = soup.find_all('a')\n            for result in results:\n                if 'eco' in result.text: \n                    s = result.text\n                    gene = s.strip('eco: ')\n                    LIST_OF_GENES.append(gene)\n                    break\n                \n        return LIST_OF_GENES\n    \n    def get_geneDict(self,uniprot_ids): \n        gene_dict = {}\n        for items in uniprot_ids:\n            # WEB SCRAPES GENE \n            URL = 'https://www.uniprot.org/uniprot/' + items\n            page = requests.get(URL)\n            soup = BeautifulSoup(page.content, 'html.parser')\n            results = soup.find_all('a')\n            for result in results:\n                if 'eco' in result.text: \n                    s = result.text\n                    gene = s.strip('eco: ')\n                    gene_dict[items] = gene\n                    break\n        \n        return gene_dict\n         \n    def pdbidToUniprot(self, pdbid): \n        URL = 'https://www.rcsb.org/structure/' + pdbid\n        page = requests.get(URL)\n        soup = BeautifulSoup(page.content, 'html.parser')\n        uniprot = 'none'\n        results = soup.find_all('a')\n        for result in results:\n            u = result.text\n            if len(u) == 6 and u[1].isdigit():\n                uniprot = u\n                break\n        \n            \n        return uniprot \n    \n    def getOrganism(self, uniprot):\n        if uniprot == 'none': \n            organism = 'none'\n        else: \n            URL = 'https://www.uniprot.org/uniprot/' + uniprot \n            page = requests.get(URL)\n            soup = BeautifulSoup(page.content, 'html.parser')\n            result = soup.find_all('em')\n            organism = str(result)\n            organism = organism[5:]\n            organism = organism[:-6]\n            \n        return organism\n    \n    def getName(self, pdbid):\n        URL = 'https://www.rcsb.org/structure/' + pdbid\n        page = requests.get(URL)\n        soup = BeautifulSoup(page.content, 'html.parser')\n        result = soup.find_all(\"span\", id= \"structureTitle\")  \n        new_string = str(result)\n        index = new_string.find('>')\n        new_string = new_string[index+1:]\n        index = new_string.find('<')\n        name = new_string[:index]\n\n        return name\n    \n    def getLigand(self, pdbid):\n        URL = 'https://www.rcsb.org/structure/' + pdbid\n        page = requests.get(URL)\n        soup = BeautifulSoup(page.content, 'html.parser')\n        results = soup.find_all(\"a\") \n        ligands = ''\n        for result in results: \n            if 'Query' in result.text:\n                string = result.text\n                index = string.find('on')\n                new_string = string[index+2:]\n                ligands = ligands + new_string + ','\n        ligands = ligands[:-1]\n\n        return ligands \n\n\n                \n    def getTaxonomy(self, uniprot):\n        if uniprot == 'none': \n            taxonomyList = 'none'\n        else:\n            taxonomyList = []\n            hiddenTaxonlist = []\n            URL = 'https://www.uniprot.org/uniprot/' + uniprot\n            page = requests.get(URL)\n            soup = BeautifulSoup(page.content, 'html.parser')\n            results = soup.find(id = 'names_and_taxonomy')\n            Newresults= results.find_all('span')\n            for result in Newresults:\n                if 'hiddenTaxon' in str(result):\n                    resultFound = False\n                    final = result.text\n                    while resultFound == False:\n                        if final[-1].isalpha():\n                            final = final\n                            resultFound = True\n                        else:\n                            final = final[:-1] \n                    hiddenTaxonlist.append(final)\n                    \n            Newerresults= results.find_all('a') \n            for result in Newerresults:\n                if 'taxonomy' in str(result):\n                    taxonomyList.append(result.text)\n            taxonomyList = taxonomyList[2:]\n            for item in taxonomyList:\n                if item in hiddenTaxonlist:\n                    taxonomyList.remove(item)\n                    \n        return taxonomyList\n    \n    def getPubmed(self, pdbid):\n        try:\n            URL = 'https://www.rcsb.org/structure/' + pdbid\n            page = requests.get(URL)\n            soup = BeautifulSoup(page.content, 'html.parser')\n            results = soup.find(id = 'pubmedLinks')\n            Newresults= results.find_all('a')\n            for result in Newresults:\n                if 'querySearchLink' in str(result):\n                    pubmedID = result.text       \n                    PubmedLink = 'https://pubmed.ncbi.nlm.nih.gov/' + pubmedID +'/'\n        except AttributeError:\n            PubmedLink = ''\n        \n        return PubmedLink\n    \n    def getSequence(self, uniprot):\n        if uniprot == 'none': \n            seq = 'none'\n        else:\n            URL = 'https://www.uniprot.org/uniprot/' + uniprot\n            page = requests.get(URL)\n            soup = BeautifulSoup(page.content, 'html.parser')\n            results = soup.find(id = 'entrySequence')\n            Newresults= results.find_all('pre')\n            for result in Newresults:\n                seq = result.text \n                \n        return seq\n    \n    def getLength(self, pdbid):\n        URL = 'https://www.rcsb.org/structure/' + pdbid\n        page = requests.get(URL)\n        soup = BeautifulSoup(page.content, 'html.parser')\n        results = soup.find(id=\"MacromoleculeTable\")\n        Newresults= results.find_all('td')\n        for result in Newresults:\n            if result.text.isdigit():\n                length = result.text\n            \n        return length \n    \n    def webscrapeData(self, pdbid):\n        featureslist = ['PDB', 'Uniprot', 'Organism', 'Name', 'Ligands', 'Global Stoichiometry', 'Sequence','Sequence Length', 'PubMed link', 'Taxonomy'] \n        pdb = pdbid\n        uni = self.pdbidToUniprot(pdbid)\n        org = self.getOrganism(uni)\n        n = self.getName(pdbid)\n        l = self.getLigand(pdbid)\n        g = self.get_global_stoich(pdbid)\n        s = self.getSequence(uni)\n        sl = self.getLength(pdbid)\n        p = self.getPubmed(pdbid)\n        t = self.getTaxonomy(uni)\n        \n        df1 = pd.DataFrame([[pdb,uni,org,n,l,g,s,sl,p,t]], columns = featureslist )\n        \n        return df1\n            \n        \n    def getHomologData(self, genename, targetprotein, pdbidlist):\n        df = pd.DataFrame()\n        targetdf = self.webscrapeData(targetprotein)\n        df = df.append(targetdf)\n        \n        for pdbid in pdbidlist: \n            df1 = self.webscrapeData(pdbid)\n    \n            df = df.append(df1)\n            \n        filename = genename +'data.xlsx'    \n        df.to_excel(filename, index=False)    \n        return df \n    \n####### FATCAT METHODS #######\n        \n    def importGlob(self, pdb1, pdb2):\n        \n        # Running FATCAT alignment, saving .cif file in output directory  \n        os.system(\"bash runFATCAT.sh -pdb1 \" + pdb1 + \" -pdb2 \" + pdb2 + \" -pdbFilePath \" + pdbDirectory + \" -autoFetch -flexible -outputPDB -outFile \" + outputDirectory + \"pdbtemporary.pdb\")\n                                                                                           \n                                \n        proteinz = outputDirectory + 'pdbtemporary.pdb'\n        \n        cmd.load(proteinz, \"proteinz\")\n        cmd.split_states(\"proteinz\")\n        cmd.delete(\"proteinz\")\n        cmd.set_name(\"proteinz_0001\", \"target\")\n        cmd.set_name(\"proteinz_0002\", \"homolog\")\n        cmd.color(\"red\", \"target\")\n        cmd.save(str(outputDirectory) + str(pdb1)+ \"_\" + str(pdb2) + \".cif\")\n        \n        temppdb = outputDirectory + 'pdbtemporary.pdb'\n        os.remove(temppdb)\n        print(\"deleted temporary pdb\")\n        print(\"done\")\n\n    def shortFatcat(self, pdb1, pdb2):\n\n        os.system(\"bash runFATCAT.sh -pdb1 \" + pdb1+ \" -pdb2 \" + pdb2 + \" -pdbFilePath \" + pdbDirectory + \" -autoFetch -flexible -printFatCat -show3d\")\n    \n        \n    def specificSingleChainAlignment(self, pdb1, pdb2, chain1, chain2):\n        \n        # Running FATCAT alignment, saving .cif file in output directory  \n        os.system(\"bash runFATCAT.sh -pdb1 \" + pdb1 + \".\" + chain1 + \" -pdb2 \" + pdb2 + \".\" + chain2 + \" -pdbFilePath \" + pdbDirectory + \" -autoFetch -flexible -outputPDB -outFile \" + outputDirectory + \"pdbtemporary.pdb\")\n      \n                                                                                                                \n        proteinz = outputDirectory + 'pdbtemporary.pdb'\n        \n        cmd.load(proteinz, \"proteinz\")\n        cmd.split_states(\"proteinz\")\n        cmd.delete(\"proteinz\")\n        cmd.set_name(\"proteinz_0001\", \"target\")\n        cmd.set_name(\"proteinz_0002\", \"homolog\")\n        cmd.color(\"red\", \"target\")\n        cmd.save(outputDirectory + pdb1 + \".\" + chain1 + \"_\" + pdb2 + \".\" + chain2 + \".cif\")\n\n        temppdb = outputDirectory + 'pdbtemporary.pdb'\n        os.remove(temppdb)\n        print(\"deleted temporary pdb\")\n        print(\"done\")\n        \n    \n    def ChainByChain(self, pdb1, pdb2):\n        cmd.fetch(pdb1)\n        for ch1 in cmd.get_chains(pdb1):\n            cmd.delete(pdb1)\n            cmd.fetch(pdb2)\n            for ch2 in cmd.get_chains(pdb2):\n                cmd.delete(pdb2)\n                self.specificSingleChainAlignment(pdb1,pdb2,ch1, ch2)\n                \n    def fatcat_savexml(self, pdb1, pdb2):  \n        \n        os.system(\"bash runFATCAT.sh -pdb1 \" + pdb1 + \" -pdb2 \" + pdb2 + \" -pdbFilePath \" + pdbDirectory + \" -autoFetch -flexible -printFatCat -printXML \" + outputDirectory + pdb1 + \"_\" + pdb2 + \".xml\")\n        print(\"done\")\n    ,\n    \n\n", "582": "# import csv, pandas,        sqlalchemy, os\nimport os\nimport scrape_utils\nimport db_utils\nimport requests\nimport re\nimport pandas as pd\nimport db_logging as lg\nimport datetime as dt\nfrom bs4 import BeautifulSoup\nfrom pprint import pprint\n\nclass getUrl:\n\turl =None\n\tpayload =None\n\t\n\tdef __init__(self,url,payload=None):\n\t\tself.url=url\n\t\tself.payload=payload\n\n\n\ndb = db_utils.Connection(dbschema='compliance',database='compliance', dbtype='POSTGRES')\nlogger=lg.logger.ImportLogger(db)\nsession = requests.session()\nfile_list=[]\nfile_url=[]\nfile_tup_list=[]\n\n# #login=getUrl(\"https://freddiemac.embs.com/FLoan/secure/login.php\")\n# print (\"hello\")\n\nRAWFILEPATH='/home/dtdata/source_data/freddie_mac/'\nusername='hung135@hotmail.com'\n\n\nform_post_login={'username':username,'password':'Ey_@82LH'}\nform_post_disclaimer={'accept':'Yes','action':'acceptTandC','acceptSubmit':'Continue'}\n\nlogin_post=getUrl(\"https://freddiemac.embs.com/FLoan/secure/auth.php\",form_post_login)\naccept_disclaimer=getUrl(\"https://freddiemac.embs.com/FLoan/Data/download.php\",form_post_disclaimer)\n\n\nscrape_utils.getUrl(login_post,session)\nraw=scrape_utils.getUrl(accept_disclaimer,session)\n\n\nsoup=scrape_utils.soupHtml(raw)\na=soup.find_all('a')\nfor x in a:\n\t#pprint((x.contents[0]))\n\tname=x.contents[0]\n\tif(re.match(re.compile('historical'),name)):\n\t\turl=\"https://freddiemac.embs.com/FLoan/Data/\"+x['href']\n\t\tprint(name)\n\t\ttup=(name,url)\n\t\tfile_list.append(name)\n\t\tfile_url.append(url)\n\t\tfile_tup_list.append(tup)\n\t\t#scrape_utils.getUrlFile(session,url,name,download=False)\n\n\t\t\n#pprint (dir(a))\npprint(a)\n\n\n\n\n\ntrg_db = db_utils.Connection(host='wdcdwl01', database='fred_lpd', dbschema='fred_lpd')\n \n\n\n\ndelta_query = \"select distinct file_name from fred_lpd.meta_source_files where file_name like '%.zip'\"\n\ntrg_df = pd.read_sql(delta_query, trg_db._conn)\n\nsrc_df =pd.DataFrame({'file_name':file_list})\n\n\n\n\nresult=pd.merge(src_df,trg_df,on='file_name',how='left',indicator=True)\n# dict = {\"src\": src_df, \"trg\": trg_df}\nx=result.query(\"_merge =='left_only'\")\n# result = pd.concat(dict)\n# result = result.drop_duplicates('file_name', keep=False)\n# delta_list = result.file_name.tolist()\n\n#print result.drop_duplicates('file_name', keep=False)\n\nnew_files=x.file_name.tolist()\nfor file in new_files:\n\tfor tup_file in file_tup_list:\n\t\tif file==tup_file[0]:\n\t\t\tx=None\n\t\t\tx=scrape_utils.getUrlFile(session,url,RAWFILEPATH+file,download=False)\n\t\t\tif x is None:\n\t\t\t\tlogger.insert_LoadStatus(table_name=\"None\",program_unit=\"WebScrape\",\n\t\t\t\tprogram_unit_type_code=\"ZipFile\",file_path=RAWFILEPATH+file,username=username)\n\t\t\telse:\n\t\t\t\tlogger.insert_ErrorLog(error_code=x[:5],error_message=url,program_unit=\"WebScrape\",user_name=username)\n\n\n\n\t\t\t#scrape_utils.getUrlFile(session,url,name,download=False)\n\t\n#logger.print_records(100)", "583": "\"\"\"\nWritten by: Christopher Hutchings | M00571702\nLast edit: 25/02/21\nModule: CST4550\n---\nDatabase handler for storing Vulnerabilities. Hook up to a webservice or webscrape from vulnerability sites.\n\"\"\"\n\nimport mysql.connector\nfrom mysql.connector import Error\nimport time\n\ndef create_connection():\n\tmydb = mysql.connector.connect(\n\t\thost=\"localhost\",\n\t\tdatabase=\"PenTestingProject\",\n\t\tuser=\"root\",\n\t\tpassword=\"123\"\n\t)\n\tconnector = mydb.cursor()\n\tgetExploits = 'SELECT * FROM Vulnerabilities'\n\tconnector.execute(getExploits)\n\treturn connector.fetchall()\n\n\n\n\n\ndef init():\n\t#create_connection()\n\tprint('Doop')\n\ttime.sleep(.1)", "584": "from selenium import webdriver\r\nimport time\r\nfrom selenium.webdriver.common.keys import Keys\r\n\r\npath = 'D:\\drive\\Colab Notebooks\\webScrape\\selenium\\chromedriver.exe'\r\ndriver = webdriver.Chrome(path)\r\nurl = 'http://orteil.dashnet.org/experiments/cookie/'\r\ndriver.get(url=url)\r\n\r\ncookie = driver.find_element_by_id(\"cookie\")\r\nstate = True\r\nwhile state:\r\n    cookie.click()\r\n\r\ntime.sleep(10)\r\ndriver.close()\r\ndriver.quit()\r\n", "585": "# Define here the models for your spider middleware\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\n\n# useful for handling different item types with a single interface\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass WebscrapeSpiderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(self, response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, or item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(self, response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Request or item objects.\n        pass\n\n    def process_start_requests(self, start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n\n\nclass WebscrapeDownloaderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_request(self, request, spider):\n        # Called for each request that goes through the downloader\n        # middleware.\n\n        # Must either:\n        # - return None: continue processing this request\n        # - or return a Response object\n        # - or return a Request object\n        # - or raise IgnoreRequest: process_exception() methods of\n        #   installed downloader middleware will be called\n        return None\n\n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n", "586": "# Define here the models for your scraped items\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass WebscrapeItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    pass\n", "587": "import requests\nfrom bs4 import BeautifulSoup\nimport time\nfrom time import sleep\nimport pandas as pd\nfrom tabulate import tabulate\nfrom multiprocessing import Pool\nfrom multiprocessing import cpu_count\nimport concurrent.futures\nfrom functools import lru_cache \nimport pickle\nimport os\nimport datetime\nfrom datetime import date\n\n\nsite_1 = input(\"Please enter the url of the site:\")\n\nt0=time.time()\n\ncontents_list = []\n\nsitemap_url = []\n\nkey_list= [\"Website Name\", \"H1 TAG\", \"H2 TAG\", \"TITLE\", \"IMG-ALT\"]\n\ndicts_list = {}\n\nMAX_THREADS = 10\n\nfor i in key_list:\n\tdicts_list[i] = []\n#getting all the tags and storing it in a list called contents_list\n\ndef webscrape(t):\n\t#print(t)\n\t#for site_url in sitemap_url:\n\t#print(site_url)\n\trequest_1 = requests.get(t)\n\tsrc_1 = request_1.content\n\tsoup_1  = BeautifulSoup(src_1, 'html.parser')\n\th1_list = []\n\th2_list = []\n\ttitle_list = []\n\timg_list = []\n\tfor h1 in soup_1.find_all(\"h1\"):\n\t\tif None in h1:\n\t\t\th1_list.append(None)\n\t\telse: \n\t\t\th1_list.append(h1.text)\n\tfor h2 in soup_1.find_all(\"h2\"):\n\t\tif None in h2:\n\t\t\th2_list.append(None)\n\t\telse:\n\t\t\th2_list.append(h2.text)\n\tfor title in soup_1.find_all(\"title\"):\n\t\tif None in title:\n\t\t\ttitle_list.append(None)\n\t\telse:\n\t\t\ttitle_list.append(title.text)\n\tfor img1 in soup_1.find_all(\"img\"):\n\t\tif img1.has_attr('alt'):\n\t\t\timg_list.append(img1['alt'])\n\tdicts_list[\"Website Name\"].append(t) \n\tdicts_list[\"H1 TAG\"].append(h1_list) \n\tdicts_list[\"H2 TAG\"].append(h2_list) \n\tdicts_list[\"TITLE\"].append(title_list) \n\tdicts_list[\"IMG-ALT\"].append(img_list)\n\tprint(\"done\")\n\n#caching function to check if the website has been scarped before\ndef cache_to_disk(func):\n    def wrapper(*args):\n        cache = '.{}{}.pkl'.format(func.__name__, args).replace('/', '_')\n        try:\n            with open(cache, 'rb') as f:\n            \ttime_created2 = os.stat(cache).st_mtime\n            \tdate_time = datetime.datetime.fromtimestamp(time_created2)\n            \tnow = datetime.datetime.today()\n            \tdelta  = now - date_time\n            \tprint(\"modified date \"+ str(date_time))\n            \tprint(\"today's date: \"+ str(now))\n            \tprint(delta.days)\n            \tdays = delta.days \n            \tif days>0:\n                \tprint(\"A\")\n                \tresult = func(*args)\n                \twith open(cache, 'wb') as g:\n                \t\tpickle.dump(result, g)\n                \treturn result\n\n            \tprint(\"B\")\n            \tt3 = time.time()\n            \tprint(args)\n            \tprint(\"This site has been scraped before\")\n            \ttotal_time = t3-t0\n            \tprint(\"The total time for scraping\" + \" \" + str(total_time))\n            \treturn pickle.load(f)     \t\t \t\t\n        except IOError:\n            result = func(*args)\n            with open(cache, 'wb') as f:\n                pickle.dump(result, f)\n            return result\n\n    return wrapper\n\n#Funtion to recursively get all the links of the sitemap\ndef get_links(url):\n\tresponse = requests.get(url)\n\tsrc_2 = response.content\n\tsoup_get_links = BeautifulSoup(src_2, 'html.parser')\n\tlocal_links = []\n\n\tfor links in soup_get_links.find_all('loc'):\n\t\tlink_url = links.text\n\n\t\tif link_url is  not  None: \n\t\t\tif link_url not in sitemap_url:\n\t\t\t\tsitemap_url.append(link_url) \n\t\t\t\tget_links(link_url)\n\t\t\t\tprint(link_url)\n\n\n@cache_to_disk\ndef multipro(site_string):\n\tsite_string = str(site_1)\n\tget_links(site_string)\n\n\tt2 = time.time()\n\tthreads = min(MAX_THREADS, len(sitemap_url))\n\n\twith concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n\t\texecutor.map(webscrape, sitemap_url)\n\n\n\tt1=time.time()\n\ttotal_time=t1-t2\n\ttotal_time2=t1-t0\n\tprint(site_string)\n\tprint(\"The total time for web scraping is\" + \" \" + str(total_time))\n\tprint(\"The total time with recursion is\" + \" \" + str(total_time2))\n\treturn dicts_list\n\ndef main():\n\tsite_string = str(site_1)\n\n\ttemp_dict_list = multipro(site_string)\n\n\tdf = pd.DataFrame(temp_dict_list)\n\tdf.to_html(\"data.html\")\n\n\t#webscrape()\n\nmain()\n", "588": "import unittest\r\nimport requests\r\n\r\nfrom webscrape import search_url\r\n\r\n\r\nclass ApiTest(unittest.TestCase):\r\n    api_url = \"http://127.10.0.0:9000/\"\r\n\r\n    def test_get_all_reviews(self):\r\n        r = requests.get(search_url)\r\n        self.assertEqual(r.status_code, 200)\r\n", "589": "import requests\r\nfrom bs4 import BeautifulSoup as bs \r\n\r\n\r\n##exchange_name = input('Input Github User: ') for searching stocks, still not responsive\r\n#attempting simple scrape this time with span class identified\r\nurl = 'https://www.tradingview.com/'\r\nr = requests.get(url)\r\nsoup = bs(r.content, 'html.parser')\r\nindex_name = soup.find(\"span\", attrs = {\"class\":\"tv-widget-watch-list__description tv-widget-watch-list__description--uppercase apply-overflow-tooltip\"})\r\nfor span in index_name:\r\n    print(span.text)\r\nprint(url)\r\n\r\n\r\n\r\n#class=\"tv-widget-watch-list__description tv-widget-watch-list__description--uppercase apply-overflow-tooltip\"\r\n\r\n\r\n\r\n#This code works due to span having a class unlike webscrape.py", "590": "from inspect import formatannotationrelativeto\nfrom django.http import HttpResponse\nfrom django.shortcuts import render\nfrom selenium import webdriver\nfrom django.views.decorators.csrf import csrf_exempt\nfrom .webscrape import flipkartscrape,amazonscrape\nfrom .dbhandler import readIntoDatabase, writeIntoDatabase\nfrom selenium.webdriver.chrome.options import Options\n\ndef index(request):\n    return render(request,'index.html')\n\n@csrf_exempt \ndef webscrape(request):\n\n    \n    if request.method == 'POST':\n        amz_link = request.POST.get('amazon')\n        flip_link = request.POST.get('flipkart')\n        c = webdriver.ChromeOptions()\n        # c.add_argument(\"--headless\")\n        c.add_argument(\"--ignore-certificate-error\")\n        c.add_argument(\"--ignore-ssl-errors\")\n        c.add_experimental_option('excludeSwitches', ['enable-logging'])\n        driver = webdriver.Chrome(chrome_options = c, executable_path= \"C:/drivers/chromedriver.exe\")\n    \n\n\n        amz_prodcut = amazonscrape(amz_link, driver)\n        flip_prodcut = flipkartscrape(flip_link, driver)\n        writeIntoDatabase(amz_prodcut ,flip_prodcut )\n        dict={}\n        count =0\n        for row in readIntoDatabase():\n            dict[count] = row\n            count +=1\n        \n        return render(request, 'dashboard.html' , {\"contex\": dict })\n    # return HttpResponse(\"Your product is added for tracking\")\ndef dashboard(request):\n    dict={}\n    counter =0\n    for row in readIntoDatabase():\n        dict[counter]=row\n        counter+=1\n     \n    return render(request, 'dashboard.html',{\"contex\": dict})\n       \n", "591": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Aug  3 16:40:08 2021\n\n@author: carlykelly\n\"\"\"\n\nimport pandas as pd\nimport requests\nimport unittest\nimport difflib\n\nclass Project1():\n    \n    def __init__(self, mylink, table_num=1, data='cotest', r=None):\n        self.mylink = mylink\n        self.table_num = table_num\n        self.data = data\n        if r == None:\n            self.r = ''\n        else:\n            self.r = r\n\n    def webscrape(self):\n        \n        #This code was taken from tyler_getWebData.py\n        # Save URL of interest\n        url = self.mylink\n        \n\n        self.r = requests.get(url)  # Obtain handle to URL\n        \n        df_list = pd.read_html(self.r.text)  # this parses all the tables in webpage\n        df = df_list[self.table_num] \n\n        # Below line creates new column names based on flattened MultiIndex\n        # Ex: \"Fossil CO2 ... \" / \"1990\" becomes \"Fossil_CO2_..._1990\"\n        df.columns = [\"_\".join(a) for a in df.columns.to_flat_index()]\n        \n        # Save DF to CSV\n        df.to_csv(f'{self.data}_data.csv')\n        return self.r\n    def merge(self, csv1, csv2, title1='csv1', title2='csv2'):\n        \n        #This code is taken from merge_data.py\n        csv1 = pd.read_csv(f'{csv1}')\n        csv2 = pd.read_csv(f'{csv2}')\n    \n    \n        co2_comp_renew = csv1['Country'].isin(csv2['Country'])\n        dif_df = csv1[~co2_comp_renew]\n    \n        database = csv2['Country']\n        similar = [difflib.get_close_matches(word, database, n = 1, cutoff = 0.8) for word in dif_df['Country']]\n    \n        dif_df['Country'] = similar\n        dif_df = dif_df[dif_df['Country'].astype(str) != '[]']\n        dif_df['Country'] = [country[0] for country in dif_df['Country']]\n        csv1.loc[dif_df.index, 'Country'] = dif_df['Country']\n    \n    \n        #SECOND PASS\n        co2_comp_renew = csv1['Country'].isin(csv2['Country'])\n        dif_df = csv1[~co2_comp_renew]\n    \n    \n    \n        translation = {\n            'Czech Republic':'Czechia',\n            'East Timor':'Timor Leste',\n            'Eswatini':'Eswatini (Swaziland)',\n            'North Korea':'Korea DPR',\n            'S\u00e3o Tom\u00e9 and Pr\u00edncipe':'Sao Tome & Principe',\n            'South Korea':'Korea Rep',\n            'Sudan South Sudan':'South Sudan',\n            'The Gambia': 'Gambia'\n            }\n    \n    \n        for original, translate in translation.items():\n            dif_df['Country'] = dif_df['Country'].str.replace(original, translate)\n        csv1.loc[dif_df.index, 'Country'] = dif_df['Country']\n        \n        #THIRD PASS\n        co2_comp_renew = csv1['Country'].isin(csv2['Country'])\n        dif_df = csv1[~co2_comp_renew]\n        \n        merged = pd.merge(csv1, csv2, on = \"Country\", how = \"inner\")\n        merged.to_csv(f'{title1}_{title2}_data.csv')\n\nclass webscrapetestsuite(unittest.TestCase): \n    \n    def test_link_input(self):\n        # test that the requests work\n        test1 = Project1('https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions')\n        test1.webscrape()\n        \n        # Testing that we access the website without an error\n        expected = 200\n        #Checking that the status code is 200 (reached without error)\n        self.assertEqual(test1.r.status_code, expected)\n        \n    def test_merge(self):\n        # test that the merge method work\n        #Creating a class from which to run the merge method.\n        #Need to put in the website as class is designed to first scrape, then merge\n        test2 = Project1('https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions')\n        test2.merge('../Carly_only_Project1/co2_data.csv', '../Carly_only_Project1/re_data.csv')\n        \n        # Testing that we have the expected number of rows in our output\n        expected = 196\n        #The program is designed that the merged csv should be saved in the same file\n        #this code so relative path is just the file name\n        file_path = \"csv1_csv2_data.csv\"\n        #Reading in csv from the path name above\n        df_to_test = pd.read_csv(file_path)\n        # Comparing the number of rows in the csv output to 196\n        self.assertEqual(len(df_to_test['Country']), expected)\n        \nif __name__ == '__main__':\n    unittest.main()", "592": "# Define here the models for your spider middleware\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\n\n# useful for handling different item types with a single interface\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass WebscrapeSpiderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(self, response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, or item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(self, response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Request or item objects.\n        pass\n\n    def process_start_requests(self, start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn\u00e2\u20ac\u2122t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n\n\nclass WebscrapeDownloaderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_request(self, request, spider):\n        # Called for each request that goes through the downloader\n        # middleware.\n\n        # Must either:\n        # - return None: continue processing this request\n        # - or return a Response object\n        # - or return a Request object\n        # - or raise IgnoreRequest: process_exception() methods of\n        #   installed downloader middleware will be called\n        return None\n\n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n", "593": "import os\nimport requests\nimport urllib.parse\n#import pandas as pd\n#import numpy as np\n\nfrom cs50 import SQL\nfrom functools import wraps\nfrom bs4 import BeautifulSoup\n\n# Configure CS50 Library to use SQLite database\ndb = SQL(\"sqlite:///webscrape.db\")\n\n\n\"\"\" 1. Web Scraping \"\"\"\ndef bswebscrape(html_site):\n    # Beautiful soup to webscrape\n    web_input = requests.get(html_site)\n\n    if web_input.status_code > 399:\n        return 1\n\n    soup = BeautifulSoup(web_input.text, 'lxml')\n    contents = soup.find_all('p')\n\n    # Store data in a new list\n    scraped_data = []\n    for content in contents:\n        scraped_data.append(content.text)\n\n    return scraped_data\n\n\n\"\"\" 2. Data Cleaning \"\"\"\ndef data_clean(scraped_data):\n\n# Declare dictionary to keep track of words - dict is the fastest\n    unique_wordlist = {}\n    word_count = 0\n\n    # Data is return in paragraphs\n    for paragraphs in scraped_data:\n\n        # Split into lists\n        paragraphs = paragraphs.split(\" \")\n\n        # Split\n        for x in range(len(paragraphs)):\n\n            # Remove punctuation\n            paragraphs[x] = paragraphs[x].replace(\",\", \"\")\n            paragraphs[x] = paragraphs[x].replace(\".\", \"\")\n            paragraphs[x] = paragraphs[x].replace(\"!\", \"\")\n            paragraphs[x] = paragraphs[x].replace(\"?\", \"\")\n            paragraphs[x] = paragraphs[x].replace(\"\u201c\", \"\")\n            paragraphs[x] = paragraphs[x].replace(\"\u201d\", \"\")\n            paragraphs[x] = paragraphs[x].replace(\"(\", \"\")\n            paragraphs[x] = paragraphs[x].replace(\")\", \"\")\n\n        for words in paragraphs:\n\n            # Insert new row\n            if words not in unique_wordlist:\n                unique_wordlist.update({words: 1})\n                word_count += 1\n\n            # Update row count\n            else:\n                int_increment = int(unique_wordlist[words]) + 1\n                unique_wordlist.update({words : int_increment})\n                word_count += 1\n\n    return {\"WordCount\" : word_count, \"Data\" : unique_wordlist}\n\n\n\"\"\" 4. Update SQL Database \"\"\"\ndef webscrapedb_update(unique_wordlist):\n\n    # Update SQL database\n    counter = 0\n    for word in unique_wordlist:\n        counter += 1\n        sqlcmd = db.execute(\"INSERT INTO word_list VALUES (?, ?, ?)\", counter, str(word), int(unique_wordlist[word]))\n    return 0\n\n\n\"\"\" 5. Delete All Rows in SQL Database \"\"\"\ndef webscrapedb_delete():\n    sqlcmd = db.execute(\"DELETE FROM word_list\")\n    return 0", "594": "# Import Libraries\nfrom flask import Flask, jsonify, render_template_string\nfrom scrape_dev import *\nfrom production.render import docs_html, get_grades\nfrom production.db import *\nimport sys\n\n# Configure as a flask server\napp = Flask(__name__)\n\n# Root endpoints only return README from github repo\n@app.route('/')\n@app.route('/v1/')\ndef render_docs():\n    return render_template_string(docs_html())\n\n\n# GET /v1///\n\n\n@app.route('/v1///', methods=['GET'])\n# Returns only the course requested\ndef single_course(term, course, section):\n    # Formatting\n    term = term.lower()\n    course = course.lower()\n    url = f\"{course}.{section}.{term}\"\n\n    # Scrape coursebook\n    class_info = webscrape_single_section(url)\n\n    # Send response\n    return jsonify({'data': class_info})\n\n# GET /v1/\n@app.route('/v1//', methods=['GET'])\n# Returns class data for all the sections in the current semester\ndef all_courses(course):\n    course = course.lower()\n    course_list = webscrape_all_sections(course)\n    return jsonify({\"data\": course_list})\n\n\n@app.route('/v1/grades///', methods=['GET'])\ndef single_course_grade(term, course, section):\n    grade_data = get_single_course_grade(term, course, section)\n    return jsonify({\"data\": grade_data})\n\n\n@app.route('/v1/grades//', methods=['GET'])\ndef all_course_grades(term, course):\n    grade_data = get_all_course_grades(term, course)\n    return jsonify({\"data\": grade_data})\n\n\n@app.route('/v1/prof/', methods=['GET'])\ndef get_prof_data(name):\n    prof_data = fetch_prof(name)\n    return jsonify({\"data\": prof_data})\n\n\n# Serve the server\nif __name__ == '__main__':\n    app.run(threaded=True)\n", "595": "# Websocket\nfrom . import wsock\n\n# PIL\nfrom . import image\n\n# Indexes\nfrom . import index\n\n# Search\nfrom . import search\n\n# Misc.\nfrom . import parallel\nfrom . import webclone\nfrom . import webscrape\nfrom . import webclone\n", "596": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Tue Apr 26 11:14:41 2022\r\n\r\n@author: DKu_7\r\n\"\"\"\r\n\r\n# Football Tables dashboard (EPL, LaLiga, Bundesliga)\r\n# Data webscraped from the official websites\r\n\r\nfrom bs4 import BeautifulSoup\r\nimport requests\r\nimport pandas as pd\r\nimport streamlit as st\r\n\r\n### Webscraping Section To Obtain Data\r\n\r\n## La Liga Table Webscraping\r\n# Reference my own webscrape work:\r\n# https://github.com/dk81/web_scrape_python/blob/main/laliga_spanishsoccer_webscrape.ipynb\r\n\r\ndef get_laliga_df():\r\n    laliga_url = \"https://www.laliga.com/en-GB/laliga-santander/standing\"\r\n\r\n    la_liga_response = requests.get(laliga_url)\r\n\r\n    laliga_soup = BeautifulSoup(la_liga_response.content, 'html.parser')\r\n\r\n    results = laliga_soup.find('div', \r\n                              {'class': 'styled__StandingTableBody-e89col-5 cDiDQb'}).find_all('div', \r\n                              {'class': 'styled__ContainerAccordion-e89col-11 HquGF'})\r\n                                                             \r\n    # Scraping La Liga Table with list comprehension\r\n\r\n    laliga_teams = [result.find('div', {'class': 'styled__ShieldContainer-lo8ov8-0 bkblFd shield-desktop'}).find('p', \r\n                                {'class': 'styled__TextRegularStyled-sc-1raci4c-0 glrfl'}).get_text() \r\n                    for result in results]\r\n\r\n    laliga_points = [result.find_all('p', \r\n                     {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[0].get_text()\r\n                     for result in results]\r\n\r\n\r\n    laliga_played = [result.find_all('p', \r\n     {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[1].get_text()\r\n     for result in results]\r\n    \r\n    laliga_wins = [result.find_all('p', \r\n     {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[2].get_text()\r\n     for result in results]\r\n    \r\n    laliga_draws = [result.find_all('p', \r\n     {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[3].get_text()\r\n     for result in results]\r\n    \r\n    laliga_losses = [result.find_all('p', \r\n     {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[4].get_text()\r\n     for result in results]\r\n    \r\n    laliga_goals_for = [result.find_all('p', \r\n     {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[5].get_text()\r\n     for result in results]\r\n    \r\n    laliga_goals_against = [result.find_all('p', \r\n     {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[6].get_text()\r\n     for result in results]\r\n    \r\n    laliga_goals_diff = [result.find_all('p', \r\n     {'class': 'styled__TextRegularStyled-sc-1raci4c-0 cIcTog'})[7].get_text()\r\n     for result in results]             \r\n    \r\n    ## Make pandas Dataframe of La Liga table:\r\n    \r\n    laliga_df = pd.DataFrame({'Rank': range(1, len(laliga_teams) + 1), 'Team': laliga_teams, \r\n                  'Pts': laliga_points, 'Pl': laliga_played, \r\n                  'W': laliga_wins, 'D': laliga_draws, \r\n                  'L': laliga_losses, 'GF': laliga_goals_for, \r\n                  'GA': laliga_goals_against, 'GD': laliga_goals_diff})  \r\n    # Return dataframe for Laliga      \r\n    return(laliga_df)\r\n\r\n## ------------\r\n## Premier League Table Webscraping\r\n# Reference own work: https://github.com/dk81/web_scrape_python/blob/main/epl_table_webscrape.ipynb\r\n\r\ndef get_epl_df():\r\n    epl_link = \"https://www.premierleague.com/tables\"\r\n    \r\n    response = requests.get(epl_link)\r\n    \r\n    epl_soup = BeautifulSoup(response.content, 'html.parser')\r\n    \r\n    # Table rows are in the tr tags. Each table row is for each EPL team\r\n    epl_table_rows = epl_soup.find_all('tr')\r\n    \r\n    # Rank, Teams, Number of Games Played:\r\n    epl_rank = [row.find_all('span', {'class': 'value'})[0].get_text() for row in epl_table_rows[1:40:2]]\r\n    epl_teams = [row.find_all('span', {'class': 'long'})[0].get_text() for row in epl_table_rows[1:40:2]]\r\n    epl_played = [row.find_all('td')[3].get_text() for row in epl_table_rows[1:40:2]]\r\n    \r\n    # Wins, Draws, Losses\r\n    epl_wins = [row.find_all('td')[4].get_text() for row in epl_table_rows[1:40:2]]\r\n    epl_draws = [row.find_all('td')[5].get_text() for row in epl_table_rows[1:40:2]]\r\n    epl_losses = [row.find_all('td')[6].get_text() for row in epl_table_rows[1:40:2]]\r\n    \r\n    # Goals For, Goals Against, Goal Diff & Points:\r\n    epl_goals_for = [row.find_all('td')[7].get_text() for row in epl_table_rows[1:40:2]]\r\n    epl_goals_against = [row.find_all('td')[8].get_text() for row in epl_table_rows[1:40:2]]\r\n    epl_goal_diff = [row.find_all('td')[9].get_text().strip() for row in epl_table_rows[1:40:2]]\r\n    epl_points = [row.find_all('td')[10].get_text().strip() for row in epl_table_rows[1:40:2]]\r\n    \r\n    # Make EPL Table:\r\n    epl_df = pd.DataFrame({\r\n             'Rank': epl_rank,\r\n             'Team': epl_teams,\r\n             'Pl': epl_played,\r\n             'W': epl_wins,\r\n             'D': epl_draws,\r\n             'L': epl_losses,\r\n             'GF': epl_goals_for,\r\n             'GA': epl_goals_against,\r\n             'GD': epl_goal_diff,\r\n             'Pts': epl_points\r\n    })\r\n    #Return EPL dataframe:\r\n    return(epl_df)\r\n\r\n## ------------\r\n## Bundesliga Table Webscraping\r\n\r\ndef get_bundesliga_df():\r\n    bundesliga_url = \"https://www.bundesliga.com/en/bundesliga/table\"\r\n    \r\n    response = requests.get(bundesliga_url)\r\n    \r\n    bundes_soup = BeautifulSoup(response.content, 'html.parser')\r\n    \r\n    bundes_results = bundes_soup.find('table', {'class': 'table'}).find_all('tr')\r\n    \r\n    # Remove first row (header):\r\n    bundes_results = bundes_results[1:]\r\n    \r\n    # Webscrape parts of the table I use list comprhension instead of for loop append method:\r\n    \r\n    bundes_teams = [result.find('td', {'class': 'team'}).find('span', {'class': 'd-none d-lg-inline'}).get_text() \r\n                    for result in bundes_results]\r\n        \r\n    bundes_matches = [result.find('td', {'class': 'matches'}).get_text() for result in bundes_results]\r\n    \r\n    bundes_points = [result.find('td', {'class': 'pts'}).get_text() for result in bundes_results]\r\n    \r\n    bundes_wins = [result.find('td', {'class': 'd-none d-lg-table-cell wins'}).get_text() for result in bundes_results]\r\n    \r\n    bundes_draws = [result.find('td', {'class': 'd-none d-lg-table-cell draws'}).get_text() for result in bundes_results]\r\n    \r\n    bundes_losses = [result.find('td', {'class': 'd-none d-lg-table-cell looses'}).get_text() for result in bundes_results]\r\n    \r\n    bundes_goals = [result.find('td', {'class': 'd-none d-md-table-cell goals'}).get_text() for result in bundes_results]\r\n    \r\n    bundes_goal_diff = [result.find('td', {'class': 'difference'}).get_text().replace(\"+\", \"\") for result in bundes_results]\r\n    \r\n    ## Make pandas Dataframe:\r\n    bundes_df = pd.DataFrame({'Rank': range(1, 19), 'Team': bundes_teams, 'Matches': bundes_matches,\r\n                              'Points': bundes_points, 'Wins': bundes_wins, 'Draws': bundes_draws,\r\n                              'Losses': bundes_losses, 'Goals': bundes_goals, 'Goal Difference': bundes_goal_diff})\r\n        \r\n    # Split Goals Into Goals For & Goals Against:\r\n    bundes_df[['Goals For','Goals Against']] = bundes_df['Goals'].str.split(\":\",expand=True,)\r\n        \r\n    # Drop Goals column\r\n    bundes_df.drop('Goals', axis = 1, inplace = True)\r\n        \r\n    # Rearrange columns\r\n    bundes_df = bundes_df.reindex(columns=['Rank', 'Team', 'Matches', 'Points',\r\n                             'Wins', 'Draws', 'Losses', 'Goals For',\r\n                             'Goals Against', 'Goal Difference'])\r\n    \r\n    bundes_df.columns = ['Rank', 'Team', 'Matches', 'Pts',\r\n                         'W', 'D', 'L', \r\n                         'GF', 'GA', 'GD']\r\n    # Get Bundesliga dataframe:\r\n    return(bundes_df)\r\n\r\n## ------------\r\n## Serie A Table Webscraping\r\n\r\ndef get_serieA_df():\r\n    serieA_url = \"https://www.legaseriea.it/en/serie-a/league-table\"\r\n    \r\n    serieA_response = requests.get(serieA_url)\r\n    \r\n    serieA_soup = BeautifulSoup(serieA_response.content, 'html.parser')\r\n    \r\n    serieA_table = serieA_soup.find('tbody')\r\n    \r\n    table_rows = serieA_table.find_all('tr')\r\n    \r\n    # Obtain parts of the table:\r\n        \r\n    serieA_ranks = [row.find_all('td')[0].find('span').text for row in table_rows]\r\n    \r\n    serieA_team_names = [row.find_all('td')[0].text.split()[1: ] for row in table_rows]\r\n    \r\n    # Unnest lists, dealing with the Hellas Verona case pretty much\r\n    serieA_teams = [\" \".join(str(x) for x in test) for test in serieA_team_names]\r\n    \r\n    serieA_points = [row.find_all('td')[1].text for row in table_rows]\r\n    \r\n    serieA_played = [row.find_all('td')[2].text for row in table_rows]\r\n    \r\n    serieA_wins = [row.find_all('td')[3].text for row in table_rows]\r\n    \r\n    serieA_draws = [row.find_all('td')[4].text for row in table_rows]\r\n    \r\n    serieA_losses = [row.find_all('td')[5].text for row in table_rows]\r\n    \r\n    serieA_goals_for = [row.find_all('td')[-2].text for row in table_rows]\r\n    \r\n    serieA_goals_against = [row.find_all('td')[-1].text for row in table_rows]\r\n    \r\n    # Make pandas Dataframe of Serie A table:\r\n    \r\n    serieA_df = pd.DataFrame({'Rank': serieA_ranks, 'Team': serieA_teams, 'Pts': serieA_points,\r\n                              'Played': serieA_played, 'W': serieA_wins, 'D': serieA_draws, 'L': serieA_losses, \r\n                              'GF': serieA_goals_for, 'GA': serieA_goals_against})\r\n    # Return serie A dataframe:\r\n    return(serieA_df)\r\n\r\n## ------------\r\n## French Ligue 1 Table Webscraping\r\n\r\ndef get_ligue1_df():\r\n    ligue_one_url = \"https://www.ligue1.com/ranking\"\r\n    \r\n    ligue_one_response = requests.get(ligue_one_url)\r\n    \r\n    ligue_one_soup = BeautifulSoup(ligue_one_response.content, 'html.parser')\r\n    \r\n    table_rows = ligue_one_soup.find('div', {'class': 'classement-table-body'}).find('ul').find_all('li')\r\n    \r\n    # Ranks\r\n    ligue_one_ranks = [x.find('a').find_all('div')[0].text for x in table_rows]\r\n    \r\n    # Team Name\r\n    ligue_one_teams = [x.find('a').find_all('div')[1].find('span').text for x in table_rows]\r\n    \r\n    # Points\r\n    ligue_one_points = [x.find('a').find_all('div')[2].text for x in table_rows]\r\n    \r\n    # Played\r\n    ligue_one_played = [x.find('a').find_all('div')[3].text for x in table_rows]\r\n    \r\n    # Wins\r\n    ligue_one_wins = [x.find('a').find_all('div')[4].text for x in table_rows]\r\n    \r\n    # Draws\r\n    ligue_one_draws = [x.find('a').find_all('div')[5].text for x in table_rows]\r\n    \r\n    # Losses\r\n    ligue_one_losses = [x.find('a').find_all('div')[6].text for x in table_rows]\r\n    \r\n    # Goals For\r\n    ligue_one_goals_for = [x.find('a').find_all('div')[7].text for x in table_rows]\r\n    \r\n    # Goals Against\r\n    ligue_one_goals_against = [x.find('a').find_all('div')[8].text for x in table_rows]\r\n    \r\n    # Goal Difference\r\n    ligue_one_goals_diff = [x.find('a').find_all('div')[9].text for x in table_rows]\r\n    \r\n    ## Create Ligue 1 Dataframe\r\n    ligue1_df = pd.DataFrame({'Rank': ligue_one_ranks, 'Team': ligue_one_teams, 'Pts': ligue_one_points,\r\n                               'Pl': ligue_one_played, 'W': ligue_one_wins, 'D': ligue_one_draws, 'L': ligue_one_losses, \r\n                               'GF': ligue_one_goals_for, 'GA': ligue_one_goals_against, 'GD': ligue_one_goals_diff})\r\n    # Return Ligue 1 dataframe:\r\n    return(ligue1_df)\r\n\r\n##-------------\r\n# Sidebar Radio Buttons Choice\r\n\r\nst.sidebar.write(\"\"\"\r\n# Football League Tables\r\n\"\"\")\r\n\r\n# Table Based On Choice\r\nleague = st.sidebar.radio(\r\n     \"Select A League\",\r\n     (\"Spain's La Liga\", 'English Premier League', 'German Bundesliga',\r\n      \"Italy's Serie A\", \"France's Ligue 1\"))\r\n\r\n# Dictionary\r\n\r\nst.sidebar.write(\"\"\"\r\n### Dictionary \\n\r\n            \r\nW = # Wins \\n\r\nD = # Draws \\n\r\nL = # Losses \\n\r\nGF = Goals For \\n \r\nGA = Goals Against \\n\r\nGD = Goals Difference      \r\n\r\n\"\"\"         \r\n)\r\n\r\n# Remove indices:\r\n# CSS to inject contained in a string\r\n# Reference: https://docs.streamlit.io/knowledge-base/using-streamlit/hide-row-indices-displaying-dataframe\r\nhide_table_row_index = \"\"\"\r\n            \r\n            \"\"\"\r\n\r\n# Inject CSS with Markdown\r\nst.markdown(hide_table_row_index, unsafe_allow_html=True)\r\n\r\n### Front Page Area\r\n\r\nst.write(f\"\"\"\r\n         Today's Date: {pd.to_datetime(\"today\").strftime('%B %d %Y')}\r\n         \"\"\")\r\n         \r\nwith st.expander(\"Info (Click To Expand)\"):\r\n     st.write(\"\"\"\r\n         This dashboard allows the user to see Football tables from Europe's top football league.\r\n         Select a league from the left sidebar and the corresponding league table will appear.\r\n     \"\"\")\r\n\r\n         \r\n# Show league table depending on choice from radio buttons:                \r\nif league == \"Spain's La Liga\":\r\n    st.write(\"### La Liga Table\")\r\n    st.table(get_laliga_df())\r\nelif league == \"English Premier League\":\r\n    st.write(\"### Premier League Table\")\r\n    st.table(get_epl_df())\r\nelif league == \"German Bundesliga\":\r\n    st.write(\"### Bundesliga Table\")\r\n    st.table(get_bundesliga_df())\r\nelif league == \"Italy's Serie A\":\r\n    st.write(\"### Serie A Table\")\r\n    st.table(get_serieA_df())\r\nelif league == \"France's Ligue 1\":\r\n    st.write(\"### Ligue 1 Table\")\r\n    st.table(get_ligue1_df())\r\n\r\n\r\n\r\n                                                                             ", "597": "def main():\n\t#url = \"https://www.monster.com/jobs/search/?q=Software-Developer&where=Seattle\"\n\t#cd = \"SearchResults,section,card-content\"\n\t#ced = \"[h2,div,div],[title,company,location]\"\n\turl = sys.argv[1]\n\tcd = sys.argv[2]\n\tced = sys.argv[3]\n\tresults = webscrape(url,cd)\n\tobjects, categories = parseContainer(results,ced)\n\tjsonCategories = json.dumps(categories)\n\tjsonObject = json.dumps(objects)\n\tprint(jsonCategories)\n\tprint(jsonObject)\n\nif __name__ == \"__main__\":\n    main()", "598": "from django.shortcuts import render\nfrom django.http import JsonResponse\nfrom datetime import datetime, timedelta\nfrom .lib.webscrape.ManitobaCourtsScraper import ManitobaCourtsScraper\n\n\n# Create your views here.\ndef index(request):\n    return JsonResponse({'gavel-api-version': '0.0.1'})\n", "599": "# from nba_bbref_webscrape import __version__\n\n\n# def test_version():\n#     assert __version__ == '0.1.0'\n", "600": "# Import libraries\nimport requests\nfrom lxml import html\nfrom bs4 import BeautifulSoup\n\ndef getPrice(coinmarketurl):\n\n    # Set the URL you want to webscrape from\n    url = coinmarketurl\n\n    # Connect to the URL\n    response = requests.get(url)\n\n    # Parse HTML and save to BeautifulSoup object\u00c2\u00b6\n    soup = BeautifulSoup(response.text, \"html.parser\")\n\n    myprice = soup.find_all(\"div\", {\"class\": \"priceValue\"})\n\n    pricestring = myprice[0].text\n    print(pricestring)\n\n    return pricestring\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "601": "#!/home/amul/Documents/webscrape/ecos/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom numpy.f2py.f2py2e import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "602": "#!/home/amul/Documents/webscrape/ecos/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom numpy.f2py.f2py2e import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "603": "#!/home/amul/Documents/webscrape/ecos/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom numpy.f2py.f2py2e import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "604": "from GraphCreator import GraphCreator\nfrom SaveSQL import *\nfrom requests_html import HTMLSession\nimport requests\nfrom bs4 import BeautifulSoup\nimport threading\nimport json\n\n# This class webscrape for DAZADAZ project.\n# Process,\n# read video list (videos) -> go to the link & check their video title\n# -> check if the video is a song (loop)\n# if so: -> get other info(views, likes, date) -> put their lists\n# not: -> continue\n# -> set total, view&title, like&title, and the sum of the view & like\n\n# when main calls gr (graph) function,\n# set top 25 view & like list -> call GraphCreator\n\n# 9/14/22\nclass Webscrape:\n    def __init__(self):\n        self.url = \"https://www.youtube.com/c/Dazbeeee/videos\"\n        self.session = HTMLSession()\n        self.song_title = [ ]\n        self.view = [ ]\n        self.like = [ ]\n        self.date = [ ]\n        self.link = [ ]\n        self.total = [ ]\n        self.totalView = 0\n        self.totalLike = 0\n        self.info = [ ]\n        self.view_title_link = [ ]\n        self.like_title_link = [ ]\n        self.top25_like = [ ]\n        self.top25_view = [ ]\n\n    def readVideoList(self):\n        response = self.session.get(self.url)\n        response.html.render(sleep=1, keep_page=True, scrolldown=26)\n        a = response.html.find('a#video-title')\n        for links in a:\n            link = next(iter(links.absolute_links))\n            thd = threading.Thread(target=self.check, args=(link, ))\n            thd.start()\n            thd.join()\n        self.setTotal()\n        self.setViewTitle()\n        self.setLikeTitle()\n        self.totalView = sum(self.view)\n        self.totalLike = sum(self.like)\n        \n    def videoLikes(self, i):\n        r = requests.get(i, headers={'User-Agent': ''})\n        likes = r.text[:r.text.find(' likes\"')]\n        like_num = int(likes[likes.rfind('\"') + 1:].replace(',', \"\"))\n        self.like.append(like_num)\n\n    def check(self, i):\n        condition.acquire()\n        res = requests.get(i)\n        soup = BeautifulSoup(res.text, 'lxml')\n        video_title = soup.find(\"meta\", itemprop=\"name\")[\"content\"]\n        if self.splitTitle(video_title):\n            self.link.append(i)\n            self.videoLikes(i)\n            self.date.append(str(soup.find(\"meta\", itemprop=\"datePublished\")[\"content\"]))\n            self.view.append(int(soup.find(\"meta\", itemprop=\"interactionCount\")[\"content\"]))\n        condition.release()\n\n\n    def sql(self):\n        print(\"\\n\\nSQL to create the table from the web\\n\")\n        sq = sqlite()\n        sq.connect()\n        sq.table(\"Database\")\n        for i in range(len(self.song_title)):\n            sql_thd = threading.Thread(target=sq.insert, args=(i + 1, self.song_title[i], self.info[i],\n                                                               self.view[i], self.like[i],\n                                                               self.date[i], self.link[i]))\n            sql_thd.start()\n            sql_thd.join()\n        print(\"\\nAll of the data inserted successfully as BLOBs into a table\\n\")\n\n    def gr(self):\n        self.set_top25_view()\n        self.set_top25_like()\n        gc = GraphCreator(self.top25_view, self.top25_like)\n        gc.createTable()\n\n    def setTotal(self):\n        for i in range(len(self.song_title)):\n            self.total.append((self.view[i], self.song_title[i], self.like[i], self.date[i], self.info[i], self.link[i]))\n\n    def setViewTitle(self):\n        for i in range(len(self.song_title)):\n            self.view_title_link.append((self.view[i], self.song_title[i], self.link[i]))\n\n    def setLikeTitle(self):\n        for i in range(len(self.song_title)):\n            self.like_title_link.append((self.like[i], self.song_title[i], self.link[i]))\n\n    def set_top25_like(self):\n        self.like_title_link.sort()\n        for i in range(25):\n            info_data = self.like_title_link[len(self.view_title_link) - i - 1]\n            self.top25_like.append(info_data)\n\n    def set_top25_view(self):\n        self.view_title_link.sort()\n        for i in range(25):\n            info_data = self.view_title_link[len(self.view_title_link) - i - 1]\n            self.top25_view.append(info_data)\n\n\n    def checkExcludes(self, title):\n        excludes = [\"Teaser\", \"#Shorts\", \"#YouTubeMusic\", \"RELEASED\", \"Selection Album\"]\n        for j in excludes:\n            if title.find(j) != -1:\n                return True\n        return False\n    \n    def splitTitle(self, t):\n        remain = r'\\(([^)]+)'\n        remove = \"\\(.*\\)|\\s-\\s.\"\n        remain_original = r'\\|'\n        excludes2 = ['  \uff0f', \" / \", \" \uff0f\",\n                     \"/\", \"|\"]\n        # M/V means the video is her original song\n        ta = t\n        if self.checkExcludes(ta):\n            return False\n        if t.find('M/V') != -1: # + 'Official Piano Arrange\n            self.info.append(\"ORIGINAL\")\n            e = re.sub(remove, '', t.split(\" | \")[1]).replace('M/V', \"\")\n            self.song_title.append(e)\n            return True\n\n        elif t.find('(') != -1 and t.find(')') != 1:  # check world.execute(me); (...)\n            if t.count('(') != 1:\n                artist = re.findall(remain, t)[-1]\n            else:\n                artist = re.findall(remain, t)[0]\n            self.info.append(artist)\n        else:\n            self.info.append(\"UNKNOWN\")\n\n        for j in excludes2:\n            if t.find(j) != -1:\n                e = re.sub(remove, '', t).split(j)\n                self.song_title.append(e[0])\n                return True\n        self.song_title.append(re.sub(remove, '', t))\n        return True\n\n\n", "605": "from bokeh.io import show, output_file\nfrom bokeh.models import ColumnDataSource, Legend\nfrom bokeh.palettes import Category20b_17\nfrom bokeh.plotting import figure\nimport pandas as pd\nimport numpy as np\n\n#pageViews = pd.DataFrame(pd.read_csv('./Webscrape/0816CountryView.csv'), names = ['By country', 'Page views per minute'])\n\ncolnames = ['Bycountry', 'Pageviewsper']\npageViews = pd.read_csv('./Webscrape/0816CountryView.csv', names=colnames)\n\ncountry = pageViews.Bycountry.tolist()\nppm = pageViews.Pageviewsper.tolist()\n#print(country)\n#print(ppm)\n\nsource = ColumnDataSource(data=dict(country=country, ppm=ppm, color=Category20b_17))\n\np = figure(x_range=country, y_range=(0, 0.02), plot_height=350, title=\"Page views per Minute by Country\",\n           toolbar_location=None, tools=\"\")\n\np.vbar(x='country', top='ppm', width=0.8, color='color', source=source)\n\np.xgrid.grid_line_color = None\n#Sorry I'm hard coding everything, last day of internship calls for brute force!!!!!\nx = np.linspace(0, 4*np.pi, 100)\ny = np.sin(x)\n\nr0 = p.square(x, 3*y, fill_color=Category20b_17[0], line_color=None)\nr1 = p.square(x, 3*y, fill_color=Category20b_17[1], line_color=None)\nr2 = p.square(x, 3*y, fill_color=Category20b_17[2], line_color=None)\nr3 = p.square(x, 3*y, fill_color=Category20b_17[3], line_color=None)\nr4 = p.square(x, 3*y, fill_color=Category20b_17[4], line_color=None)\nr5 = p.square(x, 3*y, fill_color=Category20b_17[5], line_color=None)\nr6 = p.square(x, 3*y, fill_color=Category20b_17[6], line_color=None)\nr7 = p.square(x, 3*y, fill_color=Category20b_17[7], line_color=None)\nr8 = p.square(x, 3*y, fill_color=Category20b_17[8], line_color=None)\nr9 = p.square(x, 3*y, fill_color=Category20b_17[9], line_color=None)\nr10 = p.square(x, 3*y, fill_color=Category20b_17[10], line_color=None)\nr11 = p.square(x, 3*y, fill_color=Category20b_17[11], line_color=None)\nr12 = p.square(x, 3*y, fill_color=Category20b_17[12], line_color=None)\nr13 = p.square(x, 3*y, fill_color=Category20b_17[13], line_color=None)\nr14 = p.square(x, 3*y, fill_color=Category20b_17[14], line_color=None)\nr15 = p.square(x, 3*y, fill_color=Category20b_17[15], line_color=None)\nr16 = p.square(x, 3*y, fill_color=Category20b_17[16], line_color=None)\n\nlegend = Legend(items=[\n    (\"Great Britain\"   , [r0]),\n    (\"United States\"   , [r1]),\n    (\"Ireland\"   , [r2]),\n    (\"Denmark\"   , [r3]),\n    (\"Austria\"   , [r4]),\n    (\"Brazil\"   , [r5]),\n    (\"Slovakia\"   , [r6]),\n    (\"Hungary\"   , [r7]),\n    (\"Russia\"   , [r8]),\n    (\"Netherlands\"   , [r9]),\n    (\"Venezuela\"   , [r10]),\n    (\"Chile\"   , [r11]),\n    (\"China\"   , [r12]),\n    (\"Spain\"   , [r13]),\n    (\"Japan\"   , [r14]),\n    (\"Malaysia\"   , [r15]),\n    (\"Sweden\"   , [r16]),\n], location=(0, -28), label_text_font_size='8pt', spacing=0)\n\n#countryFull = [\"Great Britain\", \"United States\", \"Ireland\", \"Denmark\", \"Austria\", \"Brazil\", \"Slovakia\", \"Hungary\"\n#              \"Hungary\", \"Russia\", \"Netherlands\", \"Chile\", \"China\", \"Spain\", \"Japan\", \"Malaysia\", \"Sweden\"]\n\np.add_layout(legend, 'right')\n\noutput_file(\"PageViews.html\")\nshow(p)\n", "606": "from urllib.request import urlopen as uReq\nimport urllib\nfrom bs4 import BeautifulSoup as soup    \nimport os, sys, time\n\ndef Webscrape_head(page_soup):\n\tNazev = page_soup.h1.text\n\t#BookInfoFile=open(\"/mnt/minerva1/nlp/projects/sentiment9/Results/\"+NazevDir+\"/BookInfo.txt\", \"w\",encoding=\"utf-8\")\n\tBookInfoFile=open(\"/mnt/minerva1/nlp/projects/sentiment9/Results/BookInfo.tsv\", \"a\",encoding=\"utf-8\")\n\n\tauthor = page_soup.find(\"a\",{\"itemprop\":\"author\"}).text\n\tgenres = page_soup.findAll(\"span\",{\"itemprop\":\"genre\"})\n\tannotation = page_soup.find(\"div\",{\"id\":\"book_annotation\"})\n\tif(annotation is not None):\n\t\tif(annotation.b is not None):\n\t\t\tannotation.b.decompose()\n\n\t\tif(annotation.text is not None):\n\t\t\tannotation=annotation.text\n\t\telse:\n\t\t\tannotation=\"\"\n\telse:\n\t\tannotation = \"\"\n\n\tgenre_list=[]\n\tfor genre in genres:\n\t\tgenre_list.append(genre.text)\n\n\tBookInfoFile.write(Nazev+'\\t'+author+'\\t'+','.join(genre_list)+'\\t'+annotation.replace('\\n',' ').replace(chr(13),'').replace('  ','').strip()+'\\n')\n\tBookInfoFile.close()\n\n\ndef WebScrape_reviews(my_url):\n\t#otevre url a precte html zadaneho url\n\tmy_url=my_url.encode('utf-8').decode('ascii', 'ignore')\n\n\ttry:\n\t\tuClient = uReq(my_url)\n\texcept:\n\t\treturn\n\n\n\tpage_html = uClient.read()\n\tuClient.close()\n\n\t#vyhledani hledanych dat v html\n\tpage_soup = soup(page_html, \"html.parser\")\n\tNazev = page_soup.h1.text\n\n\tWebscrape_head(page_soup)\n\t\t\n\n\t#pocet stranek recenzi\n\treview_page_count=len(page_soup.findAll(\"a\",{\"class\":\"textlist_item_select_width round_mini\"}))+1\n\n\n\tcbdbReviews = open(\"/mnt/minerva1/nlp/projects/sentiment9/Results/Reviews.tsv\", \"a\",encoding=\"utf-8\")\n\n\n\tfor review_page in range(1,review_page_count+1):\n\t\tif(review_page != 1):\n\t\t\t#zmena url a precteni\n\t\t\tmy_url=my_url+'&comments_page='+str(review_page)\n\t\t\t#vyhledani recenzi v html\n\t\t\tuClient = uReq(my_url)\n\t\t\tpage_html = uClient.read()\n\t\t\tuClient.close()\n\t\t\tpage_soup = soup(page_html, \"html.parser\")\n\t\treviews = page_soup.findAll(\"div\",{\"class\":\"comment\"})\n\n\t\tfor review in reviews:\n\t\t\tusername=review.div.img[\"alt\"]\n\t\t\tuserid=review.a[\"href\"].split('-')[1]\n\n\t\t\theader=review.find(\"div\",{\"class\":\"comment_header\"})\n\n\t\t\trating=header.img \n\t\t\tif(rating is not None): #pripad kde recenze nema hodnoceni\n\t\t\t\trating=header.img[\"alt\"] \n\t\t\telse:\n\t\t\t\trating=\"??\"\n\n\t\t\tdate=header.find(\"span\",{\"class\":\"date_span\"}).text\n\t\t\tcomment=review.find(\"div\",{\"class\":\"comment_content\"}).text\n\n\t\t\tcbdbReviews.write(\"cbdb\"+'\\t'+Nazev+'\\t'+username+'\\t'+userid+'\\t'+date+'\\t'+rating+'\\t'+comment.replace('\\n',' ').replace(chr(13),'').replace('  ','').strip()+'\\n') \n\t\t\n\t\ttime.sleep(2)#delay mezi pristupy na stranky recenz\u00c3\u00ad\n\ttime.sleep(2)#delay mezi pristupy na knihy\n\n\tcbdbReviews.close()", "607": "from itertools import combinations\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom regressors import stats\nfrom sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport webscrape\n\n__all__ = ['RidgeRegression', 'LassoRegression']\n\n\n# directory of data\n_DATA_DIR = os.path.join(os.path.dirname(webscrape.__file__), \"data\")\n\n\ndef get_from_csv(stat, year=None):\n    \"\"\"Imports data for specified stat and year into a pandas DataFrame. If csv\n    file doesn't exist, then it will scrape the web for the data and save it to\n    a csv file.\n\n    Parameters\n    ----------\n    stat : str\n        Name of the stat. Case doesn't matter.\n    year : int, optional\n        Year of specified statistic. If None, then it defaults to the most\n        recent (including current) season.\n    \"\"\"\n\n    # create filepath for stat and year\n    filepath = \"_\".join(stat.split(\" \"))\n    if year is not None:\n        filepath += \"_{}\".format(year)\n    filepath += \".csv\"\n    filepath = os.path.join(_DATA_DIR, filepath)\n\n    # import data for stat and year from csv file into dataframe\n    if not os.path.exists(filepath):\n        webscrape.get_stat_data(stat, year=year)\n\n    return pd.read_csv(filepath_or_buffer=filepath)\n\n\ndef create_df(main_stat, other_stats, year=None):\n    \"\"\"For a given dependent stat and a list of independent stats, returns a\n    pd.DataFrame instance where the dependent stat is in the first column and\n    predictors are in subsequent columns\n\n    Parameters\n    ----------\n    main_stat : str\n        Dependent stat.\n    other_stats : str or list\n        Independent stat(s). Can pass a single stat as a str or a list of str\n        for multiple stats.\n    year : int, optional\n        Year of specified statistic. If None, then it defaults to the most\n        recent (including current) season.\n\n    Returns\n    -------\n    df_combined : pd.DataFrame\n        DataFrame of rankings for each stat. The dependent stat is in the first\n        columns and predictors are in subsequent columns\n    \"\"\"\n\n    # if only one independent stat, turn it into list\n    if isinstance(other_stats, str):\n        other_stats = [other_stats]\n\n    # check that the main stat isn't in the other stats\n    if main_stat in other_stats:\n        raise ValueError(\"The '{}' main stat cannot be in other stats\".format(main_stat))\n\n    # get data from csv files\n    df_main = get_from_csv(main_stat, year=year)\n    df_others = []\n    for stat in other_stats:\n        df_combined = get_from_csv(stat, year=year)\n        df_combined = df_combined.set_index(\"PLAYER NAME\")\n        df_combined = df_combined.reindex(index=df_main[\"PLAYER NAME\"])\n        df_combined = df_combined.reset_index()\n        df_others.append(df_combined)\n\n    # create y and x arrays\n    y = np.array(df_main[\"RANK THIS WEEK\"])\n    x = []\n    for df_combined in df_others:\n        x.append(np.array(df_combined[\"RANK THIS WEEK\"]))\n    x = np.column_stack(tuple(x))\n\n    # create filter that removes rows with NaN in x and corresponding rows in y\n    if len(x.shape) == 1:\n        nan_filter = ~np.isnan(x)\n    else:\n        nan_filter = np.all(~np.isnan(x), axis=1)\n    x = x[nan_filter]\n    y = y[nan_filter]\n\n    # combine x and y in a pd.DataFrame instance\n    df_combined = pd.DataFrame(np.column_stack((y, x)), columns=[main_stat] + other_stats)\n\n    return df_combined\n\n\nclass AnalysisBase(object):\n\n    def __init__(self, main_stat, other_stats, year=None):\n        self.main_stat = main_stat\n        self.other_stats = other_stats\n        self.year = year\n        self.df = create_df(main_stat, other_stats, year=year)\n        self.x_mean = np.zeros(self.df.shape[1] - 1)\n        self.x_std = np.ones(self.df.shape[1] - 1)\n        self.y_mean = 0\n\n    def get_X(self, as_numpy=True):\n        X = self.df.iloc[:, 1:]\n        if as_numpy:\n            return np.array(X)\n        else:\n            return X\n\n    def get_y(self):\n        y = self.df.iloc[:, 0]\n        return y\n\n    def _resave_df(self, X, y):\n        self.df = pd.DataFrame(np.column_stack((y, X)), columns=[self.main_stat] + list(self.other_stats))\n\n    def normalize_X(self):\n        X = self.get_X()\n        self.x_mean = np.mean(X, axis=0)\n        self.x_std = np.std(X, axis=0)\n        X = (X - self.x_mean) / self.x_std\n        self._resave_df(X, self.get_y())\n\n    def center_y(self):\n        y = self.get_y()\n        self.y_mean = np.mean(y)\n        y = y - self.y_mean\n        self._resave_df(self.get_X(), y)\n\n    def fit(self, **kwargs):\n        pass\n\n    def fit_multi(self, *args, **kwargs):\n        pass\n\n    def variance_inflation_factor(self):\n        return [variance_inflation_factor(self.get_X(), i) for i in range(self.df.shape[1] - 1)]\n\n\nclass LassoRegression(AnalysisBase):\n\n    REGRESSION_TYPE = Lasso\n    REGRESSION_TYPE_CV = LassoCV\n\n    def __init__(self, main_stat, other_stats, year=None):\n        super(LassoRegression, self).__init__(main_stat, other_stats, year=year)\n        self.alpha = np.array([])\n        self.coef = None\n        self.mse = np.array([])\n        self.mse_std = np.array([])\n        self.best_coef = None\n        self.best_mse = None\n        self.CV = RepeatedKFold(n_splits=self.get_X().shape[0], n_repeats=1, random_state=2652124)\n\n    def fit(self, alpha):\n        model = self.REGRESSION_TYPE(alpha=alpha)\n        model.fit(self.get_X(), self.get_y())\n        coef = model.coef_.reshape(len(model.coef_), 1)\n        self.alpha = np.append(self.alpha, alpha)\n        if self.coef is None:\n            self.coef = coef\n        else:\n            self.coef = np.column_stack((self.coef, coef))\n        self.compute_mse(alpha)\n\n    def compute_mse(self, alpha):\n        model = self.REGRESSION_TYPE(alpha=alpha)\n        X = self.get_X()\n        y = self.get_y()\n        scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=self.CV, n_jobs=-1)\n        scores = np.abs(scores)\n        self.mse = np.append(self.mse, np.mean(scores))\n        self.mse_std = np.append(self.mse, np.std(scores))\n\n    def fit_multi(self, alphas):\n        self.alpha = np.array([])\n        self.coef = None\n        for alpha in alphas:\n            self.fit(alpha)\n\n    def plot_coef(self):\n        plt.figure()\n        plt.plot(self.alpha, self.coef.T)\n        plt.xscale('log')\n        plt.xlabel(r\"$\\lambda$\")\n        plt.ylabel(\"Standardized coefficients\")\n        plt.legend(labels=self.other_stats)\n        plt.tight_layout()\n\n    def plot_mse(self):\n        plt.figure()\n        plt.plot(self.alpha, self.mse)\n        plt.xscale('log')\n        plt.xlabel(r\"$\\lambda$\")\n        plt.ylabel(\"MSE\")\n        plt.tight_layout()\n\n    def fit_CV(self, alphas):\n        search = self.REGRESSION_TYPE_CV(alphas=alphas, cv=self.CV)\n        results = search.fit(self.get_X(), self.get_y())\n        self.best_coef = results.coef_\n        self.best_model = search\n\n    def summary(self):\n        stats.summary(self.best_model, self.get_X(), self.get_y())\n\n\nclass RidgeRegression(LassoRegression):\n\n    REGRESSION_TYPE = Ridge\n    REGRESSION_TYPE_CV = RidgeCV\n\n    def fit_CV(self, alphas):\n        search = self.REGRESSION_TYPE_CV(alphas=alphas, scoring='neg_mean_squared_error', cv=self.CV)\n        results = search.fit(self.get_X(), self.get_y())\n        self.best_mse = np.abs(results.best_score_)\n        self.best_coef = results.coef_\n        self.best_model = search\n\n\ndef best_subset(estimator, X, y, max_size=8, cv=5):\n    \"\"\"Calculates the best model of up to max_size features of X.\n       estimator must have a fit and score functions.\n       X must be a DataFrame.\"\"\"\n\n    n_features = X.shape[1]\n    subsets = (combinations(range(n_features), k + 1)\n               for k in range(min(n_features, max_size)))\n\n    best_size_subset = []\n    for subsets_k in subsets:  # for each list of subsets of the same size\n        best_score = -np.inf\n        best_subset = None\n        for subset in subsets_k: # for each subset\n            estimator.fit(X.iloc[:, list(subset)], y)\n            # get the subset with the best score among subsets of the same size\n            score = estimator.score(X.iloc[:, list(subset)], y)\n            if score > best_score:\n                best_score, best_subset = score, subset\n        # to compare subsets of different sizes we must use CV\n        # first store the best subset of each size\n        best_size_subset.append(best_subset)\n\n    # compare best subsets of each size\n    best_score = -np.inf\n    best_subset = None\n    list_scores = []\n    for subset in best_size_subset:\n        score = cross_val_score(estimator, X.iloc[:, list(subset)], y, cv=cv, scoring='neg_mean_squared_error').mean()\n        list_scores.append(score)\n        if score > best_score:\n            best_score, best_subset = score, subset\n\n    return best_subset, best_score, best_size_subset, list_scores\n\n\nif __name__ == '__main__':\n    pass\n\n", "608": "#!/home/amul/Documents/webscrape/ecos/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom pip._internal.cli.main import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "609": "#!/home/amul/Documents/webscrape/ecos/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom pip._internal.cli.main import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "610": "#!/home/amul/Documents/webscrape/ecos/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom pip._internal.cli.main import main\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "611": "#SA Beerfest WebScrape\nimport requests\nimport urllib.request\nimport time\nfrom bs4 import BeautifulSoup\n\n\n# Targeting beer name, brewery, tyoe of beer, pulling from SAbeerfest.com\nurl= 'https://sanantoniobeerfestival.com/mobile/'\nresponse = requests.get(url)\n\n\n", "612": "#Script for AT&T\n\n# Import libs and set working dir\nimport os\nos.getcwd()\n# set your working directory\nos.chdir('C:\\\\Users\\\\ACER\\\\Downloads\\\\WebScrape')\n\nimport time\nfrom re import search\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\n\n# Get Driver\ndrvPath = \"C:\\\\Users\\\\ACER\\\\Downloads\\\\WebScrape\\\\chromedriver_win32\\\\chromedriver.exe\"\nbrowser= webdriver.Chrome(drvPath)\n\n# Open Link\nbrowser.get(\"https://www.att.com/buy/phones/\")\n\ntimeout = 30\ntry:\n    WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.CLASS_NAME, \"_2rMgP\")))\nexcept TimeoutException:\n    browser.quit()\n\nSCROLL_PAUSE_TIME = 1\n\n# Get scroll height\nlast_height = browser.execute_script(\"return document.body.scrollHeight\")\n\n# Load entire page\nwhile True:\n    # Scroll down to bottom\n    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n\n    # Wait to load page\n    time.sleep(SCROLL_PAUSE_TIME)\n\n    # Calculate new scroll height and compare with last scroll height\n    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n    if new_height == last_height:\n        break\n    last_height = new_height\n\n\n# Get all the links\nelements = browser.find_elements(By.CSS_SELECTOR,\n\"div._2rMgP a\")\n\n#Loop through all the links and extract product specific links\na_elements = []\nfor elms in elements:\n    lnk=elms.get_attribute(\"href\")\n    if search(str('https://www.att.com/buy/phones/'), str(lnk)) and search(str('.html'), str(lnk)):\n        a_elements.append(lnk)\n\n# Verify Links\nprint (a_elements)\nlen(a_elements)\n\n# Loop for getting individual product information and creating dataframe\n\nfor product in a_elements:\n    browser.get(str(product))", "613": "#!/home/amul/Documents/webscrape/ecos/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom charset_normalizer.cli.normalizer import cli_detect\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(cli_detect())\n", "614": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri May 29 15:07:07 2020\n\n@author: Hayden Rampadarath (haydenrampadarath@gmail.com)\n\nScript to webscrape My Anime List\n\"\"\"\n\n\nimport urllib\nimport requests\nimport bs4\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re\nfrom tqdm import tqdm\nfrom time import sleep\nfrom bs4.element import NavigableString, Tag\n\n\n\ndef extractNavigableStrings(context):\n    \"\"\" from https://stackoverflow.com/questions/29110820/how-to-scrape-between-span-tags-using-beautifulsoup\"\"\"\n    strings = []\n    for e in context.children:\n        if isinstance(e, NavigableString):\n            strings.append(e)\n        if isinstance(e, Tag):\n            strings.extend(extractNavigableStrings(e))\n    return strings\n\n\n\ndef parse_MAl(url):\n    \"\"\"\n    Parameters\n    ----------\n    url : string\n        myanimelist.net url string \n\n    Returns\n    -------\n    df : DataFrame\n        returns a dataframe with columns \"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"\n\n    \"\"\"\n    html = requests.get(url)\n    soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n    results = soup.find_all(class_= \"ranking-list\")\n    \n    df = pd.DataFrame(columns=[\"name\",\"english_name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\", \"url\"])\n    i = 0\n    for result in results:\n        #print(i)\n        url_= result.find(class_=\"hoverinfo_trigger fl-l fs14 fw-b\")[\"href\"]\n        html_ = requests.get(url_)\n        soup_ = BeautifulSoup(html_.content, 'html.parser', from_encoding=\"utf-8\")\n        \n        t1name = extractNavigableStrings(soup_.find(class_=\"h1-title\"))\n        if len(t1name) == 1:\n            name = t1name[0]\n            english_name = None\n        elif len(t1name) >= 2:\n            name=t1name[0]\n            english_name=t1name[1]\n        else:\n            name = None\n            english_name = None\n            \n        Type, Dates, members = result.find(class_=\"information di-ib mt4\").text.strip().splitlines()\n        try:\n            members = float(\"\".join(members.split()[0].split(\",\")))\n        except:\n            members = None\n            \n        [Type_, eps, n] = [\", \".join(x.split()) for x in re.split(r'[()]',Type)]\n        \n        try:\n            eps = float(eps.split(\",\")[0])\n        except:\n            eps = None\n        \n        try:\n            genres = [genre.text.strip() for genre in soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"genre\")]\n            \n        except:\n            genres = None\n        \n        try:\n            score = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingValue\")[0].text.strip())\n        except:\n            score = None\n        #try:\n        #    score_members = float(soup_.find(class_=\"borderClass\").find_all(\"span\", itemprop=\"ratingCount\")[0].text.strip())\n        #except:\n         #   score_members = None\n        \n        df = df.append({\n            \"name\": name,\n            \"english_name\":english_name,\n            \"type\": Type_,\n            \"episodes\": eps,\n            \"members\": members,\n            #\"score_members\": score_members,\n            \"rating\": score,\n            \"genre\": genres,\n            \"dates\": Dates,\n            \"url\": url_\n        },ignore_index=True)\n        \n        i+=1\n    return df\n\n\ndef webscrape_MAl(anime_limit=16750, start=0):\n    url_template = \"https://myanimelist.net/topanime.php?limit={}\"\n    df = pd.DataFrame(columns=[\"name\",\"type\",\"episodes\",\"members\",\"score_members\", \"rating\",\"genre\",\"dates\"])\n    for limit in tqdm(range(start,anime_limit, 50)): # iterate in steps of 50\n        url = url_template.format(limit)\n        df_temp = parse_MAl(url)\n        if df_temp[\"name\"].isnull().sum() >= 40:\n            print(\"Number of missing names, for limit {} = {}\".format(limit, df_temp[\"name\"].isnull().sum()))\n            print(\"--------Halting---------\")\n            raise SystemExit()\n        save_mal_temp(df_temp, limit)\n        \n        # I think MAL has a limit on the number of conenctions per minute/second/hour\n        # and after 200-400, the site blocks access. Adding the pause for 1 minute below soleves the issue\n        sleep(60) # pause the loop for 1 minute. \n        \n\n\n\n\n\ndef save_mal_temp(df, limit):\n    csvTemp = \"temp/MAL_start_{}.csv\".format(limit)\n    df.to_csv(csvTemp)\n        \n    #print(\"Number of missing names, for limit {} = {}\".format(limit, df[\"name\"].isnull().sum()))\n    \n    \n\nwebscrape_MAl(anime_limit = 16750, start = 16550)", "615": "import re\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport requests\nimport time\nimport amadeus\nfrom amadeus import Client, ResponseError\nimport sys\nimport random as rd\n\n\nclass volare:\n    def __init__(self,name,depart,arrive,date,people):\n        self.name = name\n        self.start = depart\n        self.stop = arrive\n        self.date = date\n        self.mate = people\nc1 = volare\n\n\n### Insert Functions That will be used\nlocations=pd.read_csv('lab_3_api_and_webscrape\\Project 3\\locations.csv')\ncunt=pd.read_csv('lab_3_api_and_webscrape\\Project 3\\IATA.csv')\n\ndef ama ():\n    amadeus = Client(\n    client_id='nPw0aTw26KfiG0npH6XcoEKYCJ7N8zbf',\n    client_secret = 'dLqHifawmuDizlVZ'\n    )\n    try:\n        response = amadeus.shopping.flight_offers_search.get(\n            originLocationCode=c1.start,\n            destinationLocationCode=c1.stop,\n            departureDate=c1.date,\n            adults=c1.mate\n        )\n        print(response.data)\n    except ResponseError as error:\n        print(error)\n\n\ndef flight_retrieve(x):\n    flights=x[0:5]\n    tripdf = pd.DataFrame()\n    for i in range(len(flights)):\n        trip = {}\n        volo=flights[i]\n        flight=volo['itineraries'][0]\n        e = re.sub('PT','',flight['duration'])\n        e = re.sub('H',' Hours ',e)\n        trip['duration'] = re.sub('M',' Min',e)\n        q=flight['segments'][0]\n        trip['airline']=q['carrierCode']\n        q=q['departure']\n        trip['from']=q['iataCode']\n        trip['leave at']=re.sub('T',' at ',q['at'])\n        q=flight['segments'][0]\n        q = q['arrival']\n        trip['to']=q['iataCode']\n        trip['arrive at']=re.sub('T',' at ',q['at'])\n        cost=volo['price']\n        trip['price']=cost['total']\n        listing=pd.DataFrame([trip]).T\n        tripdf=pd.concat(objs=[listing,tripdf],axis=1,)\n    tripdf.columns=[1,2,3,4,5]\n    return(tripdf)\n\n\ndef get_id(df):\n\turl = \"https://airbnb19.p.rapidapi.com/api/v1/searchDestination\"\n\n\tquerystring = {\"query\":'tbd',\"country\":df ['Country']}\n\t\n\theaders = {\n\t\"X-RapidAPI-Key\": \"4756e43d86msh629b4ee18cb5f4cp1b8e99jsncc91f7430291\",\n\t\"X-RapidAPI-Host\": \"airbnb19.p.rapidapi.com\"}\n\t\t\t\t\t\t\t\t\t\t\t\t\n\n\tresponse = requests.request(\"GET\", url, headers=headers, params=querystring)\n\ttime.sleep(3)\n\tdata = response.json()\n\ttry:\n\t\treturn data['data'][0]['id']\n\texcept:\n\t\treturn df['City']\n\ndef get_place_info(idlist,citylist):\n\tli = []\n\tnotgood = []\n\tfor ind in range(len(idlist[0])):\n\t\tprint(idlist[0][ind])\n\t\turl = \"https://airbnb19.p.rapidapi.com/api/v1/searchPropertyByPlace\"\n\n\t\tquerystring = {\"id\":idlist[0][ind],\"totalRecords\":\"10\",\"currency\":\"EUR\",\"adults\":\"1\"}\n\n\t\theaders = {\n\t\t\"X-RapidAPI-Key\": \"4756e43d86msh629b4ee18cb5f4cp1b8e99jsncc91f7430291\",\n\t\t\"X-RapidAPI-Host\": \"airbnb19.p.rapidapi.com\"}\n\n\n\t\tresponse = requests.request(\"GET\", url, headers=headers, params=querystring)\n\t\ttime.sleep(3)\n\n\t\tdata2 = response.json()\n\t\t\n\t\ttry:\n\t\t\tfor i in range(10):\n\t\t\t\tcity = data2['data'][i]['city']\n\t\t\t\trating = data2['data'][i]['avgRating']\n\t\t\t\tprice = data2['data'][i]['price']\n\n\t\t\t\tli.append({ 'City': citylist[ind], 'Rating':rating, 'Price': price})\n\t\texcept:\n\t\t\t\tnotgood.append(citylist[ind])\n\treturn pd.DataFrame(li),notgood\n\ndef waiting(t):\n    time.sleep(t)\n    print('.')\n    print()\n    time.sleep(t)\n    print('.')\n    print()\n    time.sleep(t)\n    print('.')\n    print()\n\ndef talk_speed(dialog, speed):\n    for character in dialog:\n        sys.stdout.write(character)\n        sys.stdout.flush()\n        time.sleep(speed)\n\ndef app_title():\n    print()\n    print()\n    print()\n    print('#######################################')\n    print('############# VACATION BOT ############')\n    print('################ v.1 ##################')\n    print('#######################################')\n    waiting(0.5)\n    app_start()\n\n\ndef app_start():\n    bot2 = 'Please tell me when you want to go on vacation.(YYYY-MM-DD)\\n'\n    talk_speed(bot2,0.0001)\n    year = (input('Year()>   ')).strip()\n    month = (input('Month>   ')).strip()\n    day = (input('Year>   ')).strip()\n    date=year+'-'+month+'-'+day\n    c1.date=date\n    bot4 = \"Pick an outdoor activity\\n\"\n    talk_speed(bot4,0.0001)\n    waiting(0.5)\n    print((locations['Activities'].unique()))\n    print()\n    activity = input('>    ').strip().lower().capitalize()\n    if activity == ['Hiking', 'Cycling','Surfing']:\n        print('Good choice!!')\n    while activity not in ['Hiking', 'Cycling','Surfing']:\n        print('error: I never said that')\n        activity = input('>    ').strip().lower().capitalize()\n        if activity == ['Hiking', 'Cycling','Surfing']:\n            print('Good choice!!')\n    p=list((locations['Countries'].loc[locations['Activities']==activity]).unique())\n    p=rd.choices(p, k=10)\n    waiting(0.4)\n    bot6='Alright, here are some personal recomendations for Countries you might like to visit...\\n'\n    talk_speed(bot6,0.0001)\n    waiting(0.3)\n    print(p)\n    waiting(3)\n    bot7='Do any of these interest you?'\n    talk_speed(bot7,0.0001)\n    resp = input('Yes or No:  ').lower()\n    if resp == 'yes':\n        app_data(p)\n\n\n\ndef app_data(z):\n    print(z)\n    \n\n\n\napp_title()\n", "616": "# Modules imported\n\nfrom webscrape import *\n\nfrom collections import Counter\n\n\nclass WebAnalysis(WebsiteText):\n\n    def word_count(self):\n        count = Counter(self.remove_common_words()).most_common(7)\n        \n        return count\n    \n    def top_words(self):\n        most_count = Counter(self.remove_common_words()).most_common(1)\n        best_count = [i[0] for i in most_count]\n        \n        new = print(f\"The top word is {best_count[0]}\")\n        return new\n\n", "617": "#!/Users/shaheen/Desktop/Senior_Project_Code_Submission/Webscrape/ws_ve/bin/python3\n# -*- coding: utf-8 -*-\nimport re\nimport sys\n\nfrom pip._internal import main\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "618": "#!/Users/shaheen/Desktop/Senior_Project_Code_Submission/Webscrape/ws_ve/bin/python3\n# -*- coding: utf-8 -*-\nimport re\nimport sys\n\nfrom pip._internal import main\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "619": "#!/Users/shaheen/Desktop/Senior_Project_Code_Submission/Webscrape/ws_ve/bin/python3\n# -*- coding: utf-8 -*-\nimport re\nimport sys\n\nfrom pip._internal import main\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "620": "#!/Users/shaheen/Desktop/Senior_Project_Code_Submission/Webscrape/ws_ve/bin/python3\n# -*- coding: utf-8 -*-\nimport re\nimport sys\n\nfrom chardet.cli.chardetect import main\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "621": "from selenium import webdriver\r\n\r\npath = 'D:\\drive\\Colab Notebooks\\webScrape\\selenium\\chromedriver.exe'\r\ndriver = webdriver.Chrome(path)\r\nurlLink = 'https://www.python.org/'\r\n\r\ndriver.get(url=urlLink)\r\nevent_times = driver.find_elements_by_css_selector(\".event-widget time\")\r\nevent_names = driver.find_elements_by_css_selector(\".event-widget li a\")\r\n\r\nevents = {}\r\nfor i in range(len(event_times)):\r\n    events[i] = {\r\n        'time' : event_times[i].text,\r\n        'name' : event_names[i].text\r\n    }\r\n\r\nprint(events)\r\ndriver.close()\r\ndriver.quit()\r\n", "622": "import json\nimport time\nimport os\nfrom datetime import date\nfrom classes.page import Page\nfrom classes.adresar_page import AdresarPage\n\ntry:\n    data_path = os.environ['SREALITY_SCAPPER_DATAPATH']\n    curr_time = str(int(time.time()))\n    print(\"#Started webscrape on: \"+date.today().strftime(\"%B %d, %Y\")+\" - file: \"+curr_time+\".json\")\n    Page.addPage(AdresarPage(\"https://www.sreality.cz/adresar?strana=1\"))\n    Page.run()\n    f = open(data_path+curr_time+\".json\", \"w\")\n    f.write(json.dumps(Page.getCompanies()))\n    f.close()\nexcept KeyError:\n    print('Set SREALITY_SCAPPER_DATAPATH environment variable.')\n\n"}